Issue ID,Issue Link,Git URL,Summary,Summary Relevance,Description,Comments
KAFKA-2792,https://issues.apache.org/jira/browse/KAFKA-2792,https://github.com/apache/kafka/blob/0.9.0.0/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java,KafkaConsumer.close() can block unnecessarily due to leave group waiting for a reply,NO,"The current implementation of close() waits for a response to LeaveGroup. However, if we have an outstanding rebalance in the works, this can cause the close() operation to have to wait for the entire rebalance process to complete, which is annoying since the goal is to get rid of the consumer object anyway. This is at best surprising and at worst can cause unexpected bugs due to close() taking excessively long -- this was found due to exceeding timeouts unexpectedly causing other operations in Kafka Connect to timeout.

Waiting for a response isn't necessary since as soon as the data is in the TCP buffer, it'll be delivered to the broker. The client doesn't benefit at all from seeing the close group. So we can instead just always send the request ","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]

** Comment 4 **
  If we fire-and-forget in the close() call, it will likely lead to an EOF exception on the server side whenever some consumer closes themselves, causing some log pollutions. This is why I was deciding to not do fire-and-forget in close() but only in unsubscribe(). We need to think about if this issue can be resolved in the socket server if we stick to this plan.
"
KAFKA-2691,https://issues.apache.org/jira/browse/KAFKA-2691,https://github.com/apache/kafka/blob/0.9.0.0/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java,Improve handling of authorization failure during metadata refresh,YES,"There are two problems, one more severe than the other:

1. The consumer blocks indefinitely if there is non-transient authorization failure during metadata refresh due to KAFKA-2391
2. We get a TimeoutException instead of an AuthorizationException in the producer for the same case

If the fix for KAFKA-2391 is to add a timeout, then we will have issue `2` in both producer and consumer.","** Comment 1 **
[Comment excluded]

** Comment 2 **
 To clarify slightly, the indefinite blocking for the consumer occurs when fetching consumer/group metadata, right? I went ahead and patched this in KAFKA-2683, so hopefully this is not an issue anymore. However, we still have the problem that topic metadata authorization errors are only caught and logged by NetworkClient. 

** Comment 3 **
[Comment excluded]

** Comment 4 **
GitHub user hachikuji opened a pull request:
    [link]
    KAFKA-2691: Improve handling of authorization failure during metadata refresh
You can merge this pull request into a Git repository by running:
    $ git pull [link] KAFKA-2691
Alternatively you can review and apply these changes as the patch at:
    [link]
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #394
----
commit 722980095636f8d0f633aa1f40a6aa5736facdbe
Author: Jason Gustafson <jason@confluent.io>
Date:   2015-10-30T23:40:00Z
    KAFKA-2691: Improve handling of authorization failure during metadata refresh
----


** Comment 5 **
[Comment excluded]

** Comment 6 **
[Comment excluded]
"
KAFKA-3659,https://issues.apache.org/jira/browse/KAFKA-3659,https://github.com/apache/kafka/blob/0.10.0.0/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java,Consumer does not handle coordinator connection blackout period gracefully,NO,"Currently when the connection to the coordinator is closed, the consumer will immediately try to rediscover the coordinator and reconnect to it. This is fine as it is, but the NetworkClient enforces a blackout period before it will allow the reconnect to be attempted. This causes the following cycle which continues in a fairly tight loop until the blackout period has completed:

1. Notice connection failure (i.e. DISCONNECTED state in ConnectionStates)
2. Send GroupCoordinator request to rediscover coordinator.
3. Attempt to connect to coordinator.
4. Go back to 1.

To fix this, we should avoid rediscovery while the connection is blacked out.","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]
"
KAFKA-3627,https://issues.apache.org/jira/browse/KAFKA-3627,https://github.com/apache/kafka/blob/0.10.0.0/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java,New consumer doesn't run delayed tasks while under load,NO,"If the new consumer receives a steady flow of fetch responses it will not run delayed tasks, which means it will not heartbeat or perform automatic offset commits.

The main cause is the code that attempts to pipeline fetch responses and keep the consumer fed.  Specifically, in KafkaConsumer::pollOnce() there is a check that skips calling client.poll() if there are fetched records ready (line 903 in the 0.9.0 branch of this writing).  Then in KafkaConsumer::poll(), if records are returned it will initiate another fetch and perform a quick poll, which will send/receive fetch requests/responses but will not run delayed tasks.

If the timing works out, and the consumer is consistently receiving fetched records, it won't run delayed tasks until it doesn't receive a fetch response during its quick poll.  That leads to a rebalance since the consumer isn't heartbeating, and typically means all the consumed records will be re-delivered since the automatic offset commit wasn't able to run either.

h5. Steps to reproduce
# Start up a cluster with *at least 2 brokers*.  This seems to be required to reproduce the issue, I'm guessing because the fetch responses all arrive together when using a single broker.
# Create a topic with a good number of partitions
#* bq. bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic delayed-task-bug --partitions 10 --replication-factor 1
# Generate some test data so the consumer has plenty to consume.  In this case I'm just using uuids
#* bq. for ((i=0;i<100;++i)) do; cat /proc/sys/kernel/random/uuid >>  /tmp/test-messages; done
#* bq. bin/kafka-console-producer.sh --broker-list localhost:9092 --topic delayed-task-bug < /tmp/test-messages
# Start up a consumer with a small max fetch size to ensure it only pulls a few records at a time.  The consumer can simply sleep for a moment when it receives a record.
#* I'll attach an example in Java
# There's a timing aspect to this issue so it may take a few attempts to reproduce","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
GitHub user hachikuji opened a pull request:
    [link]
    KAFKA-3627: consumer fails to execute delayed tasks in poll when records are available
You can merge this pull request into a Git repository by running:
    $ git pull [link] KAFKA-3627
Alternatively you can review and apply these changes as the patch at:
    [link]
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #1295
----
commit 04116c65369926955d79084851f6dfda313b5fee
Author: Jason Gustafson <jason@confluent.io>
Date:   2016-04-29T21:58:35Z
    KAFKA-3627: consumer fails to execute delayed tasks in poll when records are available
----


** Comment 4 **
I believe this problem is aggravated by max.poll.messages, but since there are timing issues I haven't confirmed.  I can confirm that this is affecting us and that any failure to heartbeat or commit is an extremely serious problem as it can result in a ""rebalance storm"" where no consumers ever make progress.  Unfortunately, we are banking on max.poll.messages to address other rebalance storm problems.

** Comment 5 **
 Yes, you are right. No heartbeats are sent while data has been buffered, which is why I upgraded to blocker. I think the patch is mostly ready to go, but we're looking at some system test failures to see if they could be related. I expect a fix to go into 0.10 in any case.

** Comment 6 **
[Comment excluded]

** Comment 7 **
[Comment excluded]
"
KAFKA-3627,https://issues.apache.org/jira/browse/KAFKA-3627,https://github.com/apache/kafka/blob/0.10.0.0/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java,New consumer doesn't run delayed tasks while under load,NO,"If the new consumer receives a steady flow of fetch responses it will not run delayed tasks, which means it will not heartbeat or perform automatic offset commits.

The main cause is the code that attempts to pipeline fetch responses and keep the consumer fed.  Specifically, in KafkaConsumer::pollOnce() there is a check that skips calling client.poll() if there are fetched records ready (line 903 in the 0.9.0 branch of this writing).  Then in KafkaConsumer::poll(), if records are returned it will initiate another fetch and perform a quick poll, which will send/receive fetch requests/responses but will not run delayed tasks.

If the timing works out, and the consumer is consistently receiving fetched records, it won't run delayed tasks until it doesn't receive a fetch response during its quick poll.  That leads to a rebalance since the consumer isn't heartbeating, and typically means all the consumed records will be re-delivered since the automatic offset commit wasn't able to run either.

h5. Steps to reproduce
# Start up a cluster with *at least 2 brokers*.  This seems to be required to reproduce the issue, I'm guessing because the fetch responses all arrive together when using a single broker.
# Create a topic with a good number of partitions
#* bq. bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic delayed-task-bug --partitions 10 --replication-factor 1
# Generate some test data so the consumer has plenty to consume.  In this case I'm just using uuids
#* bq. for ((i=0;i<100;++i)) do; cat /proc/sys/kernel/random/uuid >>  /tmp/test-messages; done
#* bq. bin/kafka-console-producer.sh --broker-list localhost:9092 --topic delayed-task-bug < /tmp/test-messages
# Start up a consumer with a small max fetch size to ensure it only pulls a few records at a time.  The consumer can simply sleep for a moment when it receives a record.
#* I'll attach an example in Java
# There's a timing aspect to this issue so it may take a few attempts to reproduce","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
GitHub user hachikuji opened a pull request:
    [link]
    KAFKA-3627: consumer fails to execute delayed tasks in poll when records are available
You can merge this pull request into a Git repository by running:
    $ git pull [link] KAFKA-3627
Alternatively you can review and apply these changes as the patch at:
    [link]
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #1295
----
commit 04116c65369926955d79084851f6dfda313b5fee
Author: Jason Gustafson <jason@confluent.io>
Date:   2016-04-29T21:58:35Z
    KAFKA-3627: consumer fails to execute delayed tasks in poll when records are available
----


** Comment 4 **
I believe this problem is aggravated by max.poll.messages, but since there are timing issues I haven't confirmed.  I can confirm that this is affecting us and that any failure to heartbeat or commit is an extremely serious problem as it can result in a ""rebalance storm"" where no consumers ever make progress.  Unfortunately, we are banking on max.poll.messages to address other rebalance storm problems.

** Comment 5 **
 Yes, you are right. No heartbeats are sent while data has been buffered, which is why I upgraded to blocker. I think the patch is mostly ready to go, but we're looking at some system test failures to see if they could be related. I expect a fix to go into 0.10 in any case.

** Comment 6 **
[Comment excluded]

** Comment 7 **
[Comment excluded]
"
KAFKA-5490,https://issues.apache.org/jira/browse/KAFKA-5490,https://github.com/apache/kafka/blob/0.11.0.0/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java,Deletion of tombstones during cleaning should consider idempotent message retention,NO,"The LogCleaner always preserves the message containing last sequence from a given ProducerId when doing a round of cleaning. This is necessary to ensure that the producer is not prematurely evicted which would cause an OutOfOrderSequenceException. The problem with this approach is that the preserved message won't be considered again for cleaning until a new message with the same key is written to the topic. Generally this could result in accumulation of stale entries in the log, but the bigger problem is that the newer entry with the same key could be a tombstone. If we end up deleting this tombstone before a new record with the same key is written, then the old entry will resurface. For example, suppose the following sequence of writes:

1. ProducerId=1, Key=A, Value=1
2. ProducerId=2, Key=A, Value=null (tombstone)

We will preserve the first entry indefinitely until a new record with Key=A is written AND either ProducerId 1 has written a newer record with a larger sequence number or ProducerId 1 becomes expired. As long as the tombstone is preserved, there is no correctness violation: a consumer reading from the beginning will ignore the first entry after reading the tombstone. But it is possible that the tombstone entry will be removed from the log before a new record with Key=A is written. If that happens, then a consumer reading from the beginning would incorrectly observe the overwritten value.","** Comment 1 **
After thinking about this, the cleanest option seems to be to do what we proposed in the original KIP. We remove the record as per the normal cleaning logic, but we retain the batch even if it remains empty. On later cleaning passes, we would remove the empty batches after the producerId had either expired or had appended an entry with a new epoch or sequence. To support this, we would need a slight alteration to the message format semantics to support empty batches. One possibility is to use offsetDelta=-1.
A couple other options that we have considered:
1. We can retain the record, but change the value to null (i.e. make it a tombstone). 
2. We can retain the record, but add a record-level attribute indicating that the entry is pending deletion.
Option 1 is probably off the table since it likely violates existing semantics: a consumer materializing the records into a cache, for example, will see a state which never actually existed if it reads up to the modified record. Option 2 seems viable, but like the option to retain empty batches, it requires a change to message format semantics which would mean likely postponing a fix until 0.11.1 (or whatever major release comes next). The advantage of the empty batch approach is that we preserve existing log cleaner invariants: i.e. no duplicate keys before the dirty point. Also it does not require any attribute use.

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]

** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]

** Comment 6 **
[Comment excluded]
"
KAFKA-5474,https://issues.apache.org/jira/browse/KAFKA-5474,https://github.com/apache/kafka/blob/0.11.0.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java,Streams StandbyTask should no checkpoint on commit if EOS is enabled,NO,Discovered by system test {{streams_eos_test#test_failure_and_recovery}},"** Comment 1 **
GitHub user mjsax opened a pull request:
    [link]
    KAFKA-5474: Streams StandbyTask should no checkpoint on commit if EOS is enabled
     - actual fix for `StandbyTask#commit()`
    Additionally (for debugging):
     - EOS test, does not report ""expected"" value correctly
     - `StreamThread` does not report ""standby tasks"" correctly
     - add `IntegerDecoder` (to be use with `kafka.tools.DumpLogSegments`)
You can merge this pull request into a Git repository by running:
    $ git pull [link] kafka-5474-eos-standby-task
Alternatively you can review and apply these changes as the patch at:
    [link]
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #3375
----
commit 89ea5aa85659177907b6ab14abce1a37cb24f21a
Author: Matthias J. Sax <matthias@confluent.io>
Date:   2017-06-19T21:47:48Z
    KAFKA-5474: Streams StandbyTask should no checkpoint on commit if EOS is enabled
commit 2b6349d61c2959c671d3f5157fbc4aa40c0f47d1
Author: Matthias J. Sax <matthias@confluent.io>
Date:   2017-06-19T21:49:28Z
    Debug, cleanup, and minor fixes
----


** Comment 2 **
Failing test is cause by commit [link]#commitcomment-22667101
Closing this issue as invalid. Checkpointing is correct for StandbyTasks with EOS enabled.

** Comment 3 **
[Comment excluded]
"
KAFKA-5446,https://issues.apache.org/jira/browse/KAFKA-5446,https://github.com/apache/kafka/blob/0.11.0.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java,Annoying braces showed on log.error using streams ,YES,"Hi,
in the stream library seems to be a wrong usage of the log.error method when we want to show an exception. There are useless braces at the end of the line before showing exception information like the following example :

ERROR task [0_0] Could not close task due to {} (org.apache.kafka.streams.processor.internals.StreamTask:414)
org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is spending too much time message processing. You can address this either by increasing the session timeout or by reducing the maximum size of batches returned in poll() with max.poll.records.
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.sendOffsetCommitRequest(ConsumerCoordinator.java:725)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.commitOffsetsSync(ConsumerCoordinator.java:604)
	at org.apache.kafka.clients.consumer.KafkaConsumer.commitSync(KafkaConsumer.java:1146)

as you can see in "".... due to {}"", the braces aren't needed for showing exception info so they are printed.

Thanks,
Paolo.","** Comment 1 **
GitHub user ppatierno opened a pull request:
    [link]
    KAFKA-5446: Annoying braces showed on log.error using streams
    Fixed log.error usage with annoying braces
You can merge this pull request into a Git repository by running:
    $ git pull [link] log-error
Alternatively you can review and apply these changes as the patch at:
    [link]
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #3338
----
commit dcf8308e04663295f9310632dcfc226ea58cd947
Author: ppatierno <ppatierno@live.com>
Date:   2017-06-14T13:35:11Z
    Fixed LOG.error usage with annoying braces
----


** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]
"
KAFKA-5704,https://issues.apache.org/jira/browse/KAFKA-5704,https://github.com/apache/kafka/blob/0.11.0.1/connect/runtime/src/main/java/org/apache/kafka/connect/util/TopicAdmin.java,Auto topic creation causes failure with older clusters,YES,"The new automatic internal topic creation always tries to check the topic and create it if missing. However, older brokers that we should still be compatible with don't support some requests that are used. This results in an UnsupportedVersionException which some of the TopicAdmin code notes that it can throw but then isn't caught in the initializers, causing the entire process to fail.

We should probably just catch it, log a message, and allow things to proceed hoping that the user has already created the topics correctly (as we used to do).","** Comment 1 **
[Comment excluded]

** Comment 2 **
The fix is straightforward:  should log the appropriate message and return an empty set. Then, the initializer in  will complete by doing nothing, and the producer will get the topic metadata that will result in the broker auto-creating the topic if missing and if so configured.

** Comment 3 **
GitHub user rhauch opened a pull request:
    [link]
    KAFKA-5704 Corrected Connect distributed startup behavior to allow older brokers to auto-create topics
    When a Connect distributed worker starts up talking with broker versions 0.10.1.0 and later, it will use the AdminClient to look for the internal topics and attempt to create them if they are missing. Although the AdminClient was added in 0.11.0.0, the AdminClient uses APIs to create topics that existed in 0.10.1.0 and later. This feature works as expected when Connect uses a broker version 0.10.1.0 or later.
    However, when a Connect distributed worker starts up using a broker older than 0.10.1.0, the AdminClient is not able to find the required APIs and thus will throw an UnsupportedVersionException. Unfortunately, this exception is not caught and instead causes the Connect worker to fail even when the topics already exist.
    This change handles the UnsupportedVersionException by logging a debug message and doing nothing. The existing producer logic will get information about the topics, which will cause the broker to create them if they don’t exist and broker auto-creation of topics is enabled. This is the same behavior that existed prior to 0.11.0.0, and so this change restores that behavior for brokers older than 0.10.1.0.
    This change also adds a system test that verifies Connect works with a variety of brokers and is able to run source and sink connectors. The test verifies that Connect can read from the internal topics when the connectors are restarted.
You can merge this pull request into a Git repository by running:
    $ git pull [link] kafka-5704
Alternatively you can review and apply these changes as the patch at:
    [link]
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #3641
----
commit 0d45fd113eeaff3844742181191db2cc508353fd
Author: Randall Hauch <rhauch@gmail.com>
Date:   2017-08-07T19:32:29Z
    KAFKA-5704 Corrected Connect distributed startup behavior to allow older brokers to auto-create topics
    When a Connect distributed worker starts up talking with broker versions 0.10.1.0 and later, it will use the AdminClient to look for the internal topics and attempt to create them if they are missing. Although the AdminClient was added in 0.11.0.0, the AdminClient uses APIs to create topics that existed in 0.10.1.0 and later. This feature works as expected when Connect uses a broker version 0.10.1.0 or later.
    However, when a Connect distributed worker starts up using a broker older than 0.10.1.0, the AdminClient is not able to find the required APIs and thus will throw an UnsupportedVersionException. Unfortunately, this exception is not caught and instead causes the Connect worker to fail even when the topics already exist.
    This change handles the UnsupportedVersionException by logging a debug message and doing nothing. The existing producer logic will get information about the topics, which will cause the broker to create them if they don’t exist and broker auto-creation of topics is enabled. This is the same behavior that existed prior to 0.11.0.0, and so this change restores that behavior for brokers older than 0.10.1.0.
    This change also adds a system test that verifies Connect works with a variety of brokers and is able to run source and sink connectors. The test verifies that Connect can read from the internal topics when the connectors are restarted.
----


** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]
"
KAFKA-5603,https://issues.apache.org/jira/browse/KAFKA-5603,https://github.com/apache/kafka/blob/0.11.0.1/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java,Streams should not abort transaction when closing zombie task,NO,"The contract of the transactional producer API is to not call any transactional method after a {{ProducerFenced}} exception was thrown.

Streams however, does an unconditional call within {{StreamTask#close()}} to {{abortTransaction()}} in case of unclean shutdown. We need to distinguish between a {{ProducerFenced}} and other unclean shutdown cases.","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]

** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]

** Comment 6 **
[Comment excluded]
"
KAFKA-5152,https://issues.apache.org/jira/browse/KAFKA-5152,https://github.com/apache/kafka/blob/0.11.0.1/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java,Kafka Streams keeps restoring state after shutdown is initiated during startup,NO,"If streams shutdown is initiated during state restore (e.g. an uncaught exception is thrown) streams will not shut down until all stores are first finished restoring.

As restore progresses, stream threads appear to be taken out of service as part of the shutdown sequence, causing rebalancing of tasks. This compounds the problem by slowing down the restore process even further, since the remaining threads now have to also restore the reassigned tasks before they can shut down.

A more severe issue is that if there is a new rebalance triggered during the end of the waitingSync phase (e.g. due to a new member joining the group, or some members timed out the SyncGroup response), then some consumer clients of the group may already proceed with the {{onPartitionsAssigned}} and blocked on trying to grab the file dir lock not yet released from other clients, while the other clients holding the lock are consistently re-sending {{JoinGroup}} requests while the rebalance cannot be completed because the clients blocked on the file dir lock will not be kicked out of the group as its heartbeat thread has been consistently sending HBRequest. Hence this is a deadlock caused by not releasing the file dir locks in task suspension.","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
GitHub user guozhangwang opened a pull request:
    [link]
     Existing StreamThread exception handling issues
    This is for @dguy as a reference while working on the first step of KAFKA-5152, as a list of existing issues that need to be address at stream thread layer.
You can merge this pull request into a Git repository by running:
    $ git pull [link] KMinor-stream-thread-exception-handling
Alternatively you can review and apply these changes as the patch at:
    [link]
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #3607
----
commit 2d45430191c3dc417992c08454f9c550c1e6bb93
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-07-25T22:43:18Z
    handle commit failed exception on stream thread
commit 9655791794dfa2623ba9f109676b112779fdceca
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-07-26T00:39:07Z
    minor fixes
commit 26226d61529007acc0ccc151e6f6675fc9757d34
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-07-26T01:01:57Z
    add a bunch of TODOs for exception handling
commit 3b054f556364be04d7f83a40b212e0c7facc4a23
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-07-27T22:09:43Z
    rebase from trunk
commit 4b0f4f9cb30537fd0b45b192e2a5d81005ffa3c5
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-07-27T22:26:46Z
    minor fixes
commit 5d2dffa72443139909d3e28f1684363a6e6f5585
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-07-27T22:29:39Z
    github comments
commit 41ba5721ec9fe88b91416621a6236794d37a74de
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-08-02T00:08:13Z
    rebase from trunk
----


** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]

** Comment 6 **
[Comment excluded]

** Comment 7 **
[Comment excluded]

** Comment 8 **
[Comment excluded]

** Comment 9 **
[Comment excluded]
"
KAFKA-5603,https://issues.apache.org/jira/browse/KAFKA-5603,https://github.com/apache/kafka/blob/0.11.0.1/streams/src/main/java/org/apache/kafka/streams/processor/internals/StandbyTask.java,Streams should not abort transaction when closing zombie task,NO,"The contract of the transactional producer API is to not call any transactional method after a {{ProducerFenced}} exception was thrown.

Streams however, does an unconditional call within {{StreamTask#close()}} to {{abortTransaction()}} in case of unclean shutdown. We need to distinguish between a {{ProducerFenced}} and other unclean shutdown cases.","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]

** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]

** Comment 6 **
[Comment excluded]
"
KAFKA-5787,https://issues.apache.org/jira/browse/KAFKA-5787,https://github.com/apache/kafka/blob/0.11.0.1/streams/src/main/java/org/apache/kafka/streams/processor/internals/StoreChangelogReader.java,StoreChangeLogReader needs to restore partitions that were added post initialization,NO,"Investigation of {{KStreamRepartitionJoinTest}} failures uncovered this bug. If a task fails during initialization due to a {{LockException}}, its changelog partitions are not immediately added to the {{StoreChangelogReader}} as the thread doesn't hold the lock. However {{StoreChangelogReader#restore}} will be called and it sets the initialized flag. On a subsequent successfull call to initialize the new tasks the partitions are added to the {{StoreChangelogReader}}, however as it is already initialized these new partitions will never be restored. So the task will remain in a non-running state forever","** Comment 1 **
GitHub user dguy opened a pull request:
    [link]
    KAFKA-5787: StoreChangelogReader needs to restore partitions that were added post initialization
    If a task fails during initialization due to a LockException, its changelog partitions are not immediately added to the StoreChangelogReader as the thread doesn't hold the lock. However StoreChangelogReader#restore will be called and it sets the initialized flag. On a subsequent successfull call to initialize the new tasks the partitions are added to the StoreChangelogReader, however as it is already initialized these new partitions will never be restored. So the task would remain in a non-running state forever.
You can merge this pull request into a Git repository by running:
    $ git pull [link] kafka-5787
Alternatively you can review and apply these changes as the patch at:
    [link]
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #3736
----
commit 451b6a746cc5173b22d8110497bbbe33842d18a3
Author: Damian Guy <damian.guy@gmail.com>
Date:   2017-08-25T09:36:05Z
    StoreChangelogReader needs to restore partitions that were added post initialization
----


** Comment 2 **
[Comment excluded]

** Comment 3 **
GitHub user dguy opened a pull request:
    [link]
    KAFKA-5787: StoreChangelogReader needs to restore partitions that were added post initialization
    If a task fails during initialization due to a LockException, its changelog partitions are not immediately added to the StoreChangelogReader as the thread doesn't hold the lock. However StoreChangelogReader#restore will be called and it sets the initialized flag. On a subsequent successfull call to initialize the new tasks the partitions are added to the StoreChangelogReader, however as it is already initialized these new partitions will never be restored. So the task would remain in a non-running state forever.
You can merge this pull request into a Git repository by running:
    $ git pull [link] kafka-5787-0.11
Alternatively you can review and apply these changes as the patch at:
    [link]
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #3747
----
commit 5569a472a03dc883e3b79c6c6e232a5dbaec5379
Author: Damian Guy <damian.guy@gmail.com>
Date:   2017-08-27T09:57:20Z
    backport fix from trunk
----


** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]
"
KAFKA-4829,https://issues.apache.org/jira/browse/KAFKA-4829,https://github.com/apache/kafka/blob/0.11.0.1/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java,Improve logging of StreamTask commits,YES,"Currently I see this every commit interval:

{code}
2017-02-28T21:27:16.659Z INFO <> [StreamThread-1] o.a.k.s.p.internals.StreamThread - stream-thread [StreamThread-1] Committing task StreamTask 1_31
2017-02-28T21:27:16.659Z INFO <> [StreamThread-1] o.a.k.s.p.internals.StreamThread - stream-thread [StreamThread-1] Committing task StreamTask 2_31
{code}

We have ~10 tasks in our topology, 4 topics, and 32 partitions per topic.
This means every commit interval we log a few hundred lines of the above
which is an order of magnitude chattier than anything else in the log
during normal operations.

To improve visibility of important messages, we should reduce the chattiness of normal commits and highlight abnormal commits.  An example proposal:

existing message is fine at TRACE level for diagnostics
{{TRACE o.a.k.s.p.i.StreamThread - Committing task StreamTask 1_31}}

normal fast case, wrap them all up into one summary line
{{INFO o.a.k.s.p.i.StreamThreads - 64 stream tasks committed in 25ms}}

some kind of threshold / messaging in case it doesn't complete quickly or logs an exception
{{ERROR o.a.k.s.p.i.StreamThread - StreamTask 1_32 did not commit in 100ms}}","** Comment 1 **
I'd like to expand the scope of this JIRA a bit to make a pass over all the log4j entries in Streams and see:
1. If some INFO logging is too verbose / frequent that we can reduce.
2. If some TRACE / DEBUG logging is very vital and hence should be in DEBUG / INFO logging as long as they are not too frequent.

** Comment 2 **
Some comments on how we felt that some logs were missing at INFO level: [link]

** Comment 3 **
 I would suggest similar model to SQL Server's logging configs ([link]
I think current logging can be too much on high workloads and we need a config parameter on this. I believe this will require a KIP. WDYT?

** Comment 4 **
[Comment excluded]

** Comment 5 **
Yeah I'm with  that it to be more on the streams' log4j configures, rather than the changelog topics of the state store.

** Comment 6 **
I know these are different concepts. The main intuition is to present some level of granularity to users via config: simple/aggressive/bulk/no log4j logging. So, we can make an analogy between database logging and log4j logging, although they are not the same concepts and have different semantics.  

** Comment 7 **
We have done some similar thing on the broker side that the verboseness of the log entry depends on the log level. For example:
[link]#L193
Depending on whether it is debug or trace level we would print the full request body or not. If we do feel such controls are necessary we can do them in client-side as well, like Streams, but we may better create a different JIRA for it.

** Comment 8 **
GitHub user guozhangwang opened a pull request:
    [link]
    KAFKA-4829: Improve log4j on Streams thread / task-level
    These are the following improvements I made:
    1. On stream thread level, INFO will be demonstrating `Completed xx tasks in yy ms` or `Completed rebalance with xx state in yy ms`, 
    2. On Stream thread cache level, INFO on `Flushed xx records`.
    3. On Stream thread level, DEBUG on internal batched operations like `created xx tasks`, and TRACE on individual operation like `created x task`.
    4. Also using `isTraceEnabled` on the critical path to reduce overhead of creating `Object`.
    5. Minor cleanups in the code.
You can merge this pull request into a Git repository by running:
    $ git pull [link] K4829-tasks-log4j
Alternatively you can review and apply these changes as the patch at:
    [link]
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #3354
----
commit 00459c20ae097b9da86ba15ed1511ab3f39c9830
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   2017-06-16T00:16:20Z
    improve log4j levels
----


** Comment 9 **
[Comment excluded]

** Comment 10 **
[Comment excluded]

** Comment 11 **
[Comment excluded]
"
KAFKA-5603,https://issues.apache.org/jira/browse/KAFKA-5603,https://github.com/apache/kafka/blob/0.11.0.1/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java,Streams should not abort transaction when closing zombie task,NO,"The contract of the transactional producer API is to not call any transactional method after a {{ProducerFenced}} exception was thrown.

Streams however, does an unconditional call within {{StreamTask#close()}} to {{abortTransaction()}} in case of unclean shutdown. We need to distinguish between a {{ProducerFenced}} and other unclean shutdown cases.","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]

** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]

** Comment 6 **
[Comment excluded]
"
KAFKA-5603,https://issues.apache.org/jira/browse/KAFKA-5603,https://github.com/apache/kafka/blob/0.11.0.1/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java,Streams should not abort transaction when closing zombie task,NO,"The contract of the transactional producer API is to not call any transactional method after a {{ProducerFenced}} exception was thrown.

Streams however, does an unconditional call within {{StreamTask#close()}} to {{abortTransaction()}} in case of unclean shutdown. We need to distinguish between a {{ProducerFenced}} and other unclean shutdown cases.","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]

** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]

** Comment 6 **
[Comment excluded]
"
KAFKA-5756,https://issues.apache.org/jira/browse/KAFKA-5756,https://github.com/apache/kafka/blob/0.11.0.2/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java,Synchronization issue on flush,NO,"Access to *OffsetStorageWriter#toFlush* is not synchronized in *doFlush()* method, whereas this collection can be accessed from 2 different threads:
- *WorkerSourceTask.execute()*, finally block
- *SourceTaskOffsetCommitter*, from periodic flush task","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]

** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]

** Comment 6 **
I believe a very similar issue may still be possible and that the synchronization added in [link] while an improvement, still doesn't prevent all possible errors caused by concurrent calls to {{WorkerSourceTask::commitOffsets}} (which, as noted earlier in the ticket, can from from both the periodic offset commit from the {{SourceTaskOffsetCommitter}} class and the end-of-life offset commit from the {{WorkerSourceTask}} itself).The {{WorkerSourceTask}} class takes care to ensure that {{OffsetStorageWriter::beginFlush}} isn't invoked concurrently, which was implemented as part of [link] However, there doesn't appear to be anything in place to prevent that method from being invoked before a flush has completed (either via a call to {{OffsetStorageWriter::cancelFlush}} or to {{OffsetStorageWriter::doFlush::get}}). If this occurs,  stating that ""the framework should not allow this"".Reopening this issue until the above scenario has been addressed.

** Comment 7 **
[Comment excluded]

** Comment 8 **
I was able to reproduce this race condition with the following setup: * ConnectDistributedTest.test_bounce * clean bounces = True * tests/kafkatest/tests/connect/templates/connect-distributed.properties edited to include offset.flush.interval.ms=1This setup makes collisions between the periodic commitOffsets and the commitOffsets call from stop()/close() extremely likely. In a single run of 9 bounces, I managed to have 7 instances of duplication (12 records in 7 non-consecutive groups).An example log file when one of these race conditions happens. I've interspersed the VerifiableSourceTask's stdout messages to indicate when the records are produced and committed. DEBUG Submitting 1 entries to backing store. The offsets are: {{id=1}={seqno=15709} DEBUG Submitting 1 entries to backing store. The offsets are: {{id=1}={seqno=16209}} {""task"":1,""seqno"":16210,""time_ms"":1595285152622,""name"":""verifiable-source"",""topic"":""test""} DEBUG  Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=8, clientId=connector-producer-verifiable-source-1, correlationId=1395) and timeout 2147483647 to node 1: {acks=-1,timeout=2147483647,partitionSizes=} INFO Stopping task verifiable-source-1 {""task"":1,""seqno"":16211,""time_ms"":1595285152627,""name"":""verifiable-source"",""topic"":""test""} INFO WorkerSourceTask{id=verifiable-source-1} Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask) DEBUG  Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=8, clientId=connector-producer-verifiable-source-1, correlationId=1395): org.apache.kafka.common.requests.ProduceResponse@464a5bec (org.apache.kafka.clients.NetworkClient) ERROR Invalid call to OffsetStorageWriter flush() while already flushing, the framework should not allow this (org.apache.kafka.connect.storage.OffsetStorageWriter) ERROR WorkerSourceTask{id=verifiable-source-1} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask)org.apache.kafka.connect.errors.ConnectException: OffsetStorageWriter is already flushing        at org.apache.kafka.connect.storage.OffsetStorageWriter.beginFlush([file java]:111)        at org.apache.kafka.connect.runtime.WorkerSourceTask.commitOffsets([file java]:490)        at org.apache.kafka.connect.runtime.WorkerSourceTask.execute([file java]:274)        at org.apache.kafka.connect.runtime.WorkerTask.doRun([file java]:185)        at org.apache.kafka.connect.runtime.WorkerTask.run([file java]:235)        at java.util.concurrent.Executors$RunnableAdapter.call([file java]:511)        at java.util.concurrent.FutureTask.run([file java]:266)        at java.util.concurrent.ThreadPoolExecutor.runWorker([file java]:1149)        at java.util.concurrent.ThreadPoolExecutor$Worker.run([file java]:624)        at java.lang.Thread.run([file java]:748) {""committed"":true,""task"":1,""seqno"":16210,""time_ms"":1595285152630,""name"":""verifiable-source"",""topic"":""test""} DEBUG  Sending PRODUCE request with header RequestHeader(apiKey=PRODUCE, apiVersion=8, clientId=connector-producer-verifiable-source-1, correlationId=1396) and timeout 2147483647 to node 1: {acks=-1,timeout=2147483647,partitionSizes=} ERROR WorkerSourceTask{id=verifiable-source-1} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask) INFO WorkerSourceTask{id=verifiable-source-1} Finished commitOffsets successfully in 9 ms (org.apache.kafka.connect.runtime.WorkerSourceTask) INFO  Closing the Kafka producer with timeoutMillis = 30000 ms. DEBUG  Received PRODUCE response from node 1 for request with header RequestHeader(apiKey=PRODUCE, apiVersion=8, clientId=connector-producer-verifiable-source-1, correlationId=1396): org.apache.kafka.common.requests.ProduceResponse@6f98d76 DEBUG  Beginning shutdown of Kafka producer I/O thread, sending remaining records. ducker10 {""committed"":true,""task"":1,""seqno"":16211,""time_ms"":1595285152632,""name"":""verifiable-source"",""topic"":""test""} DEBUG  Shutdown of Kafka producer I/O thread has completed. DEBUG  Kafka producer has been closed          DEBUG Graceful stop of task verifiable-source-1 succeeded.Sequence numbers 16210 and 16211 are then duplicated once the connector restarts, since the later commit including those offsets is discarded due to the exception.When the normal interval of 5000ms is used, the test is flakey but passes ~90% of the time. In order to resolve that flakiness, we need to resolve this race condition.

** Comment 9 **
[Comment excluded]

** Comment 10 **
 I have opened a PR that I think may alleviate this failure mode.

** Comment 11 **
[Comment excluded]
"
KAFKA-5986,https://issues.apache.org/jira/browse/KAFKA-5986,https://github.com/apache/kafka/blob/0.11.0.2/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java,Streams State Restoration never completes when logging is disabled,YES,"When logging is disabled on a state store, the store restoration never completes. This is likely because there are no changelogs, but more investigation is required.","** Comment 1 **
GitHub user dguy opened a pull request:
    [link]
    KAFKA-5986: Streams State Restoration never completes when logging is disabled
    When logging is disabled and there are state stores the task never transitions from restoring to running. This is because we only ever check if the task has state stores and return false on initialization if it does. The check should be if we have changelog partitions, i.e., we need to restore.
You can merge this pull request into a Git repository by running:
    $ git pull [link] restore-test
Alternatively you can review and apply these changes as the patch at:
    [link]
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #3983
----
commit c60856362a9dfe1f8b68d76cbb5a783eef6abfff
Author: Damian Guy <damian.guy@gmail.com>
Date:   2017-09-28T11:52:34Z
    fix task initialization when logging disabled
----


** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]
"
KAFKA-5854,https://issues.apache.org/jira/browse/KAFKA-5854,https://github.com/apache/kafka/blob/1.0.0/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java,Handle SASL authentication failures as non-retriable exceptions in clients,YES,"Produce and consumer changes to avoid retries on authentication failures.

Details are in [KIP-152|https://cwiki.apache.org/confluence/display/KAFKA/KIP-152+-+Improve+diagnostics+for+SASL+authentication+failures]","** Comment 1 **
GitHub user vahidhashemian opened a pull request:
    [link]
    KAFKA-5854: (WIP) Handle SASL authentication failures as non-retriable exceptions in clients
    This PR depends on the in progress ([link]
You can merge this pull request into a Git repository by running:
    $ git pull [link] KAFKA-5854
Alternatively you can review and apply these changes as the patch at:
    [link]
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #3832
----
commit a7eca1ecb0d56004559193c716ec83601c5e95f4
Author: Rajini Sivaram <rajinisivaram@googlemail.com>
Date:   2017-08-21T18:59:43Z
    KAFKA-4764: Wrap SASL tokens in Kafka headers to improve diagnostics
commit e8cc0e663379c75447d8ec894a41b150b68d1f29
Author: Vahid Hashemian <vahidhashemian@us.ibm.com>
Date:   2017-09-11T23:15:22Z
    KAFKA-5854: (WIP) Handle SASL authentication failures as non-retriable exceptions in clients
----


** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]
"
KAFKA-5960,https://issues.apache.org/jira/browse/KAFKA-5960,https://github.com/apache/kafka/blob/1.0.0/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java,Producer uses unsupported ProduceRequest version against older brokers,NO,"Reported recently errors from a trunk producer on an 0.11.0.0 broker:
{code}
org.apache.kafka.common.errors.UnsupportedVersionException: The broker does not support the requested version 5 for api PRODUCE. Supported versions are 0 to 3.
{code}
This is likely a regression introduced in KAFKA-5793. We should be using the latest version that the broker supports.","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]

** Comment 4 **
[Comment excluded]
"
KAFKA-5990,https://issues.apache.org/jira/browse/KAFKA-5990,https://github.com/apache/kafka/blob/1.0.0/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java,Add generated documentation for Connect metrics,NO,"KAFKA-5191 recently added a new {{MetricNameTemplate}} that is used to create the {{MetricName}} objects in the producer and consumer, as we as in the newly-added generation of metric documentation. The {{Metric.toHtmlTable}} method then takes these templates and generates an HTML documentation for the metrics.

Change the Connect metrics to use these templates and update the build to generate these metrics and include them in the Kafka documentation.","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]
"
KAFKA-5949,https://issues.apache.org/jira/browse/KAFKA-5949,https://github.com/apache/kafka/blob/1.0.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java,User Callback Exceptions need to be handled properly,NO,"In Streams, we allow to register multiple user callbacks. We need to handle those exceptions gracefully, by catching and wrapping with a StreamsException.

- TimestampExtractor
- DeserializationHandler
- StateRestoreListener","** Comment 1 **
GitHub user mjsax opened a pull request:
    [link]
    KAFKA-5949: User Callback Exceptions need to be handled properly
     - catch user exception in user callback (TimestampExtractor, DeserializationHandler, StateRestoreListener) and wrap with StreamsException
    Additional cleanup:
     - rename globalRestoreListener to userRestoreListener
     - remove unnecessary interface -> collapse SourceNodeRecordDeserializer and RecordDeserializer
     - removed unused parameter loggingEnabled from ProcessorContext#register
You can merge this pull request into a Git repository by running:
    $ git pull [link] kafka-5949-exceptions-user-callbacks
Alternatively you can review and apply these changes as the patch at:
    [link]
To close this pull request, make a commit to your master/trunk branch
with (at least) the following in the commit message:
    This closes #3939
----
commit 0dbfeb64781de1a65a1d5f2c0567f6035c49e2ce
Author: Matthias J. Sax <matthias@confluent.io>
Date:   2017-09-21T21:10:26Z
    KAFKA-5949: User Callback Exceptions need to be handled properly
----


** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]

** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]

** Comment 6 **
[Comment excluded]
"
KAFKA-6590,https://issues.apache.org/jira/browse/KAFKA-6590,https://github.com/apache/kafka/blob/1.1.0/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java,Consumer bytes-fetched and records-fetched metrics are not aggregated correctly,NO,"There is a bug in the recording of these metrics which causes only the aggregated value from only one topic in the response to be recorded for the entire fetch.

This issue was found by github user kiest, who submitted a patch here: https://github.com/apache/kafka/pull/4278/files","** Comment 1 **
hachikuji closed pull request #4278: KAFKA-6590: Fix bug in aggregation of consumer fetch bytes and counts metrics
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git [file java] [file java]
index ac199bc4b0d..b46a3a29380 100644
--- [file java]
+++ [file java]
@@ -1276,8 +1276,8 @@ public void record(TopicPartition partition, int bytes, int records) {
             if (this.unrecordedPartitions.isEmpty()) {
                 // once all expected partitions from the fetch have reported in, record the metrics
-                this.sensors.bytesFetched.record(topicFetchMetric.fetchBytes);
-                this.sensors.recordsFetched.record(topicFetchMetric.fetchRecords);
+                this.sensors.bytesFetched.record(this.fetchMetrics.fetchBytes);
+                this.sensors.recordsFetched.record(this.fetchMetrics.fetchRecords);
                 // also record per-topic metrics
                 for (Map.Entry<String, FetchMetrics> entry: this.topicFetchMetrics.entrySet()) {
diff --git [file java] [file java]
index e8bb4e696ba..27aba410746 100644
--- [file java]
+++ [file java]
@@ -1447,26 +1447,50 @@ public void testReadCommittedLagMetric() {
     @Test
     public void testFetchResponseMetrics() {
-        subscriptions.assignFromUser(singleton(tp0));
-        subscriptions.seek(tp0, 0);
+        String topic1 = ""foo"";
+        String topic2 = ""bar"";
+        TopicPartition tp1 = new TopicPartition(topic1, 0);
+        TopicPartition tp2 = new TopicPartition(topic2, 0);
+
+        Map<String, Integer> partitionCounts = new HashMap<>();
+        partitionCounts.put(topic1, 1);
+        partitionCounts.put(topic2, 1);
+        Cluster cluster = TestUtils.clusterWith(1, partitionCounts);
+        metadata.update(cluster, Collections.<String>emptySet(), time.milliseconds());
-        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();
-        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));
-        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));
-
-        MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,
-                TimestampType.CREATE_TIME, 0L);
-        for (int v = 0; v < 3; v++)
-            builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, ""key"".getBytes(), (""value-"" + v).getBytes());
-        MemoryRecords records = builder.build();
+        subscriptions.assignFromUser(Utils.mkSet(tp1, tp2));
         int expectedBytes = 0;
-        for (Record record : records.records())
-            expectedBytes += record.sizeInBytes();
+        LinkedHashMap<TopicPartition, FetchResponse.PartitionData> fetchPartitionData = new LinkedHashMap<>();
-        fetchRecords(tp0, records, Errors.NONE, 100L, 0);
+        for (TopicPartition tp : Utils.mkSet(tp1, tp2)) {
+            subscriptions.seek(tp, 0);
+
+            MemoryRecordsBuilder builder = MemoryRecords.builder(ByteBuffer.allocate(1024), CompressionType.NONE,
+                    TimestampType.CREATE_TIME, 0L);
+            for (int v = 0; v < 3; v++)
+                builder.appendWithOffset(v, RecordBatch.NO_TIMESTAMP, ""key"".getBytes(), (""value-"" + v).getBytes());
+            MemoryRecords records = builder.build();
+            for (Record record : records.records())
+                expectedBytes += record.sizeInBytes();
+
+            fetchPartitionData.put(tp, new FetchResponse.PartitionData(Errors.NONE, 15L,
+                    FetchResponse.INVALID_LAST_STABLE_OFFSET, 0L, null, records));
+        }
+
+        assertEquals(1, fetcher.sendFetches());
+        client.prepareResponse(new FetchResponse(Errors.NONE, fetchPartitionData, 0, INVALID_SESSION_ID));
+        consumerClient.poll(0);
+
+        Map<TopicPartition, List<ConsumerRecord<byte, byte>>> fetchedRecords = fetcher.fetchedRecords();
+        assertEquals(3, fetchedRecords.get(tp1).size());
+        assertEquals(3, fetchedRecords.get(tp2).size());
+
+        Map<MetricName, KafkaMetric> allMetrics = metrics.metrics();
+        KafkaMetric fetchSizeAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.fetchSizeAvg));
+        KafkaMetric recordsCountAverage = allMetrics.get(metrics.metricInstance(metricsRegistry.recordsPerRequestAvg));
         assertEquals(expectedBytes, fetchSizeAverage.value(), EPSILON);
-        assertEquals(3, recordsCountAverage.value(), EPSILON);
+        assertEquals(6, recordsCountAverage.value(), EPSILON);
     }
     @Test
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-6060,https://issues.apache.org/jira/browse/KAFKA-6060,https://github.com/apache/kafka/blob/1.1.0/tools/src/main/java/org/apache/kafka/trogdor/agent/Agent.java,Add workload generation capabilities to Trogdor,NO,Add workload generation capabilities to Trogdor,"** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]
"
KAFKA-6255,https://issues.apache.org/jira/browse/KAFKA-6255,https://github.com/apache/kafka/blob/1.1.0/tools/src/main/java/org/apache/kafka/trogdor/coordinator/Coordinator.java,Add ProduceBench to Trogdor,NO,"Add ProduceBench, a benchmark of producer latency, to Trogdor.","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]
"
KAFKA-6255,https://issues.apache.org/jira/browse/KAFKA-6255,https://github.com/apache/kafka/blob/1.1.0/tools/src/main/java/org/apache/kafka/trogdor/coordinator/NodeManager.java,Add ProduceBench to Trogdor,NO,"Add ProduceBench, a benchmark of producer latency, to Trogdor.","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]
"
KAFKA-6897,https://issues.apache.org/jira/browse/KAFKA-6897,https://github.com/apache/kafka/blob/2.0.0/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java,Mirrormaker waits to shut down forever on produce failure with abort.on.send.failure=true ,YES,"Mirrormaker never shuts down after a produce failure when abort.on.send.failure=true

{code:java}
[2018-05-07 08:29:32,417] ERROR Error when sending message to topic test with key: 52 bytes, value: 32615 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)
org.apache.kafka.common.errors.TimeoutException: Expiring 1 record(s) for test-11: 45646 ms has passed since last append
[2018-05-07 08:29:32,434] INFO Closing producer due to send failure. (kafka.tools.MirrorMaker$)
[2018-05-07 08:29:32,434] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 0 ms. (org.apache.kafka.clients.producer.KafkaProducer)
{code}

A stack trace of this mirrormaker process 9 hours later shows that the main thread is still active and it is waiting for metadata that it will never get since the producer send thread is no longer running.","** Comment 1 **
dhruvilshah3 opened a new pull request #5027: KAFKA-6897: Prevent producer from blocking indefinitely on close
URL: [link]
   After successful completion of KafkaProducer#close, it is possible that an application calls KafkaProducer#send. If the send is invoked for a topic for which we do not have any metadata, the producer will block until `max.block.ms` elapses - we do not expect to receive any metadata update in this case because Sender (and NetworkClient) has already exited. It is only when RecordAccumulator#append is invoked that we notice that the producer has already exited. If `max.block.ms` is set to Long.MaxValue (or a sufficiently high value in general), the producer could this block infinitely.   This patch introduces the concept of ""poisoned metadata"". In NetworkClient#close, we poison the metadata to indicate that no further metadata updates are possible. Any consumer that is currently waiting on (or could wait for in future) for metadata updates could check the poison state of metadata and exit early, if required.   This is an early upload to get some feedback on the direction of this patch. If we agree this is the correct way to fix this issue, I will add unit tests.   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
[Comment excluded]

** Comment 3 **
hachikuji closed pull request #5027: KAFKA-6897: Prevent producer from blocking indefinitely after close
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git [file java] [file java]
index 8252cf3a9cd..ec007a69668 100644
--- [file java]
+++ [file java]
@@ -89,4 +89,8 @@ public void handleCompletedMetadataResponse(RequestHeader requestHeader, long no
     public void requestUpdate() {
         // Do nothing
     }
+
+    @Override
+    public void close() {
+    }
 }
diff --git [file java] [file java]
index 91b15875cd0..6c663cfac93 100644
--- [file java]
+++ [file java]
@@ -17,6 +17,7 @@
 package org.apache.kafka.clients;
 import org.apache.kafka.common.Cluster;
+import org.apache.kafka.common.KafkaException;
 import org.apache.kafka.common.internals.ClusterResourceListeners;
 import org.apache.kafka.common.Node;
 import org.apache.kafka.common.PartitionInfo;
@@ -25,6 +26,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import java.io.Closeable;
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
@@ -48,7 +50,7 @@
  * is removed from the metadata refresh set after an update. Consumers disable topic expiry since they explicitly
  * manage topics while producers rely on topic expiry to limit the refresh set.
  */
-public final class Metadata {
+public final class Metadata implements Closeable {
     private static final Logger log = LoggerFactory.getLogger(Metadata.class);
@@ -70,6 +72,7 @@
     private boolean needMetadataForAllTopics;
     private final boolean allowAutoTopicCreation;
     private final boolean topicExpiryEnabled;
+    private boolean isClosed;
     public Metadata(long refreshBackoffMs, long metadataExpireMs, boolean allowAutoTopicCreation) {
         this(refreshBackoffMs, metadataExpireMs, allowAutoTopicCreation, false, new ClusterResourceListeners());
@@ -100,6 +103,7 @@ public Metadata(long refreshBackoffMs, long metadataExpireMs, boolean allowAutoT
         this.listeners = new ArrayList<>();
         this.clusterResourceListeners = clusterResourceListeners;
         this.needMetadataForAllTopics = false;
+        this.isClosed = false;
     }
     /**
@@ -164,12 +168,12 @@ public synchronized AuthenticationException getAndClearAuthenticationException()
      * Wait for metadata update until the current version is larger than the last version we know of
      */
     public synchronized void awaitUpdate(final int lastVersion, final long maxWaitMs) throws InterruptedException {
-        if (maxWaitMs < 0) {
+        if (maxWaitMs < 0)
             throw new IllegalArgumentException(""Max time to wait for metadata updates should not be < 0 milliseconds"");
-        }
+
         long begin = System.currentTimeMillis();
         long remainingWaitMs = maxWaitMs;
-        while (this.version <= lastVersion) {
+        while ((this.version <= lastVersion) && !isClosed()) {
             AuthenticationException ex = getAndClearAuthenticationException();
             if (ex != null)
                 throw ex;
@@ -180,6 +184,8 @@ public synchronized void awaitUpdate(final int lastVersion, final long maxWaitMs
                 throw new TimeoutException(""Failed to update metadata after "" + maxWaitMs + "" ms."");
             remainingWaitMs = maxWaitMs - elapsed;
         }
+        if (isClosed())
+            throw new KafkaException(""Requested metadata update after close"");
     }
     /**
@@ -224,6 +230,8 @@ public synchronized boolean containsTopic(String topic) {
      */
     public synchronized void update(Cluster newCluster, Set<String> unavailableTopics, long now) {
         Objects.requireNonNull(newCluster, ""cluster should not be null"");
+        if (isClosed())
+            throw new IllegalStateException(""Update requested after metadata close"");
         this.needUpdate = false;
         this.lastRefreshMs = now;
@@ -331,6 +339,25 @@ public synchronized void removeListener(Listener listener) {
         this.listeners.remove(listener);
     }
+    /**
+     * ""Close"" this metadata instance to indicate that metadata updates are no longer possible. This is typically used
+     * when the thread responsible for performing metadata updates is exiting and needs a way to relay this information
+     * to any other thread(s) that could potentially wait on metadata update to come through.
+     */
+    @Override
+    public synchronized void close() {
+        this.isClosed = true;
+        this.notifyAll();
+    }
+
+    /**
+     * Check if this metadata instance has been closed. See {@link #close()} for more information.
+     * @return True if this instance has been closed; false otherwise
+     */
+    public synchronized boolean isClosed() {
+        return this.isClosed;
+    }
+
     /**
      * MetadataUpdate Listener
      */
diff --git [file java] [file java]
index 09ed995d14c..de765db5a8d 100644
--- [file java]
+++ [file java]
@@ -21,6 +21,7 @@
 import org.apache.kafka.common.requests.MetadataResponse;
 import org.apache.kafka.common.requests.RequestHeader;
+import java.io.Closeable;
 import java.util.List;
 /**
@@ -29,7 +30,7 @@
  * <p>
  * This class is not thread-safe!
  */
-public interface MetadataUpdater {
+public interface MetadataUpdater extends Closeable {
     /**
      * Gets the current cluster info without blocking.
@@ -82,4 +83,10 @@
      * start of the update if possible (see `maybeUpdate` for more information).
      */
     void requestUpdate();
+
+    /**
+     * Close this updater.
+     */
+    @Override
+    void close();
 }
diff --git [file java] [file java]
index 720a7814752..fd16fe608e3 100644
--- [file java]
+++ [file java]
@@ -581,6 +581,7 @@ public void wakeup() {
     @Override
     public void close() {
         this.selector.close();
+        this.metadataUpdater.close();
     }
     /**
@@ -981,6 +982,11 @@ public void requestUpdate() {
             this.metadata.requestUpdate();
         }
+        @Override
+        public void close() {
+            this.metadata.close();
+        }
+
         /**
          * Return true if there's at least one connection establishment is currently underway
          */
diff --git [file java] [file java]
index 85d3c28e8df..1ad3991ca24 100644
--- [file java]
+++ [file java]
@@ -118,6 +118,10 @@ public void handleCompletedMetadataResponse(RequestHeader requestHeader, long no
         public void requestUpdate() {
             AdminMetadataManager.this.requestUpdate();
         }
+
+        @Override
+        public void close() {
+        }
     }
     /**
diff --git [file java] [file java]
index 3a6717b7676..3519947bf15 100644
--- [file java]
+++ [file java]
@@ -790,12 +790,12 @@ public void abortTransaction() throws ProducerFencedException {
      *
      * @throws AuthenticationException if authentication fails. See the exception for more details
      * @throws AuthorizationException fatal error indicating that the producer is not allowed to write
-     * @throws IllegalStateException if a transactional.id has been configured and no transaction has been started
+     * @throws IllegalStateException if a transactional.id has been configured and no transaction has been started, or
+     *                               when send is invoked after producer has been closed.
      * @throws InterruptException If the thread is interrupted while blocked
      * @throws SerializationException If the key or value are not valid objects given the configured serializers
      * @throws TimeoutException If the time taken for fetching metadata or allocating memory for the record has surpassed <code>max.block.ms</code>.
      * @throws KafkaException If a Kafka related error occurs that does not belong to the public API exceptions.
-     *
      */
     @Override
     public Future<RecordMetadata> send(ProducerRecord<K, V> record, Callback callback) {
@@ -804,14 +804,29 @@ public void abortTransaction() throws ProducerFencedException {
         return doSend(interceptedRecord, callback);
     }
+    // Verify that this producer instance has not been closed. This method throws IllegalStateException if the producer
+    // has already been closed.
+    private void throwIfProducerClosed() {
+        if (ioThread == null || !ioThread.isAlive())
+            throw new IllegalStateException(""Cannot perform operation after producer has been closed"");
+    }
+
     /**
      * Implementation of asynchronously send a record to a topic.
      */
     private Future<RecordMetadata> doSend(ProducerRecord<K, V> record, Callback callback) {
         TopicPartition tp = null;
         try {
+            throwIfProducerClosed();
             // first make sure the metadata for the topic is available
-            ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);
+            ClusterAndWaitTime clusterAndWaitTime;
+            try {
+                clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);
+            } catch (KafkaException e) {
+                if (metadata.isClosed())
+                    throw new KafkaException(""Producer closed while send in progress"", e);
+                throw e;
+            }
             long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);
             Cluster cluster = clusterAndWaitTime.cluster;
             byte serializedKey;
@@ -896,6 +911,7 @@ private void setReadOnly(Headers headers) {
      * @param partition A specific partition expected to exist in metadata, or null if there's no preference
      * @param maxWaitMs The maximum time in ms for waiting on the metadata
      * @return The cluster containing topic metadata and the amount of time we waited in ms
+     * @throws KafkaException for all Kafka-related exceptions, including the case where this method is called after producer close
      */
     private ClusterAndWaitTime waitOnMetadata(String topic, Integer partition, long maxWaitMs) throws InterruptedException {
         // add topic to metadata topic list if it is not there already and reset expiry
@@ -1016,8 +1032,9 @@ public void flush() {
      * Get the partition metadata for the given topic. This can be used for custom partitioning.
      * @throws AuthenticationException if authentication fails. See the exception for more details
      * @throws AuthorizationException if not authorized to the specified topic. See the exception for more details
-     * @throws InterruptException If the thread is interrupted while blocked
+     * @throws InterruptException if the thread is interrupted while blocked
      * @throws TimeoutException if metadata could not be refreshed within {@code max.block.ms}
+     * @throws KafkaException for all Kafka-related exceptions, including the case where this method is called after producer close
      */
     @Override
     public List<PartitionInfo> partitionsFor(String topic) {
diff --git [file java] [file java]
index 9e9869a7e48..dc00b473027 100644
--- [file java]
+++ [file java]
@@ -311,9 +311,6 @@ public void close() {
     @Override
     public void close(long timeout, TimeUnit timeUnit) {
-        if (this.closed) {
-            throw new IllegalStateException(""MockProducer is already closed."");
-        }
         this.closed = true;
     }
diff --git [file java] [file java]
index e2b58448dc9..31c6d754c9d 100644
--- [file java]
+++ [file java]
@@ -19,6 +19,7 @@
 import org.apache.kafka.clients.ApiVersions;
 import org.apache.kafka.clients.producer.Callback;
 import org.apache.kafka.common.Cluster;
+import org.apache.kafka.common.KafkaException;
 import org.apache.kafka.common.MetricName;
 import org.apache.kafka.common.Node;
 import org.apache.kafka.common.PartitionInfo;
@@ -195,7 +196,7 @@ public RecordAppendResult append(TopicPartition tp,
             Deque<ProducerBatch> dq = getOrCreateDeque(tp);
             synchronized (dq) {
                 if (closed)
-                    throw new IllegalStateException(""Cannot send after the producer is closed."");
+                    throw new KafkaException(""Producer closed while send in progress"");
                 RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);
                 if (appendResult != null)
                     return appendResult;
@@ -209,7 +210,7 @@ public RecordAppendResult append(TopicPartition tp,
             synchronized (dq) {
                 // Need to check if producer is closed again after grabbing the dequeue lock.
                 if (closed)
-                    throw new IllegalStateException(""Cannot send after the producer is closed."");
+                    throw new KafkaException(""Producer closed while send in progress"");
                 RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);
                 if (appendResult != null) {
@@ -700,7 +701,7 @@ public void abortIncompleteBatches() {
      * Go through incomplete batches and abort them.
      */
     private void abortBatches() {
-        abortBatches(new IllegalStateException(""Producer is closed forcefully.""));
+        abortBatches(new KafkaException(""Producer is closed forcefully.""));
     }
     /**
diff --git [file java] [file java]
index 1188af776aa..969921eceeb 100644
--- [file java]
+++ [file java]
@@ -25,6 +25,7 @@
 import java.util.concurrent.atomic.AtomicReference;
 import org.apache.kafka.common.Cluster;
+import org.apache.kafka.common.KafkaException;
 import org.apache.kafka.common.internals.ClusterResourceListeners;
 import org.apache.kafka.common.Node;
 import org.apache.kafka.common.PartitionInfo;
@@ -46,7 +47,7 @@
     private long refreshBackoffMs = 100;
     private long metadataExpireMs = 1000;
     private Metadata metadata = new Metadata(refreshBackoffMs, metadataExpireMs, true);
-    private AtomicReference<String> backgroundError = new AtomicReference<>();
+    private AtomicReference<Exception> backgroundError = new AtomicReference<>();
     @After
     public void tearDown() {
@@ -83,6 +84,30 @@ public void testMetadata() throws Exception {
         assertTrue(""Update needed due to stale metadata."", metadata.timeToNextUpdate(time) == 0);
     }
+    @Test
+    public void testMetadataAwaitAfterClose() throws InterruptedException {
+        long time = 0;
+        metadata.update(Cluster.empty(), Collections.<String>emptySet(), time);
+        assertFalse(""No update needed."", metadata.timeToNextUpdate(time) == 0);
+        metadata.requestUpdate();
+        assertFalse(""Still no updated needed due to backoff"", metadata.timeToNextUpdate(time) == 0);
+        time += refreshBackoffMs;
+        assertTrue(""Update needed now that backoff time expired"", metadata.timeToNextUpdate(time) == 0);
+        String topic = ""my-topic"";
+        metadata.close();
+        Thread t1 = asyncFetch(topic, 500);
+        t1.join();
+        assertTrue(backgroundError.get().getClass() == KafkaException.class);
+        assertTrue(backgroundError.get().toString().contains(""Requested metadata update after close""));
+        clearBackgroundError();
+    }
+
+    @Test(expected = IllegalStateException.class)
+    public void testMetadataUpdateAfterClose() {
+        metadata.close();
+        metadata.update(Cluster.empty(), Collections.<String>emptySet(), 1000);
+    }
+
     private static void checkTimeToNextUpdate(long refreshBackoffMs, long metadataExpireMs) {
         long now = 10000;
@@ -409,15 +434,18 @@ public void testNonExpiringMetadata() throws Exception {
         assertTrue(""Unused topic expired when expiry disabled"", metadata.containsTopic(""topic4""));
     }
+    private void clearBackgroundError() {
+        backgroundError.set(null);
+    }
+
     private Thread asyncFetch(final String topic, final long maxWaitMs) {
         Thread thread = new Thread() {
             public void run() {
-                while (metadata.fetch().partitionsForTopic(topic).isEmpty()) {
-                    try {
+                try {
+                    while (metadata.fetch().partitionsForTopic(topic).isEmpty())
                         metadata.awaitUpdate(metadata.requestUpdate(), maxWaitMs);
-                    } catch (Exception e) {
-                        backgroundError.set(e.toString());
-                    }
+                } catch (Exception e) {
+                    backgroundError.set(e);
                 }
             }
         };
diff --git [file java] [file java]
index 0f64f13ef60..6b41a9e8779 100644
--- [file java]
+++ [file java]
@@ -533,6 +533,7 @@ public ClientRequest newClientRequest(String nodeId,
     @Override
     public void close() {
+        metadata.close();
     }
     @Override
diff --git [file java] [file java]
index bf03e46ec08..dd2dd896b28 100644
--- [file java]
+++ [file java]
@@ -632,8 +632,8 @@ public void testSendToInvalidTopic() throws Exception {
         client.setNode(node);
         Producer<String, String> producer = new KafkaProducer<>(new ProducerConfig(
-            ProducerConfig.addSerializerToConfig(props, new StringSerializer(), new StringSerializer())),
-            new StringSerializer(), new StringSerializer(), metadata, client);
+                ProducerConfig.addSerializerToConfig(props, new StringSerializer(), new StringSerializer())),
+                new StringSerializer(), new StringSerializer(), metadata, client);
         String invalidTopicName = ""topic abc"";          // Invalid topic name due to space
         ProducerRecord<String, String> record = new ProducerRecord<>(invalidTopicName, ""HelloKafka"");
@@ -641,12 +641,12 @@ public void testSendToInvalidTopic() throws Exception {
         Set<String> invalidTopic = new HashSet<String>();
         invalidTopic.add(invalidTopicName);
         Cluster metaDataUpdateResponseCluster = new Cluster(cluster.clusterResource().clusterId(),
-                                                            cluster.nodes(),
-                                                            new ArrayList<PartitionInfo>(0),
-                                                            Collections.<String>emptySet(),
-                                                            invalidTopic,
-                                                            cluster.internalTopics(),
-                                                            cluster.controller());
+                cluster.nodes(),
+                new ArrayList<PartitionInfo>(0),
+                Collections.<String>emptySet(),
+                invalidTopic,
+                cluster.internalTopics(),
+                cluster.controller());
         client.prepareMetadataUpdate(metaDataUpdateResponseCluster, Collections.<String>emptySet());
         Future<RecordMetadata> future = producer.send(record);
@@ -654,4 +654,51 @@ public void testSendToInvalidTopic() throws Exception {
         assertEquals(""Cluster has incorrect invalid topic list."", metaDataUpdateResponseCluster.invalidTopics(), metadata.fetch().invalidTopics());
         TestUtils.assertFutureError(future, InvalidTopicException.class);
     }
+
+    @Test
+    public void testCloseWhenWaitingForMetadataUpdate() throws InterruptedException {
+        Properties props = new Properties();
+        props.put(ProducerConfig.MAX_BLOCK_MS_CONFIG, Long.MAX_VALUE);
+        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, ""localhost:9000"");
+
+        // Simulate a case where metadata for a particular topic is not available. This will cause KafkaProducer#send to
+        // block in Metadata#awaitUpdate for the configured max.block.ms. When close() is invoked, KafkaProducer#send should
+        // return with a KafkaException.
+        String topicName = ""test"";
+        Time time = new MockTime();
+        Cluster cluster = TestUtils.singletonCluster();
+        Node node = cluster.nodes().get(0);
+        Metadata metadata = new Metadata(0, Long.MAX_VALUE, false);
+        metadata.update(cluster, Collections.<String>emptySet(), time.milliseconds());
+        MockClient client = new MockClient(time, metadata);
+        client.setNode(node);
+
+        Producer<String, String> producer = new KafkaProducer<>(
+                new ProducerConfig(ProducerConfig.addSerializerToConfig(props, new StringSerializer(), new StringSerializer())),
+                new StringSerializer(), new StringSerializer(), metadata, client);
+
+        ExecutorService executor = Executors.newSingleThreadExecutor();
+        final AtomicReference<Exception> sendException = new AtomicReference<>();
+
+        try {
+            executor.submit(() -> {
+                try {
+                    // Metadata for topic ""test"" will not be available which will cause us to block indefinitely until
+                    // KafkaProducer#close is invoked.
+                    producer.send(new ProducerRecord<>(topicName, ""key"", ""value""));
+                    fail();
+                } catch (Exception e) {
+                    sendException.set(e);
+                }
+            });
+
+            // Wait until metadata update for the topic has been requested
+            TestUtils.waitForCondition(() -> metadata.containsTopic(topicName), ""Timeout when waiting for topic to be added to metadata"");
+            producer.close(0, TimeUnit.MILLISECONDS);
+            TestUtils.waitForCondition(() -> sendException.get() != null, ""No producer exception within timeout"");
+            assertEquals(KafkaException.class, sendException.get().getClass());
+        } finally {
+            executor.shutdownNow();
+        }
+    }
 }
diff --git [file java] [file java]
index 27fac280afc..7a8c710b76b 100644
--- [file java]
+++ [file java]
@@ -636,16 +636,6 @@ public void shouldThrowOnAbortTransactionIfProducerIsClosed() {
         } catch (IllegalStateException e) { }
     }
-    @Test
-    public void shouldThrowOnCloseIfProducerIsClosed() {
-        buildMockProducer(true);
-        producer.close();
-        try {
-            producer.close();
-            fail(""Should have thrown as producer is already closed"");
-        } catch (IllegalStateException e) { }
-    }
-
     @Test
     public void shouldThrowOnFenceProducerIfProducerIsClosed() {
         buildMockProducer(true);
diff --git a/core/src/main/scala/kafka/tools/MirrorMaker.scala b/core/src/main/scala/kafka/tools/MirrorMaker.scala
index 92396a7bdab..9cc6ebe1c86 100755
--- a/core/src/main/scala/kafka/tools/MirrorMaker.scala
+++ b/core/src/main/scala/kafka/tools/MirrorMaker.scala
@@ -31,7 +31,7 @@ import kafka.utils.{CommandLineUtils, CoreUtils, Logging, Whitelist}
 import org.apache.kafka.clients.consumer.{CommitFailedException, Consumer, ConsumerConfig, ConsumerRebalanceListener, ConsumerRecord, KafkaConsumer, OffsetAndMetadata}
 import org.apache.kafka.clients.producer.internals.ErrorLoggingCallback
 import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord, RecordMetadata}
-import org.apache.kafka.common.TopicPartition
+import org.apache.kafka.common.{KafkaException, TopicPartition}
 import org.apache.kafka.common.serialization.ByteArrayDeserializer
 import org.apache.kafka.common.utils.Utils
 import org.apache.kafka.common.errors.WakeupException
@@ -357,6 +357,8 @@ object MirrorMaker extends Logging with KafkaMetricsGroup {
               trace(""Caught NoRecordsException, continue iteration."")
             case _: WakeupException =>
               trace(""Caught WakeupException, continue iteration."")
+            case e: KafkaException if (shuttingDown || exitingOnSendFailure) =>
+              trace(s""Ignoring caught KafkaException during shutdown. sendFailure: $exitingOnSendFailure."", e)
           }
           maybeFlushAndCommitOffsets()
         }
diff --git a/core/src/test/scala/integration/kafka/api/BaseProducerSendTest.scala b/core/src/test/scala/integration/kafka/api/BaseProducerSendTest.scala
index ee0e90f1807..dc4041f1d63 100644
--- a/core/src/test/scala/integration/kafka/api/BaseProducerSendTest.scala
+++ b/core/src/test/scala/integration/kafka/api/BaseProducerSendTest.scala
@@ -35,6 +35,7 @@ import org.junit.Assert._
 import org.junit.{After, Before, Test}
 import scala.collection.mutable.{ArrayBuffer, Buffer}
+import scala.concurrent.ExecutionException
 abstract class BaseProducerSendTest extends KafkaServerTestHarness {
@@ -446,8 +447,7 @@ abstract class BaseProducerSendTest extends KafkaServerTestHarness {
           future.get()
           fail(""No message should be sent successfully."")
         } catch {
-          case e: Exception =>
-            assertEquals(""java.lang.IllegalStateException: Producer is closed forcefully."", e.getMessage)
+          case e: ExecutionException => assertEquals(classOf, e.getCause.getClass)
         }
       }
       assertEquals(""Fetch response should have no message returned."", 0, consumer.poll(50).count)
diff --git a/core/src/test/scala/integration/kafka/api/ProducerFailureHandlingTest.scala b/core/src/test/scala/integration/kafka/api/ProducerFailureHandlingTest.scala
index 7969485d81b..9b77c2d4169 100644
--- a/core/src/test/scala/integration/kafka/api/ProducerFailureHandlingTest.scala
+++ b/core/src/test/scala/integration/kafka/api/ProducerFailureHandlingTest.scala
@@ -205,7 +205,7 @@ class ProducerFailureHandlingTest extends KafkaServerTestHarness {
     // create topic
     createTopic(topic1, replicationFactor = numServers)
-    val record = new ProducerRecord,Array](topic1, null, ""key"".getBytes, ""value"".getBytes)
+    val record = new ProducerRecord, Array](topic1, null, ""key"".getBytes, ""value"".getBytes)
     // first send a message to make sure the metadata is refreshed
     producer1.send(record).get
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-7091,https://issues.apache.org/jira/browse/KAFKA-7091,https://github.com/apache/kafka/blob/2.0.0/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java,AdminClient should handle FindCoordinatorResponse errors,YES,"Currently KafkaAdminClient.deleteConsumerGroups, listConsumerGroupOffsets method implementation ignoring FindCoordinatorResponse errors. This causes admin client request timeouts incase of authorization errors.  We should handle these errors.","** Comment 1 **
omkreddy opened a new pull request #5278: KAFKA-7091: AdminClient should handle FindCoordinatorResponse errors
URL: [link]
   - Remove scala AdminClient usage from core and streams tests   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
hachikuji closed pull request #5278: KAFKA-7091: AdminClient should handle FindCoordinatorResponse errors
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git [file java] [file java]
index 62b6b6ee752..7e245d1d1d7 100644
--- [file java]
+++ [file java]
@@ -2395,16 +2395,9 @@ public DescribeConsumerGroupsResult describeConsumerGroups(final Collection<Stri
                 @Override
                 void handleResponse(AbstractResponse abstractResponse) {
                     final FindCoordinatorResponse fcResponse = (FindCoordinatorResponse) abstractResponse;
-                    Errors error = fcResponse.error();
-                    if (error == Errors.COORDINATOR_NOT_AVAILABLE) {
-                        // Retry COORDINATOR_NOT_AVAILABLE, in case the error is temporary.
-                        throw error.exception();
-                    } else if (error != Errors.NONE) {
-                        // All other errors are immediate failures.
-                        KafkaFutureImpl<ConsumerGroupDescription> future = futures.get(groupId);
-                        future.completeExceptionally(error.exception());
+
+                    if (handleFindCoordinatorError(fcResponse, futures.get(groupId)))
                         return;
-                    }
                     final long nowDescribeConsumerGroups = time.milliseconds();
                     final int nodeId = fcResponse.node().id();
@@ -2476,6 +2469,17 @@ void handleFailure(Throwable throwable) {
         return new DescribeConsumerGroupsResult(new HashMap<String, KafkaFuture<ConsumerGroupDescription>>(futures));
     }
+    private boolean handleFindCoordinatorError(FindCoordinatorResponse response, KafkaFutureImpl<?> future) {
+        Errors error = response.error();
+        if (error.exception() instanceof RetriableException) {
+            throw error.exception();
+        } else if (response.hasError()) {
+            future.completeExceptionally(error.exception());
+            return true;
+        }
+        return false;
+    }
+
     private final static class ListConsumerGroupsResults {
         private final List<Throwable> errors;
         private final HashMap<String, ConsumerGroupListing> listings;
@@ -2610,6 +2614,9 @@ public ListConsumerGroupOffsetsResult listConsumerGroupOffsets(final String grou
             void handleResponse(AbstractResponse abstractResponse) {
                 final FindCoordinatorResponse response = (FindCoordinatorResponse) abstractResponse;
+                if (handleFindCoordinatorError(response, groupOffsetListingFuture))
+                    return;
+
                 final long nowListConsumerGroupOffsets = time.milliseconds();
                 final int nodeId = response.node().id();
@@ -2696,6 +2703,9 @@ public DeleteConsumerGroupsResult deleteConsumerGroups(Collection<String> groupI
                 void handleResponse(AbstractResponse abstractResponse) {
                     final FindCoordinatorResponse response = (FindCoordinatorResponse) abstractResponse;
+                    if (handleFindCoordinatorError(response, futures.get(groupId)))
+                        return;
+
                     final long nowDeleteConsumerGroups = time.milliseconds();
                     final int nodeId = response.node().id();
diff --git [file java] [file java]
index 39726dac6b9..bc7f654c0bb 100644
--- [file java]
+++ [file java]
@@ -68,9 +68,11 @@
     /**
      * Possible error codes:
      *
+     * COORDINATOR_LOAD_IN_PROGRESS (14)
      * COORDINATOR_NOT_AVAILABLE (15)
-     * NOT_COORDINATOR (16)
      * GROUP_AUTHORIZATION_FAILED (30)
+     * INVALID_REQUEST (42)
+     * TRANSACTIONAL_ID_AUTHORIZATION_FAILED (53)
      */
@@ -107,6 +109,10 @@ public int throttleTimeMs() {
         return throttleTimeMs;
     }
+    public boolean hasError() {
+        return this.error != Errors.NONE;
+    }
+
     public Errors error() {
         return error;
     }
diff --git [file java] [file java]
index 3566f834220..836307902f4 100644
--- [file java]
+++ [file java]
@@ -37,6 +37,7 @@
 import org.apache.kafka.common.config.ConfigResource;
 import org.apache.kafka.common.errors.AuthenticationException;
 import org.apache.kafka.common.errors.CoordinatorNotAvailableException;
+import org.apache.kafka.common.errors.GroupAuthorizationException;
 import org.apache.kafka.common.errors.InvalidTopicException;
 import org.apache.kafka.common.errors.LeaderNotAvailableException;
 import org.apache.kafka.common.errors.NotLeaderForPartitionException;
@@ -1071,6 +1072,10 @@ public void testDeleteConsumerGroups() throws Exception {
             env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());
             env.kafkaClient().setNode(env.cluster().controller());
+            //Retriable FindCoordinatorResponse errors should be retried
+            env.kafkaClient().prepareResponse(new FindCoordinatorResponse(Errors.COORDINATOR_NOT_AVAILABLE,  Node.noNode()));
+            env.kafkaClient().prepareResponse(new FindCoordinatorResponse(Errors.COORDINATOR_LOAD_IN_PROGRESS, Node.noNode()));
+
             env.kafkaClient().prepareResponse(new FindCoordinatorResponse(Errors.NONE, env.cluster().controller()));
             final Map<String, Errors> response = new HashMap<>();
@@ -1081,6 +1086,13 @@ public void testDeleteConsumerGroups() throws Exception {
             final KafkaFuture<Void> results = result.deletedGroups().get(""group-0"");
             assertNull(results.get());
+
+            //should throw error for non-retriable errors
+            env.kafkaClient().prepareResponse(new FindCoordinatorResponse(Errors.GROUP_AUTHORIZATION_FAILED,  Node.noNode()));
+
+            final DeleteConsumerGroupsResult errorResult = env.adminClient().deleteConsumerGroups(groupIds);
+            assertFutureError(errorResult.deletedGroups().get(""group-0""), GroupAuthorizationException.class);
+
         }
     }
diff --git a/core/src/test/scala/integration/kafka/api/AdminClientIntegrationTest.scala b/core/src/test/scala/integration/kafka/api/AdminClientIntegrationTest.scala
index fe98fda8785..9055e68deed 100644
--- a/core/src/test/scala/integration/kafka/api/AdminClientIntegrationTest.scala
+++ b/core/src/test/scala/integration/kafka/api/AdminClientIntegrationTest.scala
@@ -29,12 +29,13 @@ import kafka.log.LogConfig
 import kafka.server.{Defaults, KafkaConfig, KafkaServer}
 import org.apache.kafka.clients.admin._
 import kafka.utils.{Logging, TestUtils}
+import kafka.utils.TestUtils._
 import kafka.utils.Implicits._
 import org.apache.kafka.clients.admin.NewTopic
 import org.apache.kafka.clients.consumer.{ConsumerConfig, KafkaConsumer}
 import org.apache.kafka.clients.producer.KafkaProducer
 import org.apache.kafka.clients.producer.ProducerRecord
-import org.apache.kafka.common.{ConsumerGroupState, KafkaFuture, TopicPartition, TopicPartitionReplica}
+import org.apache.kafka.common.{ConsumerGroupState, TopicPartition, TopicPartitionReplica}
 import org.apache.kafka.common.acl._
 import org.apache.kafka.common.config.ConfigResource
 import org.apache.kafka.common.errors._
@@ -125,18 +126,6 @@ class AdminClientIntegrationTest extends IntegrationTestHarness with Logging {
       }, ""timed out waiting for topics"")
   }
-  def assertFutureExceptionTypeEquals(future: KafkaFuture, clazz: Class): Unit = {
-    try {
-      future.get()
-      fail(""Expected CompletableFuture.get to return an exception"")
-    } catch {
-      case e: ExecutionException =>
-        val cause = e.getCause()
-        assertTrue(""Expected an exception of type "" + clazz.getName + ""; got type "" +
-            cause.getClass().getName, clazz.isInstance(cause))
-    }
-  }
-
   @Test
   def testClose(): Unit = {
     val client = AdminClient.create(createConfig())
diff --git a/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala b/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala
index a21affcdc99..72b3b24b8c0 100644
--- a/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala
+++ b/core/src/test/scala/integration/kafka/api/AuthorizerIntegrationTest.scala
@@ -19,7 +19,6 @@ import java.util.regex.Pattern
 import java.util.{ArrayList, Collections, Properties}
 import java.time.Duration
-import kafka.admin.AdminClient
 import kafka.admin.ConsumerGroupCommand.{ConsumerGroupCommandOptions, ConsumerGroupService}
 import kafka.common.TopicAndPartition
 import kafka.log.LogConfig
@@ -27,7 +26,7 @@ import kafka.network.SocketServer
 import kafka.security.auth._
 import kafka.server.{BaseRequestTest, KafkaConfig}
 import kafka.utils.TestUtils
-import org.apache.kafka.clients.admin.NewPartitions
+import org.apache.kafka.clients.admin.{AdminClient, AdminClientConfig, NewPartitions}
 import org.apache.kafka.clients.consumer._
 import org.apache.kafka.clients.consumer.internals.NoOpConsumerRebalanceListener
 import org.apache.kafka.clients.producer._
@@ -1004,17 +1003,18 @@ class AuthorizerIntegrationTest extends BaseRequestTest {
     this.consumers.head.partitionsFor(topic)
   }
-  @Test(expected = classOf)
+  @Test
   def testDescribeGroupApiWithNoGroupAcl() {
     addAndVerifyAcls(Set(new Acl(userPrincipal, Allow, Acl.WildCardHost, Describe)), topicResource)
-    createAdminClient().describeConsumerGroup(group)
+    val result = createAdminClient().describeConsumerGroups(Seq(group).asJava)
+    TestUtils.assertFutureExceptionTypeEquals(result.describedGroups().get(group), classOf)
   }
   @Test
   def testDescribeGroupApiWithGroupDescribe() {
     addAndVerifyAcls(Set(new Acl(userPrincipal, Allow, Acl.WildCardHost, Describe)), groupResource)
     addAndVerifyAcls(Set(new Acl(userPrincipal, Allow, Acl.WildCardHost, Describe)), topicResource)
-    createAdminClient().describeConsumerGroup(group)
+    createAdminClient().describeConsumerGroups(Seq(group).asJava).describedGroups().get(group).get()
   }
   @Test
@@ -1036,8 +1036,7 @@ class AuthorizerIntegrationTest extends BaseRequestTest {
     addAndVerifyAcls(Set(new Acl(userPrincipal, Allow, Acl.WildCardHost, Delete)), groupResource)
     this.consumers.head.assign(List(tp).asJava)
     this.consumers.head.commitSync(Map(tp -> new OffsetAndMetadata(5, """")).asJava)
-    val result = createAdminClient().deleteConsumerGroups(List(group))
-    assert(result.size == 1 && result.keySet.contains(group) && result.get(group).contains(Errors.NONE))
+    createAdminClient().deleteConsumerGroups(Seq(group).asJava).deletedGroups().get(group).get()
   }
   @Test
@@ -1046,14 +1045,14 @@ class AuthorizerIntegrationTest extends BaseRequestTest {
     addAndVerifyAcls(Set(new Acl(userPrincipal, Allow, Acl.WildCardHost, Read)), topicResource)
     this.consumers.head.assign(List(tp).asJava)
     this.consumers.head.commitSync(Map(tp -> new OffsetAndMetadata(5, """")).asJava)
-    val result = createAdminClient().deleteConsumerGroups(List(group))
-    assert(result.size == 1 && result.keySet.contains(group) && result.get(group).contains(Errors.GROUP_AUTHORIZATION_FAILED))
+    val result = createAdminClient().deleteConsumerGroups(Seq(group).asJava)
+    TestUtils.assertFutureExceptionTypeEquals(result.deletedGroups().get(group), classOf)
   }
   @Test
   def testDeleteGroupApiWithNoDeleteGroupAcl2() {
-    val result = createAdminClient().deleteConsumerGroups(List(group))
-    assert(result.size == 1 && result.keySet.contains(group) && result.get(group).contains(Errors.GROUP_AUTHORIZATION_FAILED))
+    val result = createAdminClient().deleteConsumerGroups(Seq(group).asJava)
+    TestUtils.assertFutureExceptionTypeEquals(result.deletedGroups().get(group), classOf)
   }
   @Test
@@ -1462,7 +1461,9 @@ class AuthorizerIntegrationTest extends BaseRequestTest {
   }
   private def createAdminClient(): AdminClient = {
-    val adminClient = AdminClient.createSimplePlaintext(brokerList)
+    val props = new Properties()
+    props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList)
+    val adminClient = AdminClient.create(props)
     adminClients += adminClient
     adminClient
   }
diff --git a/core/src/test/scala/integration/kafka/api/ConsumerBounceTest.scala b/core/src/test/scala/integration/kafka/api/ConsumerBounceTest.scala
index f200cc2d55c..07cbf0cf414 100644
--- a/core/src/test/scala/integration/kafka/api/ConsumerBounceTest.scala
+++ b/core/src/test/scala/integration/kafka/api/ConsumerBounceTest.scala
@@ -14,28 +14,35 @@
 package kafka.api
 import java.util.concurrent._
-import java.util.{Collection, Collections}
+import java.util.{Collection, Collections, Properties}
-import kafka.admin.AdminClient
-import kafka.server.KafkaConfig
+import kafka.server.{BaseRequestTest, KafkaConfig}
 import kafka.utils.{CoreUtils, Logging, ShutdownableThread, TestUtils}
 import org.apache.kafka.clients.consumer._
-import org.apache.kafka.clients.producer.{ProducerConfig, ProducerRecord}
+import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}
 import org.apache.kafka.common.TopicPartition
+import org.apache.kafka.common.protocol.ApiKeys
+import org.apache.kafka.common.requests.{FindCoordinatorRequest, FindCoordinatorResponse}
+import org.apache.kafka.common.security.auth.SecurityProtocol
 import org.junit.Assert._
 import org.junit.{After, Before, Ignore, Test}
 import scala.collection.JavaConverters._
+import scala.collection.mutable.Buffer
 /**
  * Integration tests for the consumer that cover basic usage as well as server failures
  */
-class ConsumerBounceTest extends IntegrationTestHarness with Logging {
+class ConsumerBounceTest extends BaseRequestTest with Logging {
+
+  override def numBrokers: Int = 3
   val producerCount = 1
   val consumerCount = 2
-  val serverCount = 3
+
+  val consumers = Buffer, Array]]()
+  val producers = Buffer, Array]]()
   val topic = ""topic""
   val part = 0
@@ -45,13 +52,8 @@ class ConsumerBounceTest extends IntegrationTestHarness with Logging {
   val gracefulCloseTimeMs = 1000
   val executor = Executors.newScheduledThreadPool(2)
-  // configure the servers and clients
-  this.serverConfig.setProperty(KafkaConfig.OffsetsTopicReplicationFactorProp, ""3"") // don't want to lose offset
-  this.serverConfig.setProperty(KafkaConfig.OffsetsTopicPartitionsProp, ""1"")
-  this.serverConfig.setProperty(KafkaConfig.GroupMinSessionTimeoutMsProp, ""10"") // set small enough session timeout
-  this.serverConfig.setProperty(KafkaConfig.GroupInitialRebalanceDelayMsProp, ""0"")
-  this.serverConfig.setProperty(KafkaConfig.UncleanLeaderElectionEnableProp, ""true"")
-  this.serverConfig.setProperty(KafkaConfig.AutoCreateTopicsEnableProp, ""false"")
+  val producerConfig = new Properties
+  val consumerConfig = new Properties
   this.producerConfig.setProperty(ProducerConfig.ACKS_CONFIG, ""all"")
   this.consumerConfig.setProperty(ConsumerConfig.GROUP_ID_CONFIG, ""my-test"")
   this.consumerConfig.setProperty(ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, 4096.toString)
@@ -59,8 +61,19 @@ class ConsumerBounceTest extends IntegrationTestHarness with Logging {
   this.consumerConfig.setProperty(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, ""3000"")
   this.consumerConfig.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"")
+  def serverConfig(): Properties = {
+    val properties = new Properties
+    properties.put(KafkaConfig.OffsetsTopicReplicationFactorProp, ""3"") // don't want to lose offset
+    properties.put(KafkaConfig.OffsetsTopicPartitionsProp, ""1"")
+    properties.put(KafkaConfig.GroupMinSessionTimeoutMsProp, ""10"") // set small enough session timeout
+    properties.put(KafkaConfig.GroupInitialRebalanceDelayMsProp, ""0"")
+    properties.put(KafkaConfig.UncleanLeaderElectionEnableProp, ""true"")
+    properties.put(KafkaConfig.AutoCreateTopicsEnableProp, ""false"")
+    properties
+  }
+
   override def generateConfigs = {
-    FixedPortTestUtils.createBrokerConfigs(serverCount, zkConnect, enableControlledShutdown = false)
+    FixedPortTestUtils.createBrokerConfigs(numBrokers, zkConnect, enableControlledShutdown = false)
       .map(KafkaConfig.fromProps(_, serverConfig))
   }
@@ -68,8 +81,26 @@ class ConsumerBounceTest extends IntegrationTestHarness with Logging {
   override def setUp() {
     super.setUp()
+    for (_ <- 0 until producerCount)
+      producers += createProducer
+
+    for (_ <- 0 until consumerCount)
+      consumers += createConsumer
+
     // create the test topic with all the brokers as replicas
-    createTopic(topic, 1, serverCount)
+    createTopic(topic, 1, numBrokers)
+  }
+
+  def createProducer: KafkaProducer, Array] = {
+    TestUtils.createProducer(TestUtils.getBrokerListStrFromServers(servers),
+        securityProtocol = SecurityProtocol.PLAINTEXT,
+        props = Some(producerConfig))
+  }
+
+  def createConsumer: KafkaConsumer, Array] = {
+    TestUtils.createConsumer(TestUtils.getBrokerListStrFromServers(servers),
+        securityProtocol = SecurityProtocol.PLAINTEXT,
+        props = Some(consumerConfig))
   }
   @After
@@ -78,6 +109,8 @@ class ConsumerBounceTest extends IntegrationTestHarness with Logging {
       executor.shutdownNow()
       // Wait for any active tasks to terminate to ensure consumer is not closed while being used from another thread
       assertTrue(""Executor did not terminate"", executor.awaitTermination(5000, TimeUnit.MILLISECONDS))
+      producers.foreach(_.close())
+      consumers.foreach(_.close())
     } finally {
       super.tearDown()
     }
@@ -173,7 +206,7 @@ class ConsumerBounceTest extends IntegrationTestHarness with Logging {
     val consumer = this.consumers.head
     consumer.subscribe(Collections.singleton(newtopic))
     executor.schedule(new Runnable {
-        def run() = createTopic(newtopic, numPartitions = serverCount, replicationFactor = serverCount)
+        def run() = createTopic(newtopic, numPartitions = numBrokers, replicationFactor = numBrokers)
       }, 2, TimeUnit.SECONDS)
     consumer.poll(0)
@@ -243,9 +276,8 @@ class ConsumerBounceTest extends IntegrationTestHarness with Logging {
     val consumer1 = createConsumerAndReceive(dynamicGroup, false, numRecords)
     val consumer2 = createConsumerAndReceive(manualGroup, true, numRecords)
-    val adminClient = AdminClient.createSimplePlaintext(this.brokerList)
-    killBroker(adminClient.findCoordinator(dynamicGroup).id)
-    killBroker(adminClient.findCoordinator(manualGroup).id)
+    killBroker(findCoordinator(dynamicGroup))
+    killBroker(findCoordinator(manualGroup))
     val future1 = submitCloseAndValidate(consumer1, Long.MaxValue, None, Some(gracefulCloseTimeMs))
     val future2 = submitCloseAndValidate(consumer2, Long.MaxValue, None, Some(gracefulCloseTimeMs))
@@ -255,9 +287,16 @@ class ConsumerBounceTest extends IntegrationTestHarness with Logging {
     restartDeadBrokers()
     checkClosedState(dynamicGroup, 0)
     checkClosedState(manualGroup, numRecords)
-    adminClient.close()
   }
+  private def findCoordinator(group: String) : Int = {
+    val request = new FindCoordinatorRequest.Builder(FindCoordinatorRequest.CoordinatorType.GROUP, group).build()
+    val resp = connectAndSend(request, ApiKeys.FIND_COORDINATOR)
+    val response = FindCoordinatorResponse.parse(resp, ApiKeys.FIND_COORDINATOR.latestVersion())
+    response.node().id()
+  }
+
+
   /**
    * Consumer is closed while all brokers are unavailable. Cannot rebalance or commit offsets since
    * there is no coordinator, but close should timeout and return. If close is invoked with a very
@@ -288,7 +327,7 @@ class ConsumerBounceTest extends IntegrationTestHarness with Logging {
   @Test
   def testCloseDuringRebalance() {
     val topic = ""closetest""
-    createTopic(topic, 10, serverCount)
+    createTopic(topic, 10, numBrokers)
     this.consumerConfig.setProperty(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, ""60000"")
     this.consumerConfig.setProperty(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, ""1000"")
     this.consumerConfig.setProperty(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, ""false"")
@@ -355,7 +394,7 @@ class ConsumerBounceTest extends IntegrationTestHarness with Logging {
   private def createConsumer(groupId: String) : KafkaConsumer, Array] = {
     this.consumerConfig.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupId)
-    val consumer = super.createConsumer
+    val consumer = createConsumer
     consumers += consumer
     consumer
   }
diff --git a/core/src/test/scala/integration/kafka/api/SaslSslAdminClientIntegrationTest.scala b/core/src/test/scala/integration/kafka/api/SaslSslAdminClientIntegrationTest.scala
index 9da69370d80..3b63613419d 100644
--- a/core/src/test/scala/integration/kafka/api/SaslSslAdminClientIntegrationTest.scala
+++ b/core/src/test/scala/integration/kafka/api/SaslSslAdminClientIntegrationTest.scala
@@ -18,6 +18,8 @@ import java.util
 import kafka.security.auth.{All, Allow, Alter, AlterConfigs, Authorizer, ClusterAction, Create, Delete, Deny, Describe, Group, Operation, PermissionType, SimpleAclAuthorizer, Topic, Acl => AuthAcl, Resource => AuthResource}
 import kafka.server.KafkaConfig
 import kafka.utils.{CoreUtils, JaasTestUtils, TestUtils}
+import kafka.utils.TestUtils._
+
 import org.apache.kafka.clients.admin.{AdminClient, CreateAclsOptions, DeleteAclsOptions}
 import org.apache.kafka.common.acl._
 import org.apache.kafka.common.errors.{ClusterAuthorizationException, InvalidRequestException}
diff --git a/core/src/test/scala/unit/kafka/utils/TestUtils.scala b/core/src/test/scala/unit/kafka/utils/TestUtils.scala
index db451961337..7b68cc0a016 100755
--- a/core/src/test/scala/unit/kafka/utils/TestUtils.scala
+++ b/core/src/test/scala/unit/kafka/utils/TestUtils.scala
@@ -25,7 +25,7 @@ import java.nio.file.{Files, StandardOpenOption}
 import java.security.cert.X509Certificate
 import java.time.Duration
 import java.util.{Collections, Properties}
-import java.util.concurrent.{Callable, Executors, TimeUnit}
+import java.util.concurrent.{Callable, ExecutionException, Executors, TimeUnit}
 import javax.net.ssl.X509TrustManager
 import kafka.api._
@@ -41,7 +41,7 @@ import org.apache.kafka.clients.CommonClientConfigs
 import org.apache.kafka.clients.admin.{AdminClient, AlterConfigsResult, Config, ConfigEntry}
 import org.apache.kafka.clients.consumer.{ConsumerRecord, KafkaConsumer, OffsetAndMetadata, RangeAssignor}
 import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}
-import org.apache.kafka.common.TopicPartition
+import org.apache.kafka.common.{KafkaFuture, TopicPartition}
 import org.apache.kafka.common.config.ConfigResource
 import org.apache.kafka.common.header.Header
 import org.apache.kafka.common.internals.Topic
@@ -1374,4 +1374,15 @@ object TestUtils extends Logging {
     (out.toString, err.toString)
   }
+  def assertFutureExceptionTypeEquals(future: KafkaFuture, clazz: Class): Unit = {
+    try {
+      future.get()
+      fail(""Expected CompletableFuture.get to return an exception"")
+    } catch {
+      case e: ExecutionException =>
+        val cause = e.getCause()
+        assertTrue(""Expected an exception of type "" + clazz.getName + ""; got type "" +
+            cause.getClass().getName, clazz.isInstance(cause))
+    }
+  }
 }
diff --git [file java] [file java]
index 249e2c3cffd..64b23cb799c 100644
--- [file java]
+++ [file java]
@@ -17,12 +17,12 @@
 package org.apache.kafka.streams.integration;
 import org.apache.kafka.clients.CommonClientConfigs;
-import org.apache.kafka.clients.admin.KafkaAdminClient;
+import org.apache.kafka.clients.admin.AdminClient;
+import org.apache.kafka.clients.admin.ConsumerGroupDescription;
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.kafka.clients.producer.ProducerConfig;
 import org.apache.kafka.common.config.SslConfigs;
 import org.apache.kafka.common.config.types.Password;
-import org.apache.kafka.common.errors.TimeoutException;
 import org.apache.kafka.common.serialization.LongDeserializer;
 import org.apache.kafka.common.serialization.LongSerializer;
 import org.apache.kafka.common.serialization.Serdes;
@@ -61,9 +61,9 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
+import java.util.concurrent.ExecutionException;
 import java.util.concurrent.TimeUnit;
-import kafka.admin.AdminClient;
 import kafka.tools.StreamsResetter;
 import static org.hamcrest.CoreMatchers.equalTo;
@@ -77,20 +77,15 @@
     private static MockTime mockTime;
     private static KafkaStreams streams;
     private static AdminClient adminClient = null;
-    private static KafkaAdminClient kafkaAdminClient = null;
     abstract Map<String, Object> getClientSslConfig();
     @AfterClass
     public static void afterClassCleanup() {
         if (adminClient != null) {
-            adminClient.close();
+            adminClient.close(10, TimeUnit.SECONDS);
             adminClient = null;
         }
-        if (kafkaAdminClient != null) {
-            kafkaAdminClient.close(10, TimeUnit.SECONDS);
-            kafkaAdminClient = null;
-        }
     }
     private String appID = ""abstract-reset-integration-test"";
@@ -103,9 +98,6 @@ private void prepareEnvironment() {
         if (adminClient == null) {
             adminClient = AdminClient.create(commonClientConfig);
         }
-        if (kafkaAdminClient == null) {
-            kafkaAdminClient = (KafkaAdminClient) org.apache.kafka.clients.admin.AdminClient.create(commonClientConfig);
-        }
         boolean timeSet = false;
         while (!timeSet) {
@@ -184,8 +176,9 @@ private void prepareConfigs() {
         @Override
         public boolean conditionMet() {
             try {
-                return adminClient.describeConsumerGroup(appID, 0).consumers().get().isEmpty();
-            } catch (final TimeoutException e) {
+                ConsumerGroupDescription groupDescription = adminClient.describeConsumerGroups(Collections.singletonList(appID)).describedGroups().get(appID).get();
+                return groupDescription.members().isEmpty();
+            } catch (final ExecutionException | InterruptedException e) {
                 return false;
             }
         }
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-7194,https://issues.apache.org/jira/browse/KAFKA-7194,https://github.com/apache/kafka/blob/2.0.0/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java,Error deserializing assignment after rebalance,YES,"A simple sink connector task is failing in a test with the following exception: 
{noformat}
[2018-07-02 12:31:13,200] ERROR WorkerSinkTask{id=verifiable-sink-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask)

org.apache.kafka.common.protocol.types.SchemaException: Error reading field 'version': java.nio.BufferUnderflowException

        at org.apache.kafka.common.protocol.types.Schema.read(Schema.java:77)

        at org.apache.kafka.clients.consumer.internals.ConsumerProtocol.deserializeAssignment(ConsumerProtocol.java:105)

        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:243)

        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:421)

        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:353)

        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:338)

        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:333)

        at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1218)

        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1181)

        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1115)

        at org.apache.kafka.connect.runtime.WorkerSinkTask.pollConsumer(WorkerSinkTask.java:444)

        at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:317)

        at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:225)

        at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:193)

        at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:175)

        at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:219)

        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)

        at java.util.concurrent.FutureTask.run(FutureTask.java:266)

        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)

        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)

        at java.lang.Thread.run(Thread.java:748){noformat}
 

After dumping the consumer offsets on the partition that this consumer group is writing with: 
{noformat}
bin/kafka-dump-log.sh --offsets-decoder --files ./00000000000000000000.log {noformat}
we get: 
{noformat}
Dumping ./00000000000000000000.log

Starting offset: 0

offset: 0 position: 0 CreateTime: 1530534673177 isvalid: true keysize: 27 valuesize: 217 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: {""metadata"":""connect-verifiable-sink""} payload: {""protocolType"":""consumer"",""protocol"":""range"",""generationId"":1,""assignment"":""{consumer-4-bad84955-e702-44fe-a018-677bd3b3a9d4=[test-0]}""}

offset: 1 position: 314 CreateTime: 1530534673206 isvalid: true keysize: 27 valuesize: 32 magic: 2 compresscodec: NONE producerId: -1 producerEpoch: -1 sequence: -1 isTransactional: false headerKeys: [] key: {""metadata"":""connect-verifiable-sink""} payload: {""protocolType"":""consumer"",""protocol"":null,""generationId"":2,""assignment"":""{}""}{noformat}
 

Since the broker seems to send a non-empty response to the consumer, there's a chance that the response buffer is consumed more than once at some point when parsing the response in the client. 

Here's what the kafka-request.log shows it sends to the client with the `SYNC_GROUP` response that throws the error: 
{noformat}
[2018-07-02 12:31:13,185] DEBUG Completed request:RequestHeader(apiKey=SYNC_GROUP, apiVersion=2, clientId=consumer-4, correlationId=5) -- {group_id=connect-verifiable-sink,generation_id=1,member_id=consumer-4-bad84955-e702-44fe-a018-677bd3b3a9d4,group_assignment=[{member_id=consumer-4-bad84955-e702-44fe-a018-677bd3b3a9d4,member_assignment=java.nio.HeapByteBuffer[pos=0 lim=24 cap=24]}]},response:{throttle_time_ms=0,error_code=0,member_assignment=java.nio.HeapByteBuffer[pos=0 lim=24 cap=24]} from connection 172.31.40.44:9092-172.31.35.189:49191-25;totalTime:8.904,requestQueueTime:0.063,localTime:8.558,remoteTime:0.0,throttleTime:0.03,responseQueueTime:0.037,sendTime:0.245,securityProtocol:PLAINTEXT,principal:User:ANONYMOUS,listener:PLAINTEXT (kafka.request.logger){noformat}
 ","** Comment 1 **
hachikuji opened a new pull request #5417: KAFKA-7194; Fix buffer underflow if onJoinComplete is retried after failure
URL: [link]
   An untimely wakeup can cause `ConsumerCoordinator.onJoinComplete` to throw a `WakeupException` before completion. On the next `poll()`, it will be retried, but this leads to an underflow error because the buffer containing the assignment data will already have been advanced. The solution is to duplicate the buffer passed to `onJoinComplete`.    ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
rajinisivaram closed pull request #5417: KAFKA-7194; Fix buffer underflow if onJoinComplete is retried after failure
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git [file java] [file java]
index b5c7a66e100..53834fb81df 100644
--- [file java]
+++ [file java]
@@ -200,9 +200,8 @@ public AbstractCoordinator(LogContext logContext,
                                                                  Map<String, ByteBuffer> allMemberMetadata);
     /**
-     * Invoked when a group member has successfully joined a group. If this call is woken up (i.e.
-     * if the invocation raises {@link org.apache.kafka.common.errors.WakeupException}), then it
-     * will be retried on the next call to {@link #ensureActiveGroup()}.
+     * Invoked when a group member has successfully joined a group. If this call fails with an exception,
+     * then it will be retried using the same assignment state on the next call to {@link #ensureActiveGroup()}.
      *
      * @param generation The generation that was joined
      * @param memberId The identifier for the local member in the group
@@ -418,7 +417,9 @@ boolean joinGroupIfNeeded(final long timeoutMs, final long startTimeMs) {
             }
             if (future.succeeded()) {
-                onJoinComplete(generation.generationId, generation.memberId, generation.protocol, future.value());
+                // Duplicate the buffer in case `onJoinComplete` does not complete and needs to be retried.
+                ByteBuffer memberAssignment = future.value().duplicate();
+                onJoinComplete(generation.generationId, generation.memberId, generation.protocol, memberAssignment);
                 // We reset the join group future only after the completion callback returns. This ensures
                 // that if the callback is woken up, we will retry it on the next joinGroupIfNeeded.
diff --git [file java] [file java]
index ea6d47249eb..f9b77e921cd 100644
--- [file java]
+++ [file java]
@@ -269,10 +269,10 @@ protected void onJoinComplete(int generation,
             this.joinedSubscription = newJoinedSubscription;
         }
-        // update the metadata and enforce a refresh to make sure the fetcher can start
-        // fetching data in the next iteration
+        // Update the metadata to include the full group subscription. The leader will trigger a rebalance
+        // if there are any metadata changes affecting any of the consumed partitions (whether or not this
+        // instance is subscribed to the topics).
         this.metadata.setTopics(subscriptions.groupSubscription());
-        if (!client.ensureFreshMetadata(Long.MAX_VALUE)) throw new TimeoutException();
         // give the assignor a chance to update internal state based on the received assignment
         assignor.onAssignment(assignment);
diff --git [file java] [file java]
index 7c2638cf012..ba392c6f4cb 100644
--- [file java]
+++ [file java]
@@ -874,6 +874,57 @@ public boolean matches(AbstractRequest body) {
         assertEquals(new HashSet<>(Arrays.asList(tp1, tp2)), subscriptions.assignedPartitions());
     }
+    @Test
+    public void testWakeupFromAssignmentCallback() {
+        ConsumerCoordinator coordinator = buildCoordinator(new Metrics(), assignors,
+                ConsumerConfig.DEFAULT_EXCLUDE_INTERNAL_TOPICS, false, true);
+
+        final String topic = ""topic1"";
+        TopicPartition partition = new TopicPartition(topic, 0);
+        final String consumerId = ""follower"";
+        Set<String> topics = Collections.singleton(topic);
+        MockRebalanceListener rebalanceListener = new MockRebalanceListener() {
+            @Override
+            public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
+                boolean raiseWakeup = this.assignedCount == 0;
+                super.onPartitionsAssigned(partitions);
+
+                if (raiseWakeup)
+                    throw new WakeupException();
+            }
+        };
+
+        subscriptions.subscribe(topics, rebalanceListener);
+        metadata.setTopics(topics);
+
+        // we only have metadata for one topic initially
+        metadata.update(TestUtils.singletonCluster(topic, 1), Collections.emptySet(), time.milliseconds());
+
+        client.prepareResponse(groupCoordinatorResponse(node, Errors.NONE));
+        coordinator.ensureCoordinatorReady(Long.MAX_VALUE);
+
+        // prepare initial rebalance
+        partitionAssignor.prepare(singletonMap(consumerId, Collections.singletonList(partition)));
+
+        client.prepareResponse(joinGroupFollowerResponse(1, consumerId, ""leader"", Errors.NONE));
+        client.prepareResponse(syncGroupResponse(Collections.singletonList(partition), Errors.NONE));
+
+
+        // The first call to poll should raise the exception from the rebalance listener
+        try {
+            coordinator.poll(Long.MAX_VALUE);
+            fail(""Expected exception thrown from assignment callback"");
+        } catch (WakeupException e) {
+        }
+
+        // The second call should retry the assignment callback and succeed
+        coordinator.poll(Long.MAX_VALUE);
+
+        assertFalse(coordinator.rejoinNeededOrPending());
+        assertEquals(1, rebalanceListener.revokedCount);
+        assertEquals(2, rebalanceListener.assignedCount);
+    }
+
     @Test
     public void testRebalanceAfterTopicUnavailableWithSubscribe() {
         unavailableTopicTest(false, false, Collections.<String>emptySet());
@@ -1901,7 +1952,7 @@ private JoinGroupResponse joinGroupLeaderResponse(int generationId,
     private JoinGroupResponse joinGroupFollowerResponse(int generationId, String memberId, String leaderId, Errors error) {
         return new JoinGroupResponse(error, generationId, partitionAssignor.name(), memberId, leaderId,
-                Collections.<String, ByteBuffer>emptyMap());
+                Collections.emptyMap());
     }
     private SyncGroupResponse syncGroupResponse(List<TopicPartition> partitions, Errors error) {
@@ -1914,7 +1965,7 @@ private OffsetCommitResponse offsetCommitResponse(Map<TopicPartition, Errors> re
     }
     private OffsetFetchResponse offsetFetchResponse(Errors topLevelError) {
-        return new OffsetFetchResponse(topLevelError, Collections.<TopicPartition, OffsetFetchResponse.PartitionData>emptyMap());
+        return new OffsetFetchResponse(topLevelError, Collections.emptyMap());
     }
     private OffsetFetchResponse offsetFetchResponse(TopicPartition tp, Errors partitionLevelError, String metadata, long offset) {
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-3702,https://issues.apache.org/jira/browse/KAFKA-3702,https://github.com/apache/kafka/blob/2.0.0/clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java,SslTransportLayer.close() does not shutdown gracefully,NO,"The warning ""Failed to send SSL Close message"" occurs very frequently when SSL connections are closed. Close should write outbound data and shutdown gracefully.","** Comment 1 **
FWIW, this has been discussed at [link]Either the current behavior is normal and Kafka should not log this as a warning (maybe debug is enough?) or it is not normal so the code/logic should be changed.Any hope to get this fixed?

** Comment 2 **
Anybody have ever encountered this one? WARN: Failed to send SSL Close message java.io.IOException: Unexpected status returned by SSLEngine.wrap, expected CLOSED, received OK.Seems similar but message is a little bit different. [link] 

** Comment 3 **
I also got this issue. The frequent log is annoying.

** Comment 4 **
[Comment excluded]

** Comment 5 **
rajinisivaram opened a new pull request #5397: KAFKA-3702: Change log level of SSL close_notify failure
URL: [link]
   SslTransportLayer currently closes the SSL engine and logs a warning if `close_notify` message canot be sent because the remote end closed its connection. This tends to fill up broker logs, especially when using clients which close connections immediately. Since this log entry is not very useful anyway, it would be better to log at debug level.   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 6 **
rajinisivaram closed pull request #5397: KAFKA-3702: Change log level of SSL close_notify failure
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git [file java] [file java]
index 838a6a75af3..08a39e71d50 100644
--- [file java]
+++ [file java]
@@ -177,7 +177,7 @@ public void close() throws IOException {
                 flush(netWriteBuffer);
             }
         } catch (IOException ie) {
-            log.warn(""Failed to send SSL Close message"", ie);
+            log.debug(""Failed to send SSL Close message"", ie);
         } finally {
             socketChannel.socket().close();
             socketChannel.close();
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 7 **
We changed the log level of the entry `Failed to send SSL Close message` from WARN to DEBUG, but we haven't yet fixed the underlying issue. At the moment, we attempt to flush all outgoing data and close gracefully, but disconnect if we can't do so immediately. Reopening the JIRA to track the underlying issue.

** Comment 8 **
 Does this issue relate to KAFKA-6510?  I have seen Failed to send SSL Close message as well as Unexpected status returned by SSLEngine.wrap, expected CLOSED, received OK happening at the same in Kafka 1.1.1

** Comment 9 **
[Comment excluded]

** Comment 10 **
Hi, I came across this issue while searching in relation to the other two jiras I have been commenting for which this same error is also cropping up. I think these may all be related... Regardless, it all points to kafka 0.10 versions.. this issue has been in our clusters for near 2 years now...[link][link] 

** Comment 11 **
[Comment excluded]

** Comment 12 **
[Comment excluded]

** Comment 13 **
[Comment excluded]

** Comment 14 **
[Comment excluded]

** Comment 15 **
[Comment excluded]

** Comment 16 **
[Comment excluded]
"
KAFKA-6802,https://issues.apache.org/jira/browse/KAFKA-6802,https://github.com/apache/kafka/blob/2.0.0/streams/src/main/java/org/apache/kafka/streams/processor/DefaultPartitionGrouper.java,Improve logging when topics aren't known and assignments skipped,YES,,"** Comment 1 **
bbejeck opened a new pull request #4891: KAFKA-6802: Improved logging for missing topics during task assignment
URL: [link]
   If users don't create all topics before starting a streams application, they could get unexpected results.  For example, sharing a state store between sub-topologies where one input topic is not created ahead time results in log message that that ""Partition X is not assigned to any tasks"" does not give any clues as to how this could have occurred.   Also, this PR changes the log level from `INFO` to `WARN` when metadata does not have partitions for a given topic.   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
guozhangwang closed pull request #4891: KAFKA-6802: Improved logging for missing topics during task assignment
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git [file java] [file java]
index c86171c3ab0..cee94886854 100644
--- [file java]
+++ [file java]
@@ -82,7 +82,11 @@ protected int maxNumPartitions(Cluster metadata, Set<String> topics) {
             List<PartitionInfo> partitions = metadata.partitionsForTopic(topic);
             if (partitions.isEmpty()) {
-                log.info(""Skipping assigning topic {} to tasks since its metadata is not available yet"", topic);
+
+                log.warn(""Skipping creating tasks for the topic group {} since topic {}'s metadata is not available yet;""
+                         + "" no tasks for this topic group will be assigned to any client.\n""
+                         + "" Make sure all supplied topics in the topology are created before starting""
+                         + "" as this could lead to unexpected results"", topics, topic);
                 return StreamsPartitionAssignor.NOT_AVAILABLE;
             } else {
                 int numPartitions = partitions.size();
diff --git [file java] [file java]
index c81105ef821..1f00c0478ea 100644
--- [file java]
+++ [file java]
@@ -470,7 +470,11 @@ public Subscription subscription(final Set<String> topics) {
                 for (final PartitionInfo partitionInfo : partitionInfoList) {
                     final TopicPartition partition = new TopicPartition(partitionInfo.topic(), partitionInfo.partition());
                     if (!allAssignedPartitions.contains(partition)) {
-                        log.warn(""Partition {} is not assigned to any tasks: {}"", partition, partitionsForTask);
+                        log.warn(""Partition {} is not assigned to any tasks: {}""
+                                 + "" Possible causes of a partition not getting assigned""
+                                 + "" is that another topic defined in the topology has not been""
+                                 + "" created when starting your streams application,""
+                                 + "" resulting in no tasks created for this topology at all."", partition, partitionsForTask);
                     }
                 }
             } else {
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-6054,https://issues.apache.org/jira/browse/KAFKA-6054,https://github.com/apache/kafka/blob/2.0.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java,"ERROR ""SubscriptionInfo - unable to decode subscription data: version=2"" when upgrading from 0.10.0.0 to 0.10.2.1",YES,"KIP: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade]


We upgraded an app from kafka-streams 0.10.0.0 to 0.10.2.1. We did a rolling upgrade of the app, so that one point, there were both 0.10.0.0-based instances and 0.10.2.1-based instances running.

We observed the following stack trace:
{code:java}
2017-10-11 07:02:19.964 [StreamThread-3] ERROR o.a.k.s.p.i.a.SubscriptionInfo -
unable to decode subscription data: version=2
org.apache.kafka.streams.errors.TaskAssignmentException: unable to decode
subscription data: version=2
        at org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.decode(SubscriptionInfo.java:113)
        at org.apache.kafka.streams.processor.internals.StreamPartitionAssignor.assign(StreamPartitionAssignor.java:235)
        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.performAssignment(ConsumerCoordinator.java:260)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.onJoinLeader(AbstractCoordinator.java:404)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.access$900(AbstractCoordinator.java:81)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:358)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:340)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:679)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:658)
        at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:167)
        at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:133)
        at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:107)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.onComplete(ConsumerNetworkClient.java:426)
        at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:278)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.clientPoll(ConsumerNetworkClient.java:360)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:224)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:192)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:163)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:243)
        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.ensurePartitionAssignment(ConsumerCoordinator.java:345)
        at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:977)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:937)
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:295)
        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:218)
        
{code}
I spoke with [~mjsax] and he said this is a known issue that happens when you have both 0.10.0.0 instances and 0.10.2.1 instances running at the same time, because the internal version number of the protocol changed when adding Interactive Queries. Matthias asked me to file this JIRA>","** Comment 1 **
Here is my conversation with  from the Confluent Slack channel:{quote}James Cheng  Does this stack trace mean anything to anyone? It happened when we upgraded a kafka streams app from 0.10.0.0 to 0.10.2.1.^ @mjsax, if you have any time to look. Thanks.Matthias J Sax That makes sense. We bumped the internal version number when adding IQ feature -- thus, it seems you cannot mix instances for both version.Seems, we messed up the upgrade path :disappointed:If you can, you would need to stop all old instances, before starting with the new version.Can you also open a JIRA for this?Thus, rolling bounces to upgrade should actually work -- is this what you are doing?James Cheng  Yes, we're doing a rolling upgrade. We had (at one point, at least) both instances running.I imagine that if the 0.10.0.0 versions crashed, then restarted running 0.10.2.1, then they would be fine because they are all the same version at that point, right?Matthias J Sax Yes.James Cheng  Cool, thanks.Matthias J Sax Anyway. Please file a JIRA -- upgrading should always work without this error.James Cheng  I'll file the JIRA.Matthias J Sax Thx.{quote}

** Comment 2 **
mjsax opened a new pull request #4630: KAFKA-6054: Code cleanup to prepare the actual fix for an upgrade path
URL: [link]
   Small change in decoding version 1 metadata: don't upgrade to version 2 automatically
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 3 **
[Comment excluded]

** Comment 4 **
mjsax opened a new pull request #4636:  KAFKA-6054: Fix Kafka Streams upgrade path for v0.10.0
URL: [link]
   Fixes the upgrade path from 0.10.0.x to 0.10.1.x+   Contained in KIP-258   Adds system tests for rolling bounce upgrades.
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 5 **
mjsax closed pull request #4630: KAFKA-6054: Code cleanup to prepare the actual fix for an upgrade path
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git [file java] [file java]
index 6b3626101bd..47becfc239b 100644
--- [file java]
+++ [file java]
@@ -226,11 +226,11 @@
     public static final String DEFAULT_KEY_SERDE_CLASS_CONFIG = ""default.key.serde"";
     private static final String DEFAULT_KEY_SERDE_CLASS_DOC = "" Default serializer / deserializer class for key that implements the <code>org.apache.kafka.common.serialization.Serde</code> interface."";
-    /** {@code default timestamp.extractor} */
+    /** {@code default.timestamp.extractor} */
     public static final String DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG = ""default.timestamp.extractor"";
     private static final String DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_DOC = ""Default timestamp extractor class that implements the <code>org.apache.kafka.streams.processor.TimestampExtractor</code> interface."";
-    /** {@code default value.serde} */
+    /** {@code default.value.serde} */
     public static final String DEFAULT_VALUE_SERDE_CLASS_CONFIG = ""default.value.serde"";
     private static final String DEFAULT_VALUE_SERDE_CLASS_DOC = ""Default serializer / deserializer class for value that implements the <code>org.apache.kafka.common.serialization.Serde</code> interface."";
diff --git [file java] [file java]
index 9aa0e94c8c1..71a84b2ca73 100644
--- [file java]
+++ [file java]
@@ -66,7 +66,8 @@
         public final TaskId taskId;
         public final TopicPartition partition;
-        AssignedPartition(final TaskId taskId, final TopicPartition partition) {
+        AssignedPartition(final TaskId taskId,
+                          final TopicPartition partition) {
             this.taskId = taskId;
             this.partition = partition;
         }
@@ -77,11 +78,11 @@ public int compareTo(final AssignedPartition that) {
         }
         @Override
-        public boolean equals(Object o) {
+        public boolean equals(final Object o) {
             if (!(o instanceof AssignedPartition)) {
                 return false;
             }
-            AssignedPartition other = (AssignedPartition) o;
+            final AssignedPartition other = (AssignedPartition) o;
             return compareTo(other) == 0;
         }
@@ -104,8 +105,9 @@ public int hashCode() {
                 final String host = getHost(endPoint);
                 final Integer port = getPort(endPoint);
-                if (host == null || port == null)
+                if (host == null || port == null) {
                     throw new ConfigException(String.format(""Error parsing host address %s. Expected format host:port."", endPoint));
+                }
                 hostInfo = new HostInfo(host, port);
             } else {
@@ -119,10 +121,11 @@ public int hashCode() {
             state = new ClientState();
         }
-        void addConsumer(final String consumerMemberId, final SubscriptionInfo info) {
+        void addConsumer(final String consumerMemberId,
+                         final SubscriptionInfo info) {
             consumers.add(consumerMemberId);
-            state.addPreviousActiveTasks(info.prevTasks);
-            state.addPreviousStandbyTasks(info.standbyTasks);
+            state.addPreviousActiveTasks(info.prevTasks());
+            state.addPreviousStandbyTasks(info.standbyTasks());
             state.incrementCapacity();
         }
@@ -157,8 +160,9 @@ public String toString() {
     private static final Comparator<TopicPartition> PARTITION_COMPARATOR = new Comparator<TopicPartition>() {
         @Override
-        public int compare(TopicPartition p1, TopicPartition p2) {
-            int result = p1.topic().compareTo(p2.topic());
+        public int compare(final TopicPartition p1,
+                           final TopicPartition p2) {
+            final int result = p1.topic().compareTo(p2.topic());
             if (result != 0) {
                 return result;
@@ -194,15 +198,15 @@ public void configure(final Map<String, ?> configs) {
         final Object o = configs.get(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR);
         if (o == null) {
-            KafkaException ex = new KafkaException(""TaskManager is not specified"");
-            log.error(ex.getMessage(), ex);
-            throw ex;
+            final KafkaException fatalException = new KafkaException(""TaskManager is not specified"");
+            log.error(fatalException.getMessage(), fatalException);
+            throw fatalException;
         }
         if (!(o instanceof TaskManager)) {
-            KafkaException ex = new KafkaException(String.format(""%s is not an instance of %s"", o.getClass().getName(), TaskManager.class.getName()));
-            log.error(ex.getMessage(), ex);
-            throw ex;
+            final KafkaException fatalException = new KafkaException(String.format(""%s is not an instance of %s"", o.getClass().getName(), TaskManager.class.getName()));
+            log.error(fatalException.getMessage(), fatalException);
+            throw fatalException;
         }
         taskManager = (TaskManager) o;
@@ -214,14 +218,14 @@ public void configure(final Map<String, ?> configs) {
         final String userEndPoint = streamsConfig.getString(StreamsConfig.APPLICATION_SERVER_CONFIG);
         if (userEndPoint userEndPoint.isEmpty()) {
             try {
-                String host = getHost(userEndPoint);
-                Integer port = getPort(userEndPoint);
+                final String host = getHost(userEndPoint);
+                final Integer port = getPort(userEndPoint);
                 if (host == null || port == null)
                     throw new ConfigException(String.format(""%s Config %s isn't in the correct format. Expected a host:port pair"" +
                                     "" but received %s"",
                             logPrefix, StreamsConfig.APPLICATION_SERVER_CONFIG, userEndPoint));
-            } catch (NumberFormatException nfe) {
+            } catch (final NumberFormatException nfe) {
                 throw new ConfigException(String.format(""%s Invalid port supplied in %s for config %s"",
                         logPrefix, userEndPoint, StreamsConfig.APPLICATION_SERVER_CONFIG));
             }
@@ -240,7 +244,7 @@ public String name() {
     }
     @Override
-    public Subscription subscription(Set<String> topics) {
+    public Subscription subscription(final Set<String> topics) {
         // Adds the following information to subscription
         // 1. Client UUID (a unique id assigned to an instance of KafkaStreams)
         // 2. Task ids of previously running tasks
@@ -249,7 +253,11 @@ public Subscription subscription(Set<String> topics) {
         final Set<TaskId> previousActiveTasks = taskManager.prevActiveTaskIds();
         final Set<TaskId> standbyTasks = taskManager.cachedTasksIds();
         standbyTasks.removeAll(previousActiveTasks);
-        final SubscriptionInfo data = new SubscriptionInfo(taskManager.processId(), previousActiveTasks, standbyTasks, this.userEndPoint);
+        final SubscriptionInfo data = new SubscriptionInfo(
+            taskManager.processId(),
+            previousActiveTasks,
+            standbyTasks,
+            this.userEndPoint);
         taskManager.updateSubscriptionsFromMetadata(topics);
@@ -277,22 +285,32 @@ public Subscription subscription(Set<String> topics) {
      * 3. within each client, tasks are assigned to consumer clients in round-robin manner.
      */
     @Override
-    public Map<String, Assignment> assign(Cluster metadata, Map<String, Subscription> subscriptions) {
+    public Map<String, Assignment> assign(final Cluster metadata,
+                                          final Map<String, Subscription> subscriptions) {
         // construct the client metadata from the decoded subscription info
-        Map<UUID, ClientMetadata> clientsMetadata = new HashMap<>();
-
-        for (Map.Entry<String, Subscription> entry : subscriptions.entrySet()) {
-            String consumerId = entry.getKey();
-            Subscription subscription = entry.getValue();
-
-            SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());
+        final Map<UUID, ClientMetadata> clientsMetadata = new HashMap<>();
+
+        int minUserMetadataVersion = SubscriptionInfo.LATEST_SUPPORTED_VERSION;
+        for (final Map.Entry<String, Subscription> entry : subscriptions.entrySet()) {
+            final String consumerId = entry.getKey();
+            final Subscription subscription = entry.getValue();
+
+            final SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());
+            final int usedVersion = info.version();
+            if (usedVersion > SubscriptionInfo.LATEST_SUPPORTED_VERSION) {
+                throw new IllegalStateException(""Unknown metadata version: "" + usedVersion
+                    + ""; latest supported version: "" + SubscriptionInfo.LATEST_SUPPORTED_VERSION);
+            }
+            if (usedVersion < minUserMetadataVersion) {
+                minUserMetadataVersion = usedVersion;
+            }
             // create the new client metadata if necessary
-            ClientMetadata clientMetadata = clientsMetadata.get(info.processId);
+            ClientMetadata clientMetadata = clientsMetadata.get(info.processId());
             if (clientMetadata == null) {
-                clientMetadata = new ClientMetadata(info.userEndPoint);
-                clientsMetadata.put(info.processId, clientMetadata);
+                clientMetadata = new ClientMetadata(info.userEndPoint());
+                clientsMetadata.put(info.processId(), clientMetadata);
             }
             // add the consumer to the client
@@ -309,8 +327,8 @@ public Subscription subscription(Set<String> topics) {
         final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = taskManager.builder().topicGroups();
         final Map<String, InternalTopicMetadata> repartitionTopicMetadata = new HashMap<>();
-        for (InternalTopologyBuilder.TopicsInfo topicsInfo : topicGroups.values()) {
-            for (InternalTopicConfig topic: topicsInfo.repartitionSourceTopics.values()) {
+        for (final InternalTopologyBuilder.TopicsInfo topicsInfo : topicGroups.values()) {
+            for (final InternalTopicConfig topic: topicsInfo.repartitionSourceTopics.values()) {
                 repartitionTopicMetadata.put(topic.name(), new InternalTopicMetadata(topic));
             }
         }
@@ -319,13 +337,13 @@ public Subscription subscription(Set<String> topics) {
         do {
             numPartitionsNeeded = false;
-            for (InternalTopologyBuilder.TopicsInfo topicsInfo : topicGroups.values()) {
-                for (String topicName : topicsInfo.repartitionSourceTopics.keySet()) {
+            for (final InternalTopologyBuilder.TopicsInfo topicsInfo : topicGroups.values()) {
+                for (final String topicName : topicsInfo.repartitionSourceTopics.keySet()) {
                     int numPartitions = repartitionTopicMetadata.get(topicName).numPartitions;
                     // try set the number of partitions for this repartition topic if it is not set yet
                     if (numPartitions == UNKNOWN) {
-                        for (InternalTopologyBuilder.TopicsInfo otherTopicsInfo : topicGroups.values()) {
+                        for (final InternalTopologyBuilder.TopicsInfo otherTopicsInfo : topicGroups.values()) {
                             final Set<String> otherSinkTopics = otherTopicsInfo.sinkTopics;
                             if (otherSinkTopics.contains(topicName)) {
@@ -375,7 +393,7 @@ public Subscription subscription(Set<String> topics) {
         // augment the metadata with the newly computed number of partitions for all the
         // repartition source topics
         final Map<TopicPartition, PartitionInfo> allRepartitionTopicPartitions = new HashMap<>();
-        for (Map.Entry<String, InternalTopicMetadata> entry : repartitionTopicMetadata.entrySet()) {
+        for (final Map.Entry<String, InternalTopicMetadata> entry : repartitionTopicMetadata.entrySet()) {
             final String topic = entry.getKey();
             final int numPartitions = entry.getValue().numPartitions;
@@ -395,7 +413,7 @@ public Subscription subscription(Set<String> topics) {
         // get the tasks as partition groups from the partition grouper
         final Set<String> allSourceTopics = new HashSet<>();
         final Map<Integer, Set<String>> sourceTopicsByGroup = new HashMap<>();
-        for (Map.Entry<Integer, InternalTopologyBuilder.TopicsInfo> entry : topicGroups.entrySet()) {
+        for (final Map.Entry<Integer, InternalTopologyBuilder.TopicsInfo> entry : topicGroups.entrySet()) {
             allSourceTopics.addAll(entry.getValue().sourceTopics);
             sourceTopicsByGroup.put(entry.getKey(), entry.getValue().sourceTopics);
         }
@@ -405,9 +423,9 @@ public Subscription subscription(Set<String> topics) {
         // check if all partitions are assigned, and there are no duplicates of partitions in multiple tasks
         final Set<TopicPartition> allAssignedPartitions = new HashSet<>();
         final Map<Integer, Set<TaskId>> tasksByTopicGroup = new HashMap<>();
-        for (Map.Entry<TaskId, Set<TopicPartition>> entry : partitionsForTask.entrySet()) {
+        for (final Map.Entry<TaskId, Set<TopicPartition>> entry : partitionsForTask.entrySet()) {
             final Set<TopicPartition> partitions = entry.getValue();
-            for (TopicPartition partition : partitions) {
+            for (final TopicPartition partition : partitions) {
                 if (allAssignedPartitions.contains(partition)) {
                     log.warn(""Partition {} is assigned to more than one tasks: {}"", partition, partitionsForTask);
                 }
@@ -422,10 +440,10 @@ public Subscription subscription(Set<String> topics) {
             }
             ids.add(id);
         }
-        for (String topic : allSourceTopics) {
+        for (final String topic : allSourceTopics) {
             final List<PartitionInfo> partitionInfoList = fullMetadata.partitionsForTopic(topic);
             if (!partitionInfoList.isEmpty()) {
-                for (PartitionInfo partitionInfo : partitionInfoList) {
+                for (final PartitionInfo partitionInfo : partitionInfoList) {
                     final TopicPartition partition = new TopicPartition(partitionInfo.topic(), partitionInfo.partition());
                     if (!allAssignedPartitions.contains(partition)) {
                         log.warn(""Partition {} is not assigned to any tasks: {}"", partition, partitionsForTask);
@@ -438,15 +456,15 @@ public Subscription subscription(Set<String> topics) {
         // add tasks to state change log topic subscribers
         final Map<String, InternalTopicMetadata> changelogTopicMetadata = new HashMap<>();
-        for (Map.Entry<Integer, InternalTopologyBuilder.TopicsInfo> entry : topicGroups.entrySet()) {
+        for (final Map.Entry<Integer, InternalTopologyBuilder.TopicsInfo> entry : topicGroups.entrySet()) {
             final int topicGroupId = entry.getKey();
             final Map<String, InternalTopicConfig> stateChangelogTopics = entry.getValue().stateChangelogTopics;
-            for (InternalTopicConfig topicConfig : stateChangelogTopics.values()) {
+            for (final InternalTopicConfig topicConfig : stateChangelogTopics.values()) {
                 // the expected number of partitions is the max value of TaskId.partition + 1
                 int numPartitions = UNKNOWN;
                 if (tasksByTopicGroup.get(topicGroupId) != null) {
-                    for (TaskId task : tasksByTopicGroup.get(topicGroupId)) {
+                    for (final TaskId task : tasksByTopicGroup.get(topicGroupId)) {
                         if (numPartitions < task.partition + 1)
                             numPartitions = task.partition + 1;
                     }
@@ -468,7 +486,7 @@ public Subscription subscription(Set<String> topics) {
         // assign tasks to clients
         final Map<UUID, ClientState> states = new HashMap<>();
-        for (Map.Entry<UUID, ClientMetadata> entry : clientsMetadata.entrySet()) {
+        for (final Map.Entry<UUID, ClientMetadata> entry : clientsMetadata.entrySet()) {
             states.put(entry.getKey(), entry.getValue().state);
         }
@@ -484,25 +502,27 @@ public Subscription subscription(Set<String> topics) {
         // construct the global partition assignment per host map
         final Map<HostInfo, Set<TopicPartition>> partitionsByHostState = new HashMap<>();
-        for (Map.Entry<UUID, ClientMetadata> entry : clientsMetadata.entrySet()) {
-            final HostInfo hostInfo = entry.getValue().hostInfo;
+        if (minUserMetadataVersion == 2) {
+            for (final Map.Entry<UUID, ClientMetadata> entry : clientsMetadata.entrySet()) {
+                final HostInfo hostInfo = entry.getValue().hostInfo;
-            if (hostInfo != null) {
-                final Set<TopicPartition> topicPartitions = new HashSet<>();
-                final ClientState state = entry.getValue().state;
+                if (hostInfo != null) {
+                    final Set<TopicPartition> topicPartitions = new HashSet<>();
+                    final ClientState state = entry.getValue().state;
-                for (final TaskId id : state.activeTasks()) {
-                    topicPartitions.addAll(partitionsForTask.get(id));
-                }
+                    for (final TaskId id : state.activeTasks()) {
+                        topicPartitions.addAll(partitionsForTask.get(id));
+                    }
-                partitionsByHostState.put(hostInfo, topicPartitions);
+                    partitionsByHostState.put(hostInfo, topicPartitions);
+                }
             }
         }
         taskManager.setPartitionsByHostState(partitionsByHostState);
         // within the client, distribute tasks to its owned consumers
         final Map<String, Assignment> assignment = new HashMap<>();
-        for (Map.Entry<UUID, ClientMetadata> entry : clientsMetadata.entrySet()) {
+        for (final Map.Entry<UUID, ClientMetadata> entry : clientsMetadata.entrySet()) {
             final Set<String> consumers = entry.getValue().consumers;
             final ClientState state = entry.getValue().state;
@@ -511,7 +531,7 @@ public Subscription subscription(Set<String> topics) {
             int consumerTaskIndex = 0;
-            for (String consumer : consumers) {
+            for (final String consumer : consumers) {
                 final Map<TaskId, Set<TopicPartition>> standby = new HashMap<>();
                 final ArrayList<AssignedPartition> assignedPartitions = new ArrayList<>();
@@ -540,13 +560,15 @@ public Subscription subscription(Set<String> topics) {
                 Collections.sort(assignedPartitions);
                 final List<TaskId> active = new ArrayList<>();
                 final List<TopicPartition> activePartitions = new ArrayList<>();
-                for (AssignedPartition partition : assignedPartitions) {
+                for (final AssignedPartition partition : assignedPartitions) {
                     active.add(partition.taskId);
                     activePartitions.add(partition.partition);
                 }
                 // finally, encode the assignment before sending back to coordinator
-                assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(active, standby, partitionsByHostState).encode()));
+                assignment.put(consumer, new Assignment(
+                    activePartitions,
+                    new AssignmentInfo(minUserMetadataVersion, active, standby, partitionsByHostState).encode()));
             }
         }
@@ -577,26 +599,54 @@ public Subscription subscription(Set<String> topics) {
      * @throws TaskAssignmentException if there is no task id for one of the partitions specified
      */
     @Override
-    public void onAssignment(Assignment assignment) {
-        List<TopicPartition> partitions = new ArrayList<>(assignment.partitions());
+    public void onAssignment(final Assignment assignment) {
+        final List<TopicPartition> partitions = new ArrayList<>(assignment.partitions());
         Collections.sort(partitions, PARTITION_COMPARATOR);
-        AssignmentInfo info = AssignmentInfo.decode(assignment.userData());
+        final AssignmentInfo info = AssignmentInfo.decode(assignment.userData());
+        final int usedVersion = info.version();
-        Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();
+        // version 1 field
+        final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();
+        // version 2 fields
+        final Map<TopicPartition, PartitionInfo> topicToPartitionInfo = new HashMap<>();
+        final Map<HostInfo, Set<TopicPartition>> partitionsByHost;
+
+        switch (usedVersion) {
+            case 1:
+                processVersionOneAssignment(info, partitions, activeTasks);
+                partitionsByHost = Collections.emptyMap();
+                break;
+            case 2:
+                processVersionTwoAssignment(info, partitions, activeTasks, topicToPartitionInfo);
+                partitionsByHost = info.partitionsByHost();
+                break;
+            default:
+                throw new IllegalStateException(""Unknown metadata version: "" + usedVersion
+                    + ""; latest supported version: "" + AssignmentInfo.LATEST_SUPPORTED_VERSION);
+        }
+        taskManager.setClusterMetadata(Cluster.empty().withPartitions(topicToPartitionInfo));
+        taskManager.setPartitionsByHostState(partitionsByHost);
+        taskManager.setAssignmentMetadata(activeTasks, info.standbyTasks());
+        taskManager.updateSubscriptionsFromAssignment(partitions);
+    }
+
+    private void processVersionOneAssignment(final AssignmentInfo info,
+                                             final List<TopicPartition> partitions,
+                                             final Map<TaskId, Set<TopicPartition>> activeTasks) {
         // the number of assigned partitions should be the same as number of active tasks, which
         // could be duplicated if one task has more than one assigned partitions
-        if (partitions.size() != info.activeTasks.size()) {
+        if (partitions.size() != info.activeTasks().size()) {
             throw new TaskAssignmentException(
-                    String.format(""%sNumber of assigned partitions %d is not equal to the number of active taskIds %d"" +
-                            "", assignmentInfo=%s"", logPrefix, partitions.size(), info.activeTasks.size(), info.toString())
+                String.format(""%sNumber of assigned partitions %d is not equal to the number of active taskIds %d"" +
+                    "", assignmentInfo=%s"", logPrefix, partitions.size(), info.activeTasks().size(), info.toString())
             );
         }
         for (int i = 0; i < partitions.size(); i++) {
-            TopicPartition partition = partitions.get(i);
-            TaskId id = info.activeTasks.get(i);
+            final TopicPartition partition = partitions.get(i);
+            final TaskId id = info.activeTasks().get(i);
             Set<TopicPartition> assignedPartitions = activeTasks.get(id);
             if (assignedPartitions == null) {
@@ -605,23 +655,23 @@ public void onAssignment(Assignment assignment) {
             }
             assignedPartitions.add(partition);
         }
+    }
-        final Map<TopicPartition, PartitionInfo> topicToPartitionInfo = new HashMap<>();
-        for (Set<TopicPartition> value : info.partitionsByHost.values()) {
-            for (TopicPartition topicPartition : value) {
-                topicToPartitionInfo.put(topicPartition, new PartitionInfo(topicPartition.topic(),
-                        topicPartition.partition(),
-                        null,
-                        new Node,
-                        new Node));
+    private void processVersionTwoAssignment(final AssignmentInfo info,
+                                             final List<TopicPartition> partitions,
+                                             final Map<TaskId, Set<TopicPartition>> activeTasks,
+                                             final Map<TopicPartition, PartitionInfo> topicToPartitionInfo) {
+        processVersionOneAssignment(info, partitions, activeTasks);
+
+        // process partitions by host
+        final Map<HostInfo, Set<TopicPartition>> partitionsByHost = info.partitionsByHost();
+        for (final Set<TopicPartition> value : partitionsByHost.values()) {
+            for (final TopicPartition topicPartition : value) {
+                topicToPartitionInfo.put(
+                    topicPartition,
+                    new PartitionInfo(topicPartition.topic(), topicPartition.partition(), null, new Node, new Node));
             }
         }
-
-        taskManager.setClusterMetadata(Cluster.empty().withPartitions(topicToPartitionInfo));
-        taskManager.setPartitionsByHostState(info.partitionsByHost);
-        taskManager.setAssignmentMetadata(activeTasks, info.standbyTasks);
-
-        taskManager.updateSubscriptionsFromAssignment(partitions);
     }
     /**
@@ -658,10 +708,10 @@ private void prepareTopic(final Map<String, InternalTopicMetadata> topicPartitio
         log.debug(""Completed validating internal topics in partition assignor."");
     }
-    private void ensureCopartitioning(Collection<Set<String>> copartitionGroups,
-                                      Map<String, InternalTopicMetadata> allRepartitionTopicsNumPartitions,
-                                      Cluster metadata) {
-        for (Set<String> copartitionGroup : copartitionGroups) {
+    private void ensureCopartitioning(final Collection<Set<String>> copartitionGroups,
+                                      final Map<String, InternalTopicMetadata> allRepartitionTopicsNumPartitions,
+                                      final Cluster metadata) {
+        for (final Set<String> copartitionGroup : copartitionGroups) {
             copartitionedTopicsValidator.validate(copartitionGroup, allRepartitionTopicsNumPartitions, metadata);
         }
     }
@@ -677,7 +727,7 @@ private void ensureCopartitioning(Collection<Set<String>> copartitionGroups,
         private final Set<String> updatedTopicSubscriptions = new HashSet<>();
-        public void updateTopics(Collection<String> topicNames) {
+        public void updateTopics(final Collection<String> topicNames) {
             updatedTopicSubscriptions.clear();
             updatedTopicSubscriptions.addAll(topicNames);
         }
@@ -735,7 +785,7 @@ void validate(final Set<String> copartitionGroup,
             // if all topics for this co-partition group is repartition topics,
             // then set the number of partitions to be the maximum of the number of partitions.
             if (numPartitions == UNKNOWN) {
-                for (Map.Entry<String, InternalTopicMetadata> entry: allRepartitionTopicsNumPartitions.entrySet()) {
+                for (final Map.Entry<String, InternalTopicMetadata> entry: allRepartitionTopicsNumPartitions.entrySet()) {
                     if (copartitionGroup.contains(entry.getKey())) {
                         final int partitions = entry.getValue().numPartitions;
                         if (partitions > numPartitions) {
@@ -745,7 +795,7 @@ void validate(final Set<String> copartitionGroup,
                 }
             }
             // enforce co-partitioning restrictions to repartition topics by updating their number of partitions
-            for (Map.Entry<String, InternalTopicMetadata> entry : allRepartitionTopicsNumPartitions.entrySet()) {
+            for (final Map.Entry<String, InternalTopicMetadata> entry : allRepartitionTopicsNumPartitions.entrySet()) {
                 if (copartitionGroup.contains(entry.getKey())) {
                     entry.getValue().numPartitions = numPartitions;
                 }
@@ -755,7 +805,7 @@ void validate(final Set<String> copartitionGroup,
     }
     // following functions are for test only
-    void setInternalTopicManager(InternalTopicManager internalTopicManager) {
+    void setInternalTopicManager(final InternalTopicManager internalTopicManager) {
         this.internalTopicManager = internalTopicManager;
     }
 }
diff --git [file java] [file java]
index 8607472c281..c8df7498755 100644
--- [file java]
+++ [file java]
@@ -39,76 +39,123 @@
 public class AssignmentInfo {
     private static final Logger log = LoggerFactory.getLogger(AssignmentInfo.class);
-    /**
-     * A new field was added, partitionsByHost. CURRENT_VERSION
-     * is required so we can decode the previous version. For example, this may occur
-     * during a rolling upgrade
-     */
-    private static final int CURRENT_VERSION = 2;
-    public final int version;
-    public final List<TaskId> activeTasks; // each element corresponds to a partition
-    public final Map<TaskId, Set<TopicPartition>> standbyTasks;
-    public final Map<HostInfo, Set<TopicPartition>> partitionsByHost;
-    public AssignmentInfo(List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>> standbyTasks,
-                          Map<HostInfo, Set<TopicPartition>> hostState) {
-        this(CURRENT_VERSION, activeTasks, standbyTasks, hostState);
+    public static final int LATEST_SUPPORTED_VERSION = 2;
+
+    private final int usedVersion;
+    private List<TaskId> activeTasks;
+    private Map<TaskId, Set<TopicPartition>> standbyTasks;
+    private Map<HostInfo, Set<TopicPartition>> partitionsByHost;
+
+    private AssignmentInfo(final int version) {
+        this.usedVersion = version;
+    }
+
+    public AssignmentInfo(final List<TaskId> activeTasks,
+                          final Map<TaskId, Set<TopicPartition>> standbyTasks,
+                          final Map<HostInfo, Set<TopicPartition>> hostState) {
+        this(LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, hostState);
     }
-    protected AssignmentInfo(int version, List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>> standbyTasks,
-                             Map<HostInfo, Set<TopicPartition>> hostState) {
-        this.version = version;
+    public AssignmentInfo(final int version,
+                          final List<TaskId> activeTasks,
+                          final Map<TaskId, Set<TopicPartition>> standbyTasks,
+                          final Map<HostInfo, Set<TopicPartition>> hostState) {
+        this.usedVersion = version;
         this.activeTasks = activeTasks;
         this.standbyTasks = standbyTasks;
         this.partitionsByHost = hostState;
     }
+    public int version() {
+        return usedVersion;
+    }
+
+    public List<TaskId> activeTasks() {
+        return activeTasks;
+    }
+
+    public Map<TaskId, Set<TopicPartition>> standbyTasks() {
+        return standbyTasks;
+    }
+
+    public Map<HostInfo, Set<TopicPartition>> partitionsByHost() {
+        return partitionsByHost;
+    }
+
     /**
      * @throws TaskAssignmentException if method fails to encode the data, e.g., if there is an
      * IO exception during encoding
      */
     public ByteBuffer encode() {
-        ByteArrayOutputStream baos = new ByteArrayOutputStream();
-        DataOutputStream out = new DataOutputStream(baos);
-
-        try {
-            // Encode version
-            out.writeInt(version);
-            // Encode active tasks
-            out.writeInt(activeTasks.size());
-            for (TaskId id : activeTasks) {
-                id.writeTo(out);
-            }
-            // Encode standby tasks
-            out.writeInt(standbyTasks.size());
-            for (Map.Entry<TaskId, Set<TopicPartition>> entry : standbyTasks.entrySet()) {
-                TaskId id = entry.getKey();
-                id.writeTo(out);
-
-                Set<TopicPartition> partitions = entry.getValue();
-                writeTopicPartitions(out, partitions);
-            }
-            out.writeInt(partitionsByHost.size());
-            for (Map.Entry<HostInfo, Set<TopicPartition>> entry : partitionsByHost
-                    .entrySet()) {
-                final HostInfo hostInfo = entry.getKey();
-                out.writeUTF(hostInfo.host());
-                out.writeInt(hostInfo.port());
-                writeTopicPartitions(out, entry.getValue());
+        final ByteArrayOutputStream baos = new ByteArrayOutputStream();
+
+        try (final DataOutputStream out = new DataOutputStream(baos)) {
+            switch (usedVersion) {
+                case 1:
+                    encodeVersionOne(out);
+                    break;
+                case 2:
+                    encodeVersionTwo(out);
+                    break;
+                default:
+                    throw new IllegalStateException(""Unknown metadata version: "" + usedVersion
+                        + ""; latest supported version: "" + LATEST_SUPPORTED_VERSION);
             }
             out.flush();
             out.close();
             return ByteBuffer.wrap(baos.toByteArray());
-        } catch (IOException ex) {
+        } catch (final IOException ex) {
             throw new TaskAssignmentException(""Failed to encode AssignmentInfo"", ex);
         }
     }
-    private void writeTopicPartitions(DataOutputStream out, Set<TopicPartition> partitions) throws IOException {
+    private void encodeVersionOne(final DataOutputStream out) throws IOException {
+        out.writeInt(1); // version
+        encodeActiveAndStandbyTaskAssignment(out);
+    }
+
+    private void encodeActiveAndStandbyTaskAssignment(final DataOutputStream out) throws IOException {
+        // encode active tasks
+        out.writeInt(activeTasks.size());
+        for (final TaskId id : activeTasks) {
+            id.writeTo(out);
+        }
+
+        // encode standby tasks
+        out.writeInt(standbyTasks.size());
+        for (final Map.Entry<TaskId, Set<TopicPartition>> entry : standbyTasks.entrySet()) {
+            final TaskId id = entry.getKey();
+            id.writeTo(out);
+
+            final Set<TopicPartition> partitions = entry.getValue();
+            writeTopicPartitions(out, partitions);
+        }
+    }
+
+    private void encodeVersionTwo(final DataOutputStream out) throws IOException {
+        out.writeInt(2); // version
+        encodeActiveAndStandbyTaskAssignment(out);
+        encodePartitionsByHost(out);
+    }
+
+    private void encodePartitionsByHost(final DataOutputStream out) throws IOException {
+        // encode partitions by host
+        out.writeInt(partitionsByHost.size());
+        for (final Map.Entry<HostInfo, Set<TopicPartition>> entry : partitionsByHost.entrySet()) {
+            final HostInfo hostInfo = entry.getKey();
+            out.writeUTF(hostInfo.host());
+            out.writeInt(hostInfo.port());
+            writeTopicPartitions(out, entry.getValue());
+        }
+    }
+
+    private void writeTopicPartitions(final DataOutputStream out,
+                                      final Set<TopicPartition> partitions) throws IOException {
         out.writeInt(partitions.size());
-        for (TopicPartition partition : partitions) {
+        for (final TopicPartition partition : partitions) {
             out.writeUTF(partition.topic());
             out.writeInt(partition.partition());
         }
@@ -117,52 +164,69 @@ private void writeTopicPartitions(DataOutputStream out, Set<TopicPartition> part
     /**
      * @throws TaskAssignmentException if method fails to decode the data or if the data version is unknown
      */
-    public static AssignmentInfo decode(ByteBuffer data) {
+    public static AssignmentInfo decode(final ByteBuffer data) {
         // ensure we are at the beginning of the ByteBuffer
         data.rewind();
-        try (DataInputStream in = new DataInputStream(new ByteBufferInputStream(data))) {
-            // Decode version
-            int version = in.readInt();
-            if (version < 0 || version > CURRENT_VERSION) {
-                TaskAssignmentException ex = new TaskAssignmentException(""Unknown assignment data version: "" + version);
-                log.error(ex.getMessage(), ex);
-                throw ex;
-            }
+        try (final DataInputStream in = new DataInputStream(new ByteBufferInputStream(data))) {
+            // decode used version
+            final int usedVersion = in.readInt();
+            final AssignmentInfo assignmentInfo = new AssignmentInfo(usedVersion);
-            // Decode active tasks
-            int count = in.readInt();
-            List<TaskId> activeTasks = new ArrayList<>(count);
-            for (int i = 0; i < count; i++) {
-                activeTasks.add(TaskId.readFrom(in));
-            }
-            // Decode standby tasks
-            count = in.readInt();
-            Map<TaskId, Set<TopicPartition>> standbyTasks = new HashMap<>(count);
-            for (int i = 0; i < count; i++) {
-                TaskId id = TaskId.readFrom(in);
-                standbyTasks.put(id, readTopicPartitions(in));
+            switch (usedVersion) {
+                case 1:
+                    decodeVersionOneData(assignmentInfo, in);
+                    break;
+                case 2:
+                    decodeVersionTwoData(assignmentInfo, in);
+                    break;
+                default:
+                    TaskAssignmentException fatalException = new TaskAssignmentException(""Unable to decode subscription data: "" +
+                        ""used version: "" + usedVersion + ""; latest supported version: "" + LATEST_SUPPORTED_VERSION);
+                    log.error(fatalException.getMessage(), fatalException);
+                    throw fatalException;
             }
-            Map<HostInfo, Set<TopicPartition>> hostStateToTopicPartitions = new HashMap<>();
-            if (version == CURRENT_VERSION) {
-                int numEntries = in.readInt();
-                for (int i = 0; i < numEntries; i++) {
-                    HostInfo hostInfo = new HostInfo(in.readUTF(), in.readInt());
-                    hostStateToTopicPartitions.put(hostInfo, readTopicPartitions(in));
-                }
-            }
+            return assignmentInfo;
+        } catch (final IOException ex) {
+            throw new TaskAssignmentException(""Failed to decode AssignmentInfo"", ex);
+        }
+    }
+
+    private static void decodeVersionOneData(final AssignmentInfo assignmentInfo,
+                                             final DataInputStream in) throws IOException {
+        // decode active tasks
+        int count = in.readInt();
+        assignmentInfo.activeTasks = new ArrayList<>(count);
+        for (int i = 0; i < count; i++) {
+            assignmentInfo.activeTasks.add(TaskId.readFrom(in));
+        }
-            return new AssignmentInfo(activeTasks, standbyTasks, hostStateToTopicPartitions);
+        // decode standby tasks
+        count = in.readInt();
+        assignmentInfo.standbyTasks = new HashMap<>(count);
+        for (int i = 0; i < count; i++) {
+            TaskId id = TaskId.readFrom(in);
+            assignmentInfo.standbyTasks.put(id, readTopicPartitions(in));
+        }
+    }
-        } catch (IOException ex) {
-            throw new TaskAssignmentException(""Failed to decode AssignmentInfo"", ex);
+    private static void decodeVersionTwoData(final AssignmentInfo assignmentInfo,
+                                             final DataInputStream in) throws IOException {
+        decodeVersionOneData(assignmentInfo, in);
+
+        // decode partitions by host
+        assignmentInfo.partitionsByHost = new HashMap<>();
+        final int numEntries = in.readInt();
+        for (int i = 0; i < numEntries; i++) {
+            final HostInfo hostInfo = new HostInfo(in.readUTF(), in.readInt());
+            assignmentInfo.partitionsByHost.put(hostInfo, readTopicPartitions(in));
         }
     }
-    private static Set<TopicPartition> readTopicPartitions(DataInputStream in) throws IOException {
-        int numPartitions = in.readInt();
-        Set<TopicPartition> partitions = new HashSet<>(numPartitions);
+    private static Set<TopicPartition> readTopicPartitions(final DataInputStream in) throws IOException {
+        final int numPartitions = in.readInt();
+        final Set<TopicPartition> partitions = new HashSet<>(numPartitions);
         for (int j = 0; j < numPartitions; j++) {
             partitions.add(new TopicPartition(in.readUTF(), in.readInt()));
         }
@@ -171,14 +235,14 @@ public static AssignmentInfo decode(ByteBuffer data) {
     @Override
     public int hashCode() {
-        return version ^ activeTasks.hashCode() ^ standbyTasks.hashCode() ^ partitionsByHost.hashCode();
+        return usedVersion ^ activeTasks.hashCode() ^ standbyTasks.hashCode() ^ partitionsByHost.hashCode();
     }
     @Override
-    public boolean equals(Object o) {
+    public boolean equals(final Object o) {
         if (o instanceof AssignmentInfo) {
-            AssignmentInfo other = (AssignmentInfo) o;
-            return this.version == other.version &&
+            final AssignmentInfo other = (AssignmentInfo) o;
+            return this.usedVersion == other.usedVersion &&
                     this.activeTasks.equals(other.activeTasks) &&
                     this.standbyTasks.equals(other.standbyTasks) &&
                     this.partitionsByHost.equals(other.partitionsByHost);
@@ -189,7 +253,7 @@ public boolean equals(Object o) {
     @Override
     public String toString() {
-        return """";
+        return """";
     }
 }
diff --git [file java] [file java]
index f583dbafc94..7fee90b5402 100644
--- [file java]
+++ [file java]
@@ -31,42 +31,96 @@
     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);
-    private static final int CURRENT_VERSION = 2;
+    public static final int LATEST_SUPPORTED_VERSION = 2;
-    public final int version;
-    public final UUID processId;
-    public final Set<TaskId> prevTasks;
-    public final Set<TaskId> standbyTasks;
-    public final String userEndPoint;
+    private final int usedVersion;
+    private UUID processId;
+    private Set<TaskId> prevTasks;
+    private Set<TaskId> standbyTasks;
+    private String userEndPoint;
-    public SubscriptionInfo(UUID processId, Set<TaskId> prevTasks, Set<TaskId> standbyTasks, String userEndPoint) {
-        this(CURRENT_VERSION, processId, prevTasks, standbyTasks, userEndPoint);
+    private SubscriptionInfo(final int version) {
+        this.usedVersion = version;
     }
-    private SubscriptionInfo(int version, UUID processId, Set<TaskId> prevTasks, Set<TaskId> standbyTasks, String userEndPoint) {
-        this.version = version;
+    public SubscriptionInfo(final UUID processId,
+                            final Set<TaskId> prevTasks,
+                            final Set<TaskId> standbyTasks,
+                            final String userEndPoint) {
+        this(LATEST_SUPPORTED_VERSION, processId, prevTasks, standbyTasks, userEndPoint);
+    }
+
+    public SubscriptionInfo(final int version,
+                            final UUID processId,
+                            final Set<TaskId> prevTasks,
+                            final Set<TaskId> standbyTasks,
+                            final String userEndPoint) {
+        this.usedVersion = version;
         this.processId = processId;
         this.prevTasks = prevTasks;
         this.standbyTasks = standbyTasks;
         this.userEndPoint = userEndPoint;
     }
+    public int version() {
+        return usedVersion;
+    }
+
+    public UUID processId() {
+        return processId;
+    }
+
+    public Set<TaskId> prevTasks() {
+        return prevTasks;
+    }
+
+    public Set<TaskId> standbyTasks() {
+        return standbyTasks;
+    }
+
+    public String userEndPoint() {
+        return userEndPoint;
+    }
+
     /**
      * @throws TaskAssignmentException if method fails to encode the data
      */
     public ByteBuffer encode() {
-        byte endPointBytes;
-        if (userEndPoint == null) {
-            endPointBytes = new byte;
-        } else {
-            endPointBytes = userEndPoint.getBytes(Charset.forName(""UTF-8""));
+        final ByteBuffer buf;
+
+        switch (usedVersion) {
+            case 1:
+                buf = encodeVersionOne();
+                break;
+            case 2:
+                buf = encodeVersionTwo(prepareUserEndPoint());
+                break;
+            default:
+                throw new IllegalStateException(""Unknown metadata version: "" + usedVersion
+                    + ""; latest supported version: "" + LATEST_SUPPORTED_VERSION);
         }
-        ByteBuffer buf = ByteBuffer.allocate(4 /* version */ + 16 /* process id */ + 4 +
-                prevTasks.size() * 8 + 4 + standbyTasks.size() * 8
-                + 4 /* length of bytes */ + endPointBytes.length
-        );
-        // version
-        buf.putInt(version);
+
+        buf.rewind();
+        return buf;
+    }
+
+    private ByteBuffer encodeVersionOne() {
+        final ByteBuffer buf = ByteBuffer.allocate(getVersionOneByteLength());
+
+        buf.putInt(1); // version
+        encodeVersionOneData(buf);
+
+        return buf;
+    }
+
+    private int getVersionOneByteLength() {
+        return 4 + // version
+               16 + // client ID
+               4 + prevTasks.size() * 8 + // length + prev tasks
+               4 + standbyTasks.size() * 8; // length + standby tasks
+    }
+
+    private void encodeVersionOneData(final ByteBuffer buf) {
         // encode client UUID
         buf.putLong(processId.getMostSignificantBits());
         buf.putLong(processId.getLeastSignificantBits());
@@ -80,60 +134,104 @@ public ByteBuffer encode() {
         for (TaskId id : standbyTasks) {
             id.writeTo(buf);
         }
-        buf.putInt(endPointBytes.length);
-        buf.put(endPointBytes);
-        buf.rewind();
+    }
+
+    private byte prepareUserEndPoint() {
+        if (userEndPoint == null) {
+            return new byte;
+        } else {
+            return userEndPoint.getBytes(Charset.forName(""UTF-8""));
+        }
+    }
+
+    private ByteBuffer encodeVersionTwo(final byte endPointBytes) {
+        final ByteBuffer buf = ByteBuffer.allocate(getVersionTwoByteLength(endPointBytes));
+
+        buf.putInt(2); // version
+        encodeVersionTwoData(buf, endPointBytes);
+
         return buf;
     }
+    private int getVersionTwoByteLength(final byte endPointBytes) {
+        return getVersionOneByteLength() +
+               4 + endPointBytes.length; // length + userEndPoint
+    }
+
+    private void encodeVersionTwoData(final ByteBuffer buf,
+                                      final byte endPointBytes) {
+        encodeVersionOneData(buf);
+        if (endPointBytes != null) {
+            buf.putInt(endPointBytes.length);
+            buf.put(endPointBytes);
+        }
+    }
+
     /**
      * @throws TaskAssignmentException if method fails to decode the data
      */
-    public static SubscriptionInfo decode(ByteBuffer data) {
+    public static SubscriptionInfo decode(final ByteBuffer data) {
         // ensure we are at the beginning of the ByteBuffer
         data.rewind();
-        // Decode version
-        int version = data.getInt();
-        if (version == CURRENT_VERSION || version == 1) {
-            // Decode client UUID
-            UUID processId = new UUID(data.getLong(), data.getLong());
-            // Decode previously active tasks
-            Set<TaskId> prevTasks = new HashSet<>();
-            int numPrevs = data.getInt();
-            for (int i = 0; i < numPrevs; i++) {
-                TaskId id = TaskId.readFrom(data);
-                prevTasks.add(id);
-            }
-            // Decode previously cached tasks
-            Set<TaskId> standbyTasks = new HashSet<>();
-            int numCached = data.getInt();
-            for (int i = 0; i < numCached; i++) {
-                standbyTasks.add(TaskId.readFrom(data));
-            }
-
-            String userEndPoint = null;
-            if (version == CURRENT_VERSION) {
-                int bytesLength = data.getInt();
-                if (bytesLength != 0) {
-                    byte bytes = new byte;
-                    data.get(bytes);
-                    userEndPoint = new String(bytes, Charset.forName(""UTF-8""));
-                }
-
-            }
-            return new SubscriptionInfo(version, processId, prevTasks, standbyTasks, userEndPoint);
+        // decode used version
+        final int usedVersion = data.getInt();
+        final SubscriptionInfo subscriptionInfo = new SubscriptionInfo(usedVersion);
+
+        switch (usedVersion) {
+            case 1:
+                decodeVersionOneData(subscriptionInfo, data);
+                break;
+            case 2:
+                decodeVersionTwoData(subscriptionInfo, data);
+                break;
+            default:
+                TaskAssignmentException fatalException = new TaskAssignmentException(""Unable to decode subscription data: "" +
+                    ""used version: "" + usedVersion + ""; latest supported version: "" + LATEST_SUPPORTED_VERSION);
+                log.error(fatalException.getMessage(), fatalException);
+                throw fatalException;
+        }
+
+        return subscriptionInfo;
+    }
-        } else {
-            TaskAssignmentException ex = new TaskAssignmentException(""unable to decode subscription data: version="" + version);
-            log.error(ex.getMessage(), ex);
-            throw ex;
+    private static void decodeVersionOneData(final SubscriptionInfo subscriptionInfo,
+                                             final ByteBuffer data) {
+        // decode client UUID
+        subscriptionInfo.processId = new UUID(data.getLong(), data.getLong());
+
+        // decode previously active tasks
+        final int numPrevs = data.getInt();
+        subscriptionInfo.prevTasks = new HashSet<>();
+        for (int i = 0; i < numPrevs; i++) {
+            TaskId id = TaskId.readFrom(data);
+            subscriptionInfo.prevTasks.add(id);
+        }
+
+        // decode previously cached tasks
+        final int numCached = data.getInt();
+        subscriptionInfo.standbyTasks = new HashSet<>();
+        for (int i = 0; i < numCached; i++) {
+            subscriptionInfo.standbyTasks.add(TaskId.readFrom(data));
+        }
+    }
+
+    private static void decodeVersionTwoData(final SubscriptionInfo subscriptionInfo,
+                                             final ByteBuffer data) {
+        decodeVersionOneData(subscriptionInfo, data);
+
+        // decode user end point (can be null)
+        int bytesLength = data.getInt();
+        if (bytesLength != 0) {
+            final byte bytes = new byte;
+            data.get(bytes);
+            subscriptionInfo.userEndPoint = new String(bytes, Charset.forName(""UTF-8""));
         }
     }
     @Override
     public int hashCode() {
-        int hashCode = version ^ processId.hashCode() ^ prevTasks.hashCode() ^ standbyTasks.hashCode();
+        final int hashCode = usedVersion ^ processId.hashCode() ^ prevTasks.hashCode() ^ standbyTasks.hashCode();
         if (userEndPoint == null) {
             return hashCode;
         }
@@ -141,10 +239,10 @@ public int hashCode() {
     }
     @Override
-    public boolean equals(Object o) {
+    public boolean equals(final Object o) {
         if (o instanceof SubscriptionInfo) {
-            SubscriptionInfo other = (SubscriptionInfo) o;
-            return this.version == other.version &&
+            final SubscriptionInfo other = (SubscriptionInfo) o;
+            return this.usedVersion == other.usedVersion &&
                     this.processId.equals(other.processId) &&
                     this.prevTasks.equals(other.prevTasks) &&
                     this.standbyTasks.equals(other.standbyTasks) &&
diff --git [file java] [file java]
index 8b4e8957ed5..bb06c72d080 100644
--- [file java]
+++ [file java]
@@ -376,7 +376,6 @@ public boolean conditionMet() {
         }
     }
-
     @Test
     public void queryOnRebalance() throws InterruptedException {
         final int numThreads = STREAM_TWO_PARTITIONS;
diff --git [file java] [file java]
index bf3f1d1ac5e..b0c0d68287b 100644
--- [file java]
+++ [file java]
@@ -239,17 +239,17 @@ public void testAssignBasic() throws Exception {
         // the first consumer
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
+        allActiveTasks.addAll(info10.activeTasks());
         // the second consumer
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(""consumer11""));
-        allActiveTasks.addAll(info11.activeTasks);
+        allActiveTasks.addAll(info11.activeTasks());
         assertEquals(Utils.mkSet(task0, task1), allActiveTasks);
         // the third consumer
         AssignmentInfo info20 = checkAssignment(allTopics, assignments.get(""consumer20""));
-        allActiveTasks.addAll(info20.activeTasks);
+        allActiveTasks.addAll(info20.activeTasks());
         assertEquals(3, allActiveTasks.size());
         assertEquals(allTasks, new HashSet<>(allActiveTasks));
@@ -317,13 +317,13 @@ public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() throws E
         final AssignmentInfo info10 = AssignmentInfo.decode(assignments.get(""consumer10"").userData());
         final List<TaskId> expectedInfo10TaskIds = Arrays.asList(taskIdA1, taskIdA3, taskIdB1, taskIdB3);
-        assertEquals(expectedInfo10TaskIds, info10.activeTasks);
+        assertEquals(expectedInfo10TaskIds, info10.activeTasks());
         // the second consumer
         final AssignmentInfo info11 = AssignmentInfo.decode(assignments.get(""consumer11"").userData());
         final List<TaskId> expectedInfo11TaskIds = Arrays.asList(taskIdA0, taskIdA2, taskIdB0, taskIdB2);
-        assertEquals(expectedInfo11TaskIds, info11.activeTasks);
+        assertEquals(expectedInfo11TaskIds, info11.activeTasks());
     }
     @Test
@@ -354,7 +354,7 @@ public void testAssignWithPartialTopology() throws Exception {
         // check assignment info
         Set<TaskId> allActiveTasks = new HashSet<>();
         AssignmentInfo info10 = checkAssignment(Utils.mkSet(""topic1""), assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
+        allActiveTasks.addAll(info10.activeTasks());
         assertEquals(3, allActiveTasks.size());
         assertEquals(allTasks, new HashSet<>(allActiveTasks));
@@ -394,7 +394,7 @@ public void testAssignEmptyMetadata() throws Exception {
         // check assignment info
         Set<TaskId> allActiveTasks = new HashSet<>();
         AssignmentInfo info10 = checkAssignment(Collections.<String>emptySet(), assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
+        allActiveTasks.addAll(info10.activeTasks());
         assertEquals(0, allActiveTasks.size());
         assertEquals(Collections.<TaskId>emptySet(), new HashSet<>(allActiveTasks));
@@ -407,7 +407,7 @@ public void testAssignEmptyMetadata() throws Exception {
         // the first consumer
         info10 = checkAssignment(allTopics, assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
+        allActiveTasks.addAll(info10.activeTasks());
         assertEquals(3, allActiveTasks.size());
         assertEquals(allTasks, new HashSet<>(allActiveTasks));
@@ -455,15 +455,15 @@ public void testAssignWithNewTasks() throws Exception {
         AssignmentInfo info;
         info = AssignmentInfo.decode(assignments.get(""consumer10"").userData());
-        allActiveTasks.addAll(info.activeTasks);
+        allActiveTasks.addAll(info.activeTasks());
         allPartitions.addAll(assignments.get(""consumer10"").partitions());
         info = AssignmentInfo.decode(assignments.get(""consumer11"").userData());
-        allActiveTasks.addAll(info.activeTasks);
+        allActiveTasks.addAll(info.activeTasks());
         allPartitions.addAll(assignments.get(""consumer11"").partitions());
         info = AssignmentInfo.decode(assignments.get(""consumer20"").userData());
-        allActiveTasks.addAll(info.activeTasks);
+        allActiveTasks.addAll(info.activeTasks());
         allPartitions.addAll(assignments.get(""consumer20"").partitions());
         assertEquals(allTasks, allActiveTasks);
@@ -524,14 +524,14 @@ public void testAssignWithStates() throws Exception {
         AssignmentInfo info11 = AssignmentInfo.decode(assignments.get(""consumer11"").userData());
         AssignmentInfo info20 = AssignmentInfo.decode(assignments.get(""consumer20"").userData());
-        assertEquals(2, info10.activeTasks.size());
-        assertEquals(2, info11.activeTasks.size());
-        assertEquals(2, info20.activeTasks.size());
+        assertEquals(2, info10.activeTasks().size());
+        assertEquals(2, info11.activeTasks().size());
+        assertEquals(2, info20.activeTasks().size());
         Set<TaskId> allTasks = new HashSet<>();
-        allTasks.addAll(info10.activeTasks);
-        allTasks.addAll(info11.activeTasks);
-        allTasks.addAll(info20.activeTasks);
+        allTasks.addAll(info10.activeTasks());
+        allTasks.addAll(info11.activeTasks());
+        allTasks.addAll(info20.activeTasks());
         assertEquals(new HashSet<>(tasks), allTasks);
         // check tasks for state topics
@@ -603,15 +603,15 @@ public void testAssignWithStandbyReplicas() throws Exception {
         // the first consumer
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
-        allStandbyTasks.addAll(info10.standbyTasks.keySet());
+        allActiveTasks.addAll(info10.activeTasks());
+        allStandbyTasks.addAll(info10.standbyTasks().keySet());
         // the second consumer
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(""consumer11""));
-        allActiveTasks.addAll(info11.activeTasks);
-        allStandbyTasks.addAll(info11.standbyTasks.keySet());
+        allActiveTasks.addAll(info11.activeTasks());
+        allStandbyTasks.addAll(info11.standbyTasks().keySet());
-        assertNotEquals(""same processId has same set of standby tasks"", info11.standbyTasks.keySet(), info10.standbyTasks.keySet());
+        assertNotEquals(""same processId has same set of standby tasks"", info11.standbyTasks().keySet(), info10.standbyTasks().keySet());
         // check active tasks assigned to the first client
         assertEquals(Utils.mkSet(task0, task1), new HashSet<>(allActiveTasks));
@@ -619,8 +619,8 @@ public void testAssignWithStandbyReplicas() throws Exception {
         // the third consumer
         AssignmentInfo info20 = checkAssignment(allTopics, assignments.get(""consumer20""));
-        allActiveTasks.addAll(info20.activeTasks);
-        allStandbyTasks.addAll(info20.standbyTasks.keySet());
+        allActiveTasks.addAll(info20.activeTasks());
+        allStandbyTasks.addAll(info20.standbyTasks().keySet());
         // all task ids are in the active tasks and also in the standby tasks
@@ -847,7 +847,7 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {
         configurePartitionAssignor(Collections.singletonMap(StreamsConfig.APPLICATION_SERVER_CONFIG, (Object) userEndPoint));
         final PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""input""));
         final SubscriptionInfo subscriptionInfo = SubscriptionInfo.decode(subscription.userData());
-        assertEquals(""localhost:8080"", subscriptionInfo.userEndPoint);
+        assertEquals(""localhost:8080"", subscriptionInfo.userEndPoint());
     }
     @Test
@@ -874,7 +874,7 @@ public void shouldMapUserEndPointToTopicPartitions() throws Exception {
         final Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
         final PartitionAssignor.Assignment consumerAssignment = assignments.get(""consumer1"");
         final AssignmentInfo assignmentInfo = AssignmentInfo.decode(consumerAssignment.userData());
-        final Set<TopicPartition> topicPartitions = assignmentInfo.partitionsByHost.get(new HostInfo(""localhost"", 8080));
+        final Set<TopicPartition> topicPartitions = assignmentInfo.partitionsByHost().get(new HostInfo(""localhost"", 8080));
         assertEquals(Utils.mkSet(new TopicPartition(""topic1"", 0),
                 new TopicPartition(""topic1"", 1),
                 new TopicPartition(""topic1"", 2)), topicPartitions);
@@ -1072,8 +1072,8 @@ public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() throws Except
         final Map<String, PartitionAssignor.Assignment> assign = partitionAssignor.assign(metadata, subscriptions);
         final PartitionAssignor.Assignment consumer1Assignment = assign.get(""consumer1"");
         final AssignmentInfo assignmentInfo = AssignmentInfo.decode(consumer1Assignment.userData());
-        final Set<TopicPartition> consumer1partitions = assignmentInfo.partitionsByHost.get(new HostInfo(""localhost"", 8080));
-        final Set<TopicPartition> consumer2Partitions = assignmentInfo.partitionsByHost.get(new HostInfo(""other"", 9090));
+        final Set<TopicPartition> consumer1partitions = assignmentInfo.partitionsByHost().get(new HostInfo(""localhost"", 8080));
+        final Set<TopicPartition> consumer2Partitions = assignmentInfo.partitionsByHost().get(new HostInfo(""other"", 9090));
         final HashSet<TopicPartition> allAssignedPartitions = new HashSet<>(consumer1partitions);
         allAssignedPartitions.addAll(consumer2Partitions);
         assertThat(consumer1partitions, not(allPartitions));
@@ -1095,6 +1095,37 @@ public void shouldThrowKafkaExceptionIfStreamThreadConfigIsNotThreadDataProvider
         partitionAssignor.configure(config);
     }
+    @Test
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() throws Exception {
+        final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
+        final Set<TaskId> emptyTasks = Collections.emptySet();
+        subscriptions.put(
+            ""consumer1"",
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(""topic1""),
+                new SubscriptionInfo(1, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+        subscriptions.put(
+            ""consumer2"",
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(""topic1""),
+                new SubscriptionInfo(2, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+
+        mockTaskManager(Collections.<TaskId>emptySet(),
+            Collections.<TaskId>emptySet(),
+            UUID.randomUUID(),
+            new InternalTopologyBuilder());
+        partitionAssignor.configure(configProps());
+        final Map<String, PartitionAssignor.Assignment> assignment = partitionAssignor.assign(metadata, subscriptions);
+
+        assertThat(assignment.size(), equalTo(2));
+        assertThat(AssignmentInfo.decode(assignment.get(""consumer1"").userData()).version(), equalTo(1));
+        assertThat(AssignmentInfo.decode(assignment.get(""consumer2"").userData()).version(), equalTo(1));
+    }
+
     private PartitionAssignor.Assignment createAssignment(final Map<HostInfo, Set<TopicPartition>> firstHostState) {
         final AssignmentInfo info = new AssignmentInfo(Collections.<TaskId>emptyList(),
                                                        Collections.<TaskId, Set<TopicPartition>>emptyMap(),
@@ -1111,7 +1142,7 @@ private AssignmentInfo checkAssignment(Set<String> expectedTopics, PartitionAssi
         AssignmentInfo info = AssignmentInfo.decode(assignment.userData());
         // check if the number of assigned partitions == the size of active task id list
-        assertEquals(assignment.partitions().size(), info.activeTasks.size());
+        assertEquals(assignment.partitions().size(), info.activeTasks().size());
         // check if active tasks are consistent
         List<TaskId> activeTasks = new ArrayList<>();
@@ -1121,14 +1152,14 @@ private AssignmentInfo checkAssignment(Set<String> expectedTopics, PartitionAssi
             activeTasks.add(new TaskId(0, partition.partition()));
             activeTopics.add(partition.topic());
         }
-        assertEquals(activeTasks, info.activeTasks);
+        assertEquals(activeTasks, info.activeTasks());
         // check if active partitions cover all topics
         assertEquals(expectedTopics, activeTopics);
         // check if standby tasks are consistent
         Set<String> standbyTopics = new HashSet<>();
-        for (Map.Entry<TaskId, Set<TopicPartition>> entry : info.standbyTasks.entrySet()) {
+        for (Map.Entry<TaskId, Set<TopicPartition>> entry : info.standbyTasks().entrySet()) {
             TaskId id = entry.getKey();
             Set<TopicPartition> partitions = entry.getValue();
             for (TopicPartition partition : partitions) {
@@ -1139,7 +1170,7 @@ private AssignmentInfo checkAssignment(Set<String> expectedTopics, PartitionAssi
             }
         }
-        if (info.standbyTasks.size() > 0) {
+        if (info.standbyTasks().size() > 0) {
             // check if standby partitions cover all topics
             assertEquals(expectedTopics, standbyTopics);
         }
diff --git [file java] [file java]
index ec94ad81acd..726a5623cd5 100644
--- [file java]
+++ [file java]
@@ -33,6 +33,7 @@
 import java.util.Set;
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNull;
 public class AssignmentInfoTest {
@@ -61,10 +62,10 @@ public void shouldDecodePreviousVersion() throws IOException {
         standbyTasks.put(new TaskId(2, 0), Utils.mkSet(new TopicPartition(""t3"", 0), new TopicPartition(""t3"", 0)));
         final AssignmentInfo oldVersion = new AssignmentInfo(1, activeTasks, standbyTasks, null);
         final AssignmentInfo decoded = AssignmentInfo.decode(encodeV1(oldVersion));
-        assertEquals(oldVersion.activeTasks, decoded.activeTasks);
-        assertEquals(oldVersion.standbyTasks, decoded.standbyTasks);
-        assertEquals(0, decoded.partitionsByHost.size()); // should be empty as wasn't in V1
-        assertEquals(2, decoded.version); // automatically upgraded to v2 on decode;
+        assertEquals(oldVersion.activeTasks(), decoded.activeTasks());
+        assertEquals(oldVersion.standbyTasks(), decoded.standbyTasks());
+        assertNull(decoded.partitionsByHost()); // should be null as wasn't in V1
+        assertEquals(1, decoded.version());
     }
@@ -76,15 +77,15 @@ private ByteBuffer encodeV1(AssignmentInfo oldVersion) throws IOException {
         ByteArrayOutputStream baos = new ByteArrayOutputStream();
         DataOutputStream out = new DataOutputStream(baos);
         // Encode version
-        out.writeInt(oldVersion.version);
+        out.writeInt(oldVersion.version());
         // Encode active tasks
-        out.writeInt(oldVersion.activeTasks.size());
-        for (TaskId id : oldVersion.activeTasks) {
+        out.writeInt(oldVersion.activeTasks().size());
+        for (TaskId id : oldVersion.activeTasks()) {
             id.writeTo(out);
         }
         // Encode standby tasks
-        out.writeInt(oldVersion.standbyTasks.size());
-        for (Map.Entry<TaskId, Set<TopicPartition>> entry : oldVersion.standbyTasks.entrySet()) {
+        out.writeInt(oldVersion.standbyTasks().size());
+        for (Map.Entry<TaskId, Set<TopicPartition>> entry : oldVersion.standbyTasks().entrySet()) {
             TaskId id = entry.getKey();
             id.writeTo(out);
diff --git [file java] [file java]
index 9c011bb0cae..633285a2b4d 100644
--- [file java]
+++ [file java]
@@ -65,14 +65,12 @@ public void shouldBeBackwardCompatible() {
         final ByteBuffer v1Encoding = encodePreviousVersion(processId, activeTasks, standbyTasks);
         final SubscriptionInfo decode = SubscriptionInfo.decode(v1Encoding);
-        assertEquals(activeTasks, decode.prevTasks);
-        assertEquals(standbyTasks, decode.standbyTasks);
-        assertEquals(processId, decode.processId);
-        assertNull(decode.userEndPoint);
-
+        assertEquals(activeTasks, decode.prevTasks());
+        assertEquals(standbyTasks, decode.standbyTasks());
+        assertEquals(processId, decode.processId());
+        assertNull(decode.userEndPoint());
     }
-
     /**
      * This is a clone of what the V1 encoding did. The encode method has changed for V2
      * so it is impossible to test compatibility without having this
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 6 **
mjsax opened a new pull request #4746: KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 7 **
mjsax opened a new pull request #4758: KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 8 **
mjsax opened a new pull request #4761:  KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 9 **
mjsax opened a new pull request #4768:  KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 10 **
mjsax opened a new pull request #4773:  KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 11 **
mjsax closed pull request #4758: KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git a/bin/kafka-run-class.sh b/bin/kafka-run-class.sh
index af10f61b5c4..a25868125e9 100755
--- a/bin/kafka-run-class.sh
+++ b/bin/kafka-run-class.sh
@@ -73,28 +73,50 @@ do
   fi
 done
-for file in ""$base_dir""/clients/build/libs/kafka-clients*.jar;
-do
-  if should_include_file ""$file""; then
-    CLASSPATH=""$CLASSPATH"":""$file""
-  fi
-done
+if ; then
+  clients_lib_dir=$(dirname $0)/../clients/build/libs
+  streams_lib_dir=$(dirname $0)/../streams/build/libs
+  rocksdb_lib_dir=$(dirname $0)/../streams/build/dependant-libs-${SCALA_VERSION}
+else
+  clients_lib_dir=/opt/kafka-$UPGRADE_KAFKA_STREAMS_TEST_VERSION/libs
+  streams_lib_dir=$clients_lib_dir
+  rocksdb_lib_dir=$streams_lib_dir
+fi
+
-for file in ""$base_dir""/streams/build/libs/kafka-streams*.jar;
+for file in ""$clients_lib_dir""/kafka-clients*.jar;
 do
   if should_include_file ""$file""; then
     CLASSPATH=""$CLASSPATH"":""$file""
   fi
 done
-for file in ""$base_dir""/streams/examples/build/libs/kafka-streams-examples*.jar;
+for file in ""$streams_lib_dir""/kafka-streams*.jar;
 do
   if should_include_file ""$file""; then
     CLASSPATH=""$CLASSPATH"":""$file""
   fi
 done
-for file in ""$base_dir""/streams/build/dependant-libs-${SCALA_VERSION}/rocksdb*.jar;
+if ; then
+  for file in ""$base_dir""/streams/examples/build/libs/kafka-streams-examples*.jar;
+  do
+    if should_include_file ""$file""; then
+      CLASSPATH=""$CLASSPATH"":""$file""
+    fi
+  done
+else
+  VERSION_NO_DOTS=`echo $UPGRADE_KAFKA_STREAMS_TEST_VERSION | sed 's/\.//g'`
+  SHORT_VERSION_NO_DOTS=${VERSION_NO_DOTS:0:((${#VERSION_NO_DOTS} - 1))} # remove last char, ie, bug-fix number
+  for file in ""$base_dir""/streams/upgrade-system-tests-$SHORT_VERSION_NO_DOTS/build/libs/kafka-streams-upgrade-system-tests*.jar;
+  do
+    if should_include_file ""$file""; then
+      CLASSPATH=""$CLASSPATH"":""$file""
+    fi
+  done
+fi
+
+for file in ""$rocksdb_lib_dir""/rocksdb*.jar;
 do
   CLASSPATH=""$CLASSPATH"":""$file""
 done
diff --git a/build.gradle b/build.gradle
index 20a184c437c..5e97f901cb6 100644
--- a/build.gradle
+++ b/build.gradle
@@ -770,6 +770,30 @@ project(':streams:examples') {
   }
 }
+project(':streams:upgrade-system-tests-0100') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0100""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0100
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
+project(':streams:upgrade-system-tests-0101') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0101""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0101
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
 project(':log4j-appender') {
   archivesBaseName = ""kafka-log4j-appender""
diff --git [file java] [file java]
index 6094b547bb7..b80dfccf3d9 100644
--- [file java]
+++ [file java]
@@ -17,7 +17,9 @@
  */
 package org.apache.kafka.common.security.authenticator;
-import java.util.Map;
+import org.apache.kafka.common.config.SaslConfigs;
+import org.apache.kafka.common.network.Mode;
+import org.apache.kafka.common.security.auth.AuthCallbackHandler;
 import javax.security.auth.Subject;
 import javax.security.auth.callback.Callback;
@@ -26,10 +28,7 @@
 import javax.security.auth.callback.UnsupportedCallbackException;
 import javax.security.sasl.AuthorizeCallback;
 import javax.security.sasl.RealmCallback;
-
-import org.apache.kafka.common.config.SaslConfigs;
-import org.apache.kafka.common.network.Mode;
-import org.apache.kafka.common.security.auth.AuthCallbackHandler;
+import java.util.Map;
 /**
  * Callback handler for Sasl clients. The callbacks required for the SASL mechanism
diff --git a/docs/streams.html b/docs/streams.html
index fe0e84ee3b7..d691e63a432 100644
--- a/docs/streams.html
+++ b/docs/streams.html
@@ -807,21 +807,50 @@ <h2><a id=""streams_upgrade_and_api"" href=""#streams_upgrade_and_api"">Upgrade Guid
         See <a href=""#streams_api_changes_0102"">below</a> a complete list of 0.10.2 API and semantical changes that allow you to advance your application and/or simplify your code base, including the usage of new features.
         </p>
+        <p>
+        Upgrading from 0.10.0.x to 0.10.2.x directly is also possible.
+        See <a href=""#streams_api_changes_0102"">Streams API changes in 0.10.2</a> and <a href=""#streams_api_changes_0101"">Streams API changes in 0.10.1</a>
+        for a complete list of API changes.
+        Upgrading to 0.10.2.2 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+        (cf. <a href=""[link]"">KIP-268</a>).
+        As an alternative, and offline upgrade is also possible.
+        </p>
+        <ul>
+            <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from=</code> is set to <code>""0.10.0""</code> for new version 0.10.2.2 </li>
+            <li> bounce each instance of your application once </li>
+            <li> prepare your newly deployed 0.10.2.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+            <li> bounce each instance of your application once more to complete the upgrade </li>
+        </ul>
+        <p> Upgrading from 0.10.0.x to 0.10.2.0 or 0.10.2.1 requires an offline upgrade (rolling bounce upgrade is not supported) </p>
+        <ul>
+            <li> stop all old (0.10.0.x) application instances </li>
+            <li> update your code and swap old code and jar file with new code and new jar file </li>
+            <li> restart all new (0.10.2.0 or 0.10.2.1) application instances </li>
+        </ul>
+
         <p>
         If you want to upgrade from 0.10.0.x to 0.10.1, see the <a href=""/{{version}}/documentation/#upgrade_1010_streams"">Upgrade Section for 0.10.1</a>.
         It highlights incompatible changes you need to consider to upgrade your code and application.
         See <a href=""#streams_api_changes_0101"">below</a> a complete list of 0.10.1 API changes that allow you to advance your application and/or simplify your code base, including the usage of new features.
         </p>
-         <h3><a id=""streams_api_changes_01021"" href=""#streams_api_changes_0102"">Notable changes in 0.10.2.1</a></h3>
-         <p>
+        <h3><a id=""streams_api_changes_01022"" href=""#streams_api_changes_0102"">Notable changes in 0.10.2.2</a></h3>
+        <p>
+            Parameter updates in <code>StreamsConfig</code>:
+        </p>
+        <ul>
+            <li> New configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x </li>
+        </ul>
+
+        <h3><a id=""streams_api_changes_01021"" href=""#streams_api_changes_0102"">Notable changes in 0.10.2.1</a></h3>
+        <p>
             Parameter updates in <code>StreamsConfig</code>:
         </p>
-          <ul>
+        <ul>
             <li> of particular importance to improve the resiliency of a Kafka Streams application are two changes to default parameters of producer <code>retries</code> and consumer <code>max.poll.interval.ms</code> </li>
-          </ul>
-        <h3><a id=""streams_api_changes_0102"" href=""#streams_api_changes_0102"">Streams API changes in 0.10.2.0</a></h3>
+        </ul>
+        <h3><a id=""streams_api_changes_0102"" href=""#streams_api_changes_0102"">Streams API changes in 0.10.2.0</a></h3>
         <p>
             New methods in <code>KafkaStreams</code>:
         </p>
diff --git a/docs/upgrade.html b/docs/upgrade.html
index d7581fa8dac..77477628f4d 100644
--- a/docs/upgrade.html
+++ b/docs/upgrade.html
@@ -61,6 +61,11 @@ <h5><a id=""upgrade_1020_streams"" href=""#upgrade_1020_streams"">Upgrading a 0.10.1
     <li> See <a href=""/{{version}}/documentation/streams#streams_api_changes_0102"">Streams API changes in 0.10.2</a> for more details. </li>
 </ul>
+<h5><a id=""upgrade_10202_notable"" href=""#upgrade_10202_notable"">Notable changes in 0.10.2.2</a></h5>
+<ul>
+    <li> New configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x </li>
+</ul>
+
 <h5><a id=""upgrade_10201_notable"" href=""#upgrade_10201_notable"">Notable changes in 0.10.2.1</a></h5>
 <ul>
   <li> The default values for two configurations of the StreamsConfig class were changed to improve the resiliency of Kafka Streams applications. The internal Kafka Streams producer <code>retries</code> default value was changed from 0 to 10. The internal Kafka Streams consumer <code>max.poll.interval.ms</code>  default value was changed from 300000 to <code>Integer.MAX_VALUE</code>.
@@ -141,6 +146,23 @@ <h5><a id=""upgrade_1010_streams"" href=""#upgrade_1010_streams"">Upgrading a 0.10.0
     <li> Upgrading your Streams application from 0.10.0 to 0.10.1 does require a <a href=""#upgrade_10_1"">broker upgrade</a> because a Kafka Streams 0.10.1 application can only connect to 0.10.1 brokers. </li>
     <li> There are couple of API changes, that are not backward compatible (cf. <a href=""/{{version}}/documentation/streams#streams_api_changes_0101"">Streams API changes in 0.10.1</a> for more details).
          Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> Upgrading from 0.10.0.x to 0.10.1.2 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+         (cf. <a href=""[link]"">KIP-268</a>).
+         As an alternative, and offline upgrade is also possible.
+        <ul>
+            <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.10.1.2 </li>
+            <li> bounce each instance of your application once </li>
+            <li> prepare your newly deployed 0.10.1.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+            <li> bounce each instance of your application once more to complete the upgrade </li>
+        </ul>
+    </li>
+    <li> Upgrading from 0.10.0.x to 0.10.1.0 or 0.10.1.1 requires an offline upgrade (rolling bounce upgrade is not supported)
+        <ul>
+            <li> stop all old (0.10.0.x) application instances </li>
+            <li> update your code and swap old code and jar file with new code and new jar file </li>
+            <li> restart all new (0.10.1.0 or 0.10.1.1) application instances </li>
+        </ul>
+    </li>
 </ul>
 <h5><a id=""upgrade_1010_notable"" href=""#upgrade_1010_notable"">Notable changes in 0.10.1.0</a></h5>
diff --git a/gradle/dependencies.gradle b/gradle/dependencies.gradle
index 25faa90a78f..4084b12216d 100644
--- a/gradle/dependencies.gradle
+++ b/gradle/dependencies.gradle
@@ -31,6 +31,8 @@ versions += [
   jackson: ""2.8.5"",
   jetty: ""9.2.22.v20170606"",
   jersey: ""2.24"",
+  kafka_0100: ""0.10.0.1"",
+  kafka_0101: ""0.10.1.1"",
   log4j: ""1.2.17"",
   jopt: ""5.0.3"",
   junit: ""4.12"",
@@ -92,6 +94,8 @@ libs += [
   junit: ""junit:junit:$versions.junit"",
   log4j: ""log4j:log4j:$versions.log4j"",
   joptSimple: ""net.sf.jopt-simple:jopt-simple:$versions.jopt"",
+  kafkaStreams_0100: ""org.apache.kafka:kafka-streams:$versions.kafka_0100"",
+  kafkaStreams_0101: ""org.apache.kafka:kafka-streams:$versions.kafka_0101"",
   lz4: ""net.jpountz.lz4:lz4:$versions.lz4"",
   metrics: ""com.yammer.metrics:metrics-core:$versions.metrics"",
   powermock: ""org.powermock:powermock-module-junit4:$versions.powermock"",
diff --git a/settings.gradle b/settings.gradle
index 29d38950a8a..576b40b9ce1 100644
--- a/settings.gradle
+++ b/settings.gradle
@@ -13,5 +13,6 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
-include 'core', 'examples', 'clients', 'tools', 'streams', 'streams:examples', 'log4j-appender',
+include 'core', 'examples', 'clients', 'tools', 'streams', 'streams:examples', 'streams:upgrade-system-tests-0100',
+        'streams:upgrade-system-tests-0101', 'log4j-appender',
         'connect:api', 'connect:transforms', 'connect:runtime', 'connect:json', 'connect:file'
diff --git [file java] [file java]
index 0724571a0bc..3baa0785376 100644
--- [file java]
+++ [file java]
@@ -95,6 +95,16 @@
      */
     public static final String PRODUCER_PREFIX = ""producer."";
+    /**
+     * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 0.10.0.x}.
+     */
+    public static final String UPGRADE_FROM_0100 = ""0.10.0"";
+
+    /** {@code upgrade.from} */
+    public static final String UPGRADE_FROM_CONFIG = ""upgrade.from"";
+    public static final String UPGRADE_FROM_DOC = ""Allows upgrading from version 0.10.0 to version 0.10.1 (or newer) in a backward compatible way. "" +
+        ""Default is null. Accepted values are \"""" + UPGRADE_FROM_0100 + ""\"" (for upgrading from 0.10.0.x)."";
+
     /** {@code state.dir} */
     public static final String STATE_DIR_CONFIG = ""state.dir"";
     private static final String STATE_DIR_DOC = ""Directory location for state store."";
@@ -383,7 +393,13 @@
                     40 * 1000,
                     atLeast(0),
                     ConfigDef.Importance.MEDIUM,
-                    REQUEST_TIMEOUT_MS_DOC);
+                    REQUEST_TIMEOUT_MS_DOC)
+            .define(UPGRADE_FROM_CONFIG,
+                    ConfigDef.Type.STRING,
+                    null,
+                    in(null, UPGRADE_FROM_0100),
+                    ConfigDef.Importance.LOW,
+                    UPGRADE_FROM_DOC);
     }
     // this is the list of configs for underlying clients
@@ -501,6 +517,7 @@ public StreamsConfig(final Map<?, ?> props) {
         consumerProps.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId + ""-consumer"");
         // add configs required for stream partition assignor
+        consumerProps.put(UPGRADE_FROM_CONFIG, getString(UPGRADE_FROM_CONFIG));
         consumerProps.put(InternalConfig.STREAM_THREAD_INSTANCE, streamThread);
         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));
         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));
diff --git [file java] [file java]
index a50a81914cf..889d2ff6a32 100644
--- [file java]
+++ [file java]
@@ -14,7 +14,6 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 package org.apache.kafka.streams.processor.internals;
 import org.apache.kafka.clients.consumer.internals.PartitionAssignor;
@@ -155,6 +154,8 @@ public int compare(TopicPartition p1, TopicPartition p2) {
     private String userEndPoint;
     private int numStandbyReplicas;
+    private int userMetadataVersion = SubscriptionInfo.CURRENT_VERSION;
+
     private Cluster metadataWithInternalTopics;
     private Map<HostInfo, Set<TopicPartition>> partitionsByHostState;
@@ -182,6 +183,12 @@ void time(final Time time) {
     public void configure(Map<String, ?> configs) {
         numStandbyReplicas = (Integer) configs.get(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG);
+        final String upgradeMode = (String) configs.get(StreamsConfig.UPGRADE_FROM_CONFIG);
+        if (StreamsConfig.UPGRADE_FROM_0100.equals(upgradeMode)) {
+            log.info(""Downgrading metadata version from 2 to 1 for upgrade from 0.10.0.x."");
+            userMetadataVersion = 1;
+        }
+
         Object o = configs.get(StreamsConfig.InternalConfig.STREAM_THREAD_INSTANCE);
         if (o == null) {
             KafkaException ex = new KafkaException(""StreamThread is not specified"");
@@ -241,7 +248,7 @@ public Subscription subscription(Set<String> topics) {
         Set<TaskId> prevTasks = streamThread.prevTasks();
         Set<TaskId> standbyTasks = streamThread.cachedTasks();
         standbyTasks.removeAll(prevTasks);
-        SubscriptionInfo data = new SubscriptionInfo(streamThread.processId, prevTasks, standbyTasks, this.userEndPoint);
+        SubscriptionInfo data = new SubscriptionInfo(userMetadataVersion, streamThread.processId, prevTasks, standbyTasks, this.userEndPoint);
         if (streamThread.builder.sourceTopicPattern() != null) {
             SubscriptionUpdates subscriptionUpdates = new SubscriptionUpdates();
@@ -279,11 +286,16 @@ public Subscription subscription(Set<String> topics) {
         // construct the client metadata from the decoded subscription info
         Map<UUID, ClientMetadata> clientsMetadata = new HashMap<>();
+        int minUserMetadataVersion = SubscriptionInfo.CURRENT_VERSION;
         for (Map.Entry<String, Subscription> entry : subscriptions.entrySet()) {
             String consumerId = entry.getKey();
             Subscription subscription = entry.getValue();
             SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());
+            final int usedVersion = info.version;
+            if (usedVersion < minUserMetadataVersion) {
+                minUserMetadataVersion = usedVersion;
+            }
             // create the new client metadata if necessary
             ClientMetadata clientMetadata = clientsMetadata.get(info.processId);
@@ -539,7 +551,7 @@ public Subscription subscription(Set<String> topics) {
                 }
                 // finally, encode the assignment before sending back to coordinator
-                assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(active, standby, partitionsByHostState).encode()));
+                assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(minUserMetadataVersion, active, standby, partitionsByHostState).encode()));
                 i++;
             }
         }
diff --git [file java] [file java]
index ddbd67d8d88..7a6bf14d918 100644
--- [file java]
+++ [file java]
@@ -14,7 +14,6 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 package org.apache.kafka.streams.processor.internals.assignment;
 import org.apache.kafka.common.record.ByteBufferInputStream;
@@ -56,7 +55,7 @@ public AssignmentInfo(List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>>
         this(CURRENT_VERSION, activeTasks, standbyTasks, hostState);
     }
-    protected AssignmentInfo(int version, List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>> standbyTasks,
+    public AssignmentInfo(int version, List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>> standbyTasks,
                              Map<HostInfo, Set<TopicPartition>> hostState) {
         this.version = version;
         this.activeTasks = activeTasks;
@@ -155,9 +154,7 @@ public static AssignmentInfo decode(ByteBuffer data) {
                 }
             }
-            return new AssignmentInfo(activeTasks, standbyTasks, hostStateToTopicPartitions);
-
-
+            return new AssignmentInfo(version, activeTasks, standbyTasks, hostStateToTopicPartitions);
         } catch (IOException ex) {
             throw new TaskAssignmentException(""Failed to decode AssignmentInfo"", ex);
         }
diff --git [file java] [file java]
index c3481c05156..92c50a2a942 100644
--- [file java]
+++ [file java]
@@ -14,7 +14,6 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 package org.apache.kafka.streams.processor.internals.assignment;
 import org.apache.kafka.streams.errors.TaskAssignmentException;
@@ -32,7 +31,7 @@
     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);
-    private static final int CURRENT_VERSION = 2;
+    public static final int CURRENT_VERSION = 2;
     public final int version;
     public final UUID processId;
@@ -44,7 +43,7 @@ public SubscriptionInfo(UUID processId, Set<TaskId> prevTasks, Set<TaskId> stand
         this(CURRENT_VERSION, processId, prevTasks, standbyTasks, userEndPoint);
     }
-    private SubscriptionInfo(int version, UUID processId, Set<TaskId> prevTasks, Set<TaskId> standbyTasks, String userEndPoint) {
+    public SubscriptionInfo(int version, UUID processId, Set<TaskId> prevTasks, Set<TaskId> standbyTasks, String userEndPoint) {
         this.version = version;
         this.processId = processId;
         this.prevTasks = prevTasks;
diff --git [file java] [file java]
index 50ab1175cef..832883a0268 100644
--- [file java]
+++ [file java]
@@ -96,7 +96,7 @@ public boolean conditionMet() {
     }
     @Test
-    public void testStateCloseAfterCreate() throws Exception {
+    public void testStateCloseAfterCreate() {
         final KStreamBuilder builder = new KStreamBuilder();
         final KafkaStreams streams = new KafkaStreams(builder, props);
@@ -160,7 +160,7 @@ public void testStateThreadClose() throws Exception {
         // make sure we have the global state thread running too
         builder.globalTable(""anyTopic"", ""anyStore"");
         props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, numThreads);
-        final KafkaStreams streams = new KafkaStreams(builder, props);
+        new KafkaStreams(builder, props);
         testStateThreadCloseHelper(numThreads);
     }
@@ -200,9 +200,8 @@ public boolean conditionMet() {
     }
-
     @Test
-    public void testInitializesAndDestroysMetricsReporters() throws Exception {
+    public void testInitializesAndDestroysMetricsReporters() {
         final int oldInitCount = MockMetricsReporter.INIT_COUNT.get();
         final KStreamBuilder builder = new KStreamBuilder();
         final KafkaStreams streams = new KafkaStreams(builder, props);
@@ -217,7 +216,7 @@ public void testInitializesAndDestroysMetricsReporters() throws Exception {
     }
     @Test
-    public void testCloseIsIdempotent() throws Exception {
+    public void testCloseIsIdempotent() {
         streams.close();
         final int closeCount = MockMetricsReporter.CLOSE_COUNT.get();
@@ -227,7 +226,7 @@ public void testCloseIsIdempotent() throws Exception {
     }
     @Test(expected = IllegalStateException.class)
-    public void testCannotStartOnceClosed() throws Exception {
+    public void testCannotStartOnceClosed() {
         streams.start();
         streams.close();
         try {
@@ -241,7 +240,7 @@ public void testCannotStartOnceClosed() throws Exception {
     }
     @Test(expected = IllegalStateException.class)
-    public void testCannotStartTwice() throws Exception {
+    public void testCannotStartTwice() {
         streams.start();
         try {
@@ -267,10 +266,10 @@ public void testIllegalMetricsConfig() {
         final Properties props = new Properties();
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""appId"");
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());
         props.setProperty(StreamsConfig.METRICS_RECORDING_LEVEL_CONFIG, ""illegalConfig"");
         final KStreamBuilder builder = new KStreamBuilder();
-        final KafkaStreams streams = new KafkaStreams(builder, props);
-
+        new KafkaStreams(builder, props);
     }
     @Test
@@ -278,6 +277,7 @@ public void testLegalMetricsConfig() {
         final Properties props = new Properties();
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""appId"");
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());
         props.setProperty(StreamsConfig.METRICS_RECORDING_LEVEL_CONFIG, Sensor.RecordingLevel.INFO.toString());
         final KStreamBuilder builder1 = new KStreamBuilder();
         final KafkaStreams streams1 = new KafkaStreams(builder1, props);
@@ -285,27 +285,26 @@ public void testLegalMetricsConfig() {
         props.setProperty(StreamsConfig.METRICS_RECORDING_LEVEL_CONFIG, Sensor.RecordingLevel.DEBUG.toString());
         final KStreamBuilder builder2 = new KStreamBuilder();
-        final KafkaStreams streams2 = new KafkaStreams(builder2, props);
-
+        new KafkaStreams(builder2, props);
     }
     @Test(expected = IllegalStateException.class)
-    public void shouldNotGetAllTasksWhenNotRunning() throws Exception {
+    public void shouldNotGetAllTasksWhenNotRunning() {
         streams.allMetadata();
     }
     @Test(expected = IllegalStateException.class)
-    public void shouldNotGetAllTasksWithStoreWhenNotRunning() throws Exception {
+    public void shouldNotGetAllTasksWithStoreWhenNotRunning() {
         streams.allMetadataForStore(""store"");
     }
     @Test(expected = IllegalStateException.class)
-    public void shouldNotGetTaskWithKeyAndSerializerWhenNotRunning() throws Exception {
+    public void shouldNotGetTaskWithKeyAndSerializerWhenNotRunning() {
         streams.metadataForKey(""store"", ""key"", Serdes.String().serializer());
     }
     @Test(expected = IllegalStateException.class)
-    public void shouldNotGetTaskWithKeyAndPartitionerWhenNotRunning() throws Exception {
+    public void shouldNotGetTaskWithKeyAndPartitionerWhenNotRunning() {
         streams.metadataForKey(""store"", ""key"", new StreamPartitioner<String, Object>() {
             @Override
             public Integer partition(final String key, final Object value, final int numPartitions) {
@@ -321,6 +320,7 @@ public void shouldReturnFalseOnCloseWhenThreadsHaventTerminated() throws Excepti
             final Properties props = new Properties();
             props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""appId"");
             props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+            props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());
             props.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");
             final KStreamBuilder builder = new KStreamBuilder();
@@ -366,16 +366,18 @@ private KafkaStreams createKafkaStreams() {
         final Properties props = new Properties();
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""appId"");
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());
         final KStreamBuilder builder = new KStreamBuilder();
         return new KafkaStreams(builder, props);
     }
     @Test
-    public void testCleanup() throws Exception {
+    public void testCleanup() {
         final Properties props = new Properties();
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""testLocalCleanup"");
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());
         final KStreamBuilder builder = new KStreamBuilder();
         final KafkaStreams streams = new KafkaStreams(builder, props);
@@ -391,6 +393,7 @@ public void testCannotCleanupWhileRunning() throws Exception {
         final Properties props = new Properties();
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""testCannotCleanupWhileRunning"");
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());
         final KStreamBuilder builder = new KStreamBuilder();
         final KafkaStreams streams = new KafkaStreams(builder, props);
@@ -448,6 +451,5 @@ public void onChange(final KafkaStreams.State newState, final KafkaStreams.State
                 streams.close();
             }
         }
-
     }
 }
diff --git [file java] [file java]
index 15cc1af8e5d..ab8701fa966 100644
--- [file java]
+++ [file java]
@@ -60,7 +60,7 @@ public void setUp() {
     }
     @Test
-    public void testGetProducerConfigs() throws Exception {
+    public void testGetProducerConfigs() {
         Map<String, Object> returnedProps = streamsConfig.getProducerConfigs(""client"");
         assertEquals(returnedProps.get(ProducerConfig.CLIENT_ID_CONFIG), ""client-producer"");
         assertEquals(returnedProps.get(ProducerConfig.LINGER_MS_CONFIG), ""100"");
@@ -68,7 +68,7 @@ public void testGetProducerConfigs() throws Exception {
     }
     @Test
-    public void testGetConsumerConfigs() throws Exception {
+    public void testGetConsumerConfigs() {
         Map<String, Object> returnedProps = streamsConfig.getConsumerConfigs(null, ""example-application"", ""client"");
         assertEquals(returnedProps.get(ConsumerConfig.CLIENT_ID_CONFIG), ""client-consumer"");
         assertEquals(returnedProps.get(ConsumerConfig.GROUP_ID_CONFIG), ""example-application"");
@@ -77,7 +77,7 @@ public void testGetConsumerConfigs() throws Exception {
     }
     @Test
-    public void testGetRestoreConsumerConfigs() throws Exception {
+    public void testGetRestoreConsumerConfigs() {
         Map<String, Object> returnedProps = streamsConfig.getRestoreConsumerConfigs(""client"");
         assertEquals(returnedProps.get(ConsumerConfig.CLIENT_ID_CONFIG), ""client-restore-consumer"");
         assertNull(returnedProps.get(ConsumerConfig.GROUP_ID_CONFIG));
@@ -86,7 +86,7 @@ public void testGetRestoreConsumerConfigs() throws Exception {
     @Test
     public void defaultSerdeShouldBeConfigured() {
-        Map<String, Object> serializerConfigs = new HashMap<String, Object>();
+        Map<String, Object> serializerConfigs = new HashMap<>();
         serializerConfigs.put(""key.serializer.encoding"", ""UTF8"");
         serializerConfigs.put(""value.serializer.encoding"", ""UTF-16"");
         Serializer<String> serializer = Serdes.String().serializer();
@@ -117,7 +117,7 @@ public void shouldSupportMultipleBootstrapServers() {
     }
     @Test
-    public void shouldSupportPrefixedConsumerConfigs() throws Exception {
+    public void shouldSupportPrefixedConsumerConfigs() {
         props.put(consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), ""earliest"");
         props.put(consumerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -127,7 +127,7 @@ public void shouldSupportPrefixedConsumerConfigs() throws Exception {
     }
     @Test
-    public void shouldSupportPrefixedRestoreConsumerConfigs() throws Exception {
+    public void shouldSupportPrefixedRestoreConsumerConfigs() {
         props.put(consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), ""earliest"");
         props.put(consumerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -137,7 +137,7 @@ public void shouldSupportPrefixedRestoreConsumerConfigs() throws Exception {
     }
     @Test
-    public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() throws Exception {
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() {
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         props.put(consumerPrefix(""interceptor.statsd.host""), ""host"");
         final Map<String, Object> consumerConfigs = streamsConfig.getConsumerConfigs(null, ""groupId"", ""clientId"");
@@ -145,7 +145,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() thro
     }
     @Test
-    public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig() throws Exception {
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig() {
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         props.put(consumerPrefix(""interceptor.statsd.host""), ""host"");
         final Map<String, Object> consumerConfigs = streamsConfig.getRestoreConsumerConfigs(""clientId"");
@@ -153,7 +153,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig
     }
     @Test
-    public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() throws Exception {
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() {
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         props.put(producerPrefix(""interceptor.statsd.host""), ""host"");
         final Map<String, Object> producerConfigs = streamsConfig.getProducerConfigs(""clientId"");
@@ -162,7 +162,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() thro
     @Test
-    public void shouldSupportPrefixedProducerConfigs() throws Exception {
+    public void shouldSupportPrefixedProducerConfigs() {
         props.put(producerPrefix(ProducerConfig.BUFFER_MEMORY_CONFIG), 10);
         props.put(producerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -172,7 +172,7 @@ public void shouldSupportPrefixedProducerConfigs() throws Exception {
     }
     @Test
-    public void shouldBeSupportNonPrefixedConsumerConfigs() throws Exception {
+    public void shouldBeSupportNonPrefixedConsumerConfigs() {
         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -182,7 +182,7 @@ public void shouldBeSupportNonPrefixedConsumerConfigs() throws Exception {
     }
     @Test
-    public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() throws Exception {
+    public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() {
         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -192,7 +192,7 @@ public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() throws Exception
     }
     @Test
-    public void shouldSupportNonPrefixedProducerConfigs() throws Exception {
+    public void shouldSupportNonPrefixedProducerConfigs() {
         props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 10);
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -201,24 +201,22 @@ public void shouldSupportNonPrefixedProducerConfigs() throws Exception {
         assertEquals(1, configs.get(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG));
     }
-
-
     @Test(expected = StreamsException.class)
-    public void shouldThrowStreamsExceptionIfKeySerdeConfigFails() throws Exception {
+    public void shouldThrowStreamsExceptionIfKeySerdeConfigFails() {
         props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, MisconfiguredSerde.class);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.keySerde();
     }
     @Test(expected = StreamsException.class)
-    public void shouldThrowStreamsExceptionIfValueSerdeConfigFails() throws Exception {
+    public void shouldThrowStreamsExceptionIfValueSerdeConfigFails() {
         props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, MisconfiguredSerde.class);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.valueSerde();
     }
     @Test
-    public void shouldOverrideStreamsDefaultConsumerConfigs() throws Exception {
+    public void shouldOverrideStreamsDefaultConsumerConfigs() {
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), ""latest"");
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), ""10"");
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -228,7 +226,7 @@ public void shouldOverrideStreamsDefaultConsumerConfigs() throws Exception {
     }
     @Test
-    public void shouldOverrideStreamsDefaultProducerConfigs() throws Exception {
+    public void shouldOverrideStreamsDefaultProducerConfigs() {
         props.put(StreamsConfig.producerPrefix(ProducerConfig.LINGER_MS_CONFIG), ""10000"");
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         final Map<String, Object> producerConfigs = streamsConfig.getProducerConfigs(""client"");
@@ -236,7 +234,7 @@ public void shouldOverrideStreamsDefaultProducerConfigs() throws Exception {
     }
     @Test
-    public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() throws Exception {
+    public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() {
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), ""latest"");
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), ""10"");
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -246,14 +244,14 @@ public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() throw
     }
     @Test(expected = ConfigException.class)
-    public void shouldThrowExceptionIfConsumerAutoCommitIsOverridden() throws Exception {
+    public void shouldThrowExceptionIfConsumerAutoCommitIsOverridden() {
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG), ""true"");
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.getConsumerConfigs(null, ""a"", ""b"");
     }
     @Test(expected = ConfigException.class)
-    public void shouldThrowExceptionIfRestoreConsumerAutoCommitIsOverridden() throws Exception {
+    public void shouldThrowExceptionIfRestoreConsumerAutoCommitIsOverridden() {
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG), ""true"");
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.getRestoreConsumerConfigs(""client"");
diff --git [file java] [file java]
index 1d2a3e217e1..12568246e5d 100644
--- [file java]
+++ [file java]
@@ -32,6 +32,7 @@
 import org.apache.kafka.streams.kstream.KStream;
 import org.apache.kafka.streams.kstream.KStreamBuilder;
 import org.apache.kafka.streams.kstream.ValueMapper;
+import org.apache.kafka.test.TestUtils;
 import org.junit.BeforeClass;
 import org.junit.ClassRule;
 import org.junit.Test;
@@ -112,6 +113,7 @@ public void shouldFanoutTheInput() throws Exception {
         final Properties streamsConfiguration = new Properties();
         streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, ""fanout-integration-test"");
         streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        streamsConfiguration.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());
         streamsConfiguration.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
         streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");
         streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, cacheSizeBytes);
diff --git [file java] [file java]
index 6a8c7ff10e1..3e0d80a6b19 100644
--- [file java]
+++ [file java]
@@ -10,6 +10,7 @@
  */
 package org.apache.kafka.streams.integration;
+import kafka.utils.MockTime;
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.kafka.common.serialization.Deserializer;
 import org.apache.kafka.common.serialization.IntegerSerializer;
@@ -44,8 +45,6 @@
 import java.util.Properties;
 import java.util.concurrent.ExecutionException;
-import kafka.utils.MockTime;
-
 import static org.hamcrest.MatcherAssert.assertThat;
 import static org.hamcrest.core.Is.is;
@@ -127,7 +126,7 @@ public void shouldReduce() throws Exception {
         List<KeyValue<String, String>> results = receiveMessages(
             new StringDeserializer(),
             new StringDeserializer(),
-                5);
+            5);
         Collections.sort(results, new Comparator<KeyValue<String, String>>() {
             @Override
@@ -177,7 +176,7 @@ public String apply(Windowed<String> windowedKey, String value) {
         List<KeyValue<String, String>> windowedOutput = receiveMessages(
             new StringDeserializer(),
             new StringDeserializer(),
-                10);
+            10);
         Comparator<KeyValue<String, String>>
             comparator =
@@ -229,7 +228,7 @@ public String apply(final Windowed<Integer> windowedKey, final Long value) {
         final List<KeyValue<String, Long>> results = receiveMessages(
             new StringDeserializer(),
             new LongDeserializer(),
-                5);
+            5);
         Collections.sort(results, new Comparator<KeyValue<String, Long>>() {
             @Override
             public int compare(final KeyValue<String, Long> o1, final KeyValue<String, Long> o2) {
@@ -303,6 +302,4 @@ private void startStreams() {
     }
-
-
 }
diff --git [file java] [file java]
index bd5911d270b..13124f1cc5e 100644
--- [file java]
+++ [file java]
@@ -155,7 +155,7 @@ public void shouldReduce() throws Exception {
         final List<KeyValue<String, String>> results = receiveMessages(
             new StringDeserializer(),
             new StringDeserializer(),
-                10);
+            10);
         Collections.sort(results, new Comparator<KeyValue<String, String>>() {
             @Override
@@ -209,7 +209,7 @@ public String apply(final Windowed<String> windowedKey, final String value) {
         final List<KeyValue<String, String>> windowedOutput = receiveMessages(
             new StringDeserializer(),
             new StringDeserializer(),
-                15);
+            15);
         final Comparator<KeyValue<String, String>>
             comparator =
@@ -263,7 +263,7 @@ public void shouldAggregate() throws Exception {
         final List<KeyValue<String, Integer>> results = receiveMessages(
             new StringDeserializer(),
             new IntegerDeserializer(),
-                10);
+            10);
         Collections.sort(results, new Comparator<KeyValue<String, Integer>>() {
             @Override
@@ -313,7 +313,7 @@ public String apply(final Windowed<String> windowedKey, final Integer value) {
         final List<KeyValue<String, Integer>> windowedMessages = receiveMessages(
             new StringDeserializer(),
             new IntegerDeserializer(),
-                15);
+            15);
         final Comparator<KeyValue<String, Integer>>
             comparator =
@@ -364,7 +364,7 @@ public void shouldCount() throws Exception {
         final List<KeyValue<String, Long>> results = receiveMessages(
             new StringDeserializer(),
             new LongDeserializer(),
-                10);
+            10);
         Collections.sort(results, new Comparator<KeyValue<String, Long>>() {
             @Override
             public int compare(final KeyValue<String, Long> o1, final KeyValue<String, Long> o2) {
@@ -406,7 +406,7 @@ public String apply(final Windowed<Integer> windowedKey, final Long value) {
         final List<KeyValue<String, Long>> results = receiveMessages(
             new StringDeserializer(),
             new LongDeserializer(),
-                10);
+            10);
         Collections.sort(results, new Comparator<KeyValue<String, Long>>() {
             @Override
             public int compare(final KeyValue<String, Long> o1, final KeyValue<String, Long> o2) {
diff --git [file java] [file java]
index 64e8459b274..d09d505b5e7 100644
--- [file java]
+++ [file java]
@@ -129,7 +129,7 @@ public void createTopics() throws InterruptedException {
     }
     @Before
-    public void before() throws IOException, InterruptedException {
+    public void before() throws Exception {
         testNo++;
         createTopics();
         streamsConfiguration = new Properties();
@@ -609,15 +609,13 @@ private void verifyCanGetByKey(final String keys,
      * @param failIfKeyNotFound     if true, tests fails if an expected key is not found in store. If false,
      *                              the method merely inserts the new found key into the list of
      *                              expected keys.
-     * @throws InterruptedException
      */
     private void verifyGreaterOrEqual(final String keys,
                                       final Map<String, Long> expectedWindowedCount,
                                       final Map<String, Long> expectedCount,
                                       final ReadOnlyWindowStore<String, Long> windowStore,
                                       final ReadOnlyKeyValueStore<String, Long> keyValueStore,
-                                      final boolean failIfKeyNotFound)
-        throws InterruptedException {
+                                      final boolean failIfKeyNotFound) {
         final Map<String, Long> windowState = new HashMap<>();
         final Map<String, Long> countState = new HashMap<>();
@@ -744,5 +742,4 @@ public void run() {
         }
     }
-
 }
diff --git [file java] [file java]
index 6503038e5b3..e06ed739048 100644
--- [file java]
+++ [file java]
@@ -44,6 +44,7 @@
 import org.apache.kafka.test.MockProcessorSupplier;
 import org.apache.kafka.test.MockStateStoreSupplier;
 import org.apache.kafka.test.MockTimestampExtractor;
+import org.apache.kafka.test.TestUtils;
 import org.junit.Assert;
 import org.junit.Test;
@@ -111,6 +112,7 @@ private Properties configProps() {
             {
                 setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""stream-partition-assignor-test"");
                 setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, userEndPoint);
+                setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());
                 setProperty(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, ""3"");
                 setProperty(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, MockTimestampExtractor.class.getName());
             }
@@ -119,7 +121,7 @@ private Properties configProps() {
     @SuppressWarnings(""unchecked"")
     @Test
-    public void testSubscription() throws Exception {
+    public void testSubscription() {
         builder.addSource(""source1"", ""topic1"");
         builder.addSource(""source2"", ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
@@ -159,7 +161,7 @@ public void testSubscription() throws Exception {
     }
     @Test
-    public void testAssignBasic() throws Exception {
+    public void testAssignBasic() {
         builder.addSource(""source1"", ""topic1"");
         builder.addSource(""source2"", ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
@@ -227,7 +229,7 @@ public void testAssignBasic() throws Exception {
     }
     @Test
-    public void testAssignWithPartialTopology() throws Exception {
+    public void testAssignWithPartialTopology() {
         Properties props = configProps();
         props.put(StreamsConfig.PARTITION_GROUPER_CLASS_CONFIG, SingleGroupPartitionGrouperStub.class);
         StreamsConfig config = new StreamsConfig(props);
@@ -267,7 +269,7 @@ public void testAssignWithPartialTopology() throws Exception {
     @Test
-    public void testAssignEmptyMetadata() throws Exception {
+    public void testAssignEmptyMetadata() {
         builder.addSource(""source1"", ""topic1"");
         builder.addSource(""source2"", ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
@@ -324,7 +326,7 @@ public void testAssignEmptyMetadata() throws Exception {
     }
     @Test
-    public void testAssignWithNewTasks() throws Exception {
+    public void testAssignWithNewTasks() {
         builder.addSource(""source1"", ""topic1"");
         builder.addSource(""source2"", ""topic2"");
         builder.addSource(""source3"", ""topic3"");
@@ -381,7 +383,7 @@ public void testAssignWithNewTasks() throws Exception {
     }
     @Test
-    public void testAssignWithStates() throws Exception {
+    public void testAssignWithStates() {
         String applicationId = ""test"";
         builder.setApplicationId(applicationId);
         builder.addSource(""source1"", ""topic1"");
@@ -470,7 +472,7 @@ public void testAssignWithStates() throws Exception {
     }
     @Test
-    public void testAssignWithStandbyReplicas() throws Exception {
+    public void testAssignWithStandbyReplicas() {
         Properties props = configProps();
         props.setProperty(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, ""1"");
         StreamsConfig config = new StreamsConfig(props);
@@ -543,7 +545,7 @@ public void testAssignWithStandbyReplicas() throws Exception {
     }
     @Test
-    public void testOnAssignment() throws Exception {
+    public void testOnAssignment() {
         TopicPartition t2p3 = new TopicPartition(""topic2"", 3);
         TopologyBuilder builder = new TopologyBuilder();
@@ -576,7 +578,7 @@ public void testOnAssignment() throws Exception {
     }
     @Test
-    public void testAssignWithInternalTopics() throws Exception {
+    public void testAssignWithInternalTopics() {
         String applicationId = ""test"";
         builder.setApplicationId(applicationId);
         builder.addInternalTopic(""topicX"");
@@ -612,7 +614,7 @@ public void testAssignWithInternalTopics() throws Exception {
     }
     @Test
-    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throws Exception {
+    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() {
         String applicationId = ""test"";
         builder.setApplicationId(applicationId);
         builder.addInternalTopic(""topicX"");
@@ -650,7 +652,7 @@ public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throw
     }
     @Test
-    public void shouldAddUserDefinedEndPointToSubscription() throws Exception {
+    public void shouldAddUserDefinedEndPointToSubscription() {
         final Properties properties = configProps();
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, ""localhost:8080"");
         final StreamsConfig config = new StreamsConfig(properties);
@@ -663,8 +665,8 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {
         final UUID uuid1 = UUID.randomUUID();
         final String client1 = ""client1"";
-        final StreamThread streamThread = new StreamThread(builder, config, mockClientSupplier, applicationId, client1, uuid1, new Metrics(), Time.SYSTEM, new StreamsMetadataState(builder, StreamsMetadataState.UNKNOWN_HOST),
-                                                           0);
+        final StreamThread streamThread = new StreamThread(builder, config, mockClientSupplier, applicationId, client1,
+            uuid1, new Metrics(), Time.SYSTEM, new StreamsMetadataState(builder, StreamsMetadataState.UNKNOWN_HOST), 0);
         partitionAssignor.configure(config.getConsumerConfigs(streamThread, applicationId, client1));
         final PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""input""));
@@ -673,7 +675,80 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {
     }
     @Test
-    public void shouldMapUserEndPointToTopicPartitions() throws Exception {
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() {
+        final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
+        final Set<TaskId> emptyTasks = Collections.emptySet();
+        subscriptions.put(
+            ""consumer1"",
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(""topic1""),
+                new SubscriptionInfo(1, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+        subscriptions.put(
+            ""consumer2"",
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(""topic1""),
+                new SubscriptionInfo(2, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+
+        final StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
+        StreamsConfig config = new StreamsConfig(configProps());
+
+        final TopologyBuilder builder = new TopologyBuilder();
+        final StreamThread streamThread = new StreamThread(
+            builder,
+            config,
+            mockClientSupplier,
+            ""appId"",
+            ""clientId"",
+            UUID.randomUUID(),
+            new Metrics(),
+            Time.SYSTEM,
+            new StreamsMetadataState(builder, StreamsMetadataState.UNKNOWN_HOST),
+            0);
+
+        partitionAssignor.configure(config.getConsumerConfigs(streamThread, ""test"", ""clientId""));
+        final Map<String, PartitionAssignor.Assignment> assignment = partitionAssignor.assign(metadata, subscriptions);
+
+        assertEquals(2, assignment.size());
+        assertEquals(1, AssignmentInfo.decode(assignment.get(""consumer1"").userData()).version);
+        assertEquals(1, AssignmentInfo.decode(assignment.get(""consumer2"").userData()).version);
+    }
+
+    @Test
+    public void shouldDownGradeSubscription() {
+        final Properties properties = configProps();
+        properties.put(StreamsConfig.UPGRADE_FROM_CONFIG, StreamsConfig.UPGRADE_FROM_0100);
+        StreamsConfig config = new StreamsConfig(properties);
+
+        TopologyBuilder builder = new TopologyBuilder();
+        builder.addSource(""source1"", ""topic1"");
+
+        String clientId = ""client-id"";
+        final StreamThread streamThread = new StreamThread(
+            builder,
+            config,
+            mockClientSupplier,
+            ""appId"",
+            ""clientId"",
+            UUID.randomUUID(),
+            new Metrics(),
+            Time.SYSTEM,
+            new StreamsMetadataState(builder, StreamsMetadataState.UNKNOWN_HOST),
+            0);
+
+        StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
+        partitionAssignor.configure(config.getConsumerConfigs(streamThread, ""test"", clientId));
+
+        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""topic1""));
+
+        assertEquals(1, SubscriptionInfo.decode(subscription.userData()).version);
+    }
+
+    @Test
+    public void shouldMapUserEndPointToTopicPartitions() {
         final Properties properties = configProps();
         final String myEndPoint = ""localhost:8080"";
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);
@@ -711,7 +786,7 @@ public void shouldMapUserEndPointToTopicPartitions() throws Exception {
     }
     @Test
-    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() throws Exception {
+    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() {
         final Properties properties = configProps();
         final String myEndPoint = ""localhost"";
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);
@@ -736,7 +811,7 @@ public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() thr
     }
     @Test
-    public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() throws Exception {
+    public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() {
         final Properties properties = configProps();
         final String myEndPoint = ""localhost:j87yhk"";
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);
@@ -760,7 +835,7 @@ public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() th
     }
     @Test
-    public void shouldExposeHostStateToTopicPartitionsOnAssignment() throws Exception {
+    public void shouldExposeHostStateToTopicPartitionsOnAssignment() {
         List<TopicPartition> topic = Collections.singletonList(new TopicPartition(""topic"", 0));
         final Map<HostInfo, Set<TopicPartition>> hostState =
                 Collections.singletonMap(new HostInfo(""localhost"", 80),
@@ -773,7 +848,7 @@ public void shouldExposeHostStateToTopicPartitionsOnAssignment() throws Exceptio
     }
     @Test
-    public void shouldSetClusterMetadataOnAssignment() throws Exception {
+    public void shouldSetClusterMetadataOnAssignment() {
         final List<TopicPartition> topic = Collections.singletonList(new TopicPartition(""topic"", 0));
         final Map<HostInfo, Set<TopicPartition>> hostState =
                 Collections.singletonMap(new HostInfo(""localhost"", 80),
@@ -793,7 +868,7 @@ public void shouldSetClusterMetadataOnAssignment() throws Exception {
     }
     @Test
-    public void shouldReturnEmptyClusterMetadataIfItHasntBeenBuilt() throws Exception {
+    public void shouldReturnEmptyClusterMetadataIfItHasntBeenBuilt() {
         final Cluster cluster = partitionAssignor.clusterMetadata();
         assertNotNull(cluster);
     }
@@ -891,11 +966,11 @@ public Object apply(Object value1, Object value2) {
             new TopicPartition(applicationId + ""-count-repartition"", 1),
             new TopicPartition(applicationId + ""-count-repartition"", 2)
         );
-        assertThat(new HashSet(assignment.get(client).partitions()), equalTo(new HashSet(expectedAssignment)));
+        assertThat(new HashSet<>(assignment.get(client).partitions()), equalTo(new HashSet<>(expectedAssignment)));
     }
     @Test
-    public void shouldUpdatePartitionHostInfoMapOnAssignment() throws Exception {
+    public void shouldUpdatePartitionHostInfoMapOnAssignment() {
         final TopicPartition partitionOne = new TopicPartition(""topic"", 1);
         final TopicPartition partitionTwo = new TopicPartition(""topic"", 2);
         final Map<HostInfo, Set<TopicPartition>> firstHostState = Collections.singletonMap(
@@ -912,7 +987,7 @@ public void shouldUpdatePartitionHostInfoMapOnAssignment() throws Exception {
     }
     @Test
-    public void shouldUpdateClusterMetadataOnAssignment() throws Exception {
+    public void shouldUpdateClusterMetadataOnAssignment() {
         final TopicPartition topicOne = new TopicPartition(""topic"", 1);
         final TopicPartition topicTwo = new TopicPartition(""topic2"", 2);
         final Map<HostInfo, Set<TopicPartition>> firstHostState = Collections.singletonMap(
@@ -928,7 +1003,7 @@ public void shouldUpdateClusterMetadataOnAssignment() throws Exception {
     }
     @Test
-    public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() throws Exception {
+    public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() {
         final Properties props = configProps();
         props.setProperty(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, ""1"");
         final StreamsConfig config = new StreamsConfig(props);
@@ -976,12 +1051,12 @@ public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() throws Except
     }
     @Test(expected = KafkaException.class)
-    public void shouldThrowKafkaExceptionIfStreamThreadNotConfigured() throws Exception {
+    public void shouldThrowKafkaExceptionIfStreamThreadNotConfigured() {
         partitionAssignor.configure(Collections.singletonMap(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1));
     }
     @Test(expected = KafkaException.class)
-    public void shouldThrowKafkaExceptionIfStreamThreadConfigIsNotStreamThreadInstance() throws Exception {
+    public void shouldThrowKafkaExceptionIfStreamThreadConfigIsNotStreamThreadInstance() {
         final Map<String, Object> config = new HashMap<>();
         config.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);
         config.put(StreamsConfig.InternalConfig.STREAM_THREAD_INSTANCE, ""i am not a stream thread"");
diff --git [file java] [file java]
index cfa0e61dc85..52c753d5201 100644
--- [file java]
+++ [file java]
@@ -65,10 +65,9 @@ public void shouldDecodePreviousVersion() throws Exception {
         assertEquals(oldVersion.activeTasks, decoded.activeTasks);
         assertEquals(oldVersion.standbyTasks, decoded.standbyTasks);
         assertEquals(0, decoded.partitionsByHost.size()); // should be empty as wasn't in V1
-        assertEquals(2, decoded.version); // automatically upgraded to v2 on decode;
+        assertEquals(1, decoded.version);
     }
-
     /**
      * This is a clone of what the V1 encoding did. The encode method has changed for V2
      * so it is impossible to test compatibility without having this
diff --git [file java] [file java]
index d1921261bf7..9f59b11cc14 100644
--- [file java]
+++ [file java]
@@ -115,7 +115,7 @@ public boolean test(String key, Integer value) {
             }
         });
-        data.process(SmokeTestUtil.printProcessorSupplier(""data""));
+        data.process(SmokeTestUtil.<String, Integer>printProcessorSupplier(""data""));
         // min
         KGroupedStream<String, Integer>
@@ -141,7 +141,7 @@ public Integer apply(String aggKey, Integer value, Integer aggregate) {
         ).to(stringSerde, intSerde, ""min"");
         KTable<String, Integer> minTable = builder.table(stringSerde, intSerde, ""min"", ""minStoreName"");
-        minTable.toStream().process(SmokeTestUtil.printProcessorSupplier(""min""));
+        minTable.toStream().process(SmokeTestUtil.<String, Integer>printProcessorSupplier(""min""));
         // max
         groupedData.aggregate(
@@ -163,7 +163,7 @@ public Integer apply(String aggKey, Integer value, Integer aggregate) {
         ).to(stringSerde, intSerde, ""max"");
         KTable<String, Integer> maxTable = builder.table(stringSerde, intSerde, ""max"", ""maxStoreName"");
-        maxTable.toStream().process(SmokeTestUtil.printProcessorSupplier(""max""));
+        maxTable.toStream().process(SmokeTestUtil.<String, Integer>printProcessorSupplier(""max""));
         // sum
         groupedData.aggregate(
@@ -186,7 +186,7 @@ public Long apply(String aggKey, Integer value, Long aggregate) {
         KTable<String, Long> sumTable = builder.table(stringSerde, longSerde, ""sum"", ""sumStoreName"");
-        sumTable.toStream().process(SmokeTestUtil.printProcessorSupplier(""sum""));
+        sumTable.toStream().process(SmokeTestUtil.<String, Long>printProcessorSupplier(""sum""));
         // cnt
         groupedData.count(TimeWindows.of(TimeUnit.DAYS.toMillis(2)), ""uwin-cnt"")
@@ -195,7 +195,7 @@ public Long apply(String aggKey, Integer value, Long aggregate) {
         ).to(stringSerde, longSerde, ""cnt"");
         KTable<String, Long> cntTable = builder.table(stringSerde, longSerde, ""cnt"", ""cntStoreName"");
-        cntTable.toStream().process(SmokeTestUtil.printProcessorSupplier(""cnt""));
+        cntTable.toStream().process(SmokeTestUtil.<String, Long>printProcessorSupplier(""cnt""));
         // dif
         maxTable.join(minTable,
diff --git [file java] [file java]
index c2cfd847ccd..a0c2933e6fe 100644
--- [file java]
+++ [file java]
@@ -77,7 +77,6 @@ int next() {
     // This main() is not used by the system test. It is intended to be used for local debugging.
     public static void main(String args) throws Exception {
         final String kafka = ""localhost:9092"";
-        final String zookeeper = ""localhost:2181"";
         final File stateDir = TestUtils.tempDirectory();
         final int numKeys = 20;
@@ -131,42 +130,50 @@ public void run() {
     }
     public static Map<String, Set<Integer>> generate(String kafka, final int numKeys, final int maxRecordsPerKey) throws Exception {
+        return generate(kafka, numKeys, maxRecordsPerKey, true);
+    }
+    public static Map<String, Set<Integer>> generate(final String kafka,
+                                                     final int numKeys,
+                                                     final int maxRecordsPerKey,
+                                                     final boolean autoTerminate) throws Exception {
         final Properties producerProps = new Properties();
         producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, ""SmokeTest"");
         producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
         producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
-        // the next 4 config values make sure that all records are produced with no loss and
-        // no duplicates
+        // the next 2 config values make sure that all records are produced with no loss and no duplicates
         producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);
         producerProps.put(ProducerConfig.ACKS_CONFIG, ""all"");
-        KafkaProducer<byte, byte> producer = new KafkaProducer<>(producerProps);
+        final KafkaProducer<byte, byte> producer = new KafkaProducer<>(producerProps);
         int numRecordsProduced = 0;
-        Map<String, Set<Integer>> allData = new HashMap<>();
-        ValueList data = new ValueList;
+        final Map<String, Set<Integer>> allData = new HashMap<>();
+        final ValueList data = new ValueList;
         for (int i = 0; i < numKeys; i++) {
             data = new ValueList(i, i + maxRecordsPerKey - 1);
             allData.put(data.key, new HashSet<Integer>());
         }
-        Random rand = new Random();
+        final Random rand = new Random();
-        int remaining = data.length;
+        int remaining = 1; // dummy value must be positive if <autoTerminate> is false
+        if (autoTerminate) {
+            remaining = data.length;
+        }
         while (remaining > 0) {
-            int index = rand.nextInt(remaining);
-            String key = data.key;
+            final int index = autoTerminate ? rand.nextInt(remaining) : rand.nextInt(numKeys);
+            final String key = data.key;
             int value = data.next();
-            if (value < 0) {
+            if (autoTerminate && value < 0) {
                 remaining--;
                 data = data;
             } else {
-                ProducerRecord<byte, byte> record =
-                        new ProducerRecord<>(""data"", stringSerde.serializer().serialize("""", key), intSerde.serializer().serialize("""", value));
+                final ProducerRecord<byte, byte> record =
+                    new ProducerRecord<>(""data"", stringSerde.serializer().serialize("""", key), intSerde.serializer().serialize("""", value));
                 producer.send(record, new Callback() {
                     @Override
@@ -178,11 +185,12 @@ public void onCompletion(final RecordMetadata metadata, final Exception exceptio
                     }
                 });
-
                 numRecordsProduced++;
                 allData.get(key).add(value);
-                if (numRecordsProduced % 100 == 0)
+
+                if (numRecordsProduced % 100 == 0) {
                     System.out.println(numRecordsProduced + "" records produced"");
+                }
                 Utils.sleep(2);
             }
diff --git [file java] [file java]
index 73fe27c4659..87ab60c12ba 100644
--- [file java]
+++ [file java]
@@ -33,8 +33,6 @@
 public class SmokeTestUtil {
-    public final static int WINDOW_SIZE = 100;
-    public final static long START_TIME = 60000L * 60 * 24 * 365 * 30;
     public final static int END = Integer.MAX_VALUE;
     public static ProcessorSupplier<Object, Object> printProcessorSupplier(final String topic) {
@@ -46,18 +44,15 @@
             public Processor<Object, Object> get() {
                 return new AbstractProcessor<Object, Object>() {
                     private int numRecordsProcessed = 0;
-                    private ProcessorContext context;
                     @Override
                     public void init(ProcessorContext context) {
                         System.out.println(""initializing processor: topic="" + topic + "" taskId="" + context.taskId());
                         numRecordsProcessed = 0;
-                        this.context = context;
                     }
                     @Override
                     public void process(Object key, Object value) {
-                        if (printOffset) System.out.println("">>> "" + context.offset());
                         numRecordsProcessed++;
                         if (numRecordsProcessed % 100 == 0) {
                             System.out.println(""processed "" + numRecordsProcessed + "" records from topic="" + topic);
@@ -65,12 +60,10 @@ public void process(Object key, Object value) {
                     }
                     @Override
-                    public void punctuate(long timestamp) {
-                    }
+                    public void punctuate(long timestamp) {}
                     @Override
-                    public void close() {
-                    }
+                    public void close() {}
                 };
             }
         };
diff --git [file java] [file java]
index 304cae7e0ad..aa1def1dd22 100644
--- [file java]
+++ [file java]
@@ -24,7 +24,7 @@
 public class StreamsSmokeTest {
     /**
-     *  args ::= command kafka zookeeper stateDir
+     *  args ::= command kafka zookeeper stateDir disableAutoTerminate
      *  command := ""run"" | ""process""
      *
      * @param args
@@ -33,11 +33,13 @@ public static void main(String args) throws Exception {
         String kafka = args;
         String stateDir = args.length > 1 ? args : null;
         String command = args.length > 2 ? args : null;
+        boolean disableAutoTerminate = args.length > 3;
-        System.out.println(""StreamsTest instance started"");
+        System.out.println(""StreamsTest instance started (StreamsSmokeTest)"");
         System.out.println(""command="" + command);
         System.out.println(""kafka="" + kafka);
         System.out.println(""stateDir="" + stateDir);
+        System.out.println(""disableAutoTerminate="" + disableAutoTerminate);
         switch (command) {
             case ""standalone"":
@@ -47,8 +49,12 @@ public static void main(String args) throws Exception {
                 // this starts the driver (data generation and result verification)
                 final int numKeys = 10;
                 final int maxRecordsPerKey = 500;
-                Map<String, Set<Integer>> allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
-                SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                if (disableAutoTerminate) {
+                    SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey, false);
+                } else {
+                    Map<String, Set<Integer>> allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
+                    SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                }
                 break;
             case ""process"":
                 // this starts a KafkaStreams client
diff --git [file java] [file java]
new file mode 100644
index 00000000000..17ff97ea083
--- /dev/null
+++ [file java]
@@ -0,0 +1,73 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 2) {
+            System.err.println(""StreamsUpgradeTest requires two argument (kafka-url, state-dir, ) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args : """"));
+        }
+        final String kafka = args;
+        final String stateDir = args;
+        final String upgradeFrom = args.length > 2 ? args : null;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest trunk)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""stateDir="" + stateDir);
+        System.out.println(""upgradeFrom="" + upgradeFrom);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(SmokeTestUtil.printProcessorSupplier(""data""));
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        if (upgradeFrom != null) {
+            config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+        }
+
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..7d3ed436881
--- /dev/null
+++ [file java]
@@ -0,0 +1,104 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 3) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args + "" "" : """")
+                + (args.length > 1 ? args : """"));
+        }
+        final String kafka = args;
+        final String zookeeper = args;
+        final String stateDir = args;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.10.0)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""zookeeper="" + zookeeper);
+        System.out.println(""stateDir="" + stateDir);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..604fbe71ad3
--- /dev/null
+++ [file java]
@@ -0,0 +1,114 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    /**
+     * This test cannot be run executed, as long as Kafka 0.10.1.2 is not released
+     */
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 3) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir, ) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args + "" "" : """")
+                + (args.length > 1 ? args : """"));
+        }
+        String kafka = args;
+        String zookeeper = args;
+        String stateDir = args;
+        String upgradeFrom = args.length > 3 ? args : null;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.10.1)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""zookeeper="" + zookeeper);
+        System.out.println(""stateDir="" + stateDir);
+        System.out.println(""upgradeFrom="" + upgradeFrom);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 0.10.1.2 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(""upgrade.from"", upgradeFrom);
+        }
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git a/tests/kafkatest/services/streams.py b/tests/kafkatest/services/streams.py
index e7be9475f79..b7de568ad7a 100644
--- a/tests/kafkatest/services/streams.py
+++ b/tests/kafkatest/services/streams.py
@@ -20,6 +20,7 @@
 from ducktape.utils.util import wait_until
 from kafkatest.directory_layout.kafka_path import KafkaPathResolverMixin
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1
 class StreamsTestBaseService(KafkaPathResolverMixin, Service):
@@ -33,6 +34,8 @@ class StreamsTestBaseService(KafkaPathResolverMixin, Service):
     LOG4J_CONFIG_FILE = os.path.join(PERSISTENT_ROOT, ""tools-log4j.properties"")
     PID_FILE = os.path.join(PERSISTENT_ROOT, ""streams.pid"")
+    CLEAN_NODE_ENABLED = True
+
     logs = {
         ""streams_log"": {
             ""path"": LOG_FILE,
@@ -43,6 +46,114 @@ class StreamsTestBaseService(KafkaPathResolverMixin, Service):
         ""streams_stderr"": {
             ""path"": STDERR_FILE,
             ""collect_default"": True},
+        ""streams_log.0-1"": {
+            ""path"": LOG_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_stdout.0-1"": {
+            ""path"": STDOUT_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_stderr.0-1"": {
+            ""path"": STDERR_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_log.0-2"": {
+            ""path"": LOG_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_stdout.0-2"": {
+            ""path"": STDOUT_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_stderr.0-2"": {
+            ""path"": STDERR_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_log.0-3"": {
+            ""path"": LOG_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_stdout.0-3"": {
+            ""path"": STDOUT_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_stderr.0-3"": {
+            ""path"": STDERR_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_log.0-4"": {
+            ""path"": LOG_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_stdout.0-4"": {
+            ""path"": STDOUT_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_stderr.0-4"": {
+            ""path"": STDERR_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_log.0-5"": {
+            ""path"": LOG_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_stdout.0-5"": {
+            ""path"": STDOUT_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_stderr.0-5"": {
+            ""path"": STDERR_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_log.0-6"": {
+            ""path"": LOG_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_stdout.0-6"": {
+            ""path"": STDOUT_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_stderr.0-6"": {
+            ""path"": STDERR_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_log.1-1"": {
+            ""path"": LOG_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_stdout.1-1"": {
+            ""path"": STDOUT_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_stderr.1-1"": {
+            ""path"": STDERR_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_log.1-2"": {
+            ""path"": LOG_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_stdout.1-2"": {
+            ""path"": STDOUT_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_stderr.1-2"": {
+            ""path"": STDERR_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_log.1-3"": {
+            ""path"": LOG_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_stdout.1-3"": {
+            ""path"": STDOUT_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_stderr.1-3"": {
+            ""path"": STDERR_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_log.1-4"": {
+            ""path"": LOG_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_stdout.1-4"": {
+            ""path"": STDOUT_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_stderr.1-4"": {
+            ""path"": STDERR_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_log.1-5"": {
+            ""path"": LOG_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_stdout.1-5"": {
+            ""path"": STDOUT_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_stderr.1-5"": {
+            ""path"": STDERR_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_log.1-6"": {
+            ""path"": LOG_FILE + "".1-6"",
+            ""collect_default"": True},
+        ""streams_stdout.1-6"": {
+            ""path"": STDOUT_FILE + "".1-6"",
+            ""collect_default"": True},
+        ""streams_stderr.1-6"": {
+            ""path"": STDERR_FILE + "".1-6"",
+            ""collect_default"": True},
     }
     def __init__(self, test_context, kafka, streams_class_name, user_test_args, user_test_args1=None, user_test_args2=None):
@@ -107,7 +218,8 @@ def wait_node(self, node, timeout_sec=None):
     def clean_node(self, node):
         node.account.kill_process(""streams"", clean_shutdown=False, allow_fail=True)
-        node.account.ssh(""rm -rf "" + self.PERSISTENT_ROOT, allow_fail=False)
+        if self.CLEAN_NODE_ENABLED:
+            node.account.ssh(""rm -rf "" + self.PERSISTENT_ROOT, allow_fail=False)
     def start_cmd(self, node):
         args = self.args.copy()
@@ -153,7 +265,28 @@ def __init__(self, test_context, kafka, command):
 class StreamsSmokeTestDriverService(StreamsSmokeTestBaseService):
     def __init__(self, test_context, kafka):
         super(StreamsSmokeTestDriverService, self).__init__(test_context, kafka, ""run"")
+        self.DISABLE_AUTO_TERMINATE = """"
+
+    def disable_auto_terminate(self):
+        self.DISABLE_AUTO_TERMINATE = ""disableAutoTerminate""
+
+    def start_cmd(self, node):
+        args = self.args.copy()
+        args = self.kafka.bootstrap_servers()
+        args = self.PERSISTENT_ROOT
+        args = self.STDOUT_FILE
+        args = self.STDERR_FILE
+        args = self.PID_FILE
+        args = self.LOG4J_CONFIG_FILE
+        args = self.DISABLE_AUTO_TERMINATE
+        args = self.path.script(""kafka-run-class.sh"", node)
+        cmd = ""( export KAFKA_LOG4J_OPTS=\""-Dlog4j.configuration=file:%(log4j)s\""; "" \
+              ""INCLUDE_TEST_JARS=true %(kafka_run_class)s %(streams_class_name)s "" \
+              "" %(kafka)s %(state_dir)s %(user_test_args)s %(disable_auto_terminate)s"" \
+              "" & echo $! >&3 ) 1>> %(stdout)s 2>> %(stderr)s 3> %(pidfile)s"" % args
+
+        return cmd
 class StreamsSmokeTestJobRunnerService(StreamsSmokeTestBaseService):
     def __init__(self, test_context, kafka):
@@ -171,3 +304,41 @@ def __init__(self, test_context, kafka):
                                                                 kafka,
                                                                 ""org.apache.kafka.streams.tests.BrokerCompatibilityTest"",
                                                                 ""dummy"")
+
+class StreamsUpgradeTestJobRunnerService(StreamsTestBaseService):
+    def __init__(self, test_context, kafka):
+        super(StreamsUpgradeTestJobRunnerService, self).__init__(test_context,
+                                                                 kafka,
+                                                                 ""org.apache.kafka.streams.tests.StreamsUpgradeTest"",
+                                                                 """")
+        self.UPGRADE_FROM = """"
+
+    def set_version(self, kafka_streams_version):
+        self.KAFKA_STREAMS_VERSION = kafka_streams_version
+
+    def set_upgrade_from(self, upgrade_from):
+        self.UPGRADE_FROM = upgrade_from
+
+    def start_cmd(self, node):
+        args = self.args.copy()
+        args = self.kafka.bootstrap_servers()
+        if self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_0) or self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_1):
+            args = self.kafka.zk.connect_setting()
+        else:
+            args = """"
+        args = self.PERSISTENT_ROOT
+        args = self.STDOUT_FILE
+        args = self.STDERR_FILE
+        args = self.PID_FILE
+        args = self.LOG4J_CONFIG_FILE
+        args = self.KAFKA_STREAMS_VERSION
+        args = self.UPGRADE_FROM
+        args = self.path.script(""kafka-run-class.sh"", node)
+
+        cmd = ""( export KAFKA_LOG4J_OPTS=\""-Dlog4j.configuration=file:%(log4j)s\""; "" \
+              ""INCLUDE_TEST_JARS=true UPGRADE_KAFKA_STREAMS_TEST_VERSION=%(version)s "" \
+              "" %(kafka_run_class)s %(streams_class_name)s "" \
+              "" %(kafka)s %(zk)s %(state_dir)s %(user_test_args)s %(upgrade_from)s"" \
+              "" & echo $! >&3 ) 1>> %(stdout)s 2>> %(stderr)s 3> %(pidfile)s"" % args
+
+        return cmd
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py
new file mode 100644
index 00000000000..294e3544f79
--- /dev/null
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py
@@ -0,0 +1,242 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    [link]
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from ducktape.mark import parametrize
+from kafkatest.tests.kafka_test import KafkaTest
+from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsUpgradeTestJobRunnerService
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1, DEV_VERSION
+import random
+
+class StreamsUpgradeTest(KafkaTest):
+    """"""
+    Test upgrading Kafka Streams (all version combination)
+    If metadata was changes, upgrade is more difficult
+    Metadata version was bumped in 0.10.1.0
+    """"""
+
+    def __init__(self, test_context):
+        super(StreamsUpgradeTest, self).__init__(test_context, num_zk=1, num_brokers=1, topics={
+            'echo' : { 'partitions': 5 },
+            'data' : { 'partitions': 5 }
+        })
+
+        self.driver = StreamsSmokeTestDriverService(test_context, self.kafka)
+        self.driver.disable_auto_terminate()
+        self.processor1 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)
+        self.processor2 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)
+        self.processor3 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)
+
+    def test_simple_upgrade(self):
+        """"""
+        Starts 3 KafkaStreams instances with version 0.10.1, and upgrades one-by-one to 0.10.2
+        """"""
+
+        self.driver.start()
+        self.start_all_nodes_with(str(LATEST_0_10_1))
+
+        self.processors = 
+
+        counter = 1
+        random.seed()
+
+        random.shuffle(self.processors)
+        for p in self.processors:
+            p.CLEAN_NODE_ENABLED = False
+            self.do_rolling_bounce(p, """", str(DEV_VERSION), counter)
+            counter = counter + 1
+
+        # shutdown
+        self.driver.stop()
+        self.driver.wait()
+
+        random.shuffle(self.processors)
+        for p in self.processors:
+            node = p.node
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:
+                p.stop()
+                monitor.wait_until(""UPGRADE-TEST-CLIENT-CLOSED"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'UPGRADE-TEST-CLIENT-CLOSED' on"" + str(node.account))
+
+        self.driver.stop()
+
+    #@parametrize(new_version=str(LATEST_0_10_1)) we cannot run this test until Kafka 0.10.1.2 is released
+    @parametrize(new_version=str(DEV_VERSION))
+    def test_metadata_upgrade(self, new_version):
+        """"""
+        Starts 3 KafkaStreams instances with version 0.10.0, and upgrades one-by-one to <new_version>
+        """"""
+
+        self.driver.start()
+        self.start_all_nodes_with(str(LATEST_0_10_0))
+
+        self.processors = 
+
+        counter = 1
+        random.seed()
+
+        # first rolling bounce
+        random.shuffle(self.processors)
+        for p in self.processors:
+            p.CLEAN_NODE_ENABLED = False
+            self.do_rolling_bounce(p, ""0.10.0"", new_version, counter)
+            counter = counter + 1
+
+        # second rolling bounce
+        random.shuffle(self.processors)
+        for p in self.processors:
+            self.do_rolling_bounce(p, """", new_version, counter)
+            counter = counter + 1
+
+        # shutdown
+        self.driver.stop()
+        self.driver.wait()
+
+        random.shuffle(self.processors)
+        for p in self.processors:
+            node = p.node
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:
+                p.stop()
+                monitor.wait_until(""UPGRADE-TEST-CLIENT-CLOSED"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'UPGRADE-TEST-CLIENT-CLOSED' on"" + str(node.account))
+
+        self.driver.stop()
+
+    def start_all_nodes_with(self, version):
+        # start first with <version>
+        self.prepare_for(self.processor1, version)
+        node1 = self.processor1.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as monitor:
+            with node1.account.monitor_log(self.processor1.LOG_FILE) as log_monitor:
+                self.processor1.start()
+                log_monitor.wait_until(""Kafka version : "" + version,
+                                       timeout_sec=60,
+                                       err_msg=""Could not detect Kafka Streams version "" + version + "" "" + str(node1.account))
+                monitor.wait_until(""processed 100 records from topic"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
+
+        # start second with <version>
+        self.prepare_for(self.processor2, version)
+        node2 = self.processor2.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:
+                with node2.account.monitor_log(self.processor2.LOG_FILE) as log_monitor:
+                    self.processor2.start()
+                    log_monitor.wait_until(""Kafka version : "" + version,
+                                           timeout_sec=60,
+                                           err_msg=""Could not detect Kafka Streams version "" + version + "" "" + str(node2.account))
+                    first_monitor.wait_until(""processed 100 records from topic"",
+                                             timeout_sec=60,
+                                             err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
+                    second_monitor.wait_until(""processed 100 records from topic"",
+                                              timeout_sec=60,
+                                              err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node2.account))
+
+        # start third with <version>
+        self.prepare_for(self.processor3, version)
+        node3 = self.processor3.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:
+                with node3.account.monitor_log(self.processor3.STDOUT_FILE) as third_monitor:
+                    with node3.account.monitor_log(self.processor3.LOG_FILE) as log_monitor:
+                        self.processor3.start()
+                        log_monitor.wait_until(""Kafka version : "" + version,
+                                               timeout_sec=60,
+                                               err_msg=""Could not detect Kafka Streams version "" + version + "" "" + str(node3.account))
+                        first_monitor.wait_until(""processed 100 records from topic"",
+                                                 timeout_sec=60,
+                                                 err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
+                        second_monitor.wait_until(""processed 100 records from topic"",
+                                                  timeout_sec=60,
+                                                  err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node2.account))
+                        third_monitor.wait_until(""processed 100 records from topic"",
+                                                  timeout_sec=60,
+                                                  err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node3.account))
+
+    @staticmethod
+    def prepare_for(processor, version):
+        processor.node.account.ssh(""rm -rf "" + processor.PERSISTENT_ROOT, allow_fail=False)
+        processor.set_version(version)
+
+    def do_rolling_bounce(self, processor, upgrade_from, new_version, counter):
+        first_other_processor = None
+        second_other_processor = None
+        for p in self.processors:
+            if p != processor:
+                if first_other_processor is None:
+                    first_other_processor = p
+                else:
+                    second_other_processor = p
+
+        node = processor.node
+        first_other_node = first_other_processor.node
+        second_other_node = second_other_processor.node
+
+        # stop processor and wait for rebalance of others
+        with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:
+            with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:
+                processor.stop()
+                first_other_monitor.wait_until(""processed 100 records from topic"",
+                                               timeout_sec=60,
+                                               err_msg=""Never saw output 'processed 100 records from topic' on"" + str(first_other_node.account))
+                second_other_monitor.wait_until(""processed 100 records from topic"",
+                                                timeout_sec=60,
+                                                err_msg=""Never saw output 'processed 100 records from topic' on"" + str(second_other_node.account))
+        node.account.ssh_capture(""grep UPGRADE-TEST-CLIENT-CLOSED %s"" % processor.STDOUT_FILE, allow_fail=False)
+
+        if upgrade_from == """":  # upgrade disabled -- second round of rolling bounces
+            roll_counter = "".1-""  # second round of rolling bounces
+        else:
+            roll_counter = "".0-""  # first  round of rolling boundes
+
+        node.account.ssh(""mv "" + processor.STDOUT_FILE + "" "" + processor.STDOUT_FILE + roll_counter + str(counter), allow_fail=False)
+        node.account.ssh(""mv "" + processor.STDERR_FILE + "" "" + processor.STDERR_FILE + roll_counter + str(counter), allow_fail=False)
+        node.account.ssh(""mv "" + processor.LOG_FILE + "" "" + processor.LOG_FILE + roll_counter + str(counter), allow_fail=False)
+
+        if new_version == str(DEV_VERSION):
+            processor.set_version("""")  # set to TRUNK
+        else:
+            processor.set_version(new_version)
+        processor.set_upgrade_from(upgrade_from)
+
+        grep_metadata_error = ""grep \""org.apache.kafka.streams.errors.TaskAssignmentException: unable to decode subscription data: version=2\"" ""
+        with node.account.monitor_log(processor.STDOUT_FILE) as monitor:
+            with node.account.monitor_log(processor.LOG_FILE) as log_monitor:
+                with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:
+                    with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:
+                        processor.start()
+
+                        log_monitor.wait_until(""Kafka version : "" + new_version,
+                                               timeout_sec=60,
+                                               err_msg=""Could not detect Kafka Streams version "" + new_version + "" "" + str(node.account))
+                        first_other_monitor.wait_until(""processed 100 records from topic"",
+                                                       timeout_sec=60,
+                                                       err_msg=""Never saw output 'processed 100 records from topic' on"" + str(first_other_node.account))
+                        found = list(first_other_node.account.ssh_capture(grep_metadata_error + first_other_processor.STDERR_FILE, allow_fail=True))
+                        if len(found) > 0:
+                            raise Exception(""Kafka Streams failed with 'unable to decode subscription data: version=2'"")
+
+                        second_other_monitor.wait_until(""processed 100 records from topic"",
+                                                        timeout_sec=60,
+                                                        err_msg=""Never saw output 'processed 100 records from topic' on"" + str(second_other_node.account))
+                        found = list(second_other_node.account.ssh_capture(grep_metadata_error + second_other_processor.STDERR_FILE, allow_fail=True))
+                        if len(found) > 0:
+                            raise Exception(""Kafka Streams failed with 'unable to decode subscription data: version=2'"")
+
+                        monitor.wait_until(""processed 100 records from topic"",
+                                           timeout_sec=60,
+                                           err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node.account))
\ No newline at end of file
diff --git a/tests/kafkatest/version.py b/tests/kafkatest/version.py
index 7cd489d87ac..df956027509 100644
--- a/tests/kafkatest/version.py
+++ b/tests/kafkatest/version.py
@@ -61,6 +61,7 @@ def get_version(node=None):
         return DEV_BRANCH
 DEV_BRANCH = KafkaVersion(""dev"")
+DEV_VERSION = KafkaVersion(""0.10.2.2-SNAPSHOT"")
 # 0.8.2.X versions
 V_0_8_2_1 = KafkaVersion(""0.8.2.1"")
diff --git a/vagrant/base.sh b/vagrant/base.sh
index 0bb0f30054b..70987c6c135 100755
--- a/vagrant/base.sh
+++ b/vagrant/base.sh
@@ -52,6 +52,8 @@ get_kafka() {
     kafka_dir=/opt/kafka-$version
     url=[link]
+    # the .tgz above does not include the streams test jar hence we need to get it separately
+    url_streams_test=[link]
     if ; then
         pushd /tmp
         curl -O $url
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 12 **
mjsax closed pull request #4761:  KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git a/bin/kafka-run-class.sh b/bin/kafka-run-class.sh
index fe6aefd7321..8e2ba91bf2b 100755
--- a/bin/kafka-run-class.sh
+++ b/bin/kafka-run-class.sh
@@ -73,28 +73,50 @@ do
   fi
 done
-for file in ""$base_dir""/clients/build/libs/kafka-clients*.jar;
-do
-  if should_include_file ""$file""; then
-    CLASSPATH=""$CLASSPATH"":""$file""
-  fi
-done
+if ; then
+  clients_lib_dir=$(dirname $0)/../clients/build/libs
+  streams_lib_dir=$(dirname $0)/../streams/build/libs
+  rocksdb_lib_dir=$(dirname $0)/../streams/build/dependant-libs-${SCALA_VERSION}
+else
+  clients_lib_dir=/opt/kafka-$UPGRADE_KAFKA_STREAMS_TEST_VERSION/libs
+  streams_lib_dir=$clients_lib_dir
+  rocksdb_lib_dir=$streams_lib_dir
+fi
+
-for file in ""$base_dir""/streams/build/libs/kafka-streams*.jar;
+for file in ""$clients_lib_dir""/kafka-clients*.jar;
 do
   if should_include_file ""$file""; then
     CLASSPATH=""$CLASSPATH"":""$file""
   fi
 done
-for file in ""$base_dir""/streams/examples/build/libs/kafka-streams-examples*.jar;
+for file in ""$streams_lib_dir""/kafka-streams*.jar;
 do
   if should_include_file ""$file""; then
     CLASSPATH=""$CLASSPATH"":""$file""
   fi
 done
-for file in ""$base_dir""/streams/build/dependant-libs-${SCALA_VERSION}/rocksdb*.jar;
+if ; then
+  for file in ""$base_dir""/streams/examples/build/libs/kafka-streams-examples*.jar;
+  do
+    if should_include_file ""$file""; then
+      CLASSPATH=""$CLASSPATH"":""$file""
+    fi
+  done
+else
+  VERSION_NO_DOTS=`echo $UPGRADE_KAFKA_STREAMS_TEST_VERSION | sed 's/\.//g'`
+  SHORT_VERSION_NO_DOTS=${VERSION_NO_DOTS:0:((${#VERSION_NO_DOTS} - 1))} # remove last char, ie, bug-fix number
+  for file in ""$base_dir""/streams/upgrade-system-tests-$SHORT_VERSION_NO_DOTS/build/libs/kafka-streams-upgrade-system-tests*.jar;
+  do
+    if should_include_file ""$file""; then
+      CLASSPATH=""$CLASSPATH"":""$file""
+    fi
+  done
+fi
+
+for file in ""$rocksdb_lib_dir""/rocksdb*.jar;
 do
   CLASSPATH=""$CLASSPATH"":""$file""
 done
diff --git a/build.gradle b/build.gradle
index ce4b4e44cb2..17f3e00358d 100644
--- a/build.gradle
+++ b/build.gradle
@@ -909,6 +909,42 @@ project(':streams:examples') {
   }
 }
+project(':streams:upgrade-system-tests-0100') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0100""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0100
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
+project(':streams:upgrade-system-tests-0101') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0101""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0101
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
+project(':streams:upgrade-system-tests-0102') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0102""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0102
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
 project(':jmh-benchmarks') {
   apply plugin: 'com.github.johnrengelman.shadow'
diff --git [file java] [file java]
index 7111bad6054..7102414628a 100644
--- [file java]
+++ [file java]
@@ -16,7 +16,9 @@
  */
 package org.apache.kafka.common.security.authenticator;
-import java.util.Map;
+import org.apache.kafka.common.config.SaslConfigs;
+import org.apache.kafka.common.network.Mode;
+import org.apache.kafka.common.security.auth.AuthCallbackHandler;
 import javax.security.auth.Subject;
 import javax.security.auth.callback.Callback;
@@ -25,10 +27,7 @@
 import javax.security.auth.callback.UnsupportedCallbackException;
 import javax.security.sasl.AuthorizeCallback;
 import javax.security.sasl.RealmCallback;
-
-import org.apache.kafka.common.config.SaslConfigs;
-import org.apache.kafka.common.network.Mode;
-import org.apache.kafka.common.security.auth.AuthCallbackHandler;
+import java.util.Map;
 /**
  * Callback handler for Sasl clients. The callbacks required for the SASL mechanism
diff --git a/docs/streams/upgrade-guide.html b/docs/streams/upgrade-guide.html
index 7f2c9f6cf89..86d6d531a9c 100644
--- a/docs/streams/upgrade-guide.html
+++ b/docs/streams/upgrade-guide.html
@@ -27,16 +27,33 @@ <h1>Upgrade Guide &amp; API Changes</h1>
     </p>
     <p>
-        If you want to upgrade from 0.10.1.x to 0.10.2, see the <a href=""/{{version}}/documentation/#upgrade_1020_streams""><b>Upgrade Section for 0.10.2</b></a>.
+        If you want to upgrade from 0.10.1.x to 0.11.0, see the <a href=""/{{version}}/documentation/#upgrade_1020_streams""><b>Upgrade Section for 0.10.2</b></a>.
         It highlights incompatible changes you need to consider to upgrade your code and application.
-        See <a href=""#streams_api_changes_0102"">below</a> a complete list of 0.10.2 API and semantical changes that allow you to advance your application and/or simplify your code base, including the usage of new features.
+        See below a complete list of <a href=""#streams_api_changes_0102"">0.10.2</a> and <a href=""#streams_api_changes_0110"">0.11.0</a> API and semantical changes
+        that allow you to advance your application and/or simplify your code base, including the usage of new features.
     </p>
     <p>
-        If you want to upgrade from 0.10.0.x to 0.10.1, see the <a href=""/{{version}}/documentation/#upgrade_1010_streams""><b>Upgrade Section for 0.10.1</b></a>.
-        It highlights incompatible changes you need to consider to upgrade your code and application.
-        See <a href=""#streams_api_changes_0101"">below</a> a complete list of 0.10.1 API changes that allow you to advance your application and/or simplify your code base, including the usage of new features.
+        Upgrading from 0.10.0.x to 0.11.0.x directly is also possible.
+        Note, that a brokers must be on version 0.10.1 or higher to run a Kafka Streams application version 0.10.1 or higher.
+        See <a href=""#streams_api_changes_0101"">Streams API changes in 0.10.1</a>, <a href=""#streams_api_changes_0102"">Streams API changes in 0.10.2</a>,
+        and <a href=""#streams_api_changes_0110"">Streams API changes in 0.11.0</a> for a complete list of API changes.
+        Upgrading to 0.11.0.3 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+        (cf. <a href=""[link]"">KIP-268</a>).
+        As an alternative, an offline upgrade is also possible.
     </p>
+    <ul>
+        <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.11.0.3 </li>
+        <li> bounce each instance of your application once </li>
+        <li> prepare your newly deployed 0.11.0.3 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+        <li> bounce each instance of your application once more to complete the upgrade </li>
+    </ul>
+    <p> Upgrading from 0.10.0.x to 0.11.0.0, 0.11.0.1, or 0.11.0.2 requires an offline upgrade (rolling bounce upgrade is not supported) </p>
+    <ul>
+        <li> stop all old (0.10.0.x) application instances </li>
+        <li> update your code and swap old code and jar file with new code and new jar file </li>
+        <li> restart all new (0.11.0.0, 0.11.0.1, or 0.11.0.2) application instances </li>
+    </ul>
     <h3><a id=""streams_api_changes_0110"" href=""#streams_api_changes_0110"">Streams API changes in 0.11.0.0</a></h3>
diff --git a/docs/upgrade.html b/docs/upgrade.html
index 9f0dbdf55fb..06038753189 100644
--- a/docs/upgrade.html
+++ b/docs/upgrade.html
@@ -64,6 +64,12 @@ <h4><a id=""upgrade_11_0_0"" href=""#upgrade_11_0_0"">Upgrading from 0.8.x, 0.9.x, 0
     before you switch to 0.11.0.</li>
 </ol>
+<h5><a id=""upgrade_1103_notable"" href=""#upgrade_1103_notable"">Notable changes in 0.11.0.3</a></h5>
+<ul>
+    <li> New Kafka Streams configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x </li>
+    <li> See the <a href=""/{{version}}/documentation/streams/upgrade-guide.html""><b>Kafka Streams upgrade guide</b></a> for details about this new config.
+</ul>
+
 <h5><a id=""upgrade_1100_notable"" href=""#upgrade_1100_notable"">Notable changes in 0.11.0.0</a></h5>
 <ul>
     <li>Unclean leader election is now disabled by default. The new default favors durability over availability. Users who wish to
@@ -214,14 +220,41 @@ <h5><a id=""upgrade_1020_streams"" href=""#upgrade_1020_streams"">Upgrading a 0.10.1
     <li> See <a href=""/{{version}}/documentation/streams#streams_api_changes_0102"">Streams API changes in 0.10.2</a> for more details. </li>
 </ul>
+<h5><a id=""upgrade_1020_streams_from_0100"" href=""#upgrade_1020_streams_from_0100"">Upgrading a 0.10.0 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.0 to 0.10.2 does require a <a href=""#upgrade_10_1"">broker upgrade</a> because a Kafka Streams 0.10.2 application can only connect to 0.10.2 or 0.10.1 brokers. </li>
+    <li> There are couple of API changes, that are not backward compatible (cf. <a href=""/{{version}}/documentation/streams#streams_api_changes_0102"">Streams API changes in 0.10.2</a> for more details).
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> Upgrading from 0.10.0.x to 0.10.2.2 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+         (cf. <a href=""[link]"">KIP-268</a>).
+         As an alternative, an offline upgrade is also possible.
+        <ul>
+            <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.10.2.2 </li>
+            <li> bounce each instance of your application once </li>
+            <li> prepare your newly deployed 0.10.2.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+            <li> bounce each instance of your application once more to complete the upgrade </li>
+        </ul>
+    </li>
+    <li> Upgrading from 0.10.0.x to 0.10.2.0 or 0.10.2.1 requires an offline upgrade (rolling bounce upgrade is not supported)
+        <ul>
+            <li> stop all old (0.10.0.x) application instances </li>
+            <li> update your code and swap old code and jar file with new code and new jar file </li>
+            <li> restart all new (0.10.2.0 or 0.10.2.1) application instances </li>
+        </ul>
+    </li>
+</ul>
+
+<h5><a id=""upgrade_10202_notable"" href=""#upgrade_10202_notable"">Notable changes in 0.10.2.2</a></h5>
+<ul>
+    <li> New configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x </li>
+</ul>
+
 <h5><a id=""upgrade_10201_notable"" href=""#upgrade_10201_notable"">Notable changes in 0.10.2.1</a></h5>
 <ul>
   <li> The default values for two configurations of the StreamsConfig class were changed to improve the resiliency of Kafka Streams applications. The internal Kafka Streams producer <code>retries</code> default value was changed from 0 to 10. The internal Kafka Streams consumer <code>max.poll.interval.ms</code>  default value was changed from 300000 to <code>Integer.MAX_VALUE</code>.
   </li>
 </ul>
-
-
 <h5><a id=""upgrade_1020_notable"" href=""#upgrade_1020_notable"">Notable changes in 0.10.2.0</a></h5>
 <ul>
     <li>The Java clients (producer and consumer) have acquired the ability to communicate with older brokers. Version 0.10.2 clients
@@ -294,6 +327,23 @@ <h5><a id=""upgrade_1010_streams"" href=""#upgrade_1010_streams"">Upgrading a 0.10.0
     <li> Upgrading your Streams application from 0.10.0 to 0.10.1 does require a <a href=""#upgrade_10_1"">broker upgrade</a> because a Kafka Streams 0.10.1 application can only connect to 0.10.1 brokers. </li>
     <li> There are couple of API changes, that are not backward compatible (cf. <a href=""/{{version}}/documentation/streams#streams_api_changes_0101"">Streams API changes in 0.10.1</a> for more details).
          Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> Upgrading from 0.10.0.x to 0.10.1.2 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+         (cf. <a href=""[link]"">KIP-268</a>).
+         As an alternative, an offline upgrade is also possible.
+        <ul>
+            <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.10.1.2 </li>
+            <li> bounce each instance of your application once </li>
+            <li> prepare your newly deployed 0.10.1.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+            <li> bounce each instance of your application once more to complete the upgrade </li>
+        </ul>
+    </li>
+    <li> Upgrading from 0.10.0.x to 0.10.1.0 or 0.10.1.1 requires an offline upgrade (rolling bounce upgrade is not supported)
+        <ul>
+            <li> stop all old (0.10.0.x) application instances </li>
+            <li> update your code and swap old code and jar file with new code and new jar file </li>
+            <li> restart all new (0.10.1.0 or 0.10.1.1) application instances </li>
+        </ul>
+    </li>
 </ul>
 <h5><a id=""upgrade_1010_notable"" href=""#upgrade_1010_notable"">Notable changes in 0.10.1.0</a></h5>
diff --git a/gradle/dependencies.gradle b/gradle/dependencies.gradle
index 5d145e17ed9..d881353703f 100644
--- a/gradle/dependencies.gradle
+++ b/gradle/dependencies.gradle
@@ -55,6 +55,9 @@ versions += [
   jackson: ""2.8.5"",
   jetty: ""9.2.22.v20170606"",
   jersey: ""2.24"",
+  kafka_0100: ""0.10.0.1"",
+  kafka_0101: ""0.10.1.1"",
+  kafka_0102: ""0.10.2.1"",
   log4j: ""1.2.17"",
   jopt: ""5.0.3"",
   junit: ""4.12"",
@@ -96,6 +99,9 @@ libs += [
   junit: ""junit:junit:$versions.junit"",
   log4j: ""log4j:log4j:$versions.log4j"",
   joptSimple: ""net.sf.jopt-simple:jopt-simple:$versions.jopt"",
+  kafkaStreams_0100: ""org.apache.kafka:kafka-streams:$versions.kafka_0100"",
+  kafkaStreams_0101: ""org.apache.kafka:kafka-streams:$versions.kafka_0101"",
+  kafkaStreams_0102: ""org.apache.kafka:kafka-streams:$versions.kafka_0102"",
   lz4: ""net.jpountz.lz4:lz4:$versions.lz4"",
   metrics: ""com.yammer.metrics:metrics-core:$versions.metrics"",
   powermock: ""org.powermock:powermock-module-junit4:$versions.powermock"",
diff --git a/settings.gradle b/settings.gradle
index f0fdf07128c..769046fe556 100644
--- a/settings.gradle
+++ b/settings.gradle
@@ -13,5 +13,6 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
-include 'core', 'examples', 'clients', 'tools', 'streams', 'streams:examples', 'log4j-appender',
+include 'core', 'examples', 'clients', 'tools', 'streams', 'streams:examples', 'streams:upgrade-system-tests-0100',
+        'streams:upgrade-system-tests-0101', 'streams:upgrade-system-tests-0102', 'log4j-appender',
         'connect:api', 'connect:transforms', 'connect:runtime', 'connect:json', 'connect:file', 'jmh-benchmarks'
diff --git [file java] [file java]
index b411344aaac..d45b1357f0e 100644
--- [file java]
+++ [file java]
@@ -105,6 +105,11 @@
      */
     public static final String PRODUCER_PREFIX = ""producer."";
+    /**
+     * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 0.10.0.x}.
+     */
+    public static final String UPGRADE_FROM_0100 = ""0.10.0"";
+
     /**
      * Config value for parameter {@link #PROCESSING_GUARANTEE_CONFIG ""processing.guarantee""} for at-least-once processing guarantees.
      */
@@ -247,6 +252,11 @@
     public static final String TIMESTAMP_EXTRACTOR_CLASS_CONFIG = ""timestamp.extractor"";
     private static final String TIMESTAMP_EXTRACTOR_CLASS_DOC = ""Timestamp extractor class that implements the <code>TimestampExtractor</code> interface. This config is deprecated, use <code>"" + DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG + ""</code> instead"";
+    /** {@code upgrade.from} */
+    public static final String UPGRADE_FROM_CONFIG = ""upgrade.from"";
+    public static final String UPGRADE_FROM_DOC = ""Allows upgrading from version 0.10.0 to version 0.10.1 (or newer) in a backward compatible way. "" +
+        ""Default is null. Accepted values are \"""" + UPGRADE_FROM_0100 + ""\"" (for upgrading from 0.10.0.x)."";
+
     /**
      * {@code value.serde}
      * @deprecated Use {@link #DEFAULT_VALUE_SERDE_CLASS_CONFIG} instead.
@@ -466,6 +476,12 @@
                     null,
                     Importance.LOW,
                     TIMESTAMP_EXTRACTOR_CLASS_DOC)
+            .define(UPGRADE_FROM_CONFIG,
+                    ConfigDef.Type.STRING,
+                    null,
+                    in(null, UPGRADE_FROM_0100),
+                    Importance.LOW,
+                    UPGRADE_FROM_DOC)
             .define(VALUE_SERDE_CLASS_CONFIG,
                     Type.CLASS,
                     null,
@@ -632,6 +648,7 @@ public StreamsConfig(final Map<?, ?> props) {
         consumerProps.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId + ""-consumer"");
         // add configs required for stream partition assignor
+        consumerProps.put(UPGRADE_FROM_CONFIG, getString(UPGRADE_FROM_CONFIG));
         consumerProps.put(InternalConfig.STREAM_THREAD_INSTANCE, streamThread);
         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));
         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));
diff --git [file java] [file java]
index 0a1b2ab76cb..6e2bfa67bac 100644
--- [file java]
+++ [file java]
@@ -165,6 +165,8 @@ public int compare(TopicPartition p1, TopicPartition p2) {
     private String userEndPoint;
     private int numStandbyReplicas;
+    private int userMetadataVersion = SubscriptionInfo.CURRENT_VERSION;
+
     private Cluster metadataWithInternalTopics;
     private Map<HostInfo, Set<TopicPartition>> partitionsByHostState;
@@ -192,6 +194,12 @@ void time(final Time time) {
     public void configure(Map<String, ?> configs) {
         numStandbyReplicas = (Integer) configs.get(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG);
+        final String upgradeMode = (String) configs.get(StreamsConfig.UPGRADE_FROM_CONFIG);
+        if (StreamsConfig.UPGRADE_FROM_0100.equals(upgradeMode)) {
+            log.info(""Downgrading metadata version from 2 to 1 for upgrade from 0.10.0.x."");
+            userMetadataVersion = 1;
+        }
+
         Object o = configs.get(StreamsConfig.InternalConfig.STREAM_THREAD_INSTANCE);
         if (o == null) {
             KafkaException ex = new KafkaException(""StreamThread is not specified"");
@@ -251,7 +259,7 @@ public Subscription subscription(Set<String> topics) {
         final Set<TaskId> previousActiveTasks = streamThread.prevActiveTasks();
         Set<TaskId> standbyTasks = streamThread.cachedTasks();
         standbyTasks.removeAll(previousActiveTasks);
-        SubscriptionInfo data = new SubscriptionInfo(streamThread.processId, previousActiveTasks, standbyTasks, this.userEndPoint);
+        SubscriptionInfo data = new SubscriptionInfo(userMetadataVersion, streamThread.processId, previousActiveTasks, standbyTasks, this.userEndPoint);
         if (streamThread.builder.sourceTopicPattern() != null &&
             !streamThread.builder.subscriptionUpdates().getUpdates().equals(topics)) {
@@ -295,11 +303,16 @@ private void updateSubscribedTopics(Set<String> topics) {
         // construct the client metadata from the decoded subscription info
         Map<UUID, ClientMetadata> clientsMetadata = new HashMap<>();
+        int minUserMetadataVersion = SubscriptionInfo.CURRENT_VERSION;
         for (Map.Entry<String, Subscription> entry : subscriptions.entrySet()) {
             String consumerId = entry.getKey();
             Subscription subscription = entry.getValue();
             SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());
+            final int usedVersion = info.version;
+            if (usedVersion < minUserMetadataVersion) {
+                minUserMetadataVersion = usedVersion;
+            }
             // create the new client metadata if necessary
             ClientMetadata clientMetadata = clientsMetadata.get(info.processId);
@@ -556,7 +569,7 @@ private void updateSubscribedTopics(Set<String> topics) {
                 }
                 // finally, encode the assignment before sending back to coordinator
-                assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(active, standby, partitionsByHostState).encode()));
+                assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(minUserMetadataVersion, active, standby, partitionsByHostState).encode()));
                 i++;
             }
         }
diff --git [file java] [file java]
index 77fb58a113c..5409976d686 100644
--- [file java]
+++ [file java]
@@ -55,7 +55,7 @@ public AssignmentInfo(List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>>
         this(CURRENT_VERSION, activeTasks, standbyTasks, hostState);
     }
-    protected AssignmentInfo(int version, List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>> standbyTasks,
+    public AssignmentInfo(int version, List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>> standbyTasks,
                              Map<HostInfo, Set<TopicPartition>> hostState) {
         this.version = version;
         this.activeTasks = activeTasks;
@@ -154,9 +154,7 @@ public static AssignmentInfo decode(ByteBuffer data) {
                 }
             }
-            return new AssignmentInfo(activeTasks, standbyTasks, hostStateToTopicPartitions);
-
-
+            return new AssignmentInfo(version, activeTasks, standbyTasks, hostStateToTopicPartitions);
         } catch (IOException ex) {
             throw new TaskAssignmentException(""Failed to decode AssignmentInfo"", ex);
         }
diff --git [file java] [file java]
index f583dbafc94..00227e799b8 100644
--- [file java]
+++ [file java]
@@ -31,7 +31,7 @@
     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);
-    private static final int CURRENT_VERSION = 2;
+    public static final int CURRENT_VERSION = 2;
     public final int version;
     public final UUID processId;
@@ -43,7 +43,7 @@ public SubscriptionInfo(UUID processId, Set<TaskId> prevTasks, Set<TaskId> stand
         this(CURRENT_VERSION, processId, prevTasks, standbyTasks, userEndPoint);
     }
-    private SubscriptionInfo(int version, UUID processId, Set<TaskId> prevTasks, Set<TaskId> standbyTasks, String userEndPoint) {
+    public SubscriptionInfo(int version, UUID processId, Set<TaskId> prevTasks, Set<TaskId> standbyTasks, String userEndPoint) {
         this.version = version;
         this.processId = processId;
         this.prevTasks = prevTasks;
diff --git [file java] [file java]
index 3bbd69ea4d0..9998283928c 100644
--- [file java]
+++ [file java]
@@ -82,7 +82,7 @@ public void shouldThrowExceptionIfBootstrapServersIsNotSet() {
     }
     @Test
-    public void testGetProducerConfigs() throws Exception {
+    public void testGetProducerConfigs() {
         final String clientId = ""client"";
         final Map<String, Object> returnedProps = streamsConfig.getProducerConfigs(clientId);
         assertEquals(returnedProps.get(ProducerConfig.CLIENT_ID_CONFIG), clientId + ""-producer"");
@@ -91,7 +91,7 @@ public void testGetProducerConfigs() throws Exception {
     }
     @Test
-    public void testGetConsumerConfigs() throws Exception {
+    public void testGetConsumerConfigs() {
         final String groupId = ""example-application"";
         final String clientId = ""client"";
         final Map<String, Object> returnedProps = streamsConfig.getConsumerConfigs(null, groupId, clientId);
@@ -102,7 +102,7 @@ public void testGetConsumerConfigs() throws Exception {
     }
     @Test
-    public void testGetRestoreConsumerConfigs() throws Exception {
+    public void testGetRestoreConsumerConfigs() {
         final String clientId = ""client"";
         final Map<String, Object> returnedProps = streamsConfig.getRestoreConsumerConfigs(clientId);
         assertEquals(returnedProps.get(ConsumerConfig.CLIENT_ID_CONFIG), clientId + ""-restore-consumer"");
@@ -143,7 +143,7 @@ public void shouldSupportMultipleBootstrapServers() {
     }
     @Test
-    public void shouldSupportPrefixedConsumerConfigs() throws Exception {
+    public void shouldSupportPrefixedConsumerConfigs() {
         props.put(consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), ""earliest"");
         props.put(consumerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -153,7 +153,7 @@ public void shouldSupportPrefixedConsumerConfigs() throws Exception {
     }
     @Test
-    public void shouldSupportPrefixedRestoreConsumerConfigs() throws Exception {
+    public void shouldSupportPrefixedRestoreConsumerConfigs() {
         props.put(consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), ""earliest"");
         props.put(consumerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -163,7 +163,7 @@ public void shouldSupportPrefixedRestoreConsumerConfigs() throws Exception {
     }
     @Test
-    public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() throws Exception {
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() {
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         props.put(consumerPrefix(""interceptor.statsd.host""), ""host"");
         final Map<String, Object> consumerConfigs = streamsConfig.getConsumerConfigs(null, ""groupId"", ""clientId"");
@@ -171,7 +171,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() thro
     }
     @Test
-    public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig() throws Exception {
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig() {
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         props.put(consumerPrefix(""interceptor.statsd.host""), ""host"");
         final Map<String, Object> consumerConfigs = streamsConfig.getRestoreConsumerConfigs(""clientId"");
@@ -179,7 +179,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig
     }
     @Test
-    public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() throws Exception {
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() {
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         props.put(producerPrefix(""interceptor.statsd.host""), ""host"");
         final Map<String, Object> producerConfigs = streamsConfig.getProducerConfigs(""clientId"");
@@ -188,7 +188,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() thro
     @Test
-    public void shouldSupportPrefixedProducerConfigs() throws Exception {
+    public void shouldSupportPrefixedProducerConfigs() {
         props.put(producerPrefix(ProducerConfig.BUFFER_MEMORY_CONFIG), 10);
         props.put(producerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -198,7 +198,7 @@ public void shouldSupportPrefixedProducerConfigs() throws Exception {
     }
     @Test
-    public void shouldBeSupportNonPrefixedConsumerConfigs() throws Exception {
+    public void shouldBeSupportNonPrefixedConsumerConfigs() {
         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -208,7 +208,7 @@ public void shouldBeSupportNonPrefixedConsumerConfigs() throws Exception {
     }
     @Test
-    public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() throws Exception {
+    public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() {
         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -218,7 +218,7 @@ public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() throws Exception
     }
     @Test
-    public void shouldSupportNonPrefixedProducerConfigs() throws Exception {
+    public void shouldSupportNonPrefixedProducerConfigs() {
         props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 10);
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -227,24 +227,22 @@ public void shouldSupportNonPrefixedProducerConfigs() throws Exception {
         assertEquals(1, configs.get(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG));
     }
-
-
     @Test(expected = StreamsException.class)
-    public void shouldThrowStreamsExceptionIfKeySerdeConfigFails() throws Exception {
+    public void shouldThrowStreamsExceptionIfKeySerdeConfigFails() {
         props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, MisconfiguredSerde.class);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.defaultKeySerde();
     }
     @Test(expected = StreamsException.class)
-    public void shouldThrowStreamsExceptionIfValueSerdeConfigFails() throws Exception {
+    public void shouldThrowStreamsExceptionIfValueSerdeConfigFails() {
         props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, MisconfiguredSerde.class);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.defaultValueSerde();
     }
     @Test
-    public void shouldOverrideStreamsDefaultConsumerConfigs() throws Exception {
+    public void shouldOverrideStreamsDefaultConsumerConfigs() {
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), ""latest"");
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), ""10"");
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -254,7 +252,7 @@ public void shouldOverrideStreamsDefaultConsumerConfigs() throws Exception {
     }
     @Test
-    public void shouldOverrideStreamsDefaultProducerConfigs() throws Exception {
+    public void shouldOverrideStreamsDefaultProducerConfigs() {
         props.put(StreamsConfig.producerPrefix(ProducerConfig.LINGER_MS_CONFIG), ""10000"");
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         final Map<String, Object> producerConfigs = streamsConfig.getProducerConfigs(""clientId"");
@@ -262,7 +260,7 @@ public void shouldOverrideStreamsDefaultProducerConfigs() throws Exception {
     }
     @Test
-    public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() throws Exception {
+    public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() {
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), ""latest"");
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), ""10"");
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -272,21 +270,21 @@ public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() throw
     }
     @Test(expected = ConfigException.class)
-    public void shouldThrowExceptionIfConsumerAutoCommitIsOverridden() throws Exception {
+    public void shouldThrowExceptionIfConsumerAutoCommitIsOverridden() {
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG), ""true"");
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.getConsumerConfigs(null, ""a"", ""b"");
     }
     @Test(expected = ConfigException.class)
-    public void shouldThrowExceptionIfRestoreConsumerAutoCommitIsOverridden() throws Exception {
+    public void shouldThrowExceptionIfRestoreConsumerAutoCommitIsOverridden() {
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG), ""true"");
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.getRestoreConsumerConfigs(""client"");
     }
     @Test
-    public void shouldSetInternalLeaveGroupOnCloseConfigToFalseInConsumer() throws Exception {
+    public void shouldSetInternalLeaveGroupOnCloseConfigToFalseInConsumer() {
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         final Map<String, Object> consumerConfigs = streamsConfig.getConsumerConfigs(null, ""groupId"", ""clientId"");
         assertThat(consumerConfigs.get(""internal.leave.group.on.close""), CoreMatchers.<Object>equalTo(false));
@@ -395,6 +393,7 @@ public void shouldNotOverrideUserConfigCommitIntervalMsIfExactlyOnceEnabled() {
         assertThat(streamsConfig.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG), equalTo(commitIntervalMs));
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldBeBackwardsCompatibleWithDeprecatedConfigs() {
         final Properties props = minimalStreamsConfig();
@@ -429,6 +428,7 @@ public void shouldUseCorrectDefaultsWhenNoneSpecified() {
         assertTrue(config.defaultTimestampExtractor() instanceof FailOnInvalidTimestamp);
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldSpecifyCorrectKeySerdeClassOnErrorUsingDeprecatedConfigs() {
         final Properties props = minimalStreamsConfig();
@@ -442,6 +442,7 @@ public void shouldSpecifyCorrectKeySerdeClassOnErrorUsingDeprecatedConfigs() {
         }
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldSpecifyCorrectKeySerdeClassOnError() {
         final Properties props = minimalStreamsConfig();
@@ -455,6 +456,7 @@ public void shouldSpecifyCorrectKeySerdeClassOnError() {
         }
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldSpecifyCorrectValueSerdeClassOnErrorUsingDeprecatedConfigs() {
         final Properties props = minimalStreamsConfig();
@@ -468,6 +470,7 @@ public void shouldSpecifyCorrectValueSerdeClassOnErrorUsingDeprecatedConfigs() {
         }
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldSpecifyCorrectValueSerdeClassOnError() {
         final Properties props = minimalStreamsConfig();
diff --git [file java] [file java]
index 372b89c048e..bb4b5758cfb 100644
--- [file java]
+++ [file java]
@@ -16,6 +16,7 @@
  */
 package org.apache.kafka.streams.integration;
+import kafka.utils.MockTime;
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.kafka.common.serialization.Deserializer;
 import org.apache.kafka.common.serialization.IntegerSerializer;
@@ -51,7 +52,6 @@
 import java.util.Properties;
 import java.util.concurrent.ExecutionException;
-import kafka.utils.MockTime;
 import org.junit.experimental.categories.Category;
 import static org.hamcrest.MatcherAssert.assertThat;
@@ -315,6 +315,4 @@ private void startStreams() {
     }
-
-
 }
diff --git [file java] [file java]
index 98cd20a3527..a29380fecd0 100644
--- [file java]
+++ [file java]
@@ -135,7 +135,7 @@ public void setUp() {
     @SuppressWarnings(""unchecked"")
     @Test
-    public void testSubscription() throws Exception {
+    public void testSubscription() {
         builder.addSource(""source1"", ""topic1"");
         builder.addSource(""source2"", ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
@@ -187,7 +187,7 @@ public void testSubscription() throws Exception {
     }
     @Test
-    public void testAssignBasic() throws Exception {
+    public void testAssignBasic() {
         builder.addSource(""source1"", ""topic1"");
         builder.addSource(""source2"", ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
@@ -239,12 +239,10 @@ public void testAssignBasic() throws Exception {
         assertEquals(Utils.mkSet(t1p2, t2p2), new HashSet<>(assignments.get(""consumer20"").partitions()));
         // check assignment info
-
-        Set<TaskId> allActiveTasks = new HashSet<>();
+        AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(""consumer10""));
         // the first consumer
-        AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
+        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks);
         // the second consumer
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(""consumer11""));
@@ -264,7 +262,7 @@ public void testAssignBasic() throws Exception {
     }
     @Test
-    public void testAssignWithPartialTopology() throws Exception {
+    public void testAssignWithPartialTopology() {
         Properties props = configProps();
         props.put(StreamsConfig.PARTITION_GROUPER_CLASS_CONFIG, SingleGroupPartitionGrouperStub.class);
         StreamsConfig config = new StreamsConfig(props);
@@ -306,9 +304,8 @@ public void testAssignWithPartialTopology() throws Exception {
         Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
         // check assignment info
-        Set<TaskId> allActiveTasks = new HashSet<>();
         AssignmentInfo info10 = checkAssignment(Utils.mkSet(""topic1""), assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
+        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks);
         assertEquals(3, allActiveTasks.size());
         assertEquals(allTasks, new HashSet<>(allActiveTasks));
@@ -316,7 +313,7 @@ public void testAssignWithPartialTopology() throws Exception {
     @Test
-    public void testAssignEmptyMetadata() throws Exception {
+    public void testAssignEmptyMetadata() {
         builder.addSource(""source1"", ""topic1"");
         builder.addSource(""source2"", ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
@@ -359,9 +356,8 @@ public void testAssignEmptyMetadata() throws Exception {
             new HashSet<>(assignments.get(""consumer10"").partitions()));
         // check assignment info
-        Set<TaskId> allActiveTasks = new HashSet<>();
         AssignmentInfo info10 = checkAssignment(Collections.<String>emptySet(), assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
+        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks);
         assertEquals(0, allActiveTasks.size());
         assertEquals(Collections.<TaskId>emptySet(), new HashSet<>(allActiveTasks));
@@ -384,7 +380,7 @@ public void testAssignEmptyMetadata() throws Exception {
     }
     @Test
-    public void testAssignWithNewTasks() throws Exception {
+    public void testAssignWithNewTasks() {
         builder.addSource(""source1"", ""topic1"");
         builder.addSource(""source2"", ""topic2"");
         builder.addSource(""source3"", ""topic3"");
@@ -430,13 +426,9 @@ public void testAssignWithNewTasks() throws Exception {
         // check assigned partitions: since there is no previous task for topic 3 it will be assigned randomly so we cannot check exact match
         // also note that previously assigned partitions / tasks may not stay on the previous host since we may assign the new task first and
         // then later ones will be re-assigned to other hosts due to load balancing
-        Set<TaskId> allActiveTasks = new HashSet<>();
-        Set<TopicPartition> allPartitions = new HashSet<>();
-        AssignmentInfo info;
-
-        info = AssignmentInfo.decode(assignments.get(""consumer10"").userData());
-        allActiveTasks.addAll(info.activeTasks);
-        allPartitions.addAll(assignments.get(""consumer10"").partitions());
+        AssignmentInfo info = AssignmentInfo.decode(assignments.get(""consumer10"").userData());
+        Set<TaskId> allActiveTasks = new HashSet<>(info.activeTasks);
+        Set<TopicPartition> allPartitions = new HashSet<>(assignments.get(""consumer10"").partitions());
         info = AssignmentInfo.decode(assignments.get(""consumer11"").userData());
         allActiveTasks.addAll(info.activeTasks);
@@ -451,7 +443,7 @@ public void testAssignWithNewTasks() throws Exception {
     }
     @Test
-    public void testAssignWithStates() throws Exception {
+    public void testAssignWithStates() {
         String applicationId = ""test"";
         builder.setApplicationId(applicationId);
         builder.addSource(""source1"", ""topic1"");
@@ -551,7 +543,7 @@ public void testAssignWithStates() throws Exception {
     }
     @Test
-    public void testAssignWithStandbyReplicas() throws Exception {
+    public void testAssignWithStandbyReplicas() {
         Properties props = configProps();
         props.setProperty(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, ""1"");
         StreamsConfig config = new StreamsConfig(props);
@@ -600,13 +592,10 @@ public void testAssignWithStandbyReplicas() throws Exception {
         Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
-        Set<TaskId> allActiveTasks = new HashSet<>();
-        Set<TaskId> allStandbyTasks = new HashSet<>();
-
         // the first consumer
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
-        allStandbyTasks.addAll(info10.standbyTasks.keySet());
+        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks);
+        Set<TaskId> allStandbyTasks = new HashSet<>(info10.standbyTasks.keySet());
         // the second consumer
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(""consumer11""));
@@ -634,7 +623,7 @@ public void testAssignWithStandbyReplicas() throws Exception {
     }
     @Test
-    public void testOnAssignment() throws Exception {
+    public void testOnAssignment() {
         TopicPartition t2p3 = new TopicPartition(""topic2"", 3);
         TopologyBuilder builder = new TopologyBuilder();
@@ -677,7 +666,7 @@ public void testOnAssignment() throws Exception {
     }
     @Test
-    public void testAssignWithInternalTopics() throws Exception {
+    public void testAssignWithInternalTopics() {
         String applicationId = ""test"";
         builder.setApplicationId(applicationId);
         builder.addInternalTopic(""topicX"");
@@ -722,7 +711,7 @@ public void testAssignWithInternalTopics() throws Exception {
     }
     @Test
-    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throws Exception {
+    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() {
         String applicationId = ""test"";
         builder.setApplicationId(applicationId);
         builder.addInternalTopic(""topicX"");
@@ -760,7 +749,7 @@ public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throw
     }
     @Test
-    public void shouldAddUserDefinedEndPointToSubscription() throws Exception {
+    public void shouldAddUserDefinedEndPointToSubscription() {
         final Properties properties = configProps();
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, ""localhost:8080"");
         final StreamsConfig config = new StreamsConfig(properties);
@@ -773,8 +762,8 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {
         final UUID uuid1 = UUID.randomUUID();
         final String client1 = ""client1"";
-        final StreamThread streamThread = new StreamThread(builder, config, mockClientSupplier, applicationId, client1, uuid1, new Metrics(), Time.SYSTEM, new StreamsMetadataState(builder, StreamsMetadataState.UNKNOWN_HOST),
-                                                           0, stateDirectory);
+        final StreamThread streamThread = new StreamThread(builder, config, mockClientSupplier, applicationId, client1,
+            uuid1, new Metrics(), Time.SYSTEM, new StreamsMetadataState(builder, StreamsMetadataState.UNKNOWN_HOST), 0, stateDirectory);
         partitionAssignor.configure(config.getConsumerConfigs(streamThread, applicationId, client1));
         final PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""input""));
@@ -783,7 +772,82 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {
     }
     @Test
-    public void shouldMapUserEndPointToTopicPartitions() throws Exception {
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() {
+        final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
+        final Set<TaskId> emptyTasks = Collections.emptySet();
+        subscriptions.put(
+            ""consumer1"",
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(""topic1""),
+                new SubscriptionInfo(1, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+        subscriptions.put(
+            ""consumer2"",
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(""topic1""),
+                new SubscriptionInfo(2, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+
+        final StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
+        StreamsConfig config = new StreamsConfig(configProps());
+
+        final TopologyBuilder builder = new TopologyBuilder();
+        final StreamThread streamThread = new StreamThread(
+            builder,
+            config,
+            mockClientSupplier,
+            ""appId"",
+            ""clientId"",
+            UUID.randomUUID(),
+            new Metrics(),
+            Time.SYSTEM,
+            new StreamsMetadataState(builder, StreamsMetadataState.UNKNOWN_HOST),
+            0,
+            stateDirectory);
+
+        partitionAssignor.configure(config.getConsumerConfigs(streamThread, ""test"", ""clientId""));
+        final Map<String, PartitionAssignor.Assignment> assignment = partitionAssignor.assign(metadata, subscriptions);
+
+        assertEquals(2, assignment.size());
+        assertEquals(1, AssignmentInfo.decode(assignment.get(""consumer1"").userData()).version);
+        assertEquals(1, AssignmentInfo.decode(assignment.get(""consumer2"").userData()).version);
+    }
+
+    @Test
+    public void shouldDownGradeSubscription() {
+        final Properties properties = configProps();
+        properties.put(StreamsConfig.UPGRADE_FROM_CONFIG, StreamsConfig.UPGRADE_FROM_0100);
+        StreamsConfig config = new StreamsConfig(properties);
+
+        TopologyBuilder builder = new TopologyBuilder();
+        builder.addSource(""source1"", ""topic1"");
+
+        String clientId = ""client-id"";
+        final StreamThread streamThread = new StreamThread(
+            builder,
+            config,
+            mockClientSupplier,
+            ""appId"",
+            ""clientId"",
+            UUID.randomUUID(),
+            new Metrics(),
+            Time.SYSTEM,
+            new StreamsMetadataState(builder, StreamsMetadataState.UNKNOWN_HOST),
+            0,
+            stateDirectory);
+
+        StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
+        partitionAssignor.configure(config.getConsumerConfigs(streamThread, ""test"", clientId));
+
+        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""topic1""));
+
+        assertEquals(1, SubscriptionInfo.decode(subscription.userData()).version);
+    }
+
+    @Test
+    public void shouldMapUserEndPointToTopicPartitions() {
         final Properties properties = configProps();
         final String myEndPoint = ""localhost:8080"";
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);
@@ -831,7 +895,7 @@ public void shouldMapUserEndPointToTopicPartitions() throws Exception {
     }
     @Test
-    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() throws Exception {
+    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() {
         final Properties properties = configProps();
         final String myEndPoint = ""localhost"";
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);
@@ -865,7 +929,7 @@ public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() thr
     }
     @Test
-    public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() throws Exception {
+    public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() {
         final Properties properties = configProps();
         final String myEndPoint = ""localhost:j87yhk"";
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);
@@ -897,7 +961,7 @@ public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() th
     }
     @Test
-    public void shouldExposeHostStateToTopicPartitionsOnAssignment() throws Exception {
+    public void shouldExposeHostStateToTopicPartitionsOnAssignment() {
         List<TopicPartition> topic = Collections.singletonList(new TopicPartition(""topic"", 0));
         final Map<HostInfo, Set<TopicPartition>> hostState =
                 Collections.singletonMap(new HostInfo(""localhost"", 80),
@@ -910,7 +974,7 @@ public void shouldExposeHostStateToTopicPartitionsOnAssignment() throws Exceptio
     }
     @Test
-    public void shouldSetClusterMetadataOnAssignment() throws Exception {
+    public void shouldSetClusterMetadataOnAssignment() {
         final List<TopicPartition> topic = Collections.singletonList(new TopicPartition(""topic"", 0));
         final Map<HostInfo, Set<TopicPartition>> hostState =
                 Collections.singletonMap(new HostInfo(""localhost"", 80),
@@ -930,7 +994,7 @@ public void shouldSetClusterMetadataOnAssignment() throws Exception {
     }
     @Test
-    public void shouldReturnEmptyClusterMetadataIfItHasntBeenBuilt() throws Exception {
+    public void shouldReturnEmptyClusterMetadataIfItHasntBeenBuilt() {
         final Cluster cluster = partitionAssignor.clusterMetadata();
         assertNotNull(cluster);
     }
@@ -1039,11 +1103,11 @@ public Object apply(Object value1, Object value2) {
             new TopicPartition(applicationId + ""-count-repartition"", 1),
             new TopicPartition(applicationId + ""-count-repartition"", 2)
         );
-        assertThat(new HashSet(assignment.get(client).partitions()), equalTo(new HashSet(expectedAssignment)));
+        assertThat(new HashSet<>(assignment.get(client).partitions()), equalTo(new HashSet<>(expectedAssignment)));
     }
     @Test
-    public void shouldUpdatePartitionHostInfoMapOnAssignment() throws Exception {
+    public void shouldUpdatePartitionHostInfoMapOnAssignment() {
         final TopicPartition partitionOne = new TopicPartition(""topic"", 1);
         final TopicPartition partitionTwo = new TopicPartition(""topic"", 2);
         final Map<HostInfo, Set<TopicPartition>> firstHostState = Collections.singletonMap(
@@ -1060,7 +1124,7 @@ public void shouldUpdatePartitionHostInfoMapOnAssignment() throws Exception {
     }
     @Test
-    public void shouldUpdateClusterMetadataOnAssignment() throws Exception {
+    public void shouldUpdateClusterMetadataOnAssignment() {
         final TopicPartition topicOne = new TopicPartition(""topic"", 1);
         final TopicPartition topicTwo = new TopicPartition(""topic2"", 2);
         final Map<HostInfo, Set<TopicPartition>> firstHostState = Collections.singletonMap(
@@ -1076,7 +1140,7 @@ public void shouldUpdateClusterMetadataOnAssignment() throws Exception {
     }
     @Test
-    public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() throws Exception {
+    public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() {
         final Properties props = configProps();
         props.setProperty(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, ""1"");
         final StreamsConfig config = new StreamsConfig(props);
@@ -1135,12 +1199,12 @@ public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() throws Except
     }
     @Test(expected = KafkaException.class)
-    public void shouldThrowKafkaExceptionIfStreamThreadNotConfigured() throws Exception {
+    public void shouldThrowKafkaExceptionIfStreamThreadNotConfigured() {
         partitionAssignor.configure(Collections.singletonMap(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1));
     }
     @Test(expected = KafkaException.class)
-    public void shouldThrowKafkaExceptionIfStreamThreadConfigIsNotStreamThreadInstance() throws Exception {
+    public void shouldThrowKafkaExceptionIfStreamThreadConfigIsNotStreamThreadInstance() {
         final Map<String, Object> config = new HashMap<>();
         config.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);
         config.put(StreamsConfig.InternalConfig.STREAM_THREAD_INSTANCE, ""i am not a stream thread"");
diff --git [file java] [file java]
index 9473a4027c4..361dde87776 100644
--- [file java]
+++ [file java]
@@ -64,10 +64,9 @@ public void shouldDecodePreviousVersion() throws Exception {
         assertEquals(oldVersion.activeTasks, decoded.activeTasks);
         assertEquals(oldVersion.standbyTasks, decoded.standbyTasks);
         assertEquals(0, decoded.partitionsByHost.size()); // should be empty as wasn't in V1
-        assertEquals(2, decoded.version); // automatically upgraded to v2 on decode;
+        assertEquals(1, decoded.version);
     }
-
     /**
      * This is a clone of what the V1 encoding did. The encode method has changed for V2
      * so it is impossible to test compatibility without having this
diff --git [file java] [file java]
index 11e1ae86fc2..303061541f3 100644
--- [file java]
+++ [file java]
@@ -77,7 +77,6 @@ int next() {
     // This main() is not used by the system test. It is intended to be used for local debugging.
     public static void main(String args) throws Exception {
         final String kafka = ""localhost:9092"";
-        final String zookeeper = ""localhost:2181"";
         final File stateDir = TestUtils.tempDirectory();
         final int numKeys = 20;
@@ -131,42 +130,50 @@ public void run() {
     }
     public static Map<String, Set<Integer>> generate(String kafka, final int numKeys, final int maxRecordsPerKey) throws Exception {
+        return generate(kafka, numKeys, maxRecordsPerKey, true);
+    }
+    public static Map<String, Set<Integer>> generate(final String kafka,
+                                                     final int numKeys,
+                                                     final int maxRecordsPerKey,
+                                                     final boolean autoTerminate) throws Exception {
         final Properties producerProps = new Properties();
         producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, ""SmokeTest"");
         producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
         producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
-        // the next 4 config values make sure that all records are produced with no loss and
-        // no duplicates
+        // the next 2 config values make sure that all records are produced with no loss and no duplicates
         producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);
         producerProps.put(ProducerConfig.ACKS_CONFIG, ""all"");
-        KafkaProducer<byte, byte> producer = new KafkaProducer<>(producerProps);
+        final KafkaProducer<byte, byte> producer = new KafkaProducer<>(producerProps);
         int numRecordsProduced = 0;
-        Map<String, Set<Integer>> allData = new HashMap<>();
-        ValueList data = new ValueList;
+        final Map<String, Set<Integer>> allData = new HashMap<>();
+        final ValueList data = new ValueList;
         for (int i = 0; i < numKeys; i++) {
             data = new ValueList(i, i + maxRecordsPerKey - 1);
             allData.put(data.key, new HashSet<Integer>());
         }
-        Random rand = new Random();
+        final Random rand = new Random();
-        int remaining = data.length;
+        int remaining = 1; // dummy value must be positive if <autoTerminate> is false
+        if (autoTerminate) {
+            remaining = data.length;
+        }
         while (remaining > 0) {
-            int index = rand.nextInt(remaining);
-            String key = data.key;
+            final int index = autoTerminate ? rand.nextInt(remaining) : rand.nextInt(numKeys);
+            final String key = data.key;
             int value = data.next();
-            if (value < 0) {
+            if (autoTerminate && value < 0) {
                 remaining--;
                 data = data;
             } else {
-                ProducerRecord<byte, byte> record =
-                        new ProducerRecord<>(""data"", stringSerde.serializer().serialize("""", key), intSerde.serializer().serialize("""", value));
+                final ProducerRecord<byte, byte> record =
+                    new ProducerRecord<>(""data"", stringSerde.serializer().serialize("""", key), intSerde.serializer().serialize("""", value));
                 producer.send(record, new Callback() {
                     @Override
@@ -178,11 +185,12 @@ public void onCompletion(final RecordMetadata metadata, final Exception exceptio
                     }
                 });
-
                 numRecordsProduced++;
                 allData.get(key).add(value);
-                if (numRecordsProduced % 100 == 0)
+
+                if (numRecordsProduced % 100 == 0) {
                     System.out.println(numRecordsProduced + "" records produced"");
+                }
                 Utils.sleep(2);
             }
diff --git [file java] [file java]
index 150ec7d0c26..11845b4024c 100644
--- [file java]
+++ [file java]
@@ -44,20 +44,15 @@
             public Processor<Object, Object> get() {
                 return new AbstractProcessor<Object, Object>() {
                     private int numRecordsProcessed = 0;
-                    private ProcessorContext context;
                     @Override
                     public void init(final ProcessorContext context) {
                         System.out.println(""initializing processor: topic="" + topic + "" taskId="" + context.taskId());
                         numRecordsProcessed = 0;
-                        this.context = context;
                     }
                     @Override
                     public void process(final Object key, final Object value) {
-                        if (printOffset) {
-                            System.out.println("">>> "" + context.offset());
-                        }
                         numRecordsProcessed++;
                         if (numRecordsProcessed % 100 == 0) {
                             System.out.println(""processed "" + numRecordsProcessed + "" records from topic="" + topic);
@@ -65,10 +60,10 @@ public void process(final Object key, final Object value) {
                     }
                     @Override
-                    public void punctuate(final long timestamp) { }
+                    public void punctuate(final long timestamp) {}
                     @Override
-                    public void close() { }
+                    public void close() {}
                 };
             }
         };
diff --git [file java] [file java]
index 244aa8eef6e..699aaeba287 100644
--- [file java]
+++ [file java]
@@ -23,7 +23,7 @@
 public class StreamsSmokeTest {
     /**
-     *  args ::= command kafka zookeeper stateDir
+     *  args ::= command kafka zookeeper stateDir disableAutoTerminate
      *  command := ""run"" | ""process""
      *
      * @param args
@@ -32,11 +32,13 @@ public static void main(String args) throws Exception {
         String kafka = args;
         String stateDir = args.length > 1 ? args : null;
         String command = args.length > 2 ? args : null;
+        boolean disableAutoTerminate = args.length > 3;
-        System.out.println(""StreamsTest instance started"");
+        System.out.println(""StreamsTest instance started (StreamsSmokeTest)"");
         System.out.println(""command="" + command);
         System.out.println(""kafka="" + kafka);
         System.out.println(""stateDir="" + stateDir);
+        System.out.println(""disableAutoTerminate="" + disableAutoTerminate);
         switch (command) {
             case ""standalone"":
@@ -46,8 +48,12 @@ public static void main(String args) throws Exception {
                 // this starts the driver (data generation and result verification)
                 final int numKeys = 10;
                 final int maxRecordsPerKey = 500;
-                Map<String, Set<Integer>> allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
-                SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                if (disableAutoTerminate) {
+                    SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey, false);
+                } else {
+                    Map<String, Set<Integer>> allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
+                    SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                }
                 break;
             case ""process"":
                 // this starts a KafkaStreams client
diff --git [file java] [file java]
new file mode 100644
index 00000000000..0ee47e416ff
--- /dev/null
+++ [file java]
@@ -0,0 +1,73 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 2) {
+            System.err.println(""StreamsUpgradeTest requires two argument (kafka-url, state-dir, ) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args : """"));
+        }
+        final String kafka = args;
+        final String stateDir = args;
+        final String upgradeFrom = args.length > 2 ? args : null;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest trunk)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""stateDir="" + stateDir);
+        System.out.println(""upgradeFrom="" + upgradeFrom);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(SmokeTestUtil.printProcessorSupplier(""data""));
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        if (upgradeFrom != null) {
+            config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+        }
+
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..72d7f5a7b04
--- /dev/null
+++ [file java]
@@ -0,0 +1,104 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 3) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args + "" "" : """")
+                + (args.length > 1 ? args : """"));
+        }
+        final String kafka = args;
+        final String zookeeper = args;
+        final String stateDir = args;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.10.0)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""zookeeper="" + zookeeper);
+        System.out.println(""stateDir="" + stateDir);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..eebd0fab83c
--- /dev/null
+++ [file java]
@@ -0,0 +1,114 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    /**
+     * This test cannot be run executed, as long as Kafka 0.10.1.2 is not released
+     */
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 3) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir, ) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args + "" "" : """")
+                + (args.length > 1 ? args : """"));
+        }
+        final String kafka = args;
+        final String zookeeper = args;
+        final String stateDir = args;
+        final String upgradeFrom = args.length > 3 ? args : null;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.10.1)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""zookeeper="" + zookeeper);
+        System.out.println(""stateDir="" + stateDir);
+        System.out.println(""upgradeFrom="" + upgradeFrom);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 0.10.1.2 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(""upgrade.from"", upgradeFrom);
+        }
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..18240f04ff1
--- /dev/null
+++ [file java]
@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    /**
+     * This test cannot be run executed, as long as Kafka 0.10.2.2 is not released
+     */
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 2) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, state-dir, ) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args : """"));
+        }
+        final String kafka = args;
+        final String stateDir = args;
+        final String upgradeFrom = args.length > 2 ? args : null;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.10.2)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""stateDir="" + stateDir);
+        System.out.println(""upgradeFrom="" + upgradeFrom);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 0.10.2.2 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(""upgrade.from"", upgradeFrom);
+        }
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git a/tests/kafkatest/services/streams.py b/tests/kafkatest/services/streams.py
index e6f692b171d..eeb16816367 100644
--- a/tests/kafkatest/services/streams.py
+++ b/tests/kafkatest/services/streams.py
@@ -20,6 +20,7 @@
 from ducktape.utils.util import wait_until
 from kafkatest.directory_layout.kafka_path import KafkaPathResolverMixin
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1
 class StreamsTestBaseService(KafkaPathResolverMixin, Service):
@@ -33,6 +34,8 @@ class StreamsTestBaseService(KafkaPathResolverMixin, Service):
     LOG4J_CONFIG_FILE = os.path.join(PERSISTENT_ROOT, ""tools-log4j.properties"")
     PID_FILE = os.path.join(PERSISTENT_ROOT, ""streams.pid"")
+    CLEAN_NODE_ENABLED = True
+
     logs = {
         ""streams_log"": {
             ""path"": LOG_FILE,
@@ -43,6 +46,114 @@ class StreamsTestBaseService(KafkaPathResolverMixin, Service):
         ""streams_stderr"": {
             ""path"": STDERR_FILE,
             ""collect_default"": True},
+        ""streams_log.0-1"": {
+            ""path"": LOG_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_stdout.0-1"": {
+            ""path"": STDOUT_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_stderr.0-1"": {
+            ""path"": STDERR_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_log.0-2"": {
+            ""path"": LOG_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_stdout.0-2"": {
+            ""path"": STDOUT_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_stderr.0-2"": {
+            ""path"": STDERR_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_log.0-3"": {
+            ""path"": LOG_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_stdout.0-3"": {
+            ""path"": STDOUT_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_stderr.0-3"": {
+            ""path"": STDERR_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_log.0-4"": {
+            ""path"": LOG_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_stdout.0-4"": {
+            ""path"": STDOUT_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_stderr.0-4"": {
+            ""path"": STDERR_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_log.0-5"": {
+            ""path"": LOG_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_stdout.0-5"": {
+            ""path"": STDOUT_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_stderr.0-5"": {
+            ""path"": STDERR_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_log.0-6"": {
+            ""path"": LOG_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_stdout.0-6"": {
+            ""path"": STDOUT_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_stderr.0-6"": {
+            ""path"": STDERR_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_log.1-1"": {
+            ""path"": LOG_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_stdout.1-1"": {
+            ""path"": STDOUT_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_stderr.1-1"": {
+            ""path"": STDERR_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_log.1-2"": {
+            ""path"": LOG_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_stdout.1-2"": {
+            ""path"": STDOUT_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_stderr.1-2"": {
+            ""path"": STDERR_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_log.1-3"": {
+            ""path"": LOG_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_stdout.1-3"": {
+            ""path"": STDOUT_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_stderr.1-3"": {
+            ""path"": STDERR_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_log.1-4"": {
+            ""path"": LOG_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_stdout.1-4"": {
+            ""path"": STDOUT_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_stderr.1-4"": {
+            ""path"": STDERR_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_log.1-5"": {
+            ""path"": LOG_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_stdout.1-5"": {
+            ""path"": STDOUT_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_stderr.1-5"": {
+            ""path"": STDERR_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_log.1-6"": {
+            ""path"": LOG_FILE + "".1-6"",
+            ""collect_default"": True},
+        ""streams_stdout.1-6"": {
+            ""path"": STDOUT_FILE + "".1-6"",
+            ""collect_default"": True},
+        ""streams_stderr.1-6"": {
+            ""path"": STDERR_FILE + "".1-6"",
+            ""collect_default"": True},
     }
     def __init__(self, test_context, kafka, streams_class_name, user_test_args, user_test_args1=None, user_test_args2=None):
@@ -107,7 +218,8 @@ def wait_node(self, node, timeout_sec=None):
     def clean_node(self, node):
         node.account.kill_process(""streams"", clean_shutdown=False, allow_fail=True)
-        node.account.ssh(""rm -rf "" + self.PERSISTENT_ROOT, allow_fail=False)
+        if self.CLEAN_NODE_ENABLED:
+            node.account.ssh(""rm -rf "" + self.PERSISTENT_ROOT, allow_fail=False)
     def start_cmd(self, node):
         args = self.args.copy()
@@ -163,7 +275,28 @@ def __init__(self, test_context, kafka, command):
 class StreamsSmokeTestDriverService(StreamsSmokeTestBaseService):
     def __init__(self, test_context, kafka):
         super(StreamsSmokeTestDriverService, self).__init__(test_context, kafka, ""run"")
+        self.DISABLE_AUTO_TERMINATE = """"
+
+    def disable_auto_terminate(self):
+        self.DISABLE_AUTO_TERMINATE = ""disableAutoTerminate""
+
+    def start_cmd(self, node):
+        args = self.args.copy()
+        args = self.kafka.bootstrap_servers()
+        args = self.PERSISTENT_ROOT
+        args = self.STDOUT_FILE
+        args = self.STDERR_FILE
+        args = self.PID_FILE
+        args = self.LOG4J_CONFIG_FILE
+        args = self.DISABLE_AUTO_TERMINATE
+        args = self.path.script(""kafka-run-class.sh"", node)
+        cmd = ""( export KAFKA_LOG4J_OPTS=\""-Dlog4j.configuration=file:%(log4j)s\""; "" \
+              ""INCLUDE_TEST_JARS=true %(kafka_run_class)s %(streams_class_name)s "" \
+              "" %(kafka)s %(state_dir)s %(user_test_args)s %(disable_auto_terminate)s"" \
+              "" & echo $! >&3 ) 1>> %(stdout)s 2>> %(stderr)s 3> %(pidfile)s"" % args
+
+        return cmd
 class StreamsSmokeTestJobRunnerService(StreamsSmokeTestBaseService):
     def __init__(self, test_context, kafka):
@@ -206,3 +339,41 @@ def __init__(self, test_context, kafka, eosEnabled):
                                                                 kafka,
                                                                 ""org.apache.kafka.streams.tests.BrokerCompatibilityTest"",
                                                                 eosEnabled)
+
+class StreamsUpgradeTestJobRunnerService(StreamsTestBaseService):
+    def __init__(self, test_context, kafka):
+        super(StreamsUpgradeTestJobRunnerService, self).__init__(test_context,
+                                                                 kafka,
+                                                                 ""org.apache.kafka.streams.tests.StreamsUpgradeTest"",
+                                                                 """")
+        self.UPGRADE_FROM = """"
+
+    def set_version(self, kafka_streams_version):
+        self.KAFKA_STREAMS_VERSION = kafka_streams_version
+
+    def set_upgrade_from(self, upgrade_from):
+        self.UPGRADE_FROM = upgrade_from
+
+    def start_cmd(self, node):
+        args = self.args.copy()
+        args = self.kafka.bootstrap_servers()
+        if self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_0) or self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_1):
+            args = self.kafka.zk.connect_setting()
+        else:
+            args = """"
+        args = self.PERSISTENT_ROOT
+        args = self.STDOUT_FILE
+        args = self.STDERR_FILE
+        args = self.PID_FILE
+        args = self.LOG4J_CONFIG_FILE
+        args = self.KAFKA_STREAMS_VERSION
+        args = self.UPGRADE_FROM
+        args = self.path.script(""kafka-run-class.sh"", node)
+
+        cmd = ""( export KAFKA_LOG4J_OPTS=\""-Dlog4j.configuration=file:%(log4j)s\""; "" \
+              ""INCLUDE_TEST_JARS=true UPGRADE_KAFKA_STREAMS_TEST_VERSION=%(version)s "" \
+              "" %(kafka_run_class)s %(streams_class_name)s "" \
+              "" %(kafka)s %(zk)s %(state_dir)s %(user_test_args)s %(upgrade_from)s"" \
+              "" & echo $! >&3 ) 1>> %(stdout)s 2>> %(stderr)s 3> %(pidfile)s"" % args
+
+        return cmd
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py
new file mode 100644
index 00000000000..7aa2de67d53
--- /dev/null
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py
@@ -0,0 +1,246 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    [link]
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from ducktape.mark import parametrize
+from kafkatest.tests.kafka_test import KafkaTest
+from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsUpgradeTestJobRunnerService
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1, LATEST_0_10_2, DEV_VERSION
+import random
+
+class StreamsUpgradeTest(KafkaTest):
+    """"""
+    Test upgrading Kafka Streams (all version combination)
+    If metadata was changes, upgrade is more difficult
+    Metadata version was bumped in 0.10.1.0
+    """"""
+
+    def __init__(self, test_context):
+        super(StreamsUpgradeTest, self).__init__(test_context, num_zk=1, num_brokers=1, topics={
+            'echo' : { 'partitions': 5 },
+            'data' : { 'partitions': 5 }
+        })
+
+        self.driver = StreamsSmokeTestDriverService(test_context, self.kafka)
+        self.driver.disable_auto_terminate()
+        self.processor1 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)
+        self.processor2 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)
+        self.processor3 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)
+
+    @parametrize(old_version=str(LATEST_0_10_1), new_version=str(LATEST_0_10_2))
+    @parametrize(old_version=str(LATEST_0_10_1), new_version=str(DEV_VERSION))
+    @parametrize(old_version=str(LATEST_0_10_2), new_version=str(DEV_VERSION))
+    def test_simple_upgrade(self, old_version, new_version):
+        """"""
+        Starts 3 KafkaStreams instances with <old_version>, and upgrades one-by-one to <new_verion>
+        """"""
+
+        self.driver.start()
+        self.start_all_nodes_with(old_version)
+
+        self.processors = 
+
+        counter = 1
+        random.seed()
+
+        random.shuffle(self.processors)
+        for p in self.processors:
+            p.CLEAN_NODE_ENABLED = False
+            self.do_rolling_bounce(p, """", new_version, counter)
+            counter = counter + 1
+
+        # shutdown
+        self.driver.stop()
+        self.driver.wait()
+
+        random.shuffle(self.processors)
+        for p in self.processors:
+            node = p.node
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:
+                p.stop()
+                monitor.wait_until(""UPGRADE-TEST-CLIENT-CLOSED"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'UPGRADE-TEST-CLIENT-CLOSED' on"" + str(node.account))
+
+        self.driver.stop()
+
+    #@parametrize(new_version=str(LATEST_0_10_1)) we cannot run this test until Kafka 0.10.1.2 is released
+    #@parametrize(new_version=str(LATEST_0_10_2)) we cannot run this test until Kafka 0.10.2.2 is released
+    @parametrize(new_version=str(DEV_VERSION))
+    def test_metadata_upgrade(self, new_version):
+        """"""
+        Starts 3 KafkaStreams instances with version 0.10.0, and upgrades one-by-one to <new_version>
+        """"""
+
+        self.driver.start()
+        self.start_all_nodes_with(str(LATEST_0_10_0))
+
+        self.processors = 
+
+        counter = 1
+        random.seed()
+
+        # first rolling bounce
+        random.shuffle(self.processors)
+        for p in self.processors:
+            p.CLEAN_NODE_ENABLED = False
+            self.do_rolling_bounce(p, ""0.10.0"", new_version, counter)
+            counter = counter + 1
+
+        # second rolling bounce
+        random.shuffle(self.processors)
+        for p in self.processors:
+            self.do_rolling_bounce(p, """", new_version, counter)
+            counter = counter + 1
+
+        # shutdown
+        self.driver.stop()
+        self.driver.wait()
+
+        random.shuffle(self.processors)
+        for p in self.processors:
+            node = p.node
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:
+                p.stop()
+                monitor.wait_until(""UPGRADE-TEST-CLIENT-CLOSED"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'UPGRADE-TEST-CLIENT-CLOSED' on"" + str(node.account))
+
+        self.driver.stop()
+
+    def start_all_nodes_with(self, version):
+        # start first with <version>
+        self.prepare_for(self.processor1, version)
+        node1 = self.processor1.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as monitor:
+            with node1.account.monitor_log(self.processor1.LOG_FILE) as log_monitor:
+                self.processor1.start()
+                log_monitor.wait_until(""Kafka version : "" + version,
+                                       timeout_sec=60,
+                                       err_msg=""Could not detect Kafka Streams version "" + version + "" "" + str(node1.account))
+                monitor.wait_until(""processed 100 records from topic"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
+
+        # start second with <version>
+        self.prepare_for(self.processor2, version)
+        node2 = self.processor2.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:
+                with node2.account.monitor_log(self.processor2.LOG_FILE) as log_monitor:
+                    self.processor2.start()
+                    log_monitor.wait_until(""Kafka version : "" + version,
+                                           timeout_sec=60,
+                                           err_msg=""Could not detect Kafka Streams version "" + version + "" "" + str(node2.account))
+                    first_monitor.wait_until(""processed 100 records from topic"",
+                                             timeout_sec=60,
+                                             err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
+                    second_monitor.wait_until(""processed 100 records from topic"",
+                                              timeout_sec=60,
+                                              err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node2.account))
+
+        # start third with <version>
+        self.prepare_for(self.processor3, version)
+        node3 = self.processor3.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:
+                with node3.account.monitor_log(self.processor3.STDOUT_FILE) as third_monitor:
+                    with node3.account.monitor_log(self.processor3.LOG_FILE) as log_monitor:
+                        self.processor3.start()
+                        log_monitor.wait_until(""Kafka version : "" + version,
+                                               timeout_sec=60,
+                                               err_msg=""Could not detect Kafka Streams version "" + version + "" "" + str(node3.account))
+                        first_monitor.wait_until(""processed 100 records from topic"",
+                                                 timeout_sec=60,
+                                                 err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
+                        second_monitor.wait_until(""processed 100 records from topic"",
+                                                  timeout_sec=60,
+                                                  err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node2.account))
+                        third_monitor.wait_until(""processed 100 records from topic"",
+                                                  timeout_sec=60,
+                                                  err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node3.account))
+
+    @staticmethod
+    def prepare_for(processor, version):
+        processor.node.account.ssh(""rm -rf "" + processor.PERSISTENT_ROOT, allow_fail=False)
+        processor.set_version(version)
+
+    def do_rolling_bounce(self, processor, upgrade_from, new_version, counter):
+        first_other_processor = None
+        second_other_processor = None
+        for p in self.processors:
+            if p != processor:
+                if first_other_processor is None:
+                    first_other_processor = p
+                else:
+                    second_other_processor = p
+
+        node = processor.node
+        first_other_node = first_other_processor.node
+        second_other_node = second_other_processor.node
+
+        # stop processor and wait for rebalance of others
+        with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:
+            with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:
+                processor.stop()
+                first_other_monitor.wait_until(""processed 100 records from topic"",
+                                               timeout_sec=60,
+                                               err_msg=""Never saw output 'processed 100 records from topic' on"" + str(first_other_node.account))
+                second_other_monitor.wait_until(""processed 100 records from topic"",
+                                                timeout_sec=60,
+                                                err_msg=""Never saw output 'processed 100 records from topic' on"" + str(second_other_node.account))
+        node.account.ssh_capture(""grep UPGRADE-TEST-CLIENT-CLOSED %s"" % processor.STDOUT_FILE, allow_fail=False)
+
+        if upgrade_from == """":  # upgrade disabled -- second round of rolling bounces
+            roll_counter = "".1-""  # second round of rolling bounces
+        else:
+            roll_counter = "".0-""  # first  round of rolling boundes
+
+        node.account.ssh(""mv "" + processor.STDOUT_FILE + "" "" + processor.STDOUT_FILE + roll_counter + str(counter), allow_fail=False)
+        node.account.ssh(""mv "" + processor.STDERR_FILE + "" "" + processor.STDERR_FILE + roll_counter + str(counter), allow_fail=False)
+        node.account.ssh(""mv "" + processor.LOG_FILE + "" "" + processor.LOG_FILE + roll_counter + str(counter), allow_fail=False)
+
+        if new_version == str(DEV_VERSION):
+            processor.set_version("""")  # set to TRUNK
+        else:
+            processor.set_version(new_version)
+        processor.set_upgrade_from(upgrade_from)
+
+        grep_metadata_error = ""grep \""org.apache.kafka.streams.errors.TaskAssignmentException: unable to decode subscription data: version=2\"" ""
+        with node.account.monitor_log(processor.STDOUT_FILE) as monitor:
+            with node.account.monitor_log(processor.LOG_FILE) as log_monitor:
+                with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:
+                    with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:
+                        processor.start()
+
+                        log_monitor.wait_until(""Kafka version : "" + new_version,
+                                               timeout_sec=60,
+                                               err_msg=""Could not detect Kafka Streams version "" + new_version + "" "" + str(node.account))
+                        first_other_monitor.wait_until(""processed 100 records from topic"",
+                                                       timeout_sec=60,
+                                                       err_msg=""Never saw output 'processed 100 records from topic' on"" + str(first_other_node.account))
+                        found = list(first_other_node.account.ssh_capture(grep_metadata_error + first_other_processor.STDERR_FILE, allow_fail=True))
+                        if len(found) > 0:
+                            raise Exception(""Kafka Streams failed with 'unable to decode subscription data: version=2'"")
+
+                        second_other_monitor.wait_until(""processed 100 records from topic"",
+                                                        timeout_sec=60,
+                                                        err_msg=""Never saw output 'processed 100 records from topic' on"" + str(second_other_node.account))
+                        found = list(second_other_node.account.ssh_capture(grep_metadata_error + second_other_processor.STDERR_FILE, allow_fail=True))
+                        if len(found) > 0:
+                            raise Exception(""Kafka Streams failed with 'unable to decode subscription data: version=2'"")
+
+                        monitor.wait_until(""processed 100 records from topic"",
+                                           timeout_sec=60,
+                                           err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node.account))
diff --git a/tests/kafkatest/version.py b/tests/kafkatest/version.py
index f63a7c17ecd..94ba100bda7 100644
--- a/tests/kafkatest/version.py
+++ b/tests/kafkatest/version.py
@@ -61,6 +61,7 @@ def get_version(node=None):
         return DEV_BRANCH
 DEV_BRANCH = KafkaVersion(""dev"")
+DEV_VERSION = KafkaVersion(""0.11.0.3-SNAPSHOT"")
 # 0.8.2.X versions
 V_0_8_2_1 = KafkaVersion(""0.8.2.1"")
@@ -91,5 +92,7 @@ def get_version(node=None):
 # 0.11.0.0 versions
 V_0_11_0_0 = KafkaVersion(""0.11.0.0"")
-LATEST_0_11_0 = V_0_11_0_0
+V_0_11_0_1 = KafkaVersion(""0.11.0.1"")
+V_0_11_0_2 = KafkaVersion(""0.11.0.2"")
+LATEST_0_11_0 = V_0_11_0_2
 LATEST_0_11 = LATEST_0_11_0
diff --git a/vagrant/base.sh b/vagrant/base.sh
index 4c0add543aa..28b81ed05a8 100755
--- a/vagrant/base.sh
+++ b/vagrant/base.sh
@@ -64,6 +64,8 @@ get_kafka() {
     kafka_dir=/opt/kafka-$version
     url=[link]
+    # the .tgz above does not include the streams test jar hence we need to get it separately
+    url_streams_test=[link]
     if ; then
         pushd /tmp
         curl -O $url
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 13 **
mjsax opened a new pull request #4779: KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 14 **
mjsax closed pull request #4746: KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git a/bin/kafka-run-class.sh b/bin/kafka-run-class.sh
index 1f5140b10c8..77123ff8093 100755
--- a/bin/kafka-run-class.sh
+++ b/bin/kafka-run-class.sh
@@ -73,28 +73,48 @@ do
   fi
 done
-for file in ""$base_dir""/clients/build/libs/kafka-clients*.jar;
-do
-  if should_include_file ""$file""; then
-    CLASSPATH=""$CLASSPATH"":""$file""
-  fi
-done
+if ; then
+  clients_lib_dir=$(dirname $0)/../clients/build/libs
+  streams_lib_dir=$(dirname $0)/../streams/build/libs
+  rocksdb_lib_dir=$(dirname $0)/../streams/build/dependant-libs-${SCALA_VERSION}
+else
+  clients_lib_dir=/opt/kafka-$UPGRADE_KAFKA_STREAMS_TEST_VERSION/libs
+  streams_lib_dir=$clients_lib_dir
+  rocksdb_lib_dir=$streams_lib_dir
+fi
+
-for file in ""$base_dir""/streams/build/libs/kafka-streams*.jar;
+for file in ""$clients_lib_dir""/kafka-clients*.jar;
 do
   if should_include_file ""$file""; then
     CLASSPATH=""$CLASSPATH"":""$file""
   fi
 done
-for file in ""$base_dir""/streams/examples/build/libs/kafka-streams-examples*.jar;
+for file in ""$streams_lib_dir""/kafka-streams*.jar;
 do
   if should_include_file ""$file""; then
     CLASSPATH=""$CLASSPATH"":""$file""
   fi
 done
-for file in ""$base_dir""/streams/build/dependant-libs-${SCALA_VERSION}/rocksdb*.jar;
+if ; then
+  for file in ""$base_dir""/streams/examples/build/libs/kafka-streams-examples*.jar;
+  do
+    if should_include_file ""$file""; then
+      CLASSPATH=""$CLASSPATH"":""$file""
+    fi
+  done
+else
+  for file in ""$base_dir""/streams/upgrade-system-tests-0100/build/libs/kafka-streams-upgrade-system-tests*.jar;
+  do
+    if should_include_file ""$file""; then
+      CLASSPATH=""$CLASSPATH"":""$file""
+    fi
+  done
+fi
+
+for file in ""$rocksdb_lib_dir""/rocksdb*.jar;
 do
   CLASSPATH=""$CLASSPATH"":""$file""
 done
diff --git a/build.gradle b/build.gradle
index d221d965c5e..2a540211018 100644
--- a/build.gradle
+++ b/build.gradle
@@ -776,6 +776,19 @@ project(':streams:examples') {
   }
 }
+project(':streams:upgrade-system-tests-0100') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0100""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0100
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
+
 project(':log4j-appender') {
   archivesBaseName = ""kafka-log4j-appender""
diff --git [file java] [file java]
index 93b92bb52a3..dbbb9127199 100644
--- [file java]
+++ [file java]
@@ -918,7 +918,7 @@ public void onFailure(RuntimeException e) {
                 log.error(""Unexpected interrupt received in heartbeat thread for group {}"", groupId, e);
                 this.failed.set(new RuntimeException(e));
             } catch (RuntimeException e) {
-                log.error(""Heartbeat thread for group {} failed due to unexpected error"" , groupId, e);
+                log.error(""Heartbeat thread for group {} failed due to unexpected error"", groupId, e);
                 this.failed.set(e);
             }
         }
diff --git [file java] [file java]
index 212d701aaaa..74887483354 100644
--- [file java]
+++ [file java]
@@ -316,7 +316,7 @@ public int hashCode() {
             Field f = this.schema.get(i);
             if (f.type() instanceof ArrayOf) {
                 if (this.get(f) != null) {
-                    Object arrayObject = (Object ) this.get(f);
+                    Object arrayObject = (Object) this.get(f);
                     for (Object arrayItem: arrayObject)
                         result = prime * result + arrayItem.hashCode();
                 }
diff --git [file java] [file java]
index 8e0b8dbfa3d..b80dfccf3d9 100644
--- [file java]
+++ [file java]
@@ -17,7 +17,9 @@
  */
 package org.apache.kafka.common.security.authenticator;
-import java.util.Map;
+import org.apache.kafka.common.config.SaslConfigs;
+import org.apache.kafka.common.network.Mode;
+import org.apache.kafka.common.security.auth.AuthCallbackHandler;
 import javax.security.auth.Subject;
 import javax.security.auth.callback.Callback;
@@ -26,10 +28,7 @@
 import javax.security.auth.callback.UnsupportedCallbackException;
 import javax.security.sasl.AuthorizeCallback;
 import javax.security.sasl.RealmCallback;
-
-import org.apache.kafka.common.config.SaslConfigs;
-import org.apache.kafka.common.network.Mode;
-import org.apache.kafka.common.security.auth.AuthCallbackHandler;
+import java.util.Map;
 /**
  * Callback handler for Sasl clients. The callbacks required for the SASL mechanism
@@ -59,7 +58,7 @@ public void handle(Callback callbacks) throws UnsupportedCallbackException {
                     nc.setName(nc.getDefaultName());
             } else if (callback instanceof PasswordCallback) {
                 if (= null && !subject.getPrivateCredentials(String.class).isEmpty()) {
-                    char  password = subject.getPrivateCredentials(String.class).iterator().next().toCharArray();
+                    char password = subject.getPrivateCredentials(String.class).iterator().next().toCharArray();
                     ((PasswordCallback) callback).setPassword(password);
                 } else {
                     String errorMessage = ""Could not login: the client is being asked for a password, but the Kafka"" +
diff --git [file java] [file java]
index a844bb08917..5186d05d492 100644
--- [file java]
+++ [file java]
@@ -26,24 +26,24 @@
     @Test
     public void testEqualsAndHashCode() {
-        ProducerRecord<String, Integer> producerRecord = new ProducerRecord<>(""test"", 1 , ""key"", 1);
+        ProducerRecord<String, Integer> producerRecord = new ProducerRecord<>(""test"", 1, ""key"", 1);
         assertEquals(producerRecord, producerRecord);
         assertEquals(producerRecord.hashCode(), producerRecord.hashCode());
-        ProducerRecord<String, Integer> equalRecord = new ProducerRecord<>(""test"", 1 , ""key"", 1);
+        ProducerRecord<String, Integer> equalRecord = new ProducerRecord<>(""test"", 1, ""key"", 1);
         assertEquals(producerRecord, equalRecord);
         assertEquals(producerRecord.hashCode(), equalRecord.hashCode());
-        ProducerRecord<String, Integer> topicMisMatch = new ProducerRecord<>(""test-1"", 1 , ""key"", 1);
+        ProducerRecord<String, Integer> topicMisMatch = new ProducerRecord<>(""test-1"", 1, ""key"", 1);
         assertFalse(producerRecord.equals(topicMisMatch));
-        ProducerRecord<String, Integer> partitionMismatch = new ProducerRecord<>(""test"", 2 , ""key"", 1);
+        ProducerRecord<String, Integer> partitionMismatch = new ProducerRecord<>(""test"", 2, ""key"", 1);
         assertFalse(producerRecord.equals(partitionMismatch));
-        ProducerRecord<String, Integer> keyMisMatch = new ProducerRecord<>(""test"", 1 , ""key-1"", 1);
+        ProducerRecord<String, Integer> keyMisMatch = new ProducerRecord<>(""test"", 1, ""key-1"", 1);
         assertFalse(producerRecord.equals(keyMisMatch));
-        ProducerRecord<String, Integer> valueMisMatch = new ProducerRecord<>(""test"", 1 , ""key"", 2);
+        ProducerRecord<String, Integer> valueMisMatch = new ProducerRecord<>(""test"", 1, ""key"", 2);
         assertFalse(producerRecord.equals(valueMisMatch));
         ProducerRecord<String, Integer> nullFieldsRecord = new ProducerRecord<>(""topic"", null, null, null, null);
diff --git a/docs/upgrade.html b/docs/upgrade.html
index e6b9747d0f9..faa96c1aca6 100644
--- a/docs/upgrade.html
+++ b/docs/upgrade.html
@@ -55,6 +55,23 @@ <h5><a id=""upgrade_10_1_breaking"" href=""#upgrade_10_1_breaking"">Potential breaki
 <h5><a id=""upgrade_1010_streams"" href=""#upgrade_1010_streams"">Streams API changes in 0.10.1.0</a></h5>
 <ul>
+    <li> Upgrading from 0.10.0.x to 0.10.1.2 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+         (cf. <a href=""[link]"">KIP-268</a>).
+         As an alternative, an offline upgrade is also possible.
+        <ul>
+            <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.10.1.2 </li>
+            <li> bounce each instance of your application once </li>
+            <li> prepare your newly deployed 0.10.1.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+            <li> bounce each instance of your application once more to complete the upgrade </li>
+        </ul>
+    </li>
+    <li> Upgrading from 0.10.0.x to 0.10.1.0 or 0.10.1.1 requires an offline upgrade (rolling bounce upgrade is not supported)
+        <ul>
+            <li> stop all old (0.10.0.x) application instances </li>
+            <li> update your code and swap old code and jar file with new code and new jar file </li>
+            <li> restart all new (0.10.1.0 or 0.10.1.1) application instances </li>
+        </ul>
+    </li>
     <li> Stream grouping and aggregation split into two methods:
         <ul>
             <li> old: KStream #aggregateByKey(), #reduceByKey(), and #countByKey() </li>
diff --git a/gradle/dependencies.gradle b/gradle/dependencies.gradle
index 2ff459f1520..07944a9e42d 100644
--- a/gradle/dependencies.gradle
+++ b/gradle/dependencies.gradle
@@ -31,6 +31,7 @@ versions += [
   jackson: ""2.6.3"",
   jetty: ""9.2.22.v20170606"",
   jersey: ""2.22.2"",
+  kafka_0100: ""0.10.0.1"",
   log4j: ""1.2.17"",
   jopt: ""4.9"",
   junit: ""4.12"",
@@ -91,6 +92,7 @@ libs += [
   junit: ""junit:junit:$versions.junit"",
   log4j: ""log4j:log4j:$versions.log4j"",
   joptSimple: ""net.sf.jopt-simple:jopt-simple:$versions.jopt"",
+  kafkaStreams_0100: ""org.apache.kafka:kafka-streams:$versions.kafka_0100"",
   lz4: ""net.jpountz.lz4:lz4:$versions.lz4"",
   metrics: ""com.yammer.metrics:metrics-core:$versions.metrics"",
   powermock: ""org.powermock:powermock-module-junit4:$versions.powermock"",
diff --git a/gradle/rat.gradle b/gradle/rat.gradle
index d62b3722a4f..a51876c23ea 100644
--- a/gradle/rat.gradle
+++ b/gradle/rat.gradle
@@ -84,9 +84,15 @@ class RatTask extends DefaultTask {
     if (!reportDir.exists()) {
       reportDir.mkdirs()
     }
-    generateXmlReport(reportDir)
-    printUnknownFiles()
-    generateHtmlReport()
+    def origEncoding = System.getProperty(""file.encoding"")
+    try {
+      System.setProperty(""file.encoding"", ""UTF-8"") //affects the output of the ant rat task
+      generateXmlReport(reportDir)
+      printUnknownFiles()
+      generateHtmlReport()
+    } finally {
+      System.setProperty(""file.encoding"", origEncoding)
+    }
   }
 }
@@ -109,7 +115,7 @@ class RatPlugin implements Plugin<Project> {
       mavenCentral()
     }
     project.dependencies {
-      rat 'org.apache.rat:apache-rat-tasks:0.11'
+      rat 'org.apache.rat:apache-rat-tasks:0.12'
     }
   }
 }
diff --git a/jenkins.sh b/jenkins.sh
new file mode 100755
index 00000000000..c21eb1d8500
--- /dev/null
+++ b/jenkins.sh
@@ -0,0 +1,20 @@
+#!/bin/bash
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    [link]
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+# This script is used for verifying changes in Jenkins. In order to provide faster feedback, the tasks are ordered so
+# that faster tasks are executed in every module before slower tasks (if possible). For example, the unit tests for all
+# the modules are executed before the integration tests.
+./gradlew clean compileJava compileScala compileTestJava compileTestScala checkstyleMain checkstyleTest test rat --no-daemon -PxmlFindBugsReport=true -PtestLoggingEvents=started,passed,skipped,failed ""$@""
diff --git a/settings.gradle b/settings.gradle
index d430c2fd919..f3a1b81ba65 100644
--- a/settings.gradle
+++ b/settings.gradle
@@ -13,5 +13,5 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
-include 'core', 'examples', 'clients', 'tools', 'streams', 'streams:examples', 'log4j-appender',
-        'connect:api', 'connect:runtime', 'connect:json', 'connect:file'
+include 'core', 'examples', 'clients', 'tools', 'streams', 'streams:examples', 'streams:upgrade-system-tests-0100',
+        'log4j-appender', 'connect:api', 'connect:runtime', 'connect:json', 'connect:file'
diff --git [file java] [file java]
index 5ba438376f6..e33efefc38f 100644
--- [file java]
+++ [file java]
@@ -39,6 +39,7 @@
 import java.util.Set;
 import static org.apache.kafka.common.config.ConfigDef.Range.atLeast;
+import static org.apache.kafka.common.config.ConfigDef.ValidString.in;
 /**
  * Configuration for Kafka Streams. Documentation for these configurations can be found in the <a
@@ -54,6 +55,16 @@
     // Prefix used to isolate producer configs from consumer configs.
     public static final String PRODUCER_PREFIX = ""producer."";
+    /**
+     * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 0.10.0.x}.
+     */
+    public static final String UPGRADE_FROM_0100 = ""0.10.0"";
+
+    /** {@code upgrade.from} */
+    public static final String UPGRADE_FROM_CONFIG = ""upgrade.from"";
+    public static final String UPGRADE_FROM_DOC = ""Allows upgrading from version 0.10.0 to version 0.10.1 (or newer) in a backward compatible way. "" +
+        ""Default is null. Accepted values are \"""" + UPGRADE_FROM_0100 + ""\"" (for upgrading from 0.10.0.x)."";
+
     /** <code>state.dir</code> */
     public static final String STATE_DIR_CONFIG = ""state.dir"";
     private static final String STATE_DIR_DOC = ""Directory location for state store."";
@@ -257,14 +268,19 @@
                                         10 * 1024 * 1024L,
                                         atLeast(0),
                                         Importance.LOW,
-                                        CACHE_MAX_BYTES_BUFFERING_DOC);
+                                        CACHE_MAX_BYTES_BUFFERING_DOC)
+                                .define(UPGRADE_FROM_CONFIG,
+                                        ConfigDef.Type.STRING,
+                                        null,
+                                        in(null, UPGRADE_FROM_0100),
+                                        ConfigDef.Importance.LOW,
+                                        UPGRADE_FROM_DOC);
     }
     // this is the list of configs for underlying clients
     // that streams prefer different default values
     private static final Map<String, Object> PRODUCER_DEFAULT_OVERRIDES;
-    static
-    {
+    static {
         Map<String, Object> tempProducerDefaultOverrides = new HashMap<>();
         tempProducerDefaultOverrides.put(ProducerConfig.LINGER_MS_CONFIG, ""100"");
@@ -272,8 +288,7 @@
     }
     private static final Map<String, Object> CONSUMER_DEFAULT_OVERRIDES;
-    static
-    {
+    static {
         Map<String, Object> tempConsumerDefaultOverrides = new HashMap<>();
         tempConsumerDefaultOverrides.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, ""1000"");
         tempConsumerDefaultOverrides.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");
@@ -342,6 +357,7 @@ public StreamsConfig(Map<?, ?> props) {
         consumerProps.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId + ""-consumer"");
         // add configs required for stream partition assignor
+        consumerProps.put(UPGRADE_FROM_CONFIG, getString(UPGRADE_FROM_CONFIG));
         consumerProps.put(StreamsConfig.InternalConfig.STREAM_THREAD_INSTANCE, streamThread);
         consumerProps.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));
         consumerProps.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));
diff --git [file java] [file java]
index 81f1f63078f..bfca83a39b4 100644
--- [file java]
+++ [file java]
@@ -676,7 +676,7 @@ private void connectProcessorAndStateStore(String processorName, String stateSto
         }
     }
-    private Set<String> findSourceTopicsForProcessorParents(String  parents) {
+    private Set<String> findSourceTopicsForProcessorParents(String parents) {
         final Set<String> sourceTopics = new HashSet<>();
         for (String parent : parents) {
             NodeFactory nodeFactory = nodeFactories.get(parent);
diff --git [file java] [file java]
index dcba5437bf5..e6c407fdbae 100644
--- [file java]
+++ [file java]
@@ -14,7 +14,6 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 package org.apache.kafka.streams.processor.internals;
 import org.apache.kafka.clients.consumer.internals.PartitionAssignor;
@@ -66,7 +65,7 @@
         public final TaskId taskId;
         public final TopicPartition partition;
-        public AssignedPartition(TaskId taskId, TopicPartition partition) {
+        AssignedPartition(final TaskId taskId, final TopicPartition partition) {
             this.taskId = taskId;
             this.partition = partition;
         }
@@ -92,6 +91,7 @@ public int compare(TopicPartition p1, TopicPartition p2) {
     private StreamThread streamThread;
+    private int userMetadataVersion = SubscriptionInfo.CURRENT_VERSION;
     private int numStandbyReplicas;
     private Map<Integer, TopologyBuilder.TopicsInfo> topicGroups;
     private Map<TopicPartition, Set<TaskId>> partitionToTaskIds;
@@ -111,6 +111,11 @@ public int compare(TopicPartition p1, TopicPartition p2) {
     public void configure(Map<String, ?> configs) {
         numStandbyReplicas = (Integer) configs.get(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG);
+        final String upgradeMode = (String) configs.get(StreamsConfig.UPGRADE_FROM_CONFIG);
+        if (StreamsConfig.UPGRADE_FROM_0100.equals(upgradeMode)) {
+            log.info(""Downgrading metadata version from 2 to 1 for upgrade from 0.10.0.x."");
+            userMetadataVersion = 1;
+        }
         Object o = configs.get(StreamsConfig.InternalConfig.STREAM_THREAD_INSTANCE);
         if (o == null) {
@@ -174,7 +179,7 @@ public Subscription subscription(Set<String> topics) {
         Set<TaskId> prevTasks = streamThread.prevTasks();
         Set<TaskId> standbyTasks = streamThread.cachedTasks();
         standbyTasks.removeAll(prevTasks);
-        SubscriptionInfo data = new SubscriptionInfo(streamThread.processId, prevTasks, standbyTasks, this.userEndPointConfig);
+        SubscriptionInfo data = new SubscriptionInfo(userMetadataVersion, streamThread.processId, prevTasks, standbyTasks, this.userEndPointConfig);
         if (streamThread.builder.sourceTopicPattern() != null) {
             SubscriptionUpdates subscriptionUpdates = new SubscriptionUpdates();
@@ -265,12 +270,16 @@ public Subscription subscription(Set<String> topics) {
         Map<UUID, ClientState<TaskId>> states = new HashMap<>();
         Map<UUID, HostInfo> consumerEndPointMap = new HashMap<>();
         // decode subscription info
+        int minUserMetadataVersion = SubscriptionInfo.CURRENT_VERSION;
         for (Map.Entry<String, Subscription> entry : subscriptions.entrySet()) {
             String consumerId = entry.getKey();
             Subscription subscription = entry.getValue();
-
             SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());
+            final int usedVersion = info.version;
+            if (usedVersion < minUserMetadataVersion) {
+                minUserMetadataVersion = usedVersion;
+            }
             if (info.userEndPoint != null) {
                 final String hostPort = info.userEndPoint.split("":"");
                 consumerEndPointMap.put(info.processId, new HostInfo(hostPort, Integer.valueOf(hostPort)));
@@ -460,6 +469,7 @@ public Subscription subscription(Set<String> topics) {
                 assignmentSuppliers.add(new AssignmentSupplier(consumer,
+                                                               minUserMetadataVersion,
                                                                active,
                                                                standby,
                                                                endPointMap,
@@ -483,17 +493,20 @@ public Subscription subscription(Set<String> topics) {
     class AssignmentSupplier {
         private final String consumer;
+        private final int metadataVersion;
         private final List<TaskId> active;
         private final Map<TaskId, Set<TopicPartition>> standby;
         private final Map<HostInfo, Set<TopicPartition>> endPointMap;
         private final List<TopicPartition> activePartitions;
         AssignmentSupplier(final String consumer,
+                           final int metadataVersion,
                            final List<TaskId> active,
                            final Map<TaskId, Set<TopicPartition>> standby,
                            final Map<HostInfo, Set<TopicPartition>> endPointMap,
                            final List<TopicPartition> activePartitions) {
             this.consumer = consumer;
+            this.metadataVersion = metadataVersion;
             this.active = active;
             this.standby = standby;
             this.endPointMap = endPointMap;
@@ -501,7 +514,8 @@ public Subscription subscription(Set<String> topics) {
         }
         Assignment get() {
-            return new Assignment(activePartitions, new AssignmentInfo(active,
+            return new Assignment(activePartitions, new AssignmentInfo(metadataVersion,
+                                                                       active,
                                                                        standby,
                                                                        endPointMap).encode());
         }
diff --git [file java] [file java]
index 6569f8587bc..ce9aa6309ca 100644
--- [file java]
+++ [file java]
@@ -14,7 +14,6 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 package org.apache.kafka.streams.processor.internals.assignment;
 import org.apache.kafka.common.record.ByteBufferInputStream;
@@ -56,7 +55,7 @@ public AssignmentInfo(List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>>
         this(CURRENT_VERSION, activeTasks, standbyTasks, hostState);
     }
-    protected AssignmentInfo(int version, List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>> standbyTasks,
+    public AssignmentInfo(int version, List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>> standbyTasks,
                              Map<HostInfo, Set<TopicPartition>> hostState) {
         this.version = version;
         this.activeTasks = activeTasks;
@@ -155,9 +154,7 @@ public static AssignmentInfo decode(ByteBuffer data) {
                 }
             }
-            return new AssignmentInfo(activeTasks, standbyTasks, hostStateToTopicPartitions);
-
-
+            return new AssignmentInfo(version, activeTasks, standbyTasks, hostStateToTopicPartitions);
         } catch (IOException ex) {
             throw new TaskAssignmentException(""Failed to decode AssignmentInfo"", ex);
         }
diff --git [file java] [file java]
index c3481c05156..92c50a2a942 100644
--- [file java]
+++ [file java]
@@ -14,7 +14,6 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-
 package org.apache.kafka.streams.processor.internals.assignment;
 import org.apache.kafka.streams.errors.TaskAssignmentException;
@@ -32,7 +31,7 @@
     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);
-    private static final int CURRENT_VERSION = 2;
+    public static final int CURRENT_VERSION = 2;
     public final int version;
     public final UUID processId;
@@ -44,7 +43,7 @@ public SubscriptionInfo(UUID processId, Set<TaskId> prevTasks, Set<TaskId> stand
         this(CURRENT_VERSION, processId, prevTasks, standbyTasks, userEndPoint);
     }
-    private SubscriptionInfo(int version, UUID processId, Set<TaskId> prevTasks, Set<TaskId> standbyTasks, String userEndPoint) {
+    public SubscriptionInfo(int version, UUID processId, Set<TaskId> prevTasks, Set<TaskId> standbyTasks, String userEndPoint) {
         this.version = version;
         this.processId = processId;
         this.prevTasks = prevTasks;
diff --git [file java] [file java]
index 35b88db583b..e4ba9cdf143 100644
--- [file java]
+++ [file java]
@@ -22,6 +22,7 @@
 import org.apache.kafka.streams.kstream.KStreamBuilder;
 import org.apache.kafka.streams.processor.StreamPartitioner;
 import org.apache.kafka.test.MockMetricsReporter;
+import org.apache.kafka.test.TestUtils;
 import org.junit.Assert;
 import org.junit.ClassRule;
 import org.junit.Test;
@@ -39,11 +40,12 @@
     public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(NUM_BROKERS);
     @Test
-    public void testStartAndClose() throws Exception {
+    public void testStartAndClose() {
         final Properties props = new Properties();
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""testStartAndClose"");
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
         props.setProperty(StreamsConfig.METRIC_REPORTER_CLASSES_CONFIG, MockMetricsReporter.class.getName());
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());
         final int oldInitCount = MockMetricsReporter.INIT_COUNT.get();
         final int oldCloseCount = MockMetricsReporter.CLOSE_COUNT.get();
@@ -62,11 +64,12 @@ public void testStartAndClose() throws Exception {
     }
     @Test
-    public void testCloseIsIdempotent() throws Exception {
+    public void testCloseIsIdempotent() {
         final Properties props = new Properties();
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""testCloseIsIdempotent"");
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
         props.setProperty(StreamsConfig.METRIC_REPORTER_CLASSES_CONFIG, MockMetricsReporter.class.getName());
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());
         final KStreamBuilder builder = new KStreamBuilder();
         final KafkaStreams streams = new KafkaStreams(builder, props);
@@ -79,10 +82,11 @@ public void testCloseIsIdempotent() throws Exception {
     }
     @Test(expected = IllegalStateException.class)
-    public void testCannotStartOnceClosed() throws Exception {
+    public void testCannotStartOnceClosed() {
         final Properties props = new Properties();
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""testCannotStartOnceClosed"");
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());
         final KStreamBuilder builder = new KStreamBuilder();
         final KafkaStreams streams = new KafkaStreams(builder, props);
@@ -99,10 +103,11 @@ public void testCannotStartOnceClosed() throws Exception {
     }
     @Test(expected = IllegalStateException.class)
-    public void testCannotStartTwice() throws Exception {
+    public void testCannotStartTwice() {
         final Properties props = new Properties();
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""testCannotStartTwice"");
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());
         final KStreamBuilder builder = new KStreamBuilder();
         final KafkaStreams streams = new KafkaStreams(builder, props);
@@ -119,25 +124,25 @@ public void testCannotStartTwice() throws Exception {
     }
     @Test(expected = IllegalStateException.class)
-    public void shouldNotGetAllTasksWhenNotRunning() throws Exception {
+    public void shouldNotGetAllTasksWhenNotRunning() {
         final KafkaStreams streams = createKafkaStreams();
         streams.allMetadata();
     }
     @Test(expected = IllegalStateException.class)
-    public void shouldNotGetAllTasksWithStoreWhenNotRunning() throws Exception {
+    public void shouldNotGetAllTasksWithStoreWhenNotRunning() {
         final KafkaStreams streams = createKafkaStreams();
         streams.allMetadataForStore(""store"");
     }
     @Test(expected = IllegalStateException.class)
-    public void shouldNotGetTaskWithKeyAndSerializerWhenNotRunning() throws Exception {
+    public void shouldNotGetTaskWithKeyAndSerializerWhenNotRunning() {
         final KafkaStreams streams = createKafkaStreams();
         streams.metadataForKey(""store"", ""key"", Serdes.String().serializer());
     }
     @Test(expected = IllegalStateException.class)
-    public void shouldNotGetTaskWithKeyAndPartitionerWhenNotRunning() throws Exception {
+    public void shouldNotGetTaskWithKeyAndPartitionerWhenNotRunning() {
         final KafkaStreams streams = createKafkaStreams();
         streams.metadataForKey(""store"", ""key"", new StreamPartitioner<String, Object>() {
             @Override
@@ -152,16 +157,18 @@ private KafkaStreams createKafkaStreams() {
         final Properties props = new Properties();
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""appId"");
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());
         final KStreamBuilder builder = new KStreamBuilder();
         return new KafkaStreams(builder, props);
     }
     @Test
-    public void testCleanup() throws Exception {
+    public void testCleanup() {
         final Properties props = new Properties();
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""testLocalCleanup"");
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());
         final KStreamBuilder builder = new KStreamBuilder();
         final KafkaStreams streams = new KafkaStreams(builder, props);
@@ -173,10 +180,11 @@ public void testCleanup() throws Exception {
     }
     @Test(expected = IllegalStateException.class)
-    public void testCannotCleanupWhileRunning() throws Exception {
+    public void testCannotCleanupWhileRunning() {
         final Properties props = new Properties();
         props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""testCannotCleanupWhileRunning"");
         props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        props.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());
         final KStreamBuilder builder = new KStreamBuilder();
         final KafkaStreams streams = new KafkaStreams(builder, props);
diff --git [file java] [file java]
index f03bed95c1a..9d401487d6c 100644
--- [file java]
+++ [file java]
@@ -58,7 +58,7 @@ public void setUp() {
     }
     @Test
-    public void testGetProducerConfigs() throws Exception {
+    public void testGetProducerConfigs() {
         Map<String, Object> returnedProps = streamsConfig.getProducerConfigs(""client"");
         assertEquals(returnedProps.get(ProducerConfig.CLIENT_ID_CONFIG), ""client-producer"");
         assertEquals(returnedProps.get(ProducerConfig.LINGER_MS_CONFIG), ""100"");
@@ -66,7 +66,7 @@ public void testGetProducerConfigs() throws Exception {
     }
     @Test
-    public void testGetConsumerConfigs() throws Exception {
+    public void testGetConsumerConfigs() {
         Map<String, Object> returnedProps = streamsConfig.getConsumerConfigs(null, ""example-application"", ""client"");
         assertEquals(returnedProps.get(ConsumerConfig.CLIENT_ID_CONFIG), ""client-consumer"");
         assertEquals(returnedProps.get(ConsumerConfig.GROUP_ID_CONFIG), ""example-application"");
@@ -75,7 +75,7 @@ public void testGetConsumerConfigs() throws Exception {
     }
     @Test
-    public void testGetRestoreConsumerConfigs() throws Exception {
+    public void testGetRestoreConsumerConfigs() {
         Map<String, Object> returnedProps = streamsConfig.getRestoreConsumerConfigs(""client"");
         assertEquals(returnedProps.get(ConsumerConfig.CLIENT_ID_CONFIG), ""client-restore-consumer"");
         assertNull(returnedProps.get(ConsumerConfig.GROUP_ID_CONFIG));
@@ -84,7 +84,7 @@ public void testGetRestoreConsumerConfigs() throws Exception {
     @Test
     public void defaultSerdeShouldBeConfigured() {
-        Map<String, Object> serializerConfigs = new HashMap<String, Object>();
+        Map<String, Object> serializerConfigs = new HashMap<>();
         serializerConfigs.put(""key.serializer.encoding"", ""UTF8"");
         serializerConfigs.put(""value.serializer.encoding"", ""UTF-16"");
         Serializer<String> serializer = Serdes.String().serializer();
@@ -115,7 +115,7 @@ public void shouldSupportMultipleBootstrapServers() {
     }
     @Test
-    public void shouldSupportPrefixedConsumerConfigs() throws Exception {
+    public void shouldSupportPrefixedConsumerConfigs() {
         props.put(consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), ""earliest"");
         props.put(consumerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -125,7 +125,7 @@ public void shouldSupportPrefixedConsumerConfigs() throws Exception {
     }
     @Test
-    public void shouldSupportPrefixedRestoreConsumerConfigs() throws Exception {
+    public void shouldSupportPrefixedRestoreConsumerConfigs() {
         props.put(consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), ""earliest"");
         props.put(consumerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -135,7 +135,7 @@ public void shouldSupportPrefixedRestoreConsumerConfigs() throws Exception {
     }
     @Test
-    public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() throws Exception {
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() {
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         props.put(consumerPrefix(""interceptor.statsd.host""), ""host"");
         final Map<String, Object> consumerConfigs = streamsConfig.getConsumerConfigs(null, ""groupId"", ""clientId"");
@@ -143,7 +143,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfConsumerConfig() thro
     }
     @Test
-    public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig() throws Exception {
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig() {
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         props.put(consumerPrefix(""interceptor.statsd.host""), ""host"");
         final Map<String, Object> consumerConfigs = streamsConfig.getRestoreConsumerConfigs(""clientId"");
@@ -151,7 +151,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfRestoreConsumerConfig
     }
     @Test
-    public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() throws Exception {
+    public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() {
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         props.put(producerPrefix(""interceptor.statsd.host""), ""host"");
         final Map<String, Object> producerConfigs = streamsConfig.getProducerConfigs(""clientId"");
@@ -160,7 +160,7 @@ public void shouldSupportPrefixedPropertiesThatAreNotPartOfProducerConfig() thro
     @Test
-    public void shouldSupportPrefixedProducerConfigs() throws Exception {
+    public void shouldSupportPrefixedProducerConfigs() {
         props.put(producerPrefix(ProducerConfig.BUFFER_MEMORY_CONFIG), 10);
         props.put(producerPrefix(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG), 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -170,7 +170,7 @@ public void shouldSupportPrefixedProducerConfigs() throws Exception {
     }
     @Test
-    public void shouldBeSupportNonPrefixedConsumerConfigs() throws Exception {
+    public void shouldBeSupportNonPrefixedConsumerConfigs() {
         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -180,7 +180,7 @@ public void shouldBeSupportNonPrefixedConsumerConfigs() throws Exception {
     }
     @Test
-    public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() throws Exception {
+    public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() {
         props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -190,7 +190,7 @@ public void shouldBeSupportNonPrefixedRestoreConsumerConfigs() throws Exception
     }
     @Test
-    public void shouldSupportNonPrefixedProducerConfigs() throws Exception {
+    public void shouldSupportNonPrefixedProducerConfigs() {
         props.put(ProducerConfig.BUFFER_MEMORY_CONFIG, 10);
         props.put(ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, 1);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -199,24 +199,22 @@ public void shouldSupportNonPrefixedProducerConfigs() throws Exception {
         assertEquals(1, configs.get(ProducerConfig.METRICS_NUM_SAMPLES_CONFIG));
     }
-
-
     @Test(expected = StreamsException.class)
-    public void shouldThrowStreamsExceptionIfKeySerdeConfigFails() throws Exception {
+    public void shouldThrowStreamsExceptionIfKeySerdeConfigFails() {
         props.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, MisconfiguredSerde.class);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.keySerde();
     }
     @Test(expected = StreamsException.class)
-    public void shouldThrowStreamsExceptionIfValueSerdeConfigFails() throws Exception {
+    public void shouldThrowStreamsExceptionIfValueSerdeConfigFails() {
         props.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, MisconfiguredSerde.class);
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.valueSerde();
     }
     @Test
-    public void shouldOverrideStreamsDefaultConsumerConfigs() throws Exception {
+    public void shouldOverrideStreamsDefaultConsumerConfigs() {
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), ""latest"");
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), ""10"");
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -226,7 +224,7 @@ public void shouldOverrideStreamsDefaultConsumerConfigs() throws Exception {
     }
     @Test
-    public void shouldOverrideStreamsDefaultProducerConfigs() throws Exception {
+    public void shouldOverrideStreamsDefaultProducerConfigs() {
         props.put(StreamsConfig.producerPrefix(ProducerConfig.LINGER_MS_CONFIG), ""10000"");
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         final Map<String, Object> producerConfigs = streamsConfig.getProducerConfigs(""client"");
@@ -234,7 +232,7 @@ public void shouldOverrideStreamsDefaultProducerConfigs() throws Exception {
     }
     @Test
-    public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() throws Exception {
+    public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() {
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG), ""latest"");
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG), ""10"");
         final StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -244,14 +242,14 @@ public void shouldOverrideStreamsDefaultConsumerConifgsOnRestoreConsumer() throw
     }
     @Test(expected = ConfigException.class)
-    public void shouldThrowExceptionIfConsumerAutoCommitIsOverridden() throws Exception {
+    public void shouldThrowExceptionIfConsumerAutoCommitIsOverridden() {
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG), ""true"");
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.getConsumerConfigs(null, ""a"", ""b"");
     }
     @Test(expected = ConfigException.class)
-    public void shouldThrowExceptionIfRestoreConsumerAutoCommitIsOverridden() throws Exception {
+    public void shouldThrowExceptionIfRestoreConsumerAutoCommitIsOverridden() {
         props.put(StreamsConfig.consumerPrefix(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG), ""true"");
         final StreamsConfig streamsConfig = new StreamsConfig(props);
         streamsConfig.getRestoreConsumerConfigs(""client"");
diff --git [file java] [file java]
index a5fb0763187..88098dc679b 100644
--- [file java]
+++ [file java]
@@ -32,6 +32,7 @@
 import org.apache.kafka.streams.kstream.KStream;
 import org.apache.kafka.streams.kstream.KStreamBuilder;
 import org.apache.kafka.streams.kstream.ValueMapper;
+import org.apache.kafka.test.TestUtils;
 import org.junit.BeforeClass;
 import org.junit.ClassRule;
 import org.junit.Test;
@@ -79,13 +80,12 @@
     private static final String OUTPUT_TOPIC_C = ""C"";
     @BeforeClass
-    public static void startKafkaCluster() throws Exception {
+    public static void startKafkaCluster() {
         CLUSTER.createTopic(INPUT_TOPIC_A);
         CLUSTER.createTopic(OUTPUT_TOPIC_B);
         CLUSTER.createTopic(OUTPUT_TOPIC_C);
     }
-
     @Parameter
     public long cacheSizeBytes;
@@ -117,6 +117,7 @@ public void shouldFanoutTheInput() throws Exception {
         streamsConfiguration.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());
         streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");
         streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, cacheSizeBytes);
+        streamsConfiguration.setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());
         final KStream<byte, String> stream1 = builder.stream(INPUT_TOPIC_A);
         final KStream<byte, String> stream2 = stream1.mapValues(
diff --git [file java] [file java]
index ab08dbe0f59..eeb455bcc1e 100644
--- [file java]
+++ [file java]
@@ -122,8 +122,8 @@ public void shouldReduce() throws Exception {
         List<KeyValue<String, String>> results = receiveMessages(
             new StringDeserializer(),
-            new StringDeserializer()
-            , 5);
+            new StringDeserializer(),
+            5);
         Collections.sort(results, new Comparator<KeyValue<String, String>>() {
             @Override
@@ -172,8 +172,8 @@ public String apply(Windowed<String> windowedKey, String value) {
         List<KeyValue<String, String>> windowedOutput = receiveMessages(
             new StringDeserializer(),
-            new StringDeserializer()
-            , 10);
+            new StringDeserializer(),
+            10);
         Comparator<KeyValue<String, String>>
             comparator =
diff --git [file java] [file java]
index e5560c1b62f..383a79363a8 100644
--- [file java]
+++ [file java]
@@ -97,11 +97,10 @@ public void before() {
         streamsConfiguration = new Properties();
         final String applicationId = ""kgrouped-stream-test-"" + testNo;
         streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);
-        streamsConfiguration
-            .put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
+        streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
         streamsConfiguration.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, CLUSTER.zKConnectString());
         streamsConfiguration.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");
-        streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());
+        streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());
         streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1);
         streamsConfiguration.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, cacheSizeBytes);
@@ -155,8 +154,8 @@ public void shouldReduce() throws Exception {
         final List<KeyValue<String, String>> results = receiveMessages(
             new StringDeserializer(),
-            new StringDeserializer()
-            , 10);
+            new StringDeserializer(),
+            10);
         Collections.sort(results, new Comparator<KeyValue<String, String>>() {
             @Override
@@ -209,8 +208,8 @@ public String apply(final Windowed<String> windowedKey, final String value) {
         final List<KeyValue<String, String>> windowedOutput = receiveMessages(
             new StringDeserializer(),
-            new StringDeserializer()
-            , 15);
+            new StringDeserializer(),
+            15);
         final Comparator<KeyValue<String, String>>
             comparator =
@@ -263,8 +262,8 @@ public void shouldAggregate() throws Exception {
         final List<KeyValue<String, Integer>> results = receiveMessages(
             new StringDeserializer(),
-            new IntegerDeserializer()
-            , 10);
+            new IntegerDeserializer(),
+            10);
         Collections.sort(results, new Comparator<KeyValue<String, Integer>>() {
             @Override
@@ -313,8 +312,8 @@ public String apply(final Windowed<String> windowedKey, final Integer value) {
         final List<KeyValue<String, Integer>> windowedMessages = receiveMessages(
             new StringDeserializer(),
-            new IntegerDeserializer()
-            , 15);
+            new IntegerDeserializer(),
+            15);
         final Comparator<KeyValue<String, Integer>>
             comparator =
@@ -364,8 +363,8 @@ public void shouldCount() throws Exception {
         final List<KeyValue<String, Long>> results = receiveMessages(
             new StringDeserializer(),
-            new LongDeserializer()
-            , 10);
+            new LongDeserializer(),
+            10);
         Collections.sort(results, new Comparator<KeyValue<String, Long>>() {
             @Override
             public int compare(final KeyValue<String, Long> o1, final KeyValue<String, Long> o2) {
@@ -406,8 +405,8 @@ public String apply(final Windowed<Integer> windowedKey, final Long value) {
         final List<KeyValue<String, Long>> results = receiveMessages(
             new StringDeserializer(),
-            new LongDeserializer()
-            , 10);
+            new LongDeserializer(),
+            10);
         Collections.sort(results, new Comparator<KeyValue<String, Long>>() {
             @Override
             public int compare(final KeyValue<String, Long> o1, final KeyValue<String, Long> o2) {
diff --git [file java] [file java]
index 5f85536efe3..7848d1b3f2b 100644
--- [file java]
+++ [file java]
@@ -260,7 +260,7 @@ private Properties prepareTest() throws Exception {
         streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG, APP_ID + testNo);
         streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers());
         streamsConfiguration.put(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, CLUSTER.zKConnectString());
-        streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath());
+        streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());
         streamsConfiguration.put(StreamsConfig.KEY_SERDE_CLASS_CONFIG, Serdes.Long().getClass());
         streamsConfiguration.put(StreamsConfig.VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());
         streamsConfiguration.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 8);
diff --git [file java] [file java]
index e46a016447d..8a1e13a1925 100644
--- [file java]
+++ [file java]
@@ -37,6 +37,7 @@
 import org.apache.kafka.test.MockProcessorSupplier;
 import org.apache.kafka.test.MockStateStoreSupplier;
 import org.apache.kafka.test.MockTimestampExtractor;
+import org.apache.kafka.test.TestUtils;
 import org.junit.Assert;
 import org.junit.Test;
@@ -99,13 +100,14 @@ private Properties configProps() {
                 setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, ""localhost:2171"");
                 setProperty(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, ""3"");
                 setProperty(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, MockTimestampExtractor.class.getName());
+                setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());
             }
         };
     }
     @SuppressWarnings(""unchecked"")
     @Test
-    public void testSubscription() throws Exception {
+    public void testSubscription() {
         StreamsConfig config = new StreamsConfig(configProps());
         TopologyBuilder builder = new TopologyBuilder();
@@ -148,7 +150,7 @@ public void testSubscription() throws Exception {
     }
     @Test
-    public void testAssignBasic() throws Exception {
+    public void testAssignBasic() {
         StreamsConfig config = new StreamsConfig(configProps());
         TopologyBuilder builder = new TopologyBuilder();
@@ -215,7 +217,7 @@ public void testAssignBasic() throws Exception {
     }
     @Test
-    public void testAssignWithNewTasks() throws Exception {
+    public void testAssignWithNewTasks() {
         StreamsConfig config = new StreamsConfig(configProps());
         TopologyBuilder builder = new TopologyBuilder();
@@ -274,7 +276,7 @@ public void testAssignWithNewTasks() throws Exception {
     }
     @Test
-    public void testAssignWithStates() throws Exception {
+    public void testAssignWithStates() {
         StreamsConfig config = new StreamsConfig(configProps());
         String applicationId = ""test"";
         TopologyBuilder builder = new TopologyBuilder();
@@ -334,7 +336,7 @@ public void testAssignWithStates() throws Exception {
     }
     @Test
-    public void testAssignWithStandbyReplicas() throws Exception {
+    public void testAssignWithStandbyReplicas() {
         Properties props = configProps();
         props.setProperty(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, ""1"");
         StreamsConfig config = new StreamsConfig(props);
@@ -449,7 +451,7 @@ private AssignmentInfo checkAssignment(PartitionAssignor.Assignment assignment)
     }
     @Test
-    public void testOnAssignment() throws Exception {
+    public void testOnAssignment() {
         StreamsConfig config = new StreamsConfig(configProps());
         TopicPartition t2p3 = new TopicPartition(""topic2"", 3);
@@ -482,7 +484,7 @@ public void testOnAssignment() throws Exception {
     }
     @Test
-    public void testAssignWithInternalTopics() throws Exception {
+    public void testAssignWithInternalTopics() {
         StreamsConfig config = new StreamsConfig(configProps());
         String applicationId = ""test"";
         TopologyBuilder builder = new TopologyBuilder();
@@ -521,7 +523,7 @@ public void testAssignWithInternalTopics() throws Exception {
     }
     @Test
-    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throws Exception {
+    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() {
         StreamsConfig config = new StreamsConfig(configProps());
         String applicationId = ""test"";
         TopologyBuilder builder = new TopologyBuilder();
@@ -555,7 +557,7 @@ public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throw
         subscriptions.put(""consumer10"",
                           new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, emptyTasks, emptyTasks, userEndPoint).encode()));
-        Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
+        partitionAssignor.assign(metadata, subscriptions);
         // check prepared internal topics
         assertEquals(2, internalTopicManager.readyTopics.size());
@@ -563,7 +565,7 @@ public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throw
     }
     @Test
-    public void shouldAddUserDefinedEndPointToSubscription() throws Exception {
+    public void shouldAddUserDefinedEndPointToSubscription() {
         final Properties properties = configProps();
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, ""localhost:8080"");
         final StreamsConfig config = new StreamsConfig(properties);
@@ -589,7 +591,70 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {
     }
     @Test
-    public void shouldMapUserEndPointToTopicPartitions() throws Exception {
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() {
+        final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
+        final Set<TaskId> emptyTasks = Collections.emptySet();
+        subscriptions.put(
+            ""consumer1"",
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(""topic1""),
+                new SubscriptionInfo(1, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+        subscriptions.put(
+            ""consumer2"",
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(""topic1""),
+                new SubscriptionInfo(2, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+
+        final StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
+        StreamsConfig config = new StreamsConfig(configProps());
+
+        final TopologyBuilder builder = new TopologyBuilder();
+        final StreamThread streamThread = new StreamThread(
+            builder,
+            config,
+            new MockClientSupplier(),
+            ""applicationId"",
+            ""clientId"",
+            UUID.randomUUID(),
+            new Metrics(),
+            new SystemTime(),
+            new StreamsMetadataState(builder));
+
+        partitionAssignor.configure(config.getConsumerConfigs(streamThread, ""test"", ""clientId""));
+        final Map<String, PartitionAssignor.Assignment> assignment = partitionAssignor.assign(metadata, subscriptions);
+
+        assertEquals(2, assignment.size());
+        assertEquals(1, AssignmentInfo.decode(assignment.get(""consumer1"").userData()).version);
+        assertEquals(1, AssignmentInfo.decode(assignment.get(""consumer2"").userData()).version);
+    }
+
+    @Test
+    public void shouldDownGradeSubscription() {
+        final Properties properties = configProps();
+        properties.put(StreamsConfig.UPGRADE_FROM_CONFIG, StreamsConfig.UPGRADE_FROM_0100);
+        final StreamsConfig config = new StreamsConfig(properties);
+
+        final TopologyBuilder builder = new TopologyBuilder();
+        builder.addSource(""source1"", ""topic1"");
+
+        final String clientId = ""client-id"";
+        final StreamThread thread = new StreamThread(builder, config, new MockClientSupplier(), ""test"", clientId,
+            UUID.randomUUID(), new Metrics(), new SystemTime(), new StreamsMetadataState(builder));
+
+        final StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
+        partitionAssignor.configure(config.getConsumerConfigs(thread, ""test"", clientId));
+
+        final PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""topic1""));
+
+        assertEquals(1, SubscriptionInfo.decode(subscription.userData()).version);
+    }
+
+    @Test
+    public void shouldMapUserEndPointToTopicPartitions() {
         final Properties properties = configProps();
         final String myEndPoint = ""localhost:8080"";
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);
@@ -628,7 +693,7 @@ public void shouldMapUserEndPointToTopicPartitions() throws Exception {
     }
     @Test
-    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() throws Exception {
+    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() {
         final Properties properties = configProps();
         final String myEndPoint = ""localhost"";
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);
@@ -655,7 +720,7 @@ public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() thr
     }
     @Test
-    public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() throws Exception {
+    public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() {
         final Properties properties = configProps();
         final String myEndPoint = ""localhost:j87yhk"";
         properties.put(StreamsConfig.APPLICATION_SERVER_CONFIG, myEndPoint);
@@ -682,7 +747,7 @@ public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() th
     }
     @Test
-    public void shouldExposeHostStateToTopicPartitionsOnAssignment() throws Exception {
+    public void shouldExposeHostStateToTopicPartitionsOnAssignment() {
         final StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
         List<TopicPartition> topic = Arrays.asList(new TopicPartition(""topic"", 0));
         final Map<HostInfo, Set<TopicPartition>> hostState =
@@ -696,7 +761,7 @@ public void shouldExposeHostStateToTopicPartitionsOnAssignment() throws Exceptio
     }
     @Test
-    public void shouldSetClusterMetadataOnAssignment() throws Exception {
+    public void shouldSetClusterMetadataOnAssignment() {
         final StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
         final List<TopicPartition> topic = Arrays.asList(new TopicPartition(""topic"", 0));
@@ -718,7 +783,7 @@ public void shouldSetClusterMetadataOnAssignment() throws Exception {
     }
     @Test
-    public void shouldReturnEmptyClusterMetadataIfItHasntBeenBuilt() throws Exception {
+    public void shouldReturnEmptyClusterMetadataIfItHasntBeenBuilt() {
         final StreamPartitionAssignor partitionAssignor = new StreamPartitionAssignor();
         final Cluster cluster = partitionAssignor.clusterMetadata();
         assertNotNull(cluster);
diff --git [file java] [file java]
index 2f252e9fa60..2ea57388f2c 100644
--- [file java]
+++ [file java]
@@ -17,11 +17,6 @@
 package org.apache.kafka.streams.processor.internals;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertSame;
-import static org.junit.Assert.assertTrue;
-
 import org.apache.kafka.clients.consumer.Consumer;
 import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;
 import org.apache.kafka.clients.consumer.internals.PartitionAssignor;
@@ -40,6 +35,7 @@
 import org.apache.kafka.test.MockClientSupplier;
 import org.apache.kafka.test.MockProcessorSupplier;
 import org.apache.kafka.test.MockTimestampExtractor;
+import org.apache.kafka.test.TestUtils;
 import org.junit.Test;
 import java.io.File;
@@ -56,6 +52,11 @@
 import java.util.Set;
 import java.util.UUID;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertSame;
+import static org.junit.Assert.assertTrue;
+
 public class StreamThreadTest {
     private final String clientId = ""clientId"";
@@ -117,6 +118,7 @@ private Properties configProps() {
                 setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, ""localhost:2171"");
                 setProperty(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, ""3"");
                 setProperty(StreamsConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG, MockTimestampExtractor.class.getName());
+                setProperty(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getAbsolutePath());
             }
         };
     }
diff --git [file java] [file java]
index ce94a23a2c9..6c94c18991a 100644
--- [file java]
+++ [file java]
@@ -65,10 +65,9 @@ public void shouldDecodePreviousVersion() throws Exception {
         assertEquals(oldVersion.activeTasks, decoded.activeTasks);
         assertEquals(oldVersion.standbyTasks, decoded.standbyTasks);
         assertEquals(0, decoded.partitionsByHostState.size()); // should be empty as wasn't in V1
-        assertEquals(2, decoded.version); // automatically upgraded to v2 on decode;
+        assertEquals(1, decoded.version);
     }
-
     /**
      * This is a clone of what the V1 encoding did. The encode method has changed for V2
      * so it is impossible to test compatibility without having this
diff --git [file java] [file java]
index f920c515c0e..63ad01d8609 100644
--- [file java]
+++ [file java]
@@ -105,7 +105,7 @@ public boolean test(String key, Integer value) {
             }
         });
-        data.process(SmokeTestUtil.<Integer>printProcessorSupplier(""data""));
+        data.process(SmokeTestUtil.<String, Integer>printProcessorSupplier(""data""));
         // min
         KGroupedStream<String, Integer>
@@ -131,7 +131,7 @@ public Integer apply(String aggKey, Integer value, Integer aggregate) {
         ).to(stringSerde, intSerde, ""min"");
         KTable<String, Integer> minTable = builder.table(stringSerde, intSerde, ""min"", ""minStoreName"");
-        minTable.toStream().process(SmokeTestUtil.<Integer>printProcessorSupplier(""min""));
+        minTable.toStream().process(SmokeTestUtil.<String, Integer>printProcessorSupplier(""min""));
         // max
         groupedData.aggregate(
@@ -153,7 +153,7 @@ public Integer apply(String aggKey, Integer value, Integer aggregate) {
         ).to(stringSerde, intSerde, ""max"");
         KTable<String, Integer> maxTable = builder.table(stringSerde, intSerde, ""max"", ""maxStoreName"");
-        maxTable.toStream().process(SmokeTestUtil.<Integer>printProcessorSupplier(""max""));
+        maxTable.toStream().process(SmokeTestUtil.<String, Integer>printProcessorSupplier(""max""));
         // sum
         groupedData.aggregate(
@@ -176,7 +176,7 @@ public Long apply(String aggKey, Integer value, Long aggregate) {
         KTable<String, Long> sumTable = builder.table(stringSerde, longSerde, ""sum"", ""sumStoreName"");
-        sumTable.toStream().process(SmokeTestUtil.<Long>printProcessorSupplier(""sum""));
+        sumTable.toStream().process(SmokeTestUtil.<String, Long>printProcessorSupplier(""sum""));
         // cnt
         groupedData.count(UnlimitedWindows.of(), ""uwin-cnt"")
@@ -185,7 +185,7 @@ public Long apply(String aggKey, Integer value, Long aggregate) {
         ).to(stringSerde, longSerde, ""cnt"");
         KTable<String, Long> cntTable = builder.table(stringSerde, longSerde, ""cnt"", ""cntStoreName"");
-        cntTable.toStream().process(SmokeTestUtil.<Long>printProcessorSupplier(""cnt""));
+        cntTable.toStream().process(SmokeTestUtil.<String, Long>printProcessorSupplier(""cnt""));
         // dif
         maxTable.join(minTable,
diff --git [file java] [file java]
index f9d30d50e36..f103355ac9e 100644
--- [file java]
+++ [file java]
@@ -125,38 +125,48 @@ public void run() {
     }
     public static Map<String, Set<Integer>> generate(String kafka, final int numKeys, final int maxRecordsPerKey) throws Exception {
-        Properties props = new Properties();
+        return generate(kafka, numKeys, maxRecordsPerKey, true);
+    }
+
+    public static Map<String, Set<Integer>> generate(final String kafka,
+                                                     final int numKeys,
+                                                     final int maxRecordsPerKey,
+                                                     final boolean autoTerminate) throws Exception {
+        final Properties props = new Properties();
         props.put(ProducerConfig.CLIENT_ID_CONFIG, ""SmokeTest"");
         props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
         props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
         props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
-        KafkaProducer<byte, byte> producer = new KafkaProducer<>(props);
+        final KafkaProducer<byte, byte> producer = new KafkaProducer<>(props);
         int numRecordsProduced = 0;
-        Map<String, Set<Integer>> allData = new HashMap<>();
-        ValueList data = new ValueList;
+        final Map<String, Set<Integer>> allData = new HashMap<>();
+        final ValueList data = new ValueList;
         for (int i = 0; i < numKeys; i++) {
             data = new ValueList(i, i + maxRecordsPerKey - 1);
             allData.put(data.key, new HashSet<Integer>());
         }
-        Random rand = new Random();
+        final Random rand = new Random();
-        int remaining = data.length;
+        int remaining = 1; // dummy value must be positive if <autoTerminate> is false
+        if (autoTerminate) {
+            remaining = data.length;
+        }
         while (remaining > 0) {
-            int index = rand.nextInt(remaining);
-            String key = data.key;
+            final int index = autoTerminate ? rand.nextInt(remaining) : rand.nextInt(numKeys);
+            final String key = data.key;
             int value = data.next();
-            if (value < 0) {
+            if (autoTerminate && value < 0) {
                 remaining--;
                 data = data;
                 value = END;
             }
-            ProducerRecord<byte, byte> record =
+            final ProducerRecord<byte, byte> record =
                     new ProducerRecord<>(""data"", stringSerde.serializer().serialize("""", key), intSerde.serializer().serialize("""", value));
             producer.send(record);
@@ -165,8 +175,9 @@ public void run() {
                 numRecordsProduced++;
                 allData.get(key).add(value);
-                if (numRecordsProduced % 100 == 0)
+                if (numRecordsProduced % 100 == 0) {
                     System.out.println(numRecordsProduced + "" records produced"");
+                }
                 Thread.sleep(10);
             }
diff --git [file java] [file java]
index 7ff738f1d95..d9ad745e169 100644
--- [file java]
+++ [file java]
@@ -37,27 +37,20 @@
     public final static long START_TIME = 60000L * 60 * 24 * 365 * 30;
     public final static int END = Integer.MAX_VALUE;
-    public static <T> ProcessorSupplier<String, T> printProcessorSupplier(final String topic) {
-        return printProcessorSupplier(topic, false);
-    }
-
-    public static <T> ProcessorSupplier<String, T> printProcessorSupplier(final String topic, final boolean printOffset) {
-        return new ProcessorSupplier<String, T>() {
-            public Processor<String, T> get() {
-                return new AbstractProcessor<String, T>() {
+    public static <K, V> ProcessorSupplier<K, V> printProcessorSupplier(final String topic) {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
                     private int numRecordsProcessed = 0;
-                    private ProcessorContext context;
                     @Override
                     public void init(ProcessorContext context) {
                         System.out.println(""initializing processor: topic="" + topic + "" taskId="" + context.taskId());
                         numRecordsProcessed = 0;
-                        this.context = context;
                     }
                     @Override
-                    public void process(String key, T value) {
-                        if (printOffset) System.out.println("">>> "" + context.offset());
+                    public void process(K key, V value) {
                         numRecordsProcessed++;
                         if (numRecordsProcessed % 100 == 0) {
                             System.out.println(""processed "" + numRecordsProcessed + "" records from topic="" + topic);
@@ -65,12 +58,10 @@ public void process(String key, T value) {
                     }
                     @Override
-                    public void punctuate(long timestamp) {
-                    }
+                    public void punctuate(long timestamp) {}
                     @Override
-                    public void close() {
-                    }
+                    public void close() {}
                 };
             }
         };
diff --git [file java] [file java]
index c26544e4fc5..3328ae5e480 100644
--- [file java]
+++ [file java]
@@ -24,7 +24,7 @@
 public class StreamsSmokeTest {
     /**
-     *  args ::= command kafka zookeeper stateDir
+     *  args ::= command kafka zookeeper stateDir disableAutoTerminate
      *  command := ""run"" | ""process""
      *
      * @param args
@@ -34,12 +34,14 @@ public static void main(String args) throws Exception {
         String kafka = args.length > 1 ? args : null;
         String zookeeper = args.length > 2 ? args : null;
         String stateDir = args.length > 3 ? args : null;
+        boolean disableAutoTerminate = args.length > 4;
-        System.out.println(""StreamsSmokeTest instance started"");
+        System.out.println(""StreamsTest instance started (StreamsSmokeTest)"");
         System.out.println(""command="" + command);
         System.out.println(""kafka="" + kafka);
         System.out.println(""zookeeper="" + zookeeper);
         System.out.println(""stateDir="" + stateDir);
+        System.out.println(""disableAutoTerminate="" + disableAutoTerminate);
         switch (command) {
             case ""standalone"":
@@ -49,8 +51,12 @@ public static void main(String args) throws Exception {
                 // this starts the driver (data generation and result verification)
                 final int numKeys = 10;
                 final int maxRecordsPerKey = 500;
-                Map<String, Set<Integer>> allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
-                SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                if (disableAutoTerminate) {
+                    SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey, false);
+                } else {
+                    Map<String, Set<Integer>> allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
+                    SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                }
                 break;
             case ""process"":
                 // this starts a KafkaStreams client
diff --git [file java] [file java]
new file mode 100644
index 00000000000..e771bead6e8
--- /dev/null
+++ [file java]
@@ -0,0 +1,78 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.smoketest.SmokeTestUtil;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 3) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir, ) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args + "" "" : """")
+                + (args.length > 1 ? args : """"));
+        }
+        final String kafka = args;
+        final String zookeeper = args;
+        final String stateDir = args;
+        final String upgradeFrom = args.length > 3 ? args : null;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest trunk)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""zookeeper="" + zookeeper);
+        System.out.println(""stateDir="" + stateDir);
+        System.out.println(""upgradeFrom="" + upgradeFrom);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(SmokeTestUtil.printProcessorSupplier(""data""));
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        if (upgradeFrom != null) {
+            config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+        }
+
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+}
diff --git [file java] [file java]
index 83a9092fa20..1bedd870032 100644
--- [file java]
+++ [file java]
@@ -345,7 +345,7 @@ public synchronized long position(TopicPartition partition) {
             // consumer.subscribe(new TopicPartition(topicName, 1));
             // Set up the partition that matches the ID (which is what ProcessorStateManager expects) ...
             List<PartitionInfo> partitionInfos = new ArrayList<>();
-            partitionInfos.add(new PartitionInfo(topicName , id.partition, null, null, null));
+            partitionInfos.add(new PartitionInfo(topicName, id.partition, null, null, null));
             consumer.updatePartitions(topicName, partitionInfos);
             consumer.updateEndOffsets(Collections.singletonMap(new TopicPartition(topicName, id.partition), 0L));
         }
diff --git [file java] [file java]
new file mode 100644
index 00000000000..7d3ed436881
--- /dev/null
+++ [file java]
@@ -0,0 +1,104 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 3) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args + "" "" : """")
+                + (args.length > 1 ? args : """"));
+        }
+        final String kafka = args;
+        final String zookeeper = args;
+        final String stateDir = args;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.10.0)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""zookeeper="" + zookeeper);
+        System.out.println(""stateDir="" + stateDir);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git a/tests/kafkatest/services/streams.py b/tests/kafkatest/services/streams.py
index a63810e2bd4..b857bd53224 100644
--- a/tests/kafkatest/services/streams.py
+++ b/tests/kafkatest/services/streams.py
@@ -33,6 +33,8 @@ class StreamsSmokeTestBaseService(KafkaPathResolverMixin, Service):
     LOG4J_CONFIG_FILE = os.path.join(PERSISTENT_ROOT, ""tools-log4j.properties"")
     PID_FILE = os.path.join(PERSISTENT_ROOT, ""streams.pid"")
+    CLEAN_NODE_ENABLED = True
+
     logs = {
         ""streams_log"": {
             ""path"": LOG_FILE,
@@ -43,6 +45,114 @@ class StreamsSmokeTestBaseService(KafkaPathResolverMixin, Service):
         ""streams_stderr"": {
             ""path"": STDERR_FILE,
             ""collect_default"": True},
+        ""streams_log.0-1"": {
+            ""path"": LOG_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_stdout.0-1"": {
+            ""path"": STDOUT_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_stderr.0-1"": {
+            ""path"": STDERR_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_log.0-2"": {
+            ""path"": LOG_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_stdout.0-2"": {
+            ""path"": STDOUT_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_stderr.0-2"": {
+            ""path"": STDERR_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_log.0-3"": {
+            ""path"": LOG_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_stdout.0-3"": {
+            ""path"": STDOUT_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_stderr.0-3"": {
+            ""path"": STDERR_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_log.0-4"": {
+            ""path"": LOG_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_stdout.0-4"": {
+            ""path"": STDOUT_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_stderr.0-4"": {
+            ""path"": STDERR_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_log.0-5"": {
+            ""path"": LOG_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_stdout.0-5"": {
+            ""path"": STDOUT_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_stderr.0-5"": {
+            ""path"": STDERR_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_log.0-6"": {
+            ""path"": LOG_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_stdout.0-6"": {
+            ""path"": STDOUT_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_stderr.0-6"": {
+            ""path"": STDERR_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_log.1-1"": {
+            ""path"": LOG_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_stdout.1-1"": {
+            ""path"": STDOUT_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_stderr.1-1"": {
+            ""path"": STDERR_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_log.1-2"": {
+            ""path"": LOG_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_stdout.1-2"": {
+            ""path"": STDOUT_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_stderr.1-2"": {
+            ""path"": STDERR_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_log.1-3"": {
+            ""path"": LOG_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_stdout.1-3"": {
+            ""path"": STDOUT_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_stderr.1-3"": {
+            ""path"": STDERR_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_log.1-4"": {
+            ""path"": LOG_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_stdout.1-4"": {
+            ""path"": STDOUT_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_stderr.1-4"": {
+            ""path"": STDERR_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_log.1-5"": {
+            ""path"": LOG_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_stdout.1-5"": {
+            ""path"": STDOUT_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_stderr.1-5"": {
+            ""path"": STDERR_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_log.1-6"": {
+            ""path"": LOG_FILE + "".1-6"",
+            ""collect_default"": True},
+        ""streams_stdout.1-6"": {
+            ""path"": STDOUT_FILE + "".1-6"",
+            ""collect_default"": True},
+        ""streams_stderr.1-6"": {
+            ""path"": STDERR_FILE + "".1-6"",
+            ""collect_default"": True},
     }
     def __init__(self, context, kafka, command):
@@ -95,7 +205,8 @@ def wait(self):
     def clean_node(self, node):
         node.account.kill_process(""streams"", clean_shutdown=False, allow_fail=True)
-        node.account.ssh(""rm -rf "" + self.PERSISTENT_ROOT, allow_fail=False)
+        if self.CLEAN_NODE_ENABLED:
+            node.account.ssh(""rm -rf "" + self.PERSISTENT_ROOT, allow_fail=False)
     def start_cmd(self, node):
         args = self.args.copy()
@@ -120,10 +231,10 @@ def start_node(self, node):
         node.account.create_file(self.LOG4J_CONFIG_FILE, self.render('tools_log4j.properties', log_file=self.LOG_FILE))
-        self.logger.info(""Starting StreamsSmokeTest process on "" + str(node.account))
+        self.logger.info(""Starting StreamsTest process on "" + str(node.account))
         with node.account.monitor_log(self.STDOUT_FILE) as monitor:
             node.account.ssh(self.start_cmd(node))
-            monitor.wait_until('StreamsSmokeTest instance started', timeout_sec=15, err_msg=""Never saw message indicating StreamsSmokeTest finished startup on "" + str(node.account))
+            monitor.wait_until('StreamsTest instance started', timeout_sec=15, err_msg=""Never saw message indicating StreamsTest finished startup on "" + str(node.account))
         if len(self.pids(node)) == 0:
             raise RuntimeError(""No process ids recorded"")
@@ -132,8 +243,62 @@ def start_node(self, node):
 class StreamsSmokeTestDriverService(StreamsSmokeTestBaseService):
     def __init__(self, context, kafka):
         super(StreamsSmokeTestDriverService, self).__init__(context, kafka, ""run"")
+        self.DISABLE_AUTO_TERMINATE = """"
+    def disable_auto_terminate(self):
+        self.DISABLE_AUTO_TERMINATE = ""disableAutoTerminate""
+
+    def start_cmd(self, node):
+        args = self.args.copy()
+        args = self.kafka.bootstrap_servers()
+        args = self.kafka.zk.connect_setting()
+        args = self.PERSISTENT_ROOT
+        args = self.STDOUT_FILE
+        args = self.STDERR_FILE
+        args = self.PID_FILE
+        args = self.LOG4J_CONFIG_FILE
+        args = self.DISABLE_AUTO_TERMINATE
+        args = self.path.script(""kafka-run-class.sh"", node)
+
+        cmd = ""( export KAFKA_LOG4J_OPTS=\""-Dlog4j.configuration=file:%(log4j)s\""; "" \
+              ""INCLUDE_TEST_JARS=true %(kafka_run_class)s org.apache.kafka.streams.smoketest.StreamsSmokeTest "" \
+              "" %(command)s %(kafka)s %(zk)s %(state_dir)s %(disable_auto_terminate)s "" \
+              "" & echo $! >&3 ) 1>> %(stdout)s 2>> %(stderr)s 3> %(pidfile)s"" % args
+
+        return cmd
 class StreamsSmokeTestJobRunnerService(StreamsSmokeTestBaseService):
     def __init__(self, context, kafka):
         super(StreamsSmokeTestJobRunnerService, self).__init__(context, kafka, ""process"")
+
+class StreamsUpgradeTestJobRunnerService(StreamsSmokeTestBaseService):
+    def __init__(self, context, kafka):
+        super(StreamsUpgradeTestJobRunnerService, self).__init__(context, kafka, """")
+        self.UPGRADE_FROM = """"
+
+    def set_version(self, kafka_streams_version):
+        self.KAFKA_STREAMS_VERSION = kafka_streams_version
+
+    def set_upgrade_from(self, upgrade_from):
+        self.UPGRADE_FROM = upgrade_from
+
+    def start_cmd(self, node):
+        args = self.args.copy()
+        args = self.kafka.bootstrap_servers()
+        args = self.kafka.zk.connect_setting()
+        args = self.PERSISTENT_ROOT
+        args = self.STDOUT_FILE
+        args = self.STDERR_FILE
+        args = self.PID_FILE
+        args = self.LOG4J_CONFIG_FILE
+        args = self.KAFKA_STREAMS_VERSION
+        args = self.UPGRADE_FROM
+        args = self.path.script(""kafka-run-class.sh"", node)
+
+        cmd = ""( export KAFKA_LOG4J_OPTS=\""-Dlog4j.configuration=file:%(log4j)s\""; "" \
+              "" INCLUDE_TEST_JARS=true UPGRADE_KAFKA_STREAMS_TEST_VERSION=%(version)s "" \
+              "" %(kafka_run_class)s org.apache.kafka.streams.tests.StreamsUpgradeTest "" \
+              "" %(kafka)s %(zk)s %(state_dir)s %(upgrade_from)s "" \
+              "" & echo $! >&3 ) 1>> %(stdout)s 2>> %(stderr)s 3> %(pidfile)s"" % args
+
+        return cmd
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py
new file mode 100644
index 00000000000..8266e073d13
--- /dev/null
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py
@@ -0,0 +1,200 @@
+# Licensed to the Apache Software Foundation (ASF) under one or more
+# contributor license agreements.  See the NOTICE file distributed with
+# this work for additional information regarding copyright ownership.
+# The ASF licenses this file to You under the Apache License, Version 2.0
+# (the ""License""); you may not use this file except in compliance with
+# the License.  You may obtain a copy of the License at
+#
+#    [link]
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an ""AS IS"" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+
+from kafkatest.tests.kafka_test import KafkaTest
+from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsUpgradeTestJobRunnerService
+from kafkatest.version import LATEST_0_10_0, TRUNK_VERSION
+import random
+
+class StreamsUpgradeTest(KafkaTest):
+    """"""
+    Test upgrading Kafka Streams from 0.10.0.x to 0.10.1.x (ie, TRUNK)
+    """"""
+
+    def __init__(self, test_context):
+        super(StreamsUpgradeTest, self).__init__(test_context, num_zk=1, num_brokers=1, topics={
+            'echo' : { 'partitions': 5 },
+            'data' : { 'partitions': 5 }
+        })
+
+        self.driver = StreamsSmokeTestDriverService(test_context, self.kafka)
+        self.driver.disable_auto_terminate()
+        self.processor1 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)
+        self.processor2 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)
+        self.processor3 = StreamsUpgradeTestJobRunnerService(test_context, self.kafka)
+
+    def test_upgrade(self):
+        """"""
+        Starts 3 KafkaStreams instances with version 0.10.0, and upgrades one-by-one to 0.10.1
+        """"""
+
+        self.driver.start()
+        self.start_all_nodes_with_0100()
+
+        self.processors = 
+
+        counter = 1
+        random.seed()
+
+        # first rolling bounce
+        random.shuffle(self.processors)
+        for p in self.processors:
+            p.CLEAN_NODE_ENABLED = False
+            self.do_rolling_bounce(p, ""0.10.0"", counter)
+            counter = counter + 1
+
+        # second rolling bounce
+        random.shuffle(self.processors)
+        for p in self.processors:
+            self.do_rolling_bounce(p, """", counter)
+            counter = counter + 1
+
+        # shutdown
+        self.driver.stop()
+        self.driver.wait()
+
+        random.shuffle(self.processors)
+        for p in self.processors:
+            node = p.node
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:
+                p.stop()
+                monitor.wait_until(""UPGRADE-TEST-CLIENT-CLOSED"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'UPGRADE-TEST-CLIENT-CLOSED' on"" + str(node.account))
+
+        self.driver.stop()
+
+    def start_all_nodes_with_0100(self):
+        # start first with 0.10.0
+        self.prepare_for_0100(self.processor1)
+        node1 = self.processor1.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as monitor:
+            with node1.account.monitor_log(self.processor1.LOG_FILE) as log_monitor:
+                self.processor1.start()
+                log_monitor.wait_until(""Kafka version : 0.10.0.1"",
+                                       timeout_sec=60,
+                                       err_msg=""Could not detect Kafka Streams version 0.10.0.1"" + str(node1.account))
+                monitor.wait_until(""processed 100 records from topic"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
+
+        # start second with 0.10.0
+        self.prepare_for_0100(self.processor2)
+        node2 = self.processor2.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:
+                with node2.account.monitor_log(self.processor2.LOG_FILE) as log_monitor:
+                    self.processor2.start()
+                    log_monitor.wait_until(""Kafka version : 0.10.0.1"",
+                                           timeout_sec=60,
+                                           err_msg=""Could not detect Kafka Streams version 0.10.0.1"" + str(node2.account))
+                    first_monitor.wait_until(""processed 100 records from topic"",
+                                             timeout_sec=60,
+                                             err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
+                    second_monitor.wait_until(""processed 100 records from topic"",
+                                              timeout_sec=60,
+                                              err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node2.account))
+
+        # start third with 0.10.0
+        self.prepare_for_0100(self.processor3)
+        node3 = self.processor3.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:
+                with node3.account.monitor_log(self.processor3.STDOUT_FILE) as third_monitor:
+                    with node3.account.monitor_log(self.processor3.LOG_FILE) as log_monitor:
+                        self.processor3.start()
+                        log_monitor.wait_until(""Kafka version : 0.10.0.1"",
+                                               timeout_sec=60,
+                                               err_msg=""Could not detect Kafka Streams version 0.10.0.1"" + str(node3.account))
+                        first_monitor.wait_until(""processed 100 records from topic"",
+                                                 timeout_sec=60,
+                                                 err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
+                        second_monitor.wait_until(""processed 100 records from topic"",
+                                                  timeout_sec=60,
+                                                  err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node2.account))
+                        third_monitor.wait_until(""processed 100 records from topic"",
+                                                  timeout_sec=60,
+                                                  err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node3.account))
+
+    @staticmethod
+    def prepare_for_0100(processor):
+        processor.node.account.ssh(""rm -rf "" + processor.PERSISTENT_ROOT, allow_fail=False)
+        processor.set_version(str(LATEST_0_10_0))
+
+    def do_rolling_bounce(self, processor, upgrade_from, counter):
+        first_other_processor = None
+        second_other_processor = None
+        for p in self.processors:
+            if p != processor:
+                if first_other_processor is None:
+                    first_other_processor = p
+                else:
+                    second_other_processor = p
+
+        node = processor.node
+        first_other_node = first_other_processor.node
+        second_other_node = second_other_processor.node
+
+        # stop processor and wait for rebalance of others
+        with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:
+            with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:
+                processor.stop()
+                first_other_monitor.wait_until(""processed 100 records from topic"",
+                                               timeout_sec=60,
+                                               err_msg=""Never saw output 'processed 100 records from topic' on"" + str(first_other_node.account))
+                second_other_monitor.wait_until(""processed 100 records from topic"",
+                                                timeout_sec=60,
+                                                err_msg=""Never saw output 'processed 100 records from topic' on"" + str(second_other_node.account))
+        node.account.ssh_capture(""grep UPGRADE-TEST-CLIENT-CLOSED %s"" % processor.STDOUT_FILE, allow_fail=False)
+
+        if upgrade_from == """":  # upgrade disabled -- second round of rolling bounces
+            roll_counter = "".1-""  # second round of rolling bounces
+        else:
+            roll_counter = "".0-""  # first  round of rolling boundes
+
+        node.account.ssh(""mv "" + processor.STDOUT_FILE + "" "" + processor.STDOUT_FILE + roll_counter + str(counter), allow_fail=False)
+        node.account.ssh(""mv "" + processor.STDERR_FILE + "" "" + processor.STDERR_FILE + roll_counter + str(counter), allow_fail=False)
+        node.account.ssh(""mv "" + processor.LOG_FILE + "" "" + processor.LOG_FILE + roll_counter + str(counter), allow_fail=False)
+
+        processor.set_version("""")  # set to TRUNK
+        processor.set_upgrade_from(upgrade_from)
+
+        grep_metadata_error = ""grep \""org.apache.kafka.streams.errors.TaskAssignmentException: unable to decode subscription data: version=2\"" ""
+        with node.account.monitor_log(processor.STDOUT_FILE) as monitor:
+            with node.account.monitor_log(processor.LOG_FILE) as log_monitor:
+                with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:
+                    with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:
+                        processor.start()
+
+                        log_monitor.wait_until(""Kafka version : "" + str(TRUNK_VERSION),
+                                               timeout_sec=60,
+                                               err_msg=""Could not detect Kafka Streams version "" + str(TRUNK_VERSION) + "" "" + str(node.account))
+                        first_other_monitor.wait_until(""processed 100 records from topic"",
+                                                       timeout_sec=60,
+                                                       err_msg=""Never saw output 'processed 100 records from topic' on"" + str(first_other_node.account))
+                        found = list(first_other_node.account.ssh_capture(grep_metadata_error + first_other_processor.STDERR_FILE, allow_fail=True))
+                        if len(found) > 0:
+                            raise Exception(""Kafka Streams failed with 'unable to decode subscription data: version=2'"")
+
+                        second_other_monitor.wait_until(""processed 100 records from topic"",
+                                                        timeout_sec=60,
+                                                        err_msg=""Never saw output 'processed 100 records from topic' on"" + str(second_other_node.account))
+                        found = list(second_other_node.account.ssh_capture(grep_metadata_error + second_other_processor.STDERR_FILE, allow_fail=True))
+                        if len(found) > 0:
+                            raise Exception(""Kafka Streams failed with 'unable to decode subscription data: version=2'"")
+
+                        monitor.wait_until(""processed 100 records from topic"",
+                                           timeout_sec=60,
+                                           err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node.account))
\ No newline at end of file
diff --git a/tests/kafkatest/version.py b/tests/kafkatest/version.py
index 239a9f45d37..ebf5ecf6a51 100644
--- a/tests/kafkatest/version.py
+++ b/tests/kafkatest/version.py
@@ -64,6 +64,7 @@ def get_version(node=None):
         return TRUNK
 TRUNK = KafkaVersion(""trunk"")
+TRUNK_VERSION = KafkaVersion(""0.10.1.2-SNAPSHOT"")
 # 0.8.2.X versions
 V_0_8_2_1 = KafkaVersion(""0.8.2.1"")
@@ -78,7 +79,11 @@ def get_version(node=None):
 # 0.10.0.X versions
 V_0_10_0_0 = KafkaVersion(""0.10.0.0"")
 V_0_10_0_1 = KafkaVersion(""0.10.0.1"")
-# Adding 0.10.0 as the next version will be 0.10.1.x
 LATEST_0_10_0 = V_0_10_0_1
-LATEST_0_10 = LATEST_0_10_0
\ No newline at end of file
+# 0.10.1.X versions
+V_0_10_1_0 = KafkaVersion(""0.10.1.0"")
+V_0_10_1_1 = KafkaVersion(""0.10.1.1"")
+LATEST_0_10_1 = V_0_10_1_1
+
+LATEST_0_10 = LATEST_0_10_1
diff --git a/vagrant/base.sh b/vagrant/base.sh
index 88878dcc2ed..b8cde3a48cc 100755
--- a/vagrant/base.sh
+++ b/vagrant/base.sh
@@ -52,6 +52,8 @@ get_kafka() {
     kafka_dir=/opt/kafka-$version
     url=[link]
+    # the .tgz above does not include the streams test jar hence we need to get it separately
+    url_streams_test=[link]
     if ; then
         pushd /tmp
         curl -O $url
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 15 **
mjsax closed pull request #4768:  KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git a/bin/kafka-run-class.sh b/bin/kafka-run-class.sh
index fc89f25d9d2..81b423afa1e 100755
--- a/bin/kafka-run-class.sh
+++ b/bin/kafka-run-class.sh
@@ -73,28 +73,50 @@ do
   fi
 done
-for file in ""$base_dir""/clients/build/libs/kafka-clients*.jar;
-do
-  if should_include_file ""$file""; then
-    CLASSPATH=""$CLASSPATH"":""$file""
-  fi
-done
+if ; then
+  clients_lib_dir=$(dirname $0)/../clients/build/libs
+  streams_lib_dir=$(dirname $0)/../streams/build/libs
+  rocksdb_lib_dir=$(dirname $0)/../streams/build/dependant-libs-${SCALA_VERSION}
+else
+  clients_lib_dir=/opt/kafka-$UPGRADE_KAFKA_STREAMS_TEST_VERSION/libs
+  streams_lib_dir=$clients_lib_dir
+  rocksdb_lib_dir=$streams_lib_dir
+fi
+
-for file in ""$base_dir""/streams/build/libs/kafka-streams*.jar;
+for file in ""$clients_lib_dir""/kafka-clients*.jar;
 do
   if should_include_file ""$file""; then
     CLASSPATH=""$CLASSPATH"":""$file""
   fi
 done
-for file in ""$base_dir""/streams/examples/build/libs/kafka-streams-examples*.jar;
+for file in ""$streams_lib_dir""/kafka-streams*.jar;
 do
   if should_include_file ""$file""; then
     CLASSPATH=""$CLASSPATH"":""$file""
   fi
 done
-for file in ""$base_dir""/streams/build/dependant-libs-${SCALA_VERSION}/rocksdb*.jar;
+if ; then
+  for file in ""$base_dir""/streams/examples/build/libs/kafka-streams-examples*.jar;
+  do
+    if should_include_file ""$file""; then
+      CLASSPATH=""$CLASSPATH"":""$file""
+    fi
+  done
+else
+  VERSION_NO_DOTS=`echo $UPGRADE_KAFKA_STREAMS_TEST_VERSION | sed 's/\.//g'`
+  SHORT_VERSION_NO_DOTS=${VERSION_NO_DOTS:0:((${#VERSION_NO_DOTS} - 1))} # remove last char, ie, bug-fix number
+  for file in ""$base_dir""/streams/upgrade-system-tests-$SHORT_VERSION_NO_DOTS/build/libs/kafka-streams-upgrade-system-tests*.jar;
+  do
+    if should_include_file ""$file""; then
+      CLASSPATH=""$CLASSPATH"":""$file""
+    fi
+  done
+fi
+
+for file in ""$rocksdb_lib_dir""/rocksdb*.jar;
 do
   CLASSPATH=""$CLASSPATH"":""$file""
 done
diff --git a/build.gradle b/build.gradle
index 7a3b4119f81..727bdeae8a5 100644
--- a/build.gradle
+++ b/build.gradle
@@ -961,6 +961,54 @@ project(':streams:examples') {
   }
 }
+project(':streams:upgrade-system-tests-0100') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0100""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0100
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
+project(':streams:upgrade-system-tests-0101') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0101""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0101
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
+project(':streams:upgrade-system-tests-0102') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0102""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0102
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
+project(':streams:upgrade-system-tests-0110') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0110""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0110
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
 project(':jmh-benchmarks') {
   apply plugin: 'com.github.johnrengelman.shadow'
diff --git a/checkstyle/suppressions.xml b/checkstyle/suppressions.xml
index acff3f625bc..bc7a2bd5779 100644
--- a/checkstyle/suppressions.xml
+++ b/checkstyle/suppressions.xml
@@ -181,7 +181,7 @@
               files=""[file java]""/>
     <suppress checks=""NPathComplexity""
-              files=""[file java]""/>
+              files=""[file java]|[file java]""/>
     <suppress checks=""NPathComplexity""
               files=""[file java]""/>
diff --git [file java] [file java]
index 4756387bba8..4853f92ed6a 100644
--- [file java]
+++ [file java]
@@ -16,7 +16,8 @@
  */
 package org.apache.kafka.common.security.authenticator;
-import java.util.Map;
+import org.apache.kafka.common.config.SaslConfigs;
+import org.apache.kafka.common.network.Mode;
 import javax.security.auth.Subject;
 import javax.security.auth.callback.Callback;
@@ -25,9 +26,7 @@
 import javax.security.auth.callback.UnsupportedCallbackException;
 import javax.security.sasl.AuthorizeCallback;
 import javax.security.sasl.RealmCallback;
-
-import org.apache.kafka.common.config.SaslConfigs;
-import org.apache.kafka.common.network.Mode;
+import java.util.Map;
 /**
  * Callback handler for Sasl clients. The callbacks required for the SASL mechanism
diff --git a/docs/streams/upgrade-guide.html b/docs/streams/upgrade-guide.html
index 87d34f127a7..58cb91c7e62 100644
--- a/docs/streams/upgrade-guide.html
+++ b/docs/streams/upgrade-guide.html
@@ -34,28 +34,46 @@ <h1>Upgrade Guide &amp; API Changes</h1>
     </div>
     <p>
-        If you want to upgrade from 0.11.0.x to 1.0.0 you don't need to do any code changes as the public API is fully backward compatible.
+        If you want to upgrade from 0.10.2.x or 0.11.0.x to 1.0.x you don't need to do any code changes as the public API is fully backward compatible.
         However, some public APIs were deprecated and thus it is recommended to update your code eventually to allow for future upgrades.
-        See <a href=""#streams_api_changes_100"">below</a> a complete list of 1.0.0 API and semantic changes that allow you to advance your application and/or simplify your code base, including the usage of new features.
+        See below a complete list of <a href=""#streams_api_changes_100"">1.0</a> and <a href=""#streams_api_changes_0110"">0.11.0</a> API
+        and semantic changes that allow you to advance your application and/or simplify your code base, including the usage of new features.
+        Additionally, Streams API 1.0.x requires broker on-disk message format version 0.10 or higher; thus, you need to make sure that the message
+        format is configured correctly before you upgrade your Kafka Streams application.
     </p>
     <p>
-        If you want to upgrade from 0.10.2.x to 0.11.0 you don't need to do any code changes as the public API is fully backward compatible.
-        However, some configuration parameters were deprecated and thus it is recommended to update your code eventually to allow for future upgrades.
-        See <a href=""#streams_api_changes_0110"">below</a> a complete list of 0.11.0 API and semantic changes that allow you to advance your application and/or simplify your code base, including the usage of new features.
+        If you want to upgrade from 0.10.1.x to 1.0.x see the Upgrade Sections for <a href=""/{{version}}/documentation/#upgrade_1020_streams""><b>0.10.2</b></a>,
+        <a href=""/{{version}}/documentation/#upgrade_1100_streams""><b>0.11.0</b></a>, and
+        <a href=""/{{version}}/documentation/#upgrade_100_streams""><b>1.0</b></a>.
+        Note, that a brokers on-disk message format must be on version 0.10 or higher to run a Kafka Streams application version 1.0 or higher.
+        See below a complete list of <a href=""#streams_api_changes_0102"">0.10.2</a>, <a href=""#streams_api_changes_0110"">0.11.0</a>,
+        and <a href=""#streams_api_changes_100"">1.0</a> API and semantical changes that allow you to advance your application and/or simplify your code base, including the usage of new features.
     </p>
     <p>
-        If you want to upgrade from 0.10.1.x to 0.10.2, see the <a href=""/{{version}}/documentation/#upgrade_1020_streams""><b>Upgrade Section for 0.10.2</b></a>.
-        It highlights incompatible changes you need to consider to upgrade your code and application.
-        See <a href=""#streams_api_changes_0102"">below</a> a complete list of 0.10.2 API and semantic changes that allow you to advance your application and/or simplify your code base, including the usage of new features.
-    </p>
-
-    <p>
-        If you want to upgrade from 0.10.0.x to 0.10.1, see the <a href=""/{{version}}/documentation/#upgrade_1010_streams""><b>Upgrade Section for 0.10.1</b></a>.
-        It highlights incompatible changes you need to consider to upgrade your code and application.
-        See <a href=""#streams_api_changes_0101"">below</a> a complete list of 0.10.1 API changes that allow you to advance your application and/or simplify your code base, including the usage of new features.
+        Upgrading from 0.10.0.x to 1.0.x directly is also possible.
+        Note, that a brokers must be on version 0.10.1 or higher and on-disk message format must be on version 0.10 or higher
+        to run a Kafka Streams application version 1.0 or higher.
+        See <a href=""#streams_api_changes_0101"">Streams API changes in 0.10.1</a>, <a href=""#streams_api_changes_0102"">Streams API changes in 0.10.2</a>,
+        <a href=""#streams_api_changes_0110"">Streams API changes in 0.11.0</a>, and <a href=""#streams_api_changes_100"">Streams API changes in 1.0</a>
+        for a complete list of API changes.
+        Upgrading to 1.0.2 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+        (cf. <a href=""[link]"">KIP-268</a>).
+        As an alternative, an offline upgrade is also possible.
     </p>
+    <ul>
+        <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 1.0.2</li>
+        <li> bounce each instance of your application once </li>
+        <li> prepare your newly deployed 1.0.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+        <li> bounce each instance of your application once more to complete the upgrade </li>
+    </ul>
+    <p> Upgrading from 0.10.0.x to 1.0.0 or 1.0.1 requires an offline upgrade (rolling bounce upgrade is not supported) </p>
+    <ul>
+        <li> stop all old (0.10.0.x) application instances </li>
+        <li> update your code and swap old code and jar file with new code and new jar file </li>
+        <li> restart all new (1.0.0 or 1.0.1) application instances </li>
+    </ul>
     <h3><a id=""streams_api_changes_100"" href=""#streams_api_changes_100"">Streams API changes in 1.0.0</a></h3>
diff --git a/docs/upgrade.html b/docs/upgrade.html
index caf8e1cb276..33b94d4299c 100644
--- a/docs/upgrade.html
+++ b/docs/upgrade.html
@@ -129,17 +129,77 @@ <h5><a id=""upgrade_100_new_protocols"" href=""#upgrade_100_new_protocols"">New Prot
          be used if the SaslHandshake request version is greater than 0. </li>
 </ul>
-<h5><a id=""upgrade_100_streams"" href=""#upgrade_100_streams"">Upgrading a 1.0.0 Kafka Streams Application</a></h5>
+<h5><a id=""upgrade_100_streams"" href=""#upgrade_100_streams"">Upgrading a 0.11.0 Kafka Streams Application</a></h5>
 <ul>
     <li> Upgrading your Streams application from 0.11.0 to 1.0.0 does not require a broker upgrade.
-        A Kafka Streams 1.0.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).
-        However, Kafka Streams 1.0 requires 0.10 message format or newer and does not work with older message formats. </li>
+         A Kafka Streams 1.0.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).
+         However, Kafka Streams 1.0 requires 0.10 message format or newer and does not work with older message formats. </li>
     <li> If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. </li>
     <li> There are a few public APIs including <code>ProcessorContext#schedule()</code>, <code>Processor#punctuate()</code> and <code>KStreamBuilder</code>, <code>TopologyBuilder</code> are being deprecated by new APIs.
-        We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.
     <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_100"">Streams API changes in 1.0.0</a> for more details. </li>
 </ul>
+<h5><a id=""upgrade_100_streams_from_0102"" href=""#upgrade_100_streams_from_0102"">Upgrading a 0.10.2 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.2 to 1.0 does not require a broker upgrade.
+         A Kafka Streams 1.0 application can connect to 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
+    <li> If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. </li>
+    <li> There are a few public APIs including <code>ProcessorContext#schedule()</code>, <code>Processor#punctuate()</code> and <code>KStreamBuilder</code>, <code>TopologyBuilder</code> are being deprecated by new APIs.
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.
+    <li> If you specify customized <code>key.serde</code>, <code>value.serde</code> and <code>timestamp.extractor</code> in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. </li>
+    <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0110"">Streams API changes in 0.11.0</a> for more details. </li>
+</ul>
+
+<h5><a id=""upgrade_100_streams_from_0101"" href=""#upgrade_1100_streams_from_0101"">Upgrading a 0.10.1 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.1 to 1.0 does not require a broker upgrade.
+         A Kafka Streams 1.0 application can connect to 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
+    <li> You need to recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. </li>
+    <li> There are a few public APIs including <code>ProcessorContext#schedule()</code>, <code>Processor#punctuate()</code> and <code>KStreamBuilder</code>, <code>TopologyBuilder</code> are being deprecated by new APIs.
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.
+    <li> If you specify customized <code>key.serde</code>, <code>value.serde</code> and <code>timestamp.extractor</code> in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. </li>
+    <li> If you use a custom (i.e., user implemented) timestamp extractor, you will need to update this code, because the <code>TimestampExtractor</code> interface was changed. </li>
+    <li> If you register custom metrics, you will need to update this code, because the <code>StreamsMetric</code> interface was changed. </li>
+    <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_100"">Streams API changes in 1.0.0</a>,
+         <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0110"">Streams API changes in 0.11.0</a> and
+         <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0102"">Streams API changes in 0.10.2</a> for more details. </li>
+</ul>
+
+<h5><a id=""upgrade_100_streams_from_0100"" href=""#upgrade_100_streams_from_0100"">Upgrading a 0.10.0 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.0 to 1.0 does require a <a href=""#upgrade_10_1"">broker upgrade</a> because a Kafka Streams 1.0 application can only connect to 0.1, 0.11.0, 0.10.2, or 0.10.1 brokers. </li>
+    <li> There are couple of API changes, that are not backward compatible (cf. <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_100"">Streams API changes in 1.0.0</a>,
+         <a href=""/{{version}}/documentation/streams#streams_api_changes_0110"">Streams API changes in 0.11.0</a>,
+         <a href=""/{{version}}/documentation/streams#streams_api_changes_0102"">Streams API changes in 0.10.2</a>, and
+         <a href=""/{{version}}/documentation/streams#streams_api_changes_0101"">Streams API changes in 0.10.1</a> for more details).
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> Upgrading from 0.10.0.x to 1.0.2 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+        (cf. <a href=""[link]"">KIP-268</a>).
+        As an alternative, an offline upgrade is also possible.
+        <ul>
+            <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.11.0.3 </li>
+            <li> bounce each instance of your application once </li>
+            <li> prepare your newly deployed 1.0.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+            <li> bounce each instance of your application once more to complete the upgrade </li>
+        </ul>
+    </li>
+    <li> Upgrading from 0.10.0.x to 1.0.0 or 1.0.1 requires an offline upgrade (rolling bounce upgrade is not supported)
+        <ul>
+            <li> stop all old (0.10.0.x) application instances </li>
+            <li> update your code and swap old code and jar file with new code and new jar file </li>
+            <li> restart all new (1.0.0 or 1.0.1) application instances </li>
+        </ul>
+    </li>
+</ul>
+
+<h5><a id=""upgrade_102_notable"" href=""#upgrade_102_notable"">Notable changes in 1.0.2</a></h5>
+<ul>
+    <li> New Kafka Streams configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x </li>
+    <li> See the <a href=""/{{version}}/documentation/streams/upgrade-guide.html""><b>Kafka Streams upgrade guide</b></a> for details about this new config.
+</ul>
+
 <h4><a id=""upgrade_11_0_0"" href=""#upgrade_11_0_0"">Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x or 0.10.2.x to 0.11.0.0</a></h4>
 <p>Kafka 0.11.0.0 introduces a new message format version as well as wire protocol changes. By following the recommended rolling upgrade plan below,
   you guarantee no downtime during the upgrade. However, please review the <a href=""#upgrade_1100_notable"">notable changes in 0.11.0.0</a> before upgrading.
@@ -188,11 +248,55 @@ <h4><a id=""upgrade_11_0_0"" href=""#upgrade_11_0_0"">Upgrading from 0.8.x, 0.9.x, 0
 <h5><a id=""upgrade_1100_streams"" href=""#upgrade_1100_streams"">Upgrading a 0.10.2 Kafka Streams Application</a></h5>
 <ul>
     <li> Upgrading your Streams application from 0.10.2 to 0.11.0 does not require a broker upgrade.
-        A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
+         A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
     <li> If you specify customized <code>key.serde</code>, <code>value.serde</code> and <code>timestamp.extractor</code> in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. </li>
     <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0110"">Streams API changes in 0.11.0</a> for more details. </li>
 </ul>
+<h5><a id=""upgrade_1100_streams_from_0101"" href=""#upgrade_1100_streams_from_0101"">Upgrading a 0.10.1 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.1 to 0.11.0 does not require a broker upgrade.
+         A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
+    <li> You need to recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> If you specify customized <code>key.serde</code>, <code>value.serde</code> and <code>timestamp.extractor</code> in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. </li>
+    <li> If you use a custom (i.e., user implemented) timestamp extractor, you will need to update this code, because the <code>TimestampExtractor</code> interface was changed. </li>
+    <li> If you register custom metrics, you will need to update this code, because the <code>StreamsMetric</code> interface was changed. </li>
+    <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0110"">Streams API changes in 0.11.0</a> and
+         <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0102"">Streams API changes in 0.10.2</a> for more details. </li>
+</ul>
+
+<h5><a id=""upgrade_1100_streams_from_0100"" href=""#upgrade_1100_streams_from_0100"">Upgrading a 0.10.0 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.0 to 0.11.0 does require a <a href=""#upgrade_10_1"">broker upgrade</a> because a Kafka Streams 0.11.0 application can only connect to 0.11.0, 0.10.2, or 0.10.1 brokers. </li>
+    <li> There are couple of API changes, that are not backward compatible (cf. <a href=""/{{version}}/documentation/streams#streams_api_changes_0110"">Streams API changes in 0.11.0</a>,
+         <a href=""/{{version}}/documentation/streams#streams_api_changes_0102"">Streams API changes in 0.10.2</a>, and
+         <a href=""/{{version}}/documentation/streams#streams_api_changes_0101"">Streams API changes in 0.10.1</a> for more details).
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> Upgrading from 0.10.0.x to 0.11.0.3 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+        (cf. <a href=""[link]"">KIP-268</a>).
+        As an alternative, an offline upgrade is also possible.
+        <ul>
+            <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.11.0.3 </li>
+            <li> bounce each instance of your application once </li>
+            <li> prepare your newly deployed 0.11.0.3 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+            <li> bounce each instance of your application once more to complete the upgrade </li>
+        </ul>
+    </li>
+    <li> Upgrading from 0.10.0.x to 0.11.0.0, 0.11.0.1, or 0.11.0.2 requires an offline upgrade (rolling bounce upgrade is not supported)
+        <ul>
+            <li> stop all old (0.10.0.x) application instances </li>
+            <li> update your code and swap old code and jar file with new code and new jar file </li>
+            <li> restart all new (0.11.0.0 , 0.11.0.1, or 0.11.0.2) application instances </li>
+        </ul>
+    </li>
+</ul>
+
+<h5><a id=""upgrade_1103_notable"" href=""#upgrade_1103_notable"">Notable changes in 0.11.0.3</a></h5>
+<ul>
+    <li> New Kafka Streams configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x </li>
+    <li> See the <a href=""/{{version}}/documentation/streams/upgrade-guide.html""><b>Kafka Streams upgrade guide</b></a> for details about this new config.
+</ul>
+
 <h5><a id=""upgrade_1100_notable"" href=""#upgrade_1100_notable"">Notable changes in 0.11.0.0</a></h5>
 <ul>
     <li>Unclean leader election is now disabled by default. The new default favors durability over availability. Users who wish to
@@ -343,6 +447,35 @@ <h5><a id=""upgrade_1020_streams"" href=""#upgrade_1020_streams"">Upgrading a 0.10.1
     <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0102"">Streams API changes in 0.10.2</a> for more details. </li>
 </ul>
+<h5><a id=""upgrade_1020_streams_from_0100"" href=""#upgrade_1020_streams_from_0100"">Upgrading a 0.10.0 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.0 to 0.10.2 does require a <a href=""#upgrade_10_1"">broker upgrade</a> because a Kafka Streams 0.10.2 application can only connect to 0.10.2 or 0.10.1 brokers. </li>
+    <li> There are couple of API changes, that are not backward compatible (cf. <a href=""/{{version}}/documentation/streams#streams_api_changes_0102"">Streams API changes in 0.10.2</a> for more details).
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> Upgrading from 0.10.0.x to 0.10.2.2 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+         (cf. <a href=""[link]"">KIP-268</a>).
+         As an alternative, an offline upgrade is also possible.
+        <ul>
+            <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.10.2.2 </li>
+            <li> bounce each instance of your application once </li>
+            <li> prepare your newly deployed 0.10.2.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+            <li> bounce each instance of your application once more to complete the upgrade </li>
+        </ul>
+    </li>
+    <li> Upgrading from 0.10.0.x to 0.10.2.0 or 0.10.2.1 requires an offline upgrade (rolling bounce upgrade is not supported)
+        <ul>
+            <li> stop all old (0.10.0.x) application instances </li>
+            <li> update your code and swap old code and jar file with new code and new jar file </li>
+            <li> restart all new (0.10.2.0 or 0.10.2.1) application instances </li>
+        </ul>
+    </li>
+</ul>
+
+<h5><a id=""upgrade_10202_notable"" href=""#upgrade_10202_notable"">Notable changes in 0.10.2.2</a></h5>
+<ul>
+    <li> New configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x </li>
+</ul>
+
 <h5><a id=""upgrade_10201_notable"" href=""#upgrade_10201_notable"">Notable changes in 0.10.2.1</a></h5>
 <ul>
   <li> The default values for two configurations of the StreamsConfig class were changed to improve the resiliency of Kafka Streams applications. The internal Kafka Streams producer <code>retries</code> default value was changed from 0 to 10. The internal Kafka Streams consumer <code>max.poll.interval.ms</code>  default value was changed from 300000 to <code>Integer.MAX_VALUE</code>.
@@ -421,6 +554,23 @@ <h5><a id=""upgrade_1010_streams"" href=""#upgrade_1010_streams"">Upgrading a 0.10.0
     <li> Upgrading your Streams application from 0.10.0 to 0.10.1 does require a <a href=""#upgrade_10_1"">broker upgrade</a> because a Kafka Streams 0.10.1 application can only connect to 0.10.1 brokers. </li>
     <li> There are couple of API changes, that are not backward compatible (cf. <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0101"">Streams API changes in 0.10.1</a> for more details).
          Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> Upgrading from 0.10.0.x to 0.10.1.2 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+         (cf. <a href=""[link]"">KIP-268</a>).
+         As an alternative, an offline upgrade is also possible.
+        <ul>
+            <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.10.1.2 </li>
+            <li> bounce each instance of your application once </li>
+            <li> prepare your newly deployed 0.10.1.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+            <li> bounce each instance of your application once more to complete the upgrade </li>
+        </ul>
+    </li>
+    <li> Upgrading from 0.10.0.x to 0.10.1.0 or 0.10.1.1 requires an offline upgrade (rolling bounce upgrade is not supported)
+        <ul>
+            <li> stop all old (0.10.0.x) application instances </li>
+            <li> update your code and swap old code and jar file with new code and new jar file </li>
+            <li> restart all new (0.10.1.0 or 0.10.1.1) application instances </li>
+        </ul>
+    </li>
 </ul>
 <h5><a id=""upgrade_1010_notable"" href=""#upgrade_1010_notable"">Notable changes in 0.10.1.0</a></h5>
diff --git a/gradle/dependencies.gradle b/gradle/dependencies.gradle
index cfb0b9bb39c..f7027cf9de4 100644
--- a/gradle/dependencies.gradle
+++ b/gradle/dependencies.gradle
@@ -59,6 +59,10 @@ versions += [
   log4j: ""1.2.17"",
   jopt: ""5.0.4"",
   junit: ""4.12"",
+  kafka_0100: ""0.10.0.1"",
+  kafka_0101: ""0.10.1.1"",
+  kafka_0102: ""0.10.2.1"",
+  kafka_0110: ""0.11.0.2"",
   lz4: ""1.4"",
   metrics: ""2.2.0"",
   // PowerMock 1.x doesn't support Java 9, so use PowerMock 2.0.0 beta
@@ -95,11 +99,15 @@ libs += [
   jettyServlets: ""org.eclipse.jetty:jetty-servlets:$versions.jetty"",
   jerseyContainerServlet: ""org.glassfish.jersey.containers:jersey-container-servlet:$versions.jersey"",
   jmhCore: ""org.openjdk.jmh:jmh-core:$versions.jmh"",
-  jmhGeneratorAnnProcess: ""org.openjdk.jmh:jmh-generator-annprocess:$versions.jmh"",
   jmhCoreBenchmarks: ""org.openjdk.jmh:jmh-core-benchmarks:$versions.jmh"",
+  jmhGeneratorAnnProcess: ""org.openjdk.jmh:jmh-generator-annprocess:$versions.jmh"",
+  joptSimple: ""net.sf.jopt-simple:jopt-simple:$versions.jopt"",
   junit: ""junit:junit:$versions.junit"",
+  kafkaStreams_0100: ""org.apache.kafka:kafka-streams:$versions.kafka_0100"",
+  kafkaStreams_0101: ""org.apache.kafka:kafka-streams:$versions.kafka_0101"",
+  kafkaStreams_0102: ""org.apache.kafka:kafka-streams:$versions.kafka_0102"",
+  kafkaStreams_0110: ""org.apache.kafka:kafka-streams:$versions.kafka_0110"",
   log4j: ""log4j:log4j:$versions.log4j"",
-  joptSimple: ""net.sf.jopt-simple:jopt-simple:$versions.jopt"",
   lz4: ""org.lz4:lz4-java:$versions.lz4"",
   metrics: ""com.yammer.metrics:metrics-core:$versions.metrics"",
   powermockJunit4: ""org.powermock:powermock-module-junit4:$versions.powermock"",
diff --git a/settings.gradle b/settings.gradle
index f0fdf07128c..2820287b11e 100644
--- a/settings.gradle
+++ b/settings.gradle
@@ -13,5 +13,7 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
-include 'core', 'examples', 'clients', 'tools', 'streams', 'streams:examples', 'log4j-appender',
+include 'core', 'examples', 'clients', 'tools', 'streams', 'streams:examples', 'streams:upgrade-system-tests-0100',
+        'streams:upgrade-system-tests-0101', 'streams:upgrade-system-tests-0102', 'streams:upgrade-system-tests-0110',
+        'log4j-appender',
         'connect:api', 'connect:transforms', 'connect:runtime', 'connect:json', 'connect:file', 'jmh-benchmarks'
diff --git [file java] [file java]
index 42e65df92a6..7510e381d39 100644
--- [file java]
+++ [file java]
@@ -129,6 +129,11 @@
      */
     public static final String PRODUCER_PREFIX = ""producer."";
+    /**
+     * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 0.10.0.x}.
+     */
+    public static final String UPGRADE_FROM_0100 = ""0.10.0"";
+
     /**
      * Config value for parameter {@link #PROCESSING_GUARANTEE_CONFIG ""processing.guarantee""} for at-least-once processing guarantees.
      */
@@ -280,6 +285,11 @@
     public static final String TIMESTAMP_EXTRACTOR_CLASS_CONFIG = ""timestamp.extractor"";
     private static final String TIMESTAMP_EXTRACTOR_CLASS_DOC = ""Timestamp extractor class that implements the <code>org.apache.kafka.streams.processor.TimestampExtractor</code> interface. This config is deprecated, use <code>"" + DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG + ""</code> instead"";
+    /** {@code upgrade.from} */
+    public static final String UPGRADE_FROM_CONFIG = ""upgrade.from"";
+    public static final String UPGRADE_FROM_DOC = ""Allows upgrading from version 0.10.0 to version 0.10.1 (or newer) in a backward compatible way. "" +
+        ""Default is null. Accepted values are \"""" + UPGRADE_FROM_0100 + ""\"" (for upgrading from 0.10.0.x)."";
+
     /**
      * {@code value.serde}
      * @deprecated Use {@link #DEFAULT_VALUE_SERDE_CLASS_CONFIG} instead.
@@ -491,6 +501,12 @@
                     10 * 60 * 1000,
                     Importance.LOW,
                     STATE_CLEANUP_DELAY_MS_DOC)
+            .define(UPGRADE_FROM_CONFIG,
+                    ConfigDef.Type.STRING,
+                    null,
+                    in(null, UPGRADE_FROM_0100),
+                    Importance.LOW,
+                    UPGRADE_FROM_DOC)
             .define(WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG,
                     Type.LONG,
                     24 * 60 * 60 * 1000,
@@ -712,6 +728,7 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map<String, Obje
         consumerProps.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId + ""-consumer"");
         // add configs required for stream partition assignor
+        consumerProps.put(UPGRADE_FROM_CONFIG, getString(UPGRADE_FROM_CONFIG));
         consumerProps.put(InternalConfig.STREAM_THREAD_INSTANCE, streamThread);
         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));
         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));
diff --git [file java] [file java]
index c4ca2078fb7..ab134be8d30 100644
--- [file java]
+++ [file java]
@@ -175,6 +175,8 @@ public int compare(TopicPartition p1, TopicPartition p2) {
     private String userEndPoint;
     private int numStandbyReplicas;
+    private int userMetadataVersion = SubscriptionInfo.CURRENT_VERSION;
+
     private Cluster metadataWithInternalTopics;
     private Map<HostInfo, Set<TopicPartition>> partitionsByHostState;
@@ -205,7 +207,13 @@ public void configure(Map<String, ?> configs) {
         // Setting the logger with the passed in client thread name
         logPrefix = String.format(""stream-thread  "", configs.get(CommonClientConfigs.CLIENT_ID_CONFIG));
         final LogContext logContext = new LogContext(logPrefix);
-        this.log = logContext.logger(getClass());
+        log = logContext.logger(getClass());
+
+        final String upgradeMode = (String) configs.get(StreamsConfig.UPGRADE_FROM_CONFIG);
+        if (StreamsConfig.UPGRADE_FROM_0100.equals(upgradeMode)) {
+            log.info(""Downgrading metadata version from 2 to 1 for upgrade from 0.10.0.x."");
+            userMetadataVersion = 1;
+        }
         Object o = configs.get(StreamsConfig.InternalConfig.STREAM_THREAD_INSTANCE);
         if (o == null) {
@@ -266,7 +274,7 @@ public Subscription subscription(Set<String> topics) {
         final Set<TaskId> previousActiveTasks = threadDataProvider.prevActiveTasks();
         Set<TaskId> standbyTasks = threadDataProvider.cachedTasks();
         standbyTasks.removeAll(previousActiveTasks);
-        SubscriptionInfo data = new SubscriptionInfo(threadDataProvider.processId(), previousActiveTasks, standbyTasks, this.userEndPoint);
+        SubscriptionInfo data = new SubscriptionInfo(userMetadataVersion, threadDataProvider.processId(), previousActiveTasks, standbyTasks, this.userEndPoint);
         if (threadDataProvider.builder().sourceTopicPattern() != null &&
             !threadDataProvider.builder().subscriptionUpdates().getUpdates().equals(topics)) {
@@ -309,11 +317,16 @@ private void updateSubscribedTopics(Set<String> topics) {
         // construct the client metadata from the decoded subscription info
         Map<UUID, ClientMetadata> clientsMetadata = new HashMap<>();
+        int minUserMetadataVersion = SubscriptionInfo.CURRENT_VERSION;
         for (Map.Entry<String, Subscription> entry : subscriptions.entrySet()) {
             String consumerId = entry.getKey();
             Subscription subscription = entry.getValue();
             SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());
+            final int usedVersion = info.version;
+            if (usedVersion < minUserMetadataVersion) {
+                minUserMetadataVersion = usedVersion;
+            }
             // create the new client metadata if necessary
             ClientMetadata clientMetadata = clientsMetadata.get(info.processId);
@@ -572,7 +585,7 @@ private void updateSubscribedTopics(Set<String> topics) {
                 }
                 // finally, encode the assignment before sending back to coordinator
-                assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(active, standby, partitionsByHostState).encode()));
+                assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(minUserMetadataVersion, active, standby, partitionsByHostState).encode()));
             }
         }
diff --git [file java] [file java]
index 8607472c281..42abb4cac0b 100644
--- [file java]
+++ [file java]
@@ -55,7 +55,7 @@ public AssignmentInfo(List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>>
         this(CURRENT_VERSION, activeTasks, standbyTasks, hostState);
     }
-    protected AssignmentInfo(int version, List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>> standbyTasks,
+    public AssignmentInfo(int version, List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>> standbyTasks,
                              Map<HostInfo, Set<TopicPartition>> hostState) {
         this.version = version;
         this.activeTasks = activeTasks;
@@ -153,8 +153,7 @@ public static AssignmentInfo decode(ByteBuffer data) {
                 }
             }
-            return new AssignmentInfo(activeTasks, standbyTasks, hostStateToTopicPartitions);
-
+            return new AssignmentInfo(version, activeTasks, standbyTasks, hostStateToTopicPartitions);
         } catch (IOException ex) {
             throw new TaskAssignmentException(""Failed to decode AssignmentInfo"", ex);
         }
diff --git [file java] [file java]
index f583dbafc94..00227e799b8 100644
--- [file java]
+++ [file java]
@@ -31,7 +31,7 @@
     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);
-    private static final int CURRENT_VERSION = 2;
+    public static final int CURRENT_VERSION = 2;
     public final int version;
     public final UUID processId;
@@ -43,7 +43,7 @@ public SubscriptionInfo(UUID processId, Set<TaskId> prevTasks, Set<TaskId> stand
         this(CURRENT_VERSION, processId, prevTasks, standbyTasks, userEndPoint);
     }
-    private SubscriptionInfo(int version, UUID processId, Set<TaskId> prevTasks, Set<TaskId> standbyTasks, String userEndPoint) {
+    public SubscriptionInfo(int version, UUID processId, Set<TaskId> prevTasks, Set<TaskId> standbyTasks, String userEndPoint) {
         this.version = version;
         this.processId = processId;
         this.prevTasks = prevTasks;
diff --git [file java] [file java]
index b217bf9b5bf..30ab6922754 100644
--- [file java]
+++ [file java]
@@ -423,6 +423,7 @@ public void shouldNotOverrideUserConfigCommitIntervalMsIfExactlyOnceEnabled() {
         assertThat(streamsConfig.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG), equalTo(commitIntervalMs));
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldBeBackwardsCompatibleWithDeprecatedConfigs() {
         final Properties props = minimalStreamsConfig();
@@ -457,6 +458,7 @@ public void shouldUseCorrectDefaultsWhenNoneSpecified() {
         assertTrue(config.defaultTimestampExtractor() instanceof FailOnInvalidTimestamp);
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldSpecifyCorrectKeySerdeClassOnErrorUsingDeprecatedConfigs() {
         final Properties props = minimalStreamsConfig();
@@ -470,6 +472,7 @@ public void shouldSpecifyCorrectKeySerdeClassOnErrorUsingDeprecatedConfigs() {
         }
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldSpecifyCorrectKeySerdeClassOnError() {
         final Properties props = minimalStreamsConfig();
@@ -483,6 +486,7 @@ public void shouldSpecifyCorrectKeySerdeClassOnError() {
         }
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldSpecifyCorrectValueSerdeClassOnErrorUsingDeprecatedConfigs() {
         final Properties props = minimalStreamsConfig();
@@ -496,6 +500,7 @@ public void shouldSpecifyCorrectValueSerdeClassOnErrorUsingDeprecatedConfigs() {
         }
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldSpecifyCorrectValueSerdeClassOnError() {
         final Properties props = minimalStreamsConfig();
@@ -518,9 +523,7 @@ public void configure(final Map configs, final boolean isKey) {
         }
         @Override
-        public void close() {
-
-        }
+        public void close() {}
         @Override
         public Serializer serializer() {
diff --git [file java] [file java]
index 4c12bb93544..44e139a28bd 100644
--- [file java]
+++ [file java]
@@ -313,6 +313,4 @@ private void startStreams() {
     }
-
-
 }
diff --git [file java] [file java]
index 918b6aa5bd4..d34bea3eeb2 100644
--- [file java]
+++ [file java]
@@ -140,7 +140,7 @@ private void mockThreadDataProvider(final Set<TaskId> prevTasks,
                                         final Set<TaskId> cachedTasks,
                                         final UUID processId,
                                         final PartitionGrouper partitionGrouper,
-                                        final InternalTopologyBuilder builder) throws NoSuchFieldException, IllegalAccessException {
+                                        final InternalTopologyBuilder builder) {
         EasyMock.expect(threadDataProvider.name()).andReturn(""name"").anyTimes();
         EasyMock.expect(threadDataProvider.prevActiveTasks()).andReturn(prevTasks).anyTimes();
         EasyMock.expect(threadDataProvider.cachedTasks()).andReturn(cachedTasks).anyTimes();
@@ -179,7 +179,7 @@ public void shouldInterleaveTasksByGroupId() {
     }
     @Test
-    public void testSubscription() throws Exception {
+    public void testSubscription() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
@@ -208,7 +208,7 @@ public void testSubscription() throws Exception {
     @Test
-    public void testAssignBasic() throws Exception {
+    public void testAssignBasic() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
@@ -248,11 +248,9 @@ public void testAssignBasic() throws Exception {
         // check assignment info
-        Set<TaskId> allActiveTasks = new HashSet<>();
-
         // the first consumer
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
+        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks);
         // the second consumer
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(""consumer11""));
@@ -272,7 +270,7 @@ public void testAssignBasic() throws Exception {
     }
     @Test
-    public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() throws Exception {
+    public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"");
@@ -340,7 +338,7 @@ public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() throws E
     }
     @Test
-    public void testAssignWithPartialTopology() throws Exception {
+    public void testAssignWithPartialTopology() {
         Properties props = configProps();
         props.put(StreamsConfig.PARTITION_GROUPER_CLASS_CONFIG, SingleGroupPartitionGrouperStub.class);
         StreamsConfig config = new StreamsConfig(props);
@@ -369,9 +367,8 @@ public void testAssignWithPartialTopology() throws Exception {
         Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
         // check assignment info
-        Set<TaskId> allActiveTasks = new HashSet<>();
         AssignmentInfo info10 = checkAssignment(Utils.mkSet(""topic1""), assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
+        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks);
         assertEquals(3, allActiveTasks.size());
         assertEquals(allTasks, new HashSet<>(allActiveTasks));
@@ -379,7 +376,7 @@ public void testAssignWithPartialTopology() throws Exception {
     @Test
-    public void testAssignEmptyMetadata() throws Exception {
+    public void testAssignEmptyMetadata() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
@@ -409,9 +406,8 @@ public void testAssignEmptyMetadata() throws Exception {
             new HashSet<>(assignments.get(""consumer10"").partitions()));
         // check assignment info
-        Set<TaskId> allActiveTasks = new HashSet<>();
         AssignmentInfo info10 = checkAssignment(Collections.<String>emptySet(), assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
+        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks);
         assertEquals(0, allActiveTasks.size());
         assertEquals(Collections.<TaskId>emptySet(), new HashSet<>(allActiveTasks));
@@ -434,7 +430,7 @@ public void testAssignEmptyMetadata() throws Exception {
     }
     @Test
-    public void testAssignWithNewTasks() throws Exception {
+    public void testAssignWithNewTasks() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addSource(null, ""source3"", null, null, null, ""topic3"");
@@ -466,13 +462,9 @@ public void testAssignWithNewTasks() throws Exception {
         // check assigned partitions: since there is no previous task for topic 3 it will be assigned randomly so we cannot check exact match
         // also note that previously assigned partitions / tasks may not stay on the previous host since we may assign the new task first and
         // then later ones will be re-assigned to other hosts due to load balancing
-        Set<TaskId> allActiveTasks = new HashSet<>();
-        Set<TopicPartition> allPartitions = new HashSet<>();
-        AssignmentInfo info;
-
-        info = AssignmentInfo.decode(assignments.get(""consumer10"").userData());
-        allActiveTasks.addAll(info.activeTasks);
-        allPartitions.addAll(assignments.get(""consumer10"").partitions());
+        AssignmentInfo info = AssignmentInfo.decode(assignments.get(""consumer10"").userData());
+        Set<TaskId> allActiveTasks = new HashSet<>(info.activeTasks);
+        Set<TopicPartition> allPartitions = new HashSet<>(assignments.get(""consumer10"").partitions());
         info = AssignmentInfo.decode(assignments.get(""consumer11"").userData());
         allActiveTasks.addAll(info.activeTasks);
@@ -487,7 +479,7 @@ public void testAssignWithNewTasks() throws Exception {
     }
     @Test
-    public void testAssignWithStates() throws Exception {
+    public void testAssignWithStates() {
         String applicationId = ""test"";
         builder.setApplicationId(applicationId);
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
@@ -576,7 +568,7 @@ public void testAssignWithStates() throws Exception {
     }
     @Test
-    public void testAssignWithStandbyReplicas() throws Exception {
+    public void testAssignWithStandbyReplicas() {
         Properties props = configProps();
         props.setProperty(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, ""1"");
         StreamsConfig config = new StreamsConfig(props);
@@ -613,13 +605,10 @@ public void testAssignWithStandbyReplicas() throws Exception {
         Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
-        Set<TaskId> allActiveTasks = new HashSet<>();
-        Set<TaskId> allStandbyTasks = new HashSet<>();
-
         // the first consumer
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
-        allStandbyTasks.addAll(info10.standbyTasks.keySet());
+        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks);
+        Set<TaskId> allStandbyTasks = new HashSet<>(info10.standbyTasks.keySet());
         // the second consumer
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(""consumer11""));
@@ -647,7 +636,7 @@ public void testAssignWithStandbyReplicas() throws Exception {
     }
     @Test
-    public void testOnAssignment() throws Exception {
+    public void testOnAssignment() {
         TopicPartition t2p3 = new TopicPartition(""topic2"", 3);
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
@@ -675,7 +664,7 @@ public void testOnAssignment() throws Exception {
     }
     @Test
-    public void testAssignWithInternalTopics() throws Exception {
+    public void testAssignWithInternalTopics() {
         String applicationId = ""test"";
         builder.setApplicationId(applicationId);
         builder.addInternalTopic(""topicX"");
@@ -706,7 +695,7 @@ public void testAssignWithInternalTopics() throws Exception {
     }
     @Test
-    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throws Exception {
+    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() {
         String applicationId = ""test"";
         builder.setApplicationId(applicationId);
         builder.addInternalTopic(""topicX"");
@@ -741,9 +730,8 @@ public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throw
     }
     @Test
-    public void shouldAddUserDefinedEndPointToSubscription() throws Exception {
-        final String applicationId = ""application-id"";
-        builder.setApplicationId(applicationId);
+    public void shouldAddUserDefinedEndPointToSubscription() {
+        builder.setApplicationId(""application-id"");
         builder.addSource(null, ""source"", null, null, null, ""input"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source"");
         builder.addSink(""sink"", ""output"", null, null, null, ""processor"");
@@ -752,7 +740,8 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {
         mockThreadDataProvider(Collections.<TaskId>emptySet(),
                                Collections.<TaskId>emptySet(),
                                uuid1,
-                               defaultPartitionGrouper, builder);
+                               defaultPartitionGrouper,
+                               builder);
         configurePartitionAssignor(0, userEndPoint);
         final PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""input""));
         final SubscriptionInfo subscriptionInfo = SubscriptionInfo.decode(subscription.userData());
@@ -760,7 +749,59 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {
     }
     @Test
-    public void shouldMapUserEndPointToTopicPartitions() throws Exception {
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() {
+        final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
+        final Set<TaskId> emptyTasks = Collections.emptySet();
+        subscriptions.put(
+            ""consumer1"",
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(""topic1""),
+                new SubscriptionInfo(1, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+        subscriptions.put(
+            ""consumer2"",
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(""topic1""),
+                new SubscriptionInfo(2, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+
+        mockThreadDataProvider(
+            emptyTasks,
+            emptyTasks,
+            UUID.randomUUID(),
+            defaultPartitionGrouper,
+            builder);
+        configurePartitionAssignor(0, null);
+        final Map<String, PartitionAssignor.Assignment> assignment = partitionAssignor.assign(metadata, subscriptions);
+
+        assertEquals(2, assignment.size());
+        assertEquals(1, AssignmentInfo.decode(assignment.get(""consumer1"").userData()).version);
+        assertEquals(1, AssignmentInfo.decode(assignment.get(""consumer2"").userData()).version);
+    }
+
+    @Test
+    public void shouldDownGradeSubscription() {
+        final Set<TaskId> emptyTasks = Collections.emptySet();
+
+        mockThreadDataProvider(
+            emptyTasks,
+            emptyTasks,
+            UUID.randomUUID(),
+            defaultPartitionGrouper,
+            builder);
+
+        configurationMap.put(StreamsConfig.UPGRADE_FROM_CONFIG, StreamsConfig.UPGRADE_FROM_0100);
+        configurePartitionAssignor(0, null);
+
+        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""topic1""));
+
+        assertEquals(1, SubscriptionInfo.decode(subscription.userData()).version);
+    }
+
+    @Test
+    public void shouldMapUserEndPointToTopicPartitions() {
         final String applicationId = ""application-id"";
         builder.setApplicationId(applicationId);
         builder.addSource(null, ""source"", null, null, null, ""topic1"");
@@ -790,7 +831,7 @@ public void shouldMapUserEndPointToTopicPartitions() throws Exception {
     }
     @Test
-    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() throws Exception {
+    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() {
         final String myEndPoint = ""localhost"";
         final String applicationId = ""application-id"";
         builder.setApplicationId(applicationId);
@@ -821,7 +862,7 @@ public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() {
     }
     @Test
-    public void shouldExposeHostStateToTopicPartitionsOnAssignment() throws Exception {
+    public void shouldExposeHostStateToTopicPartitionsOnAssignment() {
         List<TopicPartition> topic = Collections.singletonList(new TopicPartition(""topic"", 0));
         final Map<HostInfo, Set<TopicPartition>> hostState =
                 Collections.singletonMap(new HostInfo(""localhost"", 80),
@@ -837,7 +878,7 @@ public void shouldExposeHostStateToTopicPartitionsOnAssignment() throws Exceptio
     }
     @Test
-    public void shouldSetClusterMetadataOnAssignment() throws Exception {
+    public void shouldSetClusterMetadataOnAssignment() {
         final List<TopicPartition> topic = Collections.singletonList(new TopicPartition(""topic"", 0));
         final Map<HostInfo, Set<TopicPartition>> hostState =
                 Collections.singletonMap(new HostInfo(""localhost"", 80),
@@ -865,7 +906,7 @@ public void shouldReturnEmptyClusterMetadataIfItHasntBeenBuilt() {
     }
     @Test
-    public void shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks() throws Exception {
+    public void shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks() {
         final String applicationId = ""application-id"";
         final StreamsBuilder builder = new StreamsBuilder();
@@ -970,7 +1011,7 @@ public Object apply(final Object value1, final Object value2) {
     }
     @Test
-    public void shouldUpdatePartitionHostInfoMapOnAssignment() throws Exception {
+    public void shouldUpdatePartitionHostInfoMapOnAssignment() {
         final TopicPartition partitionOne = new TopicPartition(""topic"", 1);
         final TopicPartition partitionTwo = new TopicPartition(""topic"", 2);
         final Map<HostInfo, Set<TopicPartition>> firstHostState = Collections.singletonMap(
@@ -993,7 +1034,7 @@ public void shouldUpdatePartitionHostInfoMapOnAssignment() throws Exception {
     }
     @Test
-    public void shouldUpdateClusterMetadataOnAssignment() throws Exception {
+    public void shouldUpdateClusterMetadataOnAssignment() {
         final TopicPartition topicOne = new TopicPartition(""topic"", 1);
         final TopicPartition topicTwo = new TopicPartition(""topic2"", 2);
         final Map<HostInfo, Set<TopicPartition>> firstHostState = Collections.singletonMap(
@@ -1015,7 +1056,7 @@ public void shouldUpdateClusterMetadataOnAssignment() throws Exception {
     }
     @Test
-    public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() throws Exception {
+    public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() {
         final String applicationId = ""appId"";
         final StreamsBuilder builder = new StreamsBuilder();
diff --git [file java] [file java]
index ec94ad81acd..8032f7da865 100644
--- [file java]
+++ [file java]
@@ -64,10 +64,9 @@ public void shouldDecodePreviousVersion() throws IOException {
         assertEquals(oldVersion.activeTasks, decoded.activeTasks);
         assertEquals(oldVersion.standbyTasks, decoded.standbyTasks);
         assertEquals(0, decoded.partitionsByHost.size()); // should be empty as wasn't in V1
-        assertEquals(2, decoded.version); // automatically upgraded to v2 on decode;
+        assertEquals(1, decoded.version);
     }
-
     /**
      * This is a clone of what the V1 encoding did. The encode method has changed for V2
      * so it is impossible to test compatibility without having this
diff --git [file java] [file java]
index 887f763e8fa..6902bd6d376 100644
--- [file java]
+++ [file java]
@@ -19,6 +19,7 @@
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.kafka.clients.producer.ProducerConfig;
 import org.apache.kafka.common.serialization.Serdes;
+import org.apache.kafka.common.utils.Bytes;
 import org.apache.kafka.streams.Consumed;
 import org.apache.kafka.streams.KafkaStreams;
 import org.apache.kafka.streams.StreamsBuilder;
@@ -30,10 +31,13 @@
 import org.apache.kafka.streams.kstream.KTable;
 import org.apache.kafka.streams.kstream.Materialized;
 import org.apache.kafka.streams.kstream.Predicate;
+import org.apache.kafka.streams.kstream.Produced;
 import org.apache.kafka.streams.kstream.Serialized;
 import org.apache.kafka.streams.kstream.TimeWindows;
 import org.apache.kafka.streams.kstream.ValueJoiner;
+import org.apache.kafka.streams.state.KeyValueStore;
 import org.apache.kafka.streams.state.Stores;
+import org.apache.kafka.streams.state.WindowStore;
 import java.io.File;
 import java.util.Properties;
@@ -47,7 +51,7 @@
     private Thread thread;
     private boolean uncaughtException = false;
-    public SmokeTestClient(File stateDir, String kafka) {
+    public SmokeTestClient(final File stateDir, final String kafka) {
         super();
         this.stateDir = stateDir;
         this.kafka = kafka;
@@ -57,7 +61,7 @@ public void start() {
         streams = createKafkaStreams(stateDir, kafka);
         streams.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
             @Override
-            public void uncaughtException(Thread t, Throwable e) {
+            public void uncaughtException(final Thread t, final Throwable e) {
                 System.out.println(""SMOKE-TEST-CLIENT-EXCEPTION"");
                 uncaughtException = true;
                 e.printStackTrace();
@@ -94,7 +98,7 @@ public void close() {
         }
     }
-    private static KafkaStreams createKafkaStreams(File stateDir, String kafka) {
+    private static Properties getStreamsConfig(final File stateDir, final String kafka) {
         final Properties props = new Properties();
         props.put(StreamsConfig.APPLICATION_ID_CONFIG, ""SmokeTest"");
         props.put(StreamsConfig.STATE_DIR_CONFIG, stateDir.toString());
@@ -109,25 +113,29 @@ private static KafkaStreams createKafkaStreams(File stateDir, String kafka) {
         props.put(ProducerConfig.ACKS_CONFIG, ""all"");
         //TODO remove this config or set to smaller value when KIP-91 is merged
         props.put(StreamsConfig.producerPrefix(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG), 60000);
+        return props;
+    }
-        StreamsBuilder builder = new StreamsBuilder();
-        Consumed<String, Integer> stringIntConsumed = Consumed.with(stringSerde, intSerde);
-        KStream<String, Integer> source = builder.stream(""data"", stringIntConsumed);
-        source.to(stringSerde, intSerde, ""echo"");
-        KStream<String, Integer> data = source.filter(new Predicate<String, Integer>() {
+    private static KafkaStreams createKafkaStreams(final File stateDir, final String kafka) {
+        final StreamsBuilder builder = new StreamsBuilder();
+        final Consumed<String, Integer> stringIntConsumed = Consumed.with(stringSerde, intSerde);
+        final KStream<String, Integer> source = builder.stream(""data"", stringIntConsumed);
+        source.to(""echo"", Produced.with(stringSerde, intSerde));
+        final KStream<String, Integer> data = source.filter(new Predicate<String, Integer>() {
             @Override
-            public boolean test(String key, Integer value) {
+            public boolean test(final String key, final Integer value) {
                 return value == null || value != END;
             }
         });
         data.process(SmokeTestUtil.printProcessorSupplier(""data""));
         // min
-        KGroupedStream<String, Integer>
-            groupedData =
+        final KGroupedStream<String, Integer> groupedData =
             data.groupByKey(Serialized.with(stringSerde, intSerde));
-        groupedData.aggregate(
+        groupedData
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(1)))
+            .aggregate(
                 new Initializer<Integer>() {
                     public Integer apply() {
                         return Integer.MAX_VALUE;
@@ -135,21 +143,24 @@ public Integer apply() {
                 },
                 new Aggregator<String, Integer, Integer>() {
                     @Override
-                    public Integer apply(String aggKey, Integer value, Integer aggregate) {
+                    public Integer apply(final String aggKey, final Integer value, final Integer aggregate) {
                         return (value < aggregate) ? value : aggregate;
                     }
                 },
-                TimeWindows.of(TimeUnit.DAYS.toMillis(1)),
-                intSerde, ""uwin-min""
-        ).toStream().map(
-                new Unwindow<String, Integer>()
-        ).to(stringSerde, intSerde, ""min"");
+                Materialized.<String, Integer, WindowStore<Bytes, byte>>as(""uwin-min"").withValueSerde(intSerde))
+            .toStream(new Unwindow<String, Integer>())
+            .to(""min"", Produced.with(stringSerde, intSerde));
-        KTable<String, Integer> minTable = builder.table(""min"", stringIntConsumed);
+        final KTable<String, Integer> minTable = builder.table(
+            ""min"",
+            Consumed.with(stringSerde, intSerde),
+            Materialized.<String, Integer, KeyValueStore<Bytes, byte>>as(""minStoreName""));
         minTable.toStream().process(SmokeTestUtil.printProcessorSupplier(""min""));
         // max
-        groupedData.aggregate(
+        groupedData
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))
+            .aggregate(
                 new Initializer<Integer>() {
                     public Integer apply() {
                         return Integer.MIN_VALUE;
@@ -157,21 +168,24 @@ public Integer apply() {
                 },
                 new Aggregator<String, Integer, Integer>() {
                     @Override
-                    public Integer apply(String aggKey, Integer value, Integer aggregate) {
+                    public Integer apply(final String aggKey, final Integer value, final Integer aggregate) {
                         return (value > aggregate) ? value : aggregate;
                     }
                 },
-                TimeWindows.of(TimeUnit.DAYS.toMillis(2)),
-                intSerde, ""uwin-max""
-        ).toStream().map(
-                new Unwindow<String, Integer>()
-        ).to(stringSerde, intSerde, ""max"");
+                Materialized.<String, Integer, WindowStore<Bytes, byte>>as(""uwin-max"").withValueSerde(intSerde))
+            .toStream(new Unwindow<String, Integer>())
+            .to(""max"", Produced.with(stringSerde, intSerde));
-        KTable<String, Integer> maxTable = builder.table(""max"", stringIntConsumed);
+        final KTable<String, Integer> maxTable = builder.table(
+            ""max"",
+            Consumed.with(stringSerde, intSerde),
+            Materialized.<String, Integer, KeyValueStore<Bytes, byte>>as(""maxStoreName""));
         maxTable.toStream().process(SmokeTestUtil.printProcessorSupplier(""max""));
         // sum
-        groupedData.aggregate(
+        groupedData
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))
+            .aggregate(
                 new Initializer<Long>() {
                     public Long apply() {
                         return 0L;
@@ -179,70 +193,74 @@ public Long apply() {
                 },
                 new Aggregator<String, Integer, Long>() {
                     @Override
-                    public Long apply(String aggKey, Integer value, Long aggregate) {
+                    public Long apply(final String aggKey, final Integer value, final Long aggregate) {
                         return (long) value + aggregate;
                     }
                 },
-                TimeWindows.of(TimeUnit.DAYS.toMillis(2)),
-                longSerde, ""win-sum""
-        ).toStream().map(
-                new Unwindow<String, Long>()
-        ).to(stringSerde, longSerde, ""sum"");
-
-        Consumed<String, Long> stringLongConsumed = Consumed.with(stringSerde, longSerde);
-        KTable<String, Long> sumTable = builder.table(""sum"", stringLongConsumed);
+                Materialized.<String, Long, WindowStore<Bytes, byte>>as(""win-sum"").withValueSerde(longSerde))
+            .toStream(new Unwindow<String, Long>())
+            .to(""sum"", Produced.with(stringSerde, longSerde));
+
+        final Consumed<String, Long> stringLongConsumed = Consumed.with(stringSerde, longSerde);
+        final KTable<String, Long> sumTable = builder.table(""sum"", stringLongConsumed);
         sumTable.toStream().process(SmokeTestUtil.printProcessorSupplier(""sum""));
+
         // cnt
-        groupedData.count(TimeWindows.of(TimeUnit.DAYS.toMillis(2)), ""uwin-cnt"")
-            .toStream().map(
-                new Unwindow<String, Long>()
-        ).to(stringSerde, longSerde, ""cnt"");
+        groupedData
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))
+            .count(Materialized.<String, Long, WindowStore<Bytes, byte>>as(""uwin-cnt""))
+            .toStream(new Unwindow<String, Long>())
+            .to(""cnt"", Produced.with(stringSerde, longSerde));
-        KTable<String, Long> cntTable = builder.table(""cnt"", stringLongConsumed);
+        final KTable<String, Long> cntTable = builder.table(
+            ""cnt"",
+            Consumed.with(stringSerde, longSerde),
+            Materialized.<String, Long, KeyValueStore<Bytes, byte>>as(""cntStoreName""));
         cntTable.toStream().process(SmokeTestUtil.printProcessorSupplier(""cnt""));
         // dif
-        maxTable.join(minTable,
+        maxTable
+            .join(
+                minTable,
                 new ValueJoiner<Integer, Integer, Integer>() {
-                    public Integer apply(Integer value1, Integer value2) {
+                    public Integer apply(final Integer value1, final Integer value2) {
                         return value1 - value2;
                     }
-                }
-        ).to(stringSerde, intSerde, ""dif"");
+                })
+            .toStream()
+            .to(""dif"", Produced.with(stringSerde, intSerde));
         // avg
-        sumTable.join(
+        sumTable
+            .join(
                 cntTable,
                 new ValueJoiner<Long, Long, Double>() {
-                    public Double apply(Long value1, Long value2) {
+                    public Double apply(final Long value1, final Long value2) {
                         return (double) value1 / (double) value2;
                     }
-                }
-        ).to(stringSerde, doubleSerde, ""avg"");
+                })
+            .toStream()
+            .to(""avg"", Produced.with(stringSerde, doubleSerde));
         // test repartition
-        Agg agg = new Agg();
-        cntTable.groupBy(agg.selector(),
-                         Serialized.with(stringSerde, longSerde)
-        ).aggregate(agg.init(),
-                    agg.adder(),
-                    agg.remover(),
-                    Materialized.<String, Long>as(Stores.inMemoryKeyValueStore(""cntByCnt""))
-                            .withKeySerde(Serdes.String())
-                            .withValueSerde(Serdes.Long())
-        ).to(stringSerde, longSerde, ""tagg"");
-
-        final KafkaStreams streamsClient = new KafkaStreams(builder.build(), props);
+        final Agg agg = new Agg();
+        cntTable.groupBy(agg.selector(), Serialized.with(stringSerde, longSerde))
+            .aggregate(agg.init(), agg.adder(), agg.remover(),
+                Materialized.<String, Long>as(Stores.inMemoryKeyValueStore(""cntByCnt""))
+                    .withKeySerde(Serdes.String())
+                    .withValueSerde(Serdes.Long()))
+            .toStream()
+            .to(""tagg"", Produced.with(stringSerde, longSerde));
+
+        final KafkaStreams streamsClient = new KafkaStreams(builder.build(), getStreamsConfig(stateDir, kafka));
         streamsClient.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
             @Override
-            public void uncaughtException(Thread t, Throwable e) {
+            public void uncaughtException(final Thread t, final Throwable e) {
                 System.out.println(""FATAL: An unexpected exception is encountered on thread "" + t + "": "" + e);
-                
                 streamsClient.close(30, TimeUnit.SECONDS);
             }
         });
         return streamsClient;
     }
-
 }
diff --git [file java] [file java]
index fdc9326427c..1e661baad4a 100644
--- [file java]
+++ [file java]
@@ -130,53 +130,65 @@ public void run() {
         System.out.println(""shutdown"");
     }
-    public static Map<String, Set<Integer>> generate(String kafka, final int numKeys, final int maxRecordsPerKey) {
+    public static Map<String, Set<Integer>> generate(final String kafka,
+                                                     final int numKeys,
+                                                     final int maxRecordsPerKey) {
+        return generate(kafka, numKeys, maxRecordsPerKey, true);
+    }
+
+    public static Map<String, Set<Integer>> generate(final String kafka,
+                                                     final int numKeys,
+                                                     final int maxRecordsPerKey,
+                                                     final boolean autoTerminate) {
         final Properties producerProps = new Properties();
         producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, ""SmokeTest"");
         producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
         producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
-        // the next 4 config values make sure that all records are produced with no loss and
-        // no duplicates
+        // the next 2 config values make sure that all records are produced with no loss and no duplicates
         producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);
         producerProps.put(ProducerConfig.ACKS_CONFIG, ""all"");
         producerProps.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 45000);
-        KafkaProducer<byte, byte> producer = new KafkaProducer<>(producerProps);
+        final KafkaProducer<byte, byte> producer = new KafkaProducer<>(producerProps);
         int numRecordsProduced = 0;
-        Map<String, Set<Integer>> allData = new HashMap<>();
-        ValueList data = new ValueList;
+        final Map<String, Set<Integer>> allData = new HashMap<>();
+        final ValueList data = new ValueList;
         for (int i = 0; i < numKeys; i++) {
             data = new ValueList(i, i + maxRecordsPerKey - 1);
             allData.put(data.key, new HashSet<Integer>());
         }
-        Random rand = new Random();
+        final Random rand = new Random();
-        int remaining = data.length;
+        int remaining = 1; // dummy value must be positive if <autoTerminate> is false
+        if (autoTerminate) {
+            remaining = data.length;
+        }
         List<ProducerRecord<byte, byte>> needRetry = new ArrayList<>();
         while (remaining > 0) {
-            int index = rand.nextInt(remaining);
-            String key = data.key;
+            final int index = autoTerminate ? rand.nextInt(remaining) : rand.nextInt(numKeys);
+            final String key = data.key;
             int value = data.next();
-            if (value < 0) {
+            if (autoTerminate && value < 0) {
                 remaining--;
                 data = data;
             } else {
-                ProducerRecord<byte, byte> record =
-                        new ProducerRecord<>(""data"", stringSerde.serializer().serialize("""", key), intSerde.serializer().serialize("""", value));
+                final ProducerRecord<byte, byte> record =
+                    new ProducerRecord<>(""data"", stringSerde.serializer().serialize("""", key), intSerde.serializer().serialize("""", value));
                 producer.send(record, new TestCallback(record, needRetry));
                 numRecordsProduced++;
                 allData.get(key).add(value);
-                if (numRecordsProduced % 100 == 0)
+                if (numRecordsProduced % 100 == 0) {
                     System.out.println(numRecordsProduced + "" records produced"");
+                }
                 Utils.sleep(2);
             }
         }
diff --git [file java] [file java]
index dc4c91b4097..87ca82918a9 100644
--- [file java]
+++ [file java]
@@ -44,20 +44,15 @@
             public Processor<Object, Object> get() {
                 return new AbstractProcessor<Object, Object>() {
                     private int numRecordsProcessed = 0;
-                    private ProcessorContext context;
                     @Override
                     public void init(final ProcessorContext context) {
                         System.out.println(""initializing processor: topic="" + topic + "" taskId="" + context.taskId());
                         numRecordsProcessed = 0;
-                        this.context = context;
                     }
                     @Override
                     public void process(final Object key, final Object value) {
-                        if (printOffset) {
-                            System.out.println("">>> "" + context.offset());
-                        }
                         numRecordsProcessed++;
                         if (numRecordsProcessed % 100 == 0) {
                             System.out.println(System.currentTimeMillis());
@@ -66,19 +61,19 @@ public void process(final Object key, final Object value) {
                     }
                     @Override
-                    public void punctuate(final long timestamp) { }
+                    public void punctuate(final long timestamp) {}
                     @Override
-                    public void close() { }
+                    public void close() {}
                 };
             }
         };
     }
-    public static final class Unwindow<K, V> implements KeyValueMapper<Windowed<K>, V, KeyValue<K, V>> {
+    public static final class Unwindow<K, V> implements KeyValueMapper<Windowed<K>, V, K> {
         @Override
-        public KeyValue<K, V> apply(final Windowed<K> winKey, final V value) {
-            return new KeyValue<>(winKey.key(), value);
+        public K apply(final Windowed<K> winKey, final V value) {
+            return winKey.key();
         }
     }
diff --git [file java] [file java]
index 64597bdcda6..c0e345ff2b6 100644
--- [file java]
+++ [file java]
@@ -23,7 +23,7 @@
 public class StreamsSmokeTest {
     /**
-     *  args ::= command kafka zookeeper stateDir
+     *  args ::= command kafka zookeeper stateDir disableAutoTerminate
      *  command := ""run"" | ""process""
      *
      * @param args
@@ -32,11 +32,13 @@ public static void main(String args) throws InterruptedException {
         String kafka = args;
         String stateDir = args.length > 1 ? args : null;
         String command = args.length > 2 ? args : null;
+        boolean disableAutoTerminate = args.length > 3;
-        System.out.println(""StreamsTest instance started"");
+        System.out.println(""StreamsTest instance started (StreamsSmokeTest)"");
         System.out.println(""command="" + command);
         System.out.println(""kafka="" + kafka);
         System.out.println(""stateDir="" + stateDir);
+        System.out.println(""disableAutoTerminate="" + disableAutoTerminate);
         switch (command) {
             case ""standalone"":
@@ -46,8 +48,12 @@ public static void main(String args) throws InterruptedException {
                 // this starts the driver (data generation and result verification)
                 final int numKeys = 10;
                 final int maxRecordsPerKey = 500;
-                Map<String, Set<Integer>> allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
-                SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                if (disableAutoTerminate) {
+                    SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey, false);
+                } else {
+                    Map<String, Set<Integer>> allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
+                    SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                }
                 break;
             case ""process"":
                 // this starts a KafkaStreams client
diff --git [file java] [file java]
new file mode 100644
index 00000000000..5486374b62c
--- /dev/null
+++ [file java]
@@ -0,0 +1,72 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsBuilder;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 2) {
+            System.err.println(""StreamsUpgradeTest requires two argument (kafka-url, state-dir, ) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args : """"));
+        }
+        final String kafka = args;
+        final String stateDir = args;
+        final String upgradeFrom = args.length > 2 ? args : null;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest trunk)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""stateDir="" + stateDir);
+        System.out.println(""upgradeFrom="" + upgradeFrom);
+
+        final StreamsBuilder builder = new StreamsBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(SmokeTestUtil.printProcessorSupplier(""data""));
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        if (upgradeFrom != null) {
+            config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+        }
+
+
+        final KafkaStreams streams = new KafkaStreams(builder.build(), config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..72d7f5a7b04
--- /dev/null
+++ [file java]
@@ -0,0 +1,104 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 3) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args + "" "" : """")
+                + (args.length > 1 ? args : """"));
+        }
+        final String kafka = args;
+        final String zookeeper = args;
+        final String stateDir = args;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.10.0)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""zookeeper="" + zookeeper);
+        System.out.println(""stateDir="" + stateDir);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..eebd0fab83c
--- /dev/null
+++ [file java]
@@ -0,0 +1,114 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    /**
+     * This test cannot be run executed, as long as Kafka 0.10.1.2 is not released
+     */
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 3) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir, ) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args + "" "" : """")
+                + (args.length > 1 ? args : """"));
+        }
+        final String kafka = args;
+        final String zookeeper = args;
+        final String stateDir = args;
+        final String upgradeFrom = args.length > 3 ? args : null;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.10.1)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""zookeeper="" + zookeeper);
+        System.out.println(""stateDir="" + stateDir);
+        System.out.println(""upgradeFrom="" + upgradeFrom);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 0.10.1.2 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(""upgrade.from"", upgradeFrom);
+        }
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..18240f04ff1
--- /dev/null
+++ [file java]
@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    /**
+     * This test cannot be run executed, as long as Kafka 0.10.2.2 is not released
+     */
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 2) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, state-dir, ) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args : """"));
+        }
+        final String kafka = args;
+        final String stateDir = args;
+        final String upgradeFrom = args.length > 2 ? args : null;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.10.2)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""stateDir="" + stateDir);
+        System.out.println(""upgradeFrom="" + upgradeFrom);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 0.10.2.2 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(""upgrade.from"", upgradeFrom);
+        }
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..779021d8672
--- /dev/null
+++ [file java]
@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    /**
+     * This test cannot be run executed, as long as Kafka 0.11.0.3 is not released
+     */
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 2) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, state-dir, ) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args : """"));
+        }
+        final String kafka = args;
+        final String stateDir = args;
+        final String upgradeFrom = args.length > 2 ? args : null;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.11.0)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""stateDir="" + stateDir);
+        System.out.println(""upgradeFrom="" + upgradeFrom);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 0.11.0.3 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(""upgrade.from"", upgradeFrom);
+        }
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git a/tests/docker/Dockerfile b/tests/docker/Dockerfile
index 3ca99938fb6..6fe05ffed6d 100644
--- a/tests/docker/Dockerfile
+++ b/tests/docker/Dockerfile
@@ -40,13 +40,13 @@ COPY ./ssh-config /root/.ssh/config
 RUN ssh-keygen -q -t rsa -N '' -f /root/.ssh/id_rsa && cp -f /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys
 # Install binary test dependencies.
-ENV MIRROR=""[link]""
-RUN mkdir -p ""/opt/kafka-0.8.2.2"" && curl -s ""${MIRROR}kafka/0.8.2.2/kafka_2.10-0.8.2.2.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.8.2.2""
-RUN mkdir -p ""/opt/kafka-0.9.0.1"" && curl -s ""${MIRROR}kafka/0.9.0.1/kafka_2.11-0.9.0.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.9.0.1""
-RUN mkdir -p ""/opt/kafka-0.10.0.1"" && curl -s ""${MIRROR}kafka/0.10.0.1/kafka_2.11-0.10.0.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.0.1""
-RUN mkdir -p ""/opt/kafka-0.10.1.1"" && curl -s ""${MIRROR}kafka/0.10.1.1/kafka_2.11-0.10.1.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.1.1""
-RUN mkdir -p ""/opt/kafka-0.10.2.1"" && curl -s ""${MIRROR}kafka/0.10.2.1/kafka_2.11-0.10.2.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.2.1""
-RUN mkdir -p ""/opt/kafka-0.11.0.0"" && curl -s ""${MIRROR}kafka/0.11.0.0/kafka_2.11-0.11.0.0.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.11.0.0""
+ENV MIRROR=""[link]""
+RUN mkdir -p ""/opt/kafka-0.8.2.2"" && curl -s ""${MIRROR}/kafka_2.10-0.8.2.2.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.8.2.2""
+RUN mkdir -p ""/opt/kafka-0.9.0.1"" && curl -s ""${MIRROR}/kafka_2.11-0.9.0.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.9.0.1""
+RUN mkdir -p ""/opt/kafka-0.10.0.1"" && curl -s ""${MIRROR}/kafka_2.11-0.10.0.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.0.1""
+RUN mkdir -p ""/opt/kafka-0.10.1.1"" && curl -s ""${MIRROR}/kafka_2.11-0.10.1.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.1.1""
+RUN mkdir -p ""/opt/kafka-0.10.2.1"" && curl -s ""${MIRROR}/kafka_2.11-0.10.2.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.2.1""
+RUN mkdir -p ""/opt/kafka-0.11.0.2"" && curl -s ""${MIRROR}/kafka_2.11-0.11.0.2.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.11.0.2""
 # Set up the ducker user.
 RUN useradd -ms /bin/bash ducker && mkdir -p /home/ducker/ && rsync -aiq /root/.ssh/ /home/ducker/.ssh && chown -R ducker /home/ducker/ /mnt/ && echo 'ducker ALL=(ALL) NOPASSWD: ALL' >> /etc/sudoers
diff --git a/tests/kafkatest/services/streams.py b/tests/kafkatest/services/streams.py
index f3f7348bf4e..f60074c7c6b 100644
--- a/tests/kafkatest/services/streams.py
+++ b/tests/kafkatest/services/streams.py
@@ -20,6 +20,7 @@
 from ducktape.utils.util import wait_until
 from kafkatest.directory_layout.kafka_path import KafkaPathResolverMixin
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1
 class StreamsTestBaseService(KafkaPathResolverMixin, Service):
@@ -33,6 +34,8 @@ class StreamsTestBaseService(KafkaPathResolverMixin, Service):
     LOG4J_CONFIG_FILE = os.path.join(PERSISTENT_ROOT, ""tools-log4j.properties"")
     PID_FILE = os.path.join(PERSISTENT_ROOT, ""streams.pid"")
+    CLEAN_NODE_ENABLED = True
+
     logs = {
         ""streams_log"": {
             ""path"": LOG_FILE,
@@ -43,6 +46,114 @@ class StreamsTestBaseService(KafkaPathResolverMixin, Service):
         ""streams_stderr"": {
             ""path"": STDERR_FILE,
             ""collect_default"": True},
+        ""streams_log.0-1"": {
+            ""path"": LOG_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_stdout.0-1"": {
+            ""path"": STDOUT_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_stderr.0-1"": {
+            ""path"": STDERR_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_log.0-2"": {
+            ""path"": LOG_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_stdout.0-2"": {
+            ""path"": STDOUT_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_stderr.0-2"": {
+            ""path"": STDERR_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_log.0-3"": {
+            ""path"": LOG_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_stdout.0-3"": {
+            ""path"": STDOUT_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_stderr.0-3"": {
+            ""path"": STDERR_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_log.0-4"": {
+            ""path"": LOG_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_stdout.0-4"": {
+            ""path"": STDOUT_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_stderr.0-4"": {
+            ""path"": STDERR_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_log.0-5"": {
+            ""path"": LOG_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_stdout.0-5"": {
+            ""path"": STDOUT_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_stderr.0-5"": {
+            ""path"": STDERR_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_log.0-6"": {
+            ""path"": LOG_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_stdout.0-6"": {
+            ""path"": STDOUT_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_stderr.0-6"": {
+            ""path"": STDERR_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_log.1-1"": {
+            ""path"": LOG_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_stdout.1-1"": {
+            ""path"": STDOUT_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_stderr.1-1"": {
+            ""path"": STDERR_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_log.1-2"": {
+            ""path"": LOG_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_stdout.1-2"": {
+            ""path"": STDOUT_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_stderr.1-2"": {
+            ""path"": STDERR_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_log.1-3"": {
+            ""path"": LOG_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_stdout.1-3"": {
+            ""path"": STDOUT_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_stderr.1-3"": {
+            ""path"": STDERR_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_log.1-4"": {
+            ""path"": LOG_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_stdout.1-4"": {
+            ""path"": STDOUT_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_stderr.1-4"": {
+            ""path"": STDERR_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_log.1-5"": {
+            ""path"": LOG_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_stdout.1-5"": {
+            ""path"": STDOUT_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_stderr.1-5"": {
+            ""path"": STDERR_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_log.1-6"": {
+            ""path"": LOG_FILE + "".1-6"",
+            ""collect_default"": True},
+        ""streams_stdout.1-6"": {
+            ""path"": STDOUT_FILE + "".1-6"",
+            ""collect_default"": True},
+        ""streams_stderr.1-6"": {
+            ""path"": STDERR_FILE + "".1-6"",
+            ""collect_default"": True},
     }
     def __init__(self, test_context, kafka, streams_class_name, user_test_args, user_test_args1=None, user_test_args2=None, user_test_args3=None):
@@ -108,7 +219,8 @@ def wait_node(self, node, timeout_sec=None):
     def clean_node(self, node):
         node.account.kill_process(""streams"", clean_shutdown=False, allow_fail=True)
-        node.account.ssh(""rm -rf "" + self.PERSISTENT_ROOT, allow_fail=False)
+        if self.CLEAN_NODE_ENABLED:
+            node.account.ssh(""rm -rf "" + self.PERSISTENT_ROOT, allow_fail=False)
     def start_cmd(self, node):
         args = self.args.copy()
@@ -170,7 +282,28 @@ def clean_node(self, node):
 class StreamsSmokeTestDriverService(StreamsSmokeTestBaseService):
     def __init__(self, test_context, kafka):
         super(StreamsSmokeTestDriverService, self).__init__(test_context, kafka, ""run"")
+        self.DISABLE_AUTO_TERMINATE = """"
+
+    def disable_auto_terminate(self):
+        self.DISABLE_AUTO_TERMINATE = ""disableAutoTerminate""
+
+    def start_cmd(self, node):
+        args = self.args.copy()
+        args = self.kafka.bootstrap_servers()
+        args = self.PERSISTENT_ROOT
+        args = self.STDOUT_FILE
+        args = self.STDERR_FILE
+        args = self.PID_FILE
+        args = self.LOG4J_CONFIG_FILE
+        args = self.DISABLE_AUTO_TERMINATE
+        args = self.path.script(""kafka-run-class.sh"", node)
+        cmd = ""( export KAFKA_LOG4J_OPTS=\""-Dlog4j.configuration=file:%(log4j)s\""; "" \
+              ""INCLUDE_TEST_JARS=true %(kafka_run_class)s %(streams_class_name)s "" \
+              "" %(kafka)s %(state_dir)s %(user_test_args)s %(disable_auto_terminate)s"" \
+              "" & echo $! >&3 ) 1>> %(stdout)s 2>> %(stderr)s 3> %(pidfile)s"" % args
+
+        return cmd
 class StreamsSmokeTestJobRunnerService(StreamsSmokeTestBaseService):
     def __init__(self, test_context, kafka):
@@ -211,3 +344,41 @@ def __init__(self, test_context, kafka, eosEnabled):
                                                                 kafka,
                                                                 ""org.apache.kafka.streams.tests.BrokerCompatibilityTest"",
                                                                 eosEnabled)
+
+class StreamsUpgradeTestJobRunnerService(StreamsTestBaseService):
+    def __init__(self, test_context, kafka):
+        super(StreamsUpgradeTestJobRunnerService, self).__init__(test_context,
+                                                                 kafka,
+                                                                 ""org.apache.kafka.streams.tests.StreamsUpgradeTest"",
+                                                                 """")
+        self.UPGRADE_FROM = """"
+
+    def set_version(self, kafka_streams_version):
+        self.KAFKA_STREAMS_VERSION = kafka_streams_version
+
+    def set_upgrade_from(self, upgrade_from):
+        self.UPGRADE_FROM = upgrade_from
+
+    def start_cmd(self, node):
+        args = self.args.copy()
+        args = self.kafka.bootstrap_servers()
+        if self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_0) or self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_1):
+            args = self.kafka.zk.connect_setting()
+        else:
+            args = """"
+        args = self.PERSISTENT_ROOT
+        args = self.STDOUT_FILE
+        args = self.STDERR_FILE
+        args = self.PID_FILE
+        args = self.LOG4J_CONFIG_FILE
+        args = self.KAFKA_STREAMS_VERSION
+        args = self.UPGRADE_FROM
+        args = self.path.script(""kafka-run-class.sh"", node)
+
+        cmd = ""( export KAFKA_LOG4J_OPTS=\""-Dlog4j.configuration=file:%(log4j)s\""; "" \
+              ""INCLUDE_TEST_JARS=true UPGRADE_KAFKA_STREAMS_TEST_VERSION=%(version)s "" \
+              "" %(kafka_run_class)s %(streams_class_name)s "" \
+              "" %(kafka)s %(zk)s %(state_dir)s %(user_test_args)s %(upgrade_from)s"" \
+              "" & echo $! >&3 ) 1>> %(stdout)s 2>> %(stderr)s 3> %(pidfile)s"" % args
+
+        return cmd
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py
index 81b7ffe7047..e97bbb7fcfd 100644
--- a/tests/kafkatest/tests/streams/streams_upgrade_test.py
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py
@@ -15,21 +15,48 @@
 from ducktape.mark.resource import cluster
 from ducktape.tests.test import Test
-from ducktape.mark import parametrize, ignore
+from ducktape.mark import ignore, matrix, parametrize
 from kafkatest.services.kafka import KafkaService
 from kafkatest.services.zookeeper import ZookeeperService
-from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsSmokeTestJobRunnerService
-from kafkatest.version import LATEST_0_10_1, LATEST_0_10_2, LATEST_0_11_0, DEV_BRANCH, KafkaVersion
+from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsSmokeTestJobRunnerService, StreamsUpgradeTestJobRunnerService
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1, LATEST_0_10_2, LATEST_0_11_0, DEV_BRANCH, DEV_VERSION, KafkaVersion
+import random
 import time
+broker_upgrade_versions = 
+simple_upgrade_versions_metadata_version_2 = 
 class StreamsUpgradeTest(Test):
     """"""
-    Tests rolling upgrades and downgrades of the Kafka Streams library.
+    Test upgrading Kafka Streams (all version combination)
+    If metadata was changes, upgrade is more difficult
+    Metadata version was bumped in 0.10.1.0
     """"""
     def __init__(self, test_context):
         super(StreamsUpgradeTest, self).__init__(test_context)
+        self.topics = {
+            'echo' : { 'partitions': 5 },
+            'data' : { 'partitions': 5 },
+        }
+
+    def perform_broker_upgrade(self, to_version):
+        self.logger.info(""First pass bounce - rolling broker upgrade"")
+        for node in self.kafka.nodes:
+            self.kafka.stop_node(node)
+            node.version = KafkaVersion(to_version)
+            self.kafka.start_node(node)
+
+    @cluster(num_nodes=6)
+    @matrix(from_version=broker_upgrade_versions, to_version=broker_upgrade_versions)
+    def test_upgrade_downgrade_brokers(self, from_version, to_version):
+        """"""
+        Start a smoke test client then perform rolling upgrades on the broker. 
+        """"""
+
+        if from_version == to_version:
+            return
+
         self.replication = 3
         self.partitions = 1
         self.isr = 2
@@ -55,45 +82,7 @@ def __init__(self, test_context):
             'tagg' : { 'partitions': self.partitions, 'replication-factor': self.replication,
                        'configs': {""min.insync.replicas"": self.isr} }
         }
-        
-
-    def perform_streams_upgrade(self, to_version):
-        self.logger.info(""First pass bounce - rolling streams upgrade"")
-
-        # get the node running the streams app
-        node = self.processor1.node
-        self.processor1.stop()
-
-        # change it's version. This will automatically make it pick up a different
-        # JAR when it starts again
-        node.version = KafkaVersion(to_version)
-        self.processor1.start()
-
-    def perform_broker_upgrade(self, to_version):
-        self.logger.info(""First pass bounce - rolling broker upgrade"")
-        for node in self.kafka.nodes:
-            self.kafka.stop_node(node)
-            node.version = KafkaVersion(to_version)
-            self.kafka.start_node(node)
-
-    @cluster(num_nodes=6)
-    @parametrize(from_version=str(LATEST_0_10_1), to_version=str(DEV_BRANCH))
-    @parametrize(from_version=str(LATEST_0_10_2), to_version=str(DEV_BRANCH))
-    @parametrize(from_version=str(LATEST_0_10_1), to_version=str(LATEST_0_11_0))
-    @parametrize(from_version=str(LATEST_0_10_2), to_version=str(LATEST_0_11_0))
-    @parametrize(from_version=str(LATEST_0_11_0), to_version=str(LATEST_0_10_2))
-    @parametrize(from_version=str(DEV_BRANCH), to_version=str(LATEST_0_10_2))
-    def test_upgrade_downgrade_streams(self, from_version, to_version):
-        """"""
-        Start a smoke test client, then abort (kill -9) and restart it a few times.
-        Ensure that all records are delivered.
-
-        Note, that just like tests/core/upgrade_test.py, a prerequisite for this test to succeed
-        if the inclusion of all parametrized versions of kafka in kafka/vagrant/base.sh 
-        (search for get_kafka()). For streams in particular, that means that someone has manually
-        copies the kafka-stream-$version-test.jar in the right S3 bucket as shown in base.sh.
-        """"""
         # Setup phase
         self.zk = ZookeeperService(self.test_context, num_nodes=1)
         self.zk.start()
@@ -108,13 +97,12 @@ def test_upgrade_downgrade_streams(self, from_version, to_version):
         self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)
         self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka)
-
         self.driver.start()
         self.processor1.start()
         time.sleep(15)
-        self.perform_streams_upgrade(to_version)
+        self.perform_broker_upgrade(to_version)
         time.sleep(15)
         self.driver.wait()
@@ -126,42 +114,239 @@ def test_upgrade_downgrade_streams(self, from_version, to_version):
         node.account.ssh(""grep ALL-RECORDS-DELIVERED %s"" % self.driver.STDOUT_FILE, allow_fail=False)
         self.processor1.node.account.ssh_capture(""grep SMOKE-TEST-CLIENT-CLOSED %s"" % self.processor1.STDOUT_FILE, allow_fail=False)
+    @matrix(from_version=simple_upgrade_versions_metadata_version_2, to_version=simple_upgrade_versions_metadata_version_2)
+    def test_simple_upgrade_downgrade(self, from_version, to_version):
+        """"""
+        Starts 3 KafkaStreams instances with <old_version>, and upgrades one-by-one to <new_version>
+        """"""
+        if from_version == to_version:
+            return
-    @cluster(num_nodes=6)
-    @parametrize(from_version=str(LATEST_0_10_2), to_version=str(DEV_BRANCH))
-    def test_upgrade_brokers(self, from_version, to_version):
+        self.zk = ZookeeperService(self.test_context, num_nodes=1)
+        self.zk.start()
+
+        self.kafka = KafkaService(self.test_context, num_nodes=1, zk=self.zk, topics=self.topics)
+        self.kafka.start()
+
+        self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)
+        self.driver.disable_auto_terminate()
+        self.processor1 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+        self.processor2 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+        self.processor3 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+
+        self.driver.start()
+        self.start_all_nodes_with(from_version)
+
+        self.processors = 
+
+        counter = 1
+        random.seed()
+
+        # upgrade one-by-one via rolling bounce
+        random.shuffle(self.processors)
+        for p in self.processors:
+            p.CLEAN_NODE_ENABLED = False
+            self.do_rolling_bounce(p, """", to_version, counter)
+            counter = counter + 1
+
+        # shutdown
+        self.driver.stop()
+        self.driver.wait()
+
+        random.shuffle(self.processors)
+        for p in self.processors:
+            node = p.node
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:
+                p.stop()
+                monitor.wait_until(""UPGRADE-TEST-CLIENT-CLOSED"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'UPGRADE-TEST-CLIENT-CLOSED' on"" + str(node.account))
+
+        self.driver.stop()
+
+    #@parametrize(new_version=str(LATEST_0_10_1)) we cannot run this test until Kafka 0.10.1.2 is released
+    #@parametrize(new_version=str(LATEST_0_10_2)) we cannot run this test until Kafka 0.10.2.2 is released
+    #@parametrize(new_version=str(LATEST_0_11_0)) we cannot run this test until Kafka 0.11.0.3 is released
+    @parametrize(new_version=str(DEV_VERSION))
+    def test_metadata_upgrade(self, new_version):
         """"""
-        Start a smoke test client then perform rolling upgrades on the broker. 
+        Starts 3 KafkaStreams instances with version 0.10.0, and upgrades one-by-one to <new_version>
         """"""
-        # Setup phase
+
         self.zk = ZookeeperService(self.test_context, num_nodes=1)
         self.zk.start()
-        # number of nodes needs to be >= 3 for the smoke test
-        self.kafka = KafkaService(self.test_context, num_nodes=3,
-                                  zk=self.zk, version=KafkaVersion(from_version), topics=self.topics)
+        self.kafka = KafkaService(self.test_context, num_nodes=1, zk=self.zk, topics=self.topics)
         self.kafka.start()
-        
-        # allow some time for topics to be created
-        time.sleep(10)
-        
+
         self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)
-        self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka)
+        self.driver.disable_auto_terminate()
+        self.processor1 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+        self.processor2 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+        self.processor3 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
-        
         self.driver.start()
-        self.processor1.start()
-        time.sleep(15)
+        self.start_all_nodes_with(str(LATEST_0_10_0))
-        self.perform_broker_upgrade(to_version)
+        self.processors = 
-        time.sleep(15)
+        counter = 1
+        random.seed()
+
+        # first rolling bounce
+        random.shuffle(self.processors)
+        for p in self.processors:
+            p.CLEAN_NODE_ENABLED = False
+            self.do_rolling_bounce(p, ""0.10.0"", new_version, counter)
+            counter = counter + 1
+
+        # second rolling bounce
+        random.shuffle(self.processors)
+        for p in self.processors:
+            self.do_rolling_bounce(p, """", new_version, counter)
+            counter = counter + 1
+
+        # shutdown
+        self.driver.stop()
         self.driver.wait()
+
+        random.shuffle(self.processors)
+        for p in self.processors:
+            node = p.node
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:
+                p.stop()
+                monitor.wait_until(""UPGRADE-TEST-CLIENT-CLOSED"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'UPGRADE-TEST-CLIENT-CLOSED' on"" + str(node.account))
+
         self.driver.stop()
-        self.processor1.stop()
+    def start_all_nodes_with(self, version):
+        # start first with <version>
+        self.prepare_for(self.processor1, version)
+        node1 = self.processor1.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as monitor:
+            with node1.account.monitor_log(self.processor1.LOG_FILE) as log_monitor:
+                self.processor1.start()
+                log_monitor.wait_until(""Kafka version : "" + version,
+                                       timeout_sec=60,
+                                       err_msg=""Could not detect Kafka Streams version "" + version + "" "" + str(node1.account))
+                monitor.wait_until(""processed 100 records from topic"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
-        node = self.driver.node
-        node.account.ssh(""grep ALL-RECORDS-DELIVERED %s"" % self.driver.STDOUT_FILE, allow_fail=False)
-        self.processor1.node.account.ssh_capture(""grep SMOKE-TEST-CLIENT-CLOSED %s"" % self.processor1.STDOUT_FILE, allow_fail=False)
+        # start second with <version>
+        self.prepare_for(self.processor2, version)
+        node2 = self.processor2.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:
+                with node2.account.monitor_log(self.processor2.LOG_FILE) as log_monitor:
+                    self.processor2.start()
+                    log_monitor.wait_until(""Kafka version : "" + version,
+                                           timeout_sec=60,
+                                           err_msg=""Could not detect Kafka Streams version "" + version + "" "" + str(node2.account))
+                    first_monitor.wait_until(""processed 100 records from topic"",
+                                             timeout_sec=60,
+                                             err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
+                    second_monitor.wait_until(""processed 100 records from topic"",
+                                              timeout_sec=60,
+                                              err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node2.account))
+
+        # start third with <version>
+        self.prepare_for(self.processor3, version)
+        node3 = self.processor3.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:
+                with node3.account.monitor_log(self.processor3.STDOUT_FILE) as third_monitor:
+                    with node3.account.monitor_log(self.processor3.LOG_FILE) as log_monitor:
+                        self.processor3.start()
+                        log_monitor.wait_until(""Kafka version : "" + version,
+                                               timeout_sec=60,
+                                               err_msg=""Could not detect Kafka Streams version "" + version + "" "" + str(node3.account))
+                        first_monitor.wait_until(""processed 100 records from topic"",
+                                                 timeout_sec=60,
+                                                 err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
+                        second_monitor.wait_until(""processed 100 records from topic"",
+                                                  timeout_sec=60,
+                                                  err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node2.account))
+                        third_monitor.wait_until(""processed 100 records from topic"",
+                                                  timeout_sec=60,
+                                                  err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node3.account))
+
+    @staticmethod
+    def prepare_for(processor, version):
+        processor.node.account.ssh(""rm -rf "" + processor.PERSISTENT_ROOT, allow_fail=False)
+        if version == str(DEV_VERSION):
+            processor.set_version("""")  # set to TRUNK
+        else:
+            processor.set_version(version)
+
+    def do_rolling_bounce(self, processor, upgrade_from, new_version, counter):
+        first_other_processor = None
+        second_other_processor = None
+        for p in self.processors:
+            if p != processor:
+                if first_other_processor is None:
+                    first_other_processor = p
+                else:
+                    second_other_processor = p
+
+        node = processor.node
+        first_other_node = first_other_processor.node
+        second_other_node = second_other_processor.node
+
+        # stop processor and wait for rebalance of others
+        with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:
+            with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:
+                processor.stop()
+                first_other_monitor.wait_until(""processed 100 records from topic"",
+                                               timeout_sec=60,
+                                               err_msg=""Never saw output 'processed 100 records from topic' on"" + str(first_other_node.account))
+                second_other_monitor.wait_until(""processed 100 records from topic"",
+                                                timeout_sec=60,
+                                                err_msg=""Never saw output 'processed 100 records from topic' on"" + str(second_other_node.account))
+        node.account.ssh_capture(""grep UPGRADE-TEST-CLIENT-CLOSED %s"" % processor.STDOUT_FILE, allow_fail=False)
+
+        if upgrade_from == """":  # upgrade disabled -- second round of rolling bounces
+            roll_counter = "".1-""  # second round of rolling bounces
+        else:
+            roll_counter = "".0-""  # first  round of rolling boundes
+
+        node.account.ssh(""mv "" + processor.STDOUT_FILE + "" "" + processor.STDOUT_FILE + roll_counter + str(counter), allow_fail=False)
+        node.account.ssh(""mv "" + processor.STDERR_FILE + "" "" + processor.STDERR_FILE + roll_counter + str(counter), allow_fail=False)
+        node.account.ssh(""mv "" + processor.LOG_FILE + "" "" + processor.LOG_FILE + roll_counter + str(counter), allow_fail=False)
+
+        if new_version == str(DEV_VERSION):
+            processor.set_version("""")  # set to TRUNK
+        else:
+            processor.set_version(new_version)
+        processor.set_upgrade_from(upgrade_from)
+
+        grep_metadata_error = ""grep \""org.apache.kafka.streams.errors.TaskAssignmentException: unable to decode subscription data: version=2\"" ""
+        with node.account.monitor_log(processor.STDOUT_FILE) as monitor:
+            with node.account.monitor_log(processor.LOG_FILE) as log_monitor:
+                with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:
+                    with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:
+                        processor.start()
+
+                        log_monitor.wait_until(""Kafka version : "" + new_version,
+                                               timeout_sec=60,
+                                               err_msg=""Could not detect Kafka Streams version "" + new_version + "" "" + str(node.account))
+                        first_other_monitor.wait_until(""processed 100 records from topic"",
+                                                       timeout_sec=60,
+                                                       err_msg=""Never saw output 'processed 100 records from topic' on"" + str(first_other_node.account))
+                        found = list(first_other_node.account.ssh_capture(grep_metadata_error + first_other_processor.STDERR_FILE, allow_fail=True))
+                        if len(found) > 0:
+                            raise Exception(""Kafka Streams failed with 'unable to decode subscription data: version=2'"")
+
+                        second_other_monitor.wait_until(""processed 100 records from topic"",
+                                                        timeout_sec=60,
+                                                        err_msg=""Never saw output 'processed 100 records from topic' on"" + str(second_other_node.account))
+                        found = list(second_other_node.account.ssh_capture(grep_metadata_error + second_other_processor.STDERR_FILE, allow_fail=True))
+                        if len(found) > 0:
+                            raise Exception(""Kafka Streams failed with 'unable to decode subscription data: version=2'"")
+
+                        monitor.wait_until(""processed 100 records from topic"",
+                                           timeout_sec=60,
+                                           err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node.account))
diff --git a/tests/kafkatest/version.py b/tests/kafkatest/version.py
index 8a9373c1413..b49a3ef2fcc 100644
--- a/tests/kafkatest/version.py
+++ b/tests/kafkatest/version.py
@@ -61,6 +61,7 @@ def get_version(node=None):
         return DEV_BRANCH
 DEV_BRANCH = KafkaVersion(""dev"")
+DEV_VERSION = KafkaVersion(""1.0.2-SNAPSHOT"")
 # 0.8.2.X versions
 V_0_8_2_1 = KafkaVersion(""0.8.2.1"")
@@ -91,7 +92,9 @@ def get_version(node=None):
 # 0.11.0.0 versions
 V_0_11_0_0 = KafkaVersion(""0.11.0.0"")
-LATEST_0_11_0 = V_0_11_0_0
+V_0_11_0_1 = KafkaVersion(""0.11.0.1"")
+V_0_11_0_2 = KafkaVersion(""0.11.0.2"")
+LATEST_0_11_0 = V_0_11_0_2
 LATEST_0_11 = LATEST_0_11_0
 # 1.0.0 versions
diff --git a/vagrant/base.sh b/vagrant/base.sh
index 2ebebf9adaa..86ceefe6897 100755
--- a/vagrant/base.sh
+++ b/vagrant/base.sh
@@ -93,8 +93,8 @@ get_kafka 0.10.1.1 2.11
 chmod a+rw /opt/kafka-0.10.1.1
 get_kafka 0.10.2.1 2.11
 chmod a+rw /opt/kafka-0.10.2.1
-get_kafka 0.11.0.0 2.11
-chmod a+rw /opt/kafka-0.11.0.0
+get_kafka 0.11.0.2 2.11
+chmod a+rw /opt/kafka-0.11.0.2
 # For EC2 nodes, we want to use /mnt, which should have the local disk. On local
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 16 **
mjsax closed pull request #4773:  KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git a/bin/kafka-run-class.sh b/bin/kafka-run-class.sh
index bb786da43ca..4dd092323b0 100755
--- a/bin/kafka-run-class.sh
+++ b/bin/kafka-run-class.sh
@@ -69,28 +69,50 @@ do
   fi
 done
-for file in ""$base_dir""/clients/build/libs/kafka-clients*.jar;
-do
-  if should_include_file ""$file""; then
-    CLASSPATH=""$CLASSPATH"":""$file""
-  fi
-done
+if ; then
+  clients_lib_dir=$(dirname $0)/../clients/build/libs
+  streams_lib_dir=$(dirname $0)/../streams/build/libs
+  rocksdb_lib_dir=$(dirname $0)/../streams/build/dependant-libs-${SCALA_VERSION}
+else
+  clients_lib_dir=/opt/kafka-$UPGRADE_KAFKA_STREAMS_TEST_VERSION/libs
+  streams_lib_dir=$clients_lib_dir
+  rocksdb_lib_dir=$streams_lib_dir
+fi
+
-for file in ""$base_dir""/streams/build/libs/kafka-streams*.jar;
+for file in ""$clients_lib_dir""/kafka-clients*.jar;
 do
   if should_include_file ""$file""; then
     CLASSPATH=""$CLASSPATH"":""$file""
   fi
 done
-for file in ""$base_dir""/streams/examples/build/libs/kafka-streams-examples*.jar;
+for file in ""$streams_lib_dir""/kafka-streams*.jar;
 do
   if should_include_file ""$file""; then
     CLASSPATH=""$CLASSPATH"":""$file""
   fi
 done
-for file in ""$base_dir""/streams/build/dependant-libs-${SCALA_VERSION}/rocksdb*.jar;
+if ; then
+  for file in ""$base_dir""/streams/examples/build/libs/kafka-streams-examples*.jar;
+  do
+    if should_include_file ""$file""; then
+      CLASSPATH=""$CLASSPATH"":""$file""
+    fi
+  done
+else
+  VERSION_NO_DOTS=`echo $UPGRADE_KAFKA_STREAMS_TEST_VERSION | sed 's/\.//g'`
+  SHORT_VERSION_NO_DOTS=${VERSION_NO_DOTS:0:((${#VERSION_NO_DOTS} - 1))} # remove last char, ie, bug-fix number
+  for file in ""$base_dir""/streams/upgrade-system-tests-$SHORT_VERSION_NO_DOTS/build/libs/kafka-streams-upgrade-system-tests*.jar;
+  do
+    if should_include_file ""$file""; then
+      CLASSPATH=""$CLASSPATH"":""$file""
+    fi
+  done
+fi
+
+for file in ""$rocksdb_lib_dir""/rocksdb*.jar;
 do
   CLASSPATH=""$CLASSPATH"":""$file""
 done
diff --git a/build.gradle b/build.gradle
index 5e4c35643c2..f4d1fb3e090 100644
--- a/build.gradle
+++ b/build.gradle
@@ -1019,6 +1019,66 @@ project(':streams:examples') {
   }
 }
+project(':streams:upgrade-system-tests-0100') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0100""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0100
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
+project(':streams:upgrade-system-tests-0101') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0101""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0101
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
+project(':streams:upgrade-system-tests-0102') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0102""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0102
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
+project(':streams:upgrade-system-tests-0110') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0110""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0110
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
+project(':streams:upgrade-system-tests-10') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-10""
+
+  dependencies {
+    testCompile libs.kafkaStreams_10
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
 project(':jmh-benchmarks') {
   apply plugin: 'com.github.johnrengelman.shadow'
diff --git a/checkstyle/suppressions.xml b/checkstyle/suppressions.xml
index f23805e665f..1ec535f6f32 100644
--- a/checkstyle/suppressions.xml
+++ b/checkstyle/suppressions.xml
@@ -189,7 +189,7 @@
               files=""[file java]""/>
     <suppress checks=""NPathComplexity""
-              files=""[file java]""/>
+              files=""[file java]|[file java]""/>
     <suppress checks=""NPathComplexity""
               files=""[file java]""/>
diff --git [file java] [file java]
index 31c51c22ca3..4c17e596b32 100644
--- [file java]
+++ [file java]
@@ -16,7 +16,9 @@
  */
 package org.apache.kafka.common.security.authenticator;
-import java.util.Map;
+import org.apache.kafka.common.config.SaslConfigs;
+import org.apache.kafka.common.network.Mode;
+import org.apache.kafka.common.security.scram.ScramExtensionsCallback;
 import javax.security.auth.Subject;
 import javax.security.auth.callback.Callback;
@@ -25,10 +27,7 @@
 import javax.security.auth.callback.UnsupportedCallbackException;
 import javax.security.sasl.AuthorizeCallback;
 import javax.security.sasl.RealmCallback;
-
-import org.apache.kafka.common.config.SaslConfigs;
-import org.apache.kafka.common.network.Mode;
-import org.apache.kafka.common.security.scram.ScramExtensionsCallback;
+import java.util.Map;
 /**
  * Callback handler for Sasl clients. The callbacks required for the SASL mechanism
diff --git a/docs/streams/upgrade-guide.html b/docs/streams/upgrade-guide.html
index b9b4a4e48cf..09557fa2c63 100644
--- a/docs/streams/upgrade-guide.html
+++ b/docs/streams/upgrade-guide.html
@@ -34,7 +34,7 @@ <h1>Upgrade Guide and API Changes</h1>
     </div>
     <p>
-        If you want to upgrade from 1.0.x to 1.1.0 and you have customized window store implementations on the <code>ReadOnlyWindowStore</code> interface
+        If you want to upgrade from 1.0.x to 1.1.x and you have customized window store implementations on the <code>ReadOnlyWindowStore</code> interface
         you'd need to update your code to incorporate the newly added public APIs.
         Otherwise, if you are using Java 7 you don't need to make any code changes as the public API is fully backward compatible;
         but if you are using Java 8 method references in your Kafka Streams code you might need to update your code to resolve method ambiguities.
@@ -43,28 +43,52 @@ <h1>Upgrade Guide and API Changes</h1>
     </p>
     <p>
-        If you want to upgrade from 0.11.0.x to 1.0.0 you don't need to make any code changes as the public API is fully backward compatible.
+        If you want to upgrade from 0.10.2.x or 0.11.0.x to 1.1.x and you have customized window store implementations on the <code>ReadOnlyWindowStore</code> interface
+        you'd need to update your code to incorporate the newly added public APIs.
+        Otherwise, if you are using Java 7 you don't need to do any code changes as the public API is fully backward compatible;
+        but if you are using Java 8 method references in your Kafka Streams code you might need to update your code to resolve method ambiguities.
         However, some public APIs were deprecated and thus it is recommended to update your code eventually to allow for future upgrades.
-        See <a href=""#streams_api_changes_100"">below</a> for a complete list of 1.0.0 API and semantic changes that allow you to advance your application and/or simplify your code base.
-    </p>
-
-    <p>
-        If you want to upgrade from 0.10.2.x to 0.11.0 you don't need to make any code changes as the public API is fully backward compatible.
-        However, some configuration parameters were deprecated and thus it is recommended to update your code eventually to allow for future upgrades.
-        See <a href=""#streams_api_changes_0110"">below</a> for a complete list of 0.11.0 API and semantic changes that allow you to advance your application and/or simplify your code base.
+        See below a complete list of <a href=""#streams_api_changes_110"">1.1</a>, <a href=""#streams_api_changes_100"">1.0</a>,
+        and <a href=""#streams_api_changes_0110"">0.11.0</a> API
+        and semantic changes that allow you to advance your application and/or simplify your code base, including the usage of new features.
+        Additionally, Streams API 1.1.x requires broker on-disk message format version 0.10 or higher; thus, you need to make sure that the message
+        format is configured correctly before you upgrade your Kafka Streams application.
     </p>
     <p>
-        If you want to upgrade from 0.10.1.x to 0.10.2, see the <a href=""/{{version}}/documentation/#upgrade_1020_streams""><b>Upgrade Section for 0.10.2</b></a>.
-        It highlights incompatible changes you need to consider to upgrade your code and application.
-        See <a href=""#streams_api_changes_0102"">below</a> for a complete list of 0.10.2 API and semantic changes that allow you to advance your application and/or simplify your code base.
+        If you want to upgrade from 0.10.1.x to 1.1.x see the Upgrade Sections for <a href=""/{{version}}/documentation/#upgrade_1020_streams""><b>0.10.2</b></a>,
+        <a href=""/{{version}}/documentation/#upgrade_1100_streams""><b>0.11.0</b></a>, and
+        <a href=""/{{version}}/documentation/#upgrade_100_streams""><b>1.0</b></a>, and
+        <a href=""/{{version}}/documentation/#upgrade_110_streams""><b>1.1</b></a>.
+        Note, that a brokers on-disk message format must be on version 0.10 or higher to run a Kafka Streams application version 1.1 or higher.
+        See below a complete list of <a href=""#streams_api_changes_0102"">0.10.2</a>, <a href=""#streams_api_changes_0110"">0.11.0</a>,
+        <a href=""#streams_api_changes_100"">1.0</a>, and <a href=""#streams_api_changes_110"">1.1</a> API and semantical changes
+        that allow you to advance your application and/or simplify your code base, including the usage of new features.
     </p>
     <p>
-        If you want to upgrade from 0.10.0.x to 0.10.1, see the <a href=""/{{version}}/documentation/#upgrade_1010_streams""><b>Upgrade Section for 0.10.1</b></a>.
-        It highlights incompatible changes you need to consider to upgrade your code and application.
-        See <a href=""#streams_api_changes_0101"">below</a> a complete list of 0.10.1 API changes that allow you to advance your application and/or simplify your code base, including the usage of new features.
+        Upgrading from 0.10.0.x to 1.1.x directly is also possible.
+        Note, that a brokers must be on version 0.10.1 or higher and on-disk message format must be on version 0.10 or higher
+        to run a Kafka Streams application version 1.1 or higher.
+        See <a href=""#streams_api_changes_0101"">Streams API changes in 0.10.1</a>, <a href=""#streams_api_changes_0102"">Streams API changes in 0.10.2</a>,
+        <a href=""#streams_api_changes_0110"">Streams API changes in 0.11.0</a>, and <a href=""#streams_api_changes_100"">Streams API changes in 1.0</a>
+        for a complete list of API changes.
+        Upgrading to 1.1.1 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+        (cf. <a href=""[link]"">KIP-268</a>).
+        As an alternative, an offline upgrade is also possible.
     </p>
+    <ul>
+        <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 1.1.1</li>
+        <li> bounce each instance of your application once </li>
+        <li> prepare your newly deployed 1.1.1 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+        <li> bounce each instance of your application once more to complete the upgrade </li>
+    </ul>
+    <p> Upgrading from 0.10.0.x to 1.1.0 requires an offline upgrade (rolling bounce upgrade is not supported) </p>
+    <ul>
+        <li> stop all old (0.10.0.x) application instances </li>
+        <li> update your code and swap old code and jar file with new code and new jar file </li>
+        <li> restart all new (1.1.0) application instances </li>
+    </ul>
     <h3><a id=""streams_api_changes_110"" href=""#streams_api_changes_110"">Streams API changes in 1.1.0</a></h3>
     <p>
diff --git a/docs/upgrade.html b/docs/upgrade.html
index 3ac293d8498..26c4779a133 100644
--- a/docs/upgrade.html
+++ b/docs/upgrade.html
@@ -63,6 +63,12 @@ <h4><a id=""upgrade_1_1_0"" href=""#upgrade_1_1_0"">Upgrading from 0.8.x, 0.9.x, 0.1
         Hot-swaping the jar-file only might not work.</li>
 </ol>
+<h5><a id=""upgrade_111_notable"" href=""#upgrade_111_notable"">Notable changes in 1.1.1</a></h5>
+<ul>
+    <li> New Kafka Streams configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x </li>
+    <li> See the <a href=""/{{version}}/documentation/streams/upgrade-guide.html""><b>Kafka Streams upgrade guide</b></a> for details about this new config.
+</ul>
+
 <h5><a id=""upgrade_110_notable"" href=""#upgrade_110_notable"">Notable changes in 1.1.0</a></h5>
 <ul>
     <li>The kafka artifact in Maven no longer depends on log4j or slf4j-log4j12. Similarly to the kafka-clients artifact, users
@@ -134,6 +140,12 @@ <h4><a id=""upgrade_1_0_0"" href=""#upgrade_1_0_0"">Upgrading from 0.8.x, 0.9.x, 0.1
         Similarly for the message format version.</li>
 </ol>
+<h5><a id=""upgrade_102_notable"" href=""#upgrade_102_notable"">Notable changes in 1.0.2</a></h5>
+<ul>
+    <li> New Kafka Streams configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x </li>
+    <li> See the <a href=""/{{version}}/documentation/streams/upgrade-guide.html""><b>Kafka Streams upgrade guide</b></a> for details about this new config.
+</ul>
+
 <h5><a id=""upgrade_101_notable"" href=""#upgrade_101_notable"">Notable changes in 1.0.1</a></h5>
 <ul>
     <li>Restored binary compatibility of AdminClient's Options classes (e.g. CreateTopicsOptions, DeleteTopicsOptions, etc.) with
@@ -199,17 +211,71 @@ <h5><a id=""upgrade_100_new_protocols"" href=""#upgrade_100_new_protocols"">New Prot
          be used if the SaslHandshake request version is greater than 0. </li>
 </ul>
-<h5><a id=""upgrade_100_streams"" href=""#upgrade_100_streams"">Upgrading a 1.0.0 Kafka Streams Application</a></h5>
+<h5><a id=""upgrade_100_streams"" href=""#upgrade_100_streams"">Upgrading a 0.11.0 Kafka Streams Application</a></h5>
 <ul>
     <li> Upgrading your Streams application from 0.11.0 to 1.0.0 does not require a broker upgrade.
-        A Kafka Streams 1.0.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).
-        However, Kafka Streams 1.0 requires 0.10 message format or newer and does not work with older message formats. </li>
+         A Kafka Streams 1.0.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).
+         However, Kafka Streams 1.0 requires 0.10 message format or newer and does not work with older message formats. </li>
     <li> If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. </li>
     <li> There are a few public APIs including <code>ProcessorContext#schedule()</code>, <code>Processor#punctuate()</code> and <code>KStreamBuilder</code>, <code>TopologyBuilder</code> are being deprecated by new APIs.
-        We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.
     <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_100"">Streams API changes in 1.0.0</a> for more details. </li>
 </ul>
+<h5><a id=""upgrade_100_streams_from_0102"" href=""#upgrade_100_streams_from_0102"">Upgrading a 0.10.2 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.2 to 1.0 does not require a broker upgrade.
+         A Kafka Streams 1.0 application can connect to 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
+    <li> If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. </li>
+    <li> There are a few public APIs including <code>ProcessorContext#schedule()</code>, <code>Processor#punctuate()</code> and <code>KStreamBuilder</code>, <code>TopologyBuilder</code> are being deprecated by new APIs.
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.
+    <li> If you specify customized <code>key.serde</code>, <code>value.serde</code> and <code>timestamp.extractor</code> in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. </li>
+    <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0110"">Streams API changes in 0.11.0</a> for more details. </li>
+</ul>
+
+<h5><a id=""upgrade_100_streams_from_0101"" href=""#upgrade_1100_streams_from_0101"">Upgrading a 0.10.1 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.1 to 1.0 does not require a broker upgrade.
+         A Kafka Streams 1.0 application can connect to 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
+    <li> You need to recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. </li>
+    <li> There are a few public APIs including <code>ProcessorContext#schedule()</code>, <code>Processor#punctuate()</code> and <code>KStreamBuilder</code>, <code>TopologyBuilder</code> are being deprecated by new APIs.
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.
+    <li> If you specify customized <code>key.serde</code>, <code>value.serde</code> and <code>timestamp.extractor</code> in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. </li>
+    <li> If you use a custom (i.e., user implemented) timestamp extractor, you will need to update this code, because the <code>TimestampExtractor</code> interface was changed. </li>
+    <li> If you register custom metrics, you will need to update this code, because the <code>StreamsMetric</code> interface was changed. </li>
+    <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_100"">Streams API changes in 1.0.0</a>,
+         <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0110"">Streams API changes in 0.11.0</a> and
+         <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0102"">Streams API changes in 0.10.2</a> for more details. </li>
+</ul>
+
+<h5><a id=""upgrade_100_streams_from_0100"" href=""#upgrade_100_streams_from_0100"">Upgrading a 0.10.0 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.0 to 1.0 does require a <a href=""#upgrade_10_1"">broker upgrade</a> because a Kafka Streams 1.0 application can only connect to 0.1, 0.11.0, 0.10.2, or 0.10.1 brokers. </li>
+    <li> There are couple of API changes, that are not backward compatible (cf. <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_100"">Streams API changes in 1.0.0</a>,
+         <a href=""/{{version}}/documentation/streams#streams_api_changes_0110"">Streams API changes in 0.11.0</a>,
+         <a href=""/{{version}}/documentation/streams#streams_api_changes_0102"">Streams API changes in 0.10.2</a>, and
+         <a href=""/{{version}}/documentation/streams#streams_api_changes_0101"">Streams API changes in 0.10.1</a> for more details).
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> Upgrading from 0.10.0.x to 1.0.2 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+        (cf. <a href=""[link]"">KIP-268</a>).
+        As an alternative, an offline upgrade is also possible.
+        <ul>
+            <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.11.0.3 </li>
+            <li> bounce each instance of your application once </li>
+            <li> prepare your newly deployed 1.0.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+            <li> bounce each instance of your application once more to complete the upgrade </li>
+        </ul>
+    </li>
+    <li> Upgrading from 0.10.0.x to 1.0.0 or 1.0.1 requires an offline upgrade (rolling bounce upgrade is not supported)
+        <ul>
+            <li> stop all old (0.10.0.x) application instances </li>
+            <li> update your code and swap old code and jar file with new code and new jar file </li>
+            <li> restart all new (1.0.0 or 1.0.1) application instances </li>
+        </ul>
+    </li>
+</ul>
+
 <h4><a id=""upgrade_11_0_0"" href=""#upgrade_11_0_0"">Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x or 0.10.2.x to 0.11.0.0</a></h4>
 <p>Kafka 0.11.0.0 introduces a new message format version as well as wire protocol changes. By following the recommended rolling upgrade plan below,
   you guarantee no downtime during the upgrade. However, please review the <a href=""#upgrade_1100_notable"">notable changes in 0.11.0.0</a> before upgrading.
@@ -258,11 +324,55 @@ <h4><a id=""upgrade_11_0_0"" href=""#upgrade_11_0_0"">Upgrading from 0.8.x, 0.9.x, 0
 <h5><a id=""upgrade_1100_streams"" href=""#upgrade_1100_streams"">Upgrading a 0.10.2 Kafka Streams Application</a></h5>
 <ul>
     <li> Upgrading your Streams application from 0.10.2 to 0.11.0 does not require a broker upgrade.
-        A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
+         A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
     <li> If you specify customized <code>key.serde</code>, <code>value.serde</code> and <code>timestamp.extractor</code> in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. </li>
     <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0110"">Streams API changes in 0.11.0</a> for more details. </li>
 </ul>
+<h5><a id=""upgrade_1100_streams_from_0101"" href=""#upgrade_1100_streams_from_0101"">Upgrading a 0.10.1 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.1 to 0.11.0 does not require a broker upgrade.
+         A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
+    <li> You need to recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> If you specify customized <code>key.serde</code>, <code>value.serde</code> and <code>timestamp.extractor</code> in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. </li>
+    <li> If you use a custom (i.e., user implemented) timestamp extractor, you will need to update this code, because the <code>TimestampExtractor</code> interface was changed. </li>
+    <li> If you register custom metrics, you will need to update this code, because the <code>StreamsMetric</code> interface was changed. </li>
+    <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0110"">Streams API changes in 0.11.0</a> and
+         <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0102"">Streams API changes in 0.10.2</a> for more details. </li>
+</ul>
+
+<h5><a id=""upgrade_1100_streams_from_0100"" href=""#upgrade_1100_streams_from_0100"">Upgrading a 0.10.0 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.0 to 0.11.0 does require a <a href=""#upgrade_10_1"">broker upgrade</a> because a Kafka Streams 0.11.0 application can only connect to 0.11.0, 0.10.2, or 0.10.1 brokers. </li>
+    <li> There are couple of API changes, that are not backward compatible (cf. <a href=""/{{version}}/documentation/streams#streams_api_changes_0110"">Streams API changes in 0.11.0</a>,
+         <a href=""/{{version}}/documentation/streams#streams_api_changes_0102"">Streams API changes in 0.10.2</a>, and
+         <a href=""/{{version}}/documentation/streams#streams_api_changes_0101"">Streams API changes in 0.10.1</a> for more details).
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> Upgrading from 0.10.0.x to 0.11.0.3 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+        (cf. <a href=""[link]"">KIP-268</a>).
+        As an alternative, an offline upgrade is also possible.
+        <ul>
+            <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.11.0.3 </li>
+            <li> bounce each instance of your application once </li>
+            <li> prepare your newly deployed 0.11.0.3 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+            <li> bounce each instance of your application once more to complete the upgrade </li>
+        </ul>
+    </li>
+    <li> Upgrading from 0.10.0.x to 0.11.0.0, 0.11.0.1, or 0.11.0.2 requires an offline upgrade (rolling bounce upgrade is not supported)
+        <ul>
+            <li> stop all old (0.10.0.x) application instances </li>
+            <li> update your code and swap old code and jar file with new code and new jar file </li>
+            <li> restart all new (0.11.0.0 , 0.11.0.1, or 0.11.0.2) application instances </li>
+        </ul>
+    </li>
+</ul>
+
+<h5><a id=""upgrade_1103_notable"" href=""#upgrade_1103_notable"">Notable changes in 0.11.0.3</a></h5>
+<ul>
+    <li> New Kafka Streams configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x </li>
+    <li> See the <a href=""/{{version}}/documentation/streams/upgrade-guide.html""><b>Kafka Streams upgrade guide</b></a> for details about this new config.
+</ul>
+
 <h5><a id=""upgrade_1100_notable"" href=""#upgrade_1100_notable"">Notable changes in 0.11.0.0</a></h5>
 <ul>
     <li>Unclean leader election is now disabled by default. The new default favors durability over availability. Users who wish to
@@ -413,6 +523,35 @@ <h5><a id=""upgrade_1020_streams"" href=""#upgrade_1020_streams"">Upgrading a 0.10.1
     <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0102"">Streams API changes in 0.10.2</a> for more details. </li>
 </ul>
+<h5><a id=""upgrade_1020_streams_from_0100"" href=""#upgrade_1020_streams_from_0100"">Upgrading a 0.10.0 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.0 to 0.10.2 does require a <a href=""#upgrade_10_1"">broker upgrade</a> because a Kafka Streams 0.10.2 application can only connect to 0.10.2 or 0.10.1 brokers. </li>
+    <li> There are couple of API changes, that are not backward compatible (cf. <a href=""/{{version}}/documentation/streams#streams_api_changes_0102"">Streams API changes in 0.10.2</a> for more details).
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> Upgrading from 0.10.0.x to 0.10.2.2 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+         (cf. <a href=""[link]"">KIP-268</a>).
+         As an alternative, an offline upgrade is also possible.
+        <ul>
+            <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.10.2.2 </li>
+            <li> bounce each instance of your application once </li>
+            <li> prepare your newly deployed 0.10.2.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+            <li> bounce each instance of your application once more to complete the upgrade </li>
+        </ul>
+    </li>
+    <li> Upgrading from 0.10.0.x to 0.10.2.0 or 0.10.2.1 requires an offline upgrade (rolling bounce upgrade is not supported)
+        <ul>
+            <li> stop all old (0.10.0.x) application instances </li>
+            <li> update your code and swap old code and jar file with new code and new jar file </li>
+            <li> restart all new (0.10.2.0 or 0.10.2.1) application instances </li>
+        </ul>
+    </li>
+</ul>
+
+<h5><a id=""upgrade_10202_notable"" href=""#upgrade_10202_notable"">Notable changes in 0.10.2.2</a></h5>
+<ul>
+    <li> New configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x </li>
+</ul>
+
 <h5><a id=""upgrade_10201_notable"" href=""#upgrade_10201_notable"">Notable changes in 0.10.2.1</a></h5>
 <ul>
   <li> The default values for two configurations of the StreamsConfig class were changed to improve the resiliency of Kafka Streams applications. The internal Kafka Streams producer <code>retries</code> default value was changed from 0 to 10. The internal Kafka Streams consumer <code>max.poll.interval.ms</code>  default value was changed from 300000 to <code>Integer.MAX_VALUE</code>.
@@ -491,6 +630,23 @@ <h5><a id=""upgrade_1010_streams"" href=""#upgrade_1010_streams"">Upgrading a 0.10.0
     <li> Upgrading your Streams application from 0.10.0 to 0.10.1 does require a <a href=""#upgrade_10_1"">broker upgrade</a> because a Kafka Streams 0.10.1 application can only connect to 0.10.1 brokers. </li>
     <li> There are couple of API changes, that are not backward compatible (cf. <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0101"">Streams API changes in 0.10.1</a> for more details).
          Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> Upgrading from 0.10.0.x to 0.10.1.2 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+         (cf. <a href=""[link]"">KIP-268</a>).
+         As an alternative, an offline upgrade is also possible.
+        <ul>
+            <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.10.1.2 </li>
+            <li> bounce each instance of your application once </li>
+            <li> prepare your newly deployed 0.10.1.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+            <li> bounce each instance of your application once more to complete the upgrade </li>
+        </ul>
+    </li>
+    <li> Upgrading from 0.10.0.x to 0.10.1.0 or 0.10.1.1 requires an offline upgrade (rolling bounce upgrade is not supported)
+        <ul>
+            <li> stop all old (0.10.0.x) application instances </li>
+            <li> update your code and swap old code and jar file with new code and new jar file </li>
+            <li> restart all new (0.10.1.0 or 0.10.1.1) application instances </li>
+        </ul>
+    </li>
 </ul>
 <h5><a id=""upgrade_1010_notable"" href=""#upgrade_1010_notable"">Notable changes in 0.10.1.0</a></h5>
@@ -534,14 +690,17 @@ <h5><a id=""upgrade_1010_new_protocols"" href=""#upgrade_1010_new_protocols"">New Pr
 </ul>
 <h4><a id=""upgrade_10"" href=""#upgrade_10"">Upgrading from 0.8.x or 0.9.x to 0.10.0.0</a></h4>
+<p>
 0.10.0.0 has <a href=""#upgrade_10_breaking"">potential breaking changes</a> (please review before upgrading) and possible <a href=""#upgrade_10_performance_impact"">  performance impact following the upgrade</a>. By following the recommended rolling upgrade plan below, you guarantee no downtime and no performance impact during and following the upgrade.
 <br>
 Note: Because new protocols are introduced, it is important to upgrade your Kafka clusters before upgrading your clients.
-<p/>
+</p>
+<p>
 <b>Notes to clients with version 0.9.0.0: </b>Due to a bug introduced in 0.9.0.0,
 clients that depend on ZooKeeper (old Scala high-level Consumer and MirrorMaker if used with the old consumer) will not
 work with 0.10.0.x brokers. Therefore, 0.9.0.0 clients should be upgraded to 0.9.0.1 <b>before</b> brokers are upgraded to
 0.10.0.x. This step is not necessary for 0.8.X or 0.9.0.1 clients.
+</p>
 <p><b>For a rolling upgrade:</b></p>
diff --git a/gradle/dependencies.gradle b/gradle/dependencies.gradle
index b7a03dcd752..d82ec4b3d6a 100644
--- a/gradle/dependencies.gradle
+++ b/gradle/dependencies.gradle
@@ -60,6 +60,11 @@ versions += [
   scalaLogging: ""3.7.2"",
   jopt: ""5.0.4"",
   junit: ""4.12"",
+  kafka_0100: ""0.10.0.1"",
+  kafka_0101: ""0.10.1.1"",
+  kafka_0102: ""0.10.2.1"",
+  kafka_0110: ""0.11.0.2"",
+  kafka_10: ""1.0.1"",
   lz4: ""1.4"",
   metrics: ""2.2.0"",
   // PowerMock 1.x doesn't support Java 9, so use PowerMock 2.0.0 beta
@@ -97,12 +102,16 @@ libs += [
   jettyServlets: ""org.eclipse.jetty:jetty-servlets:$versions.jetty"",
   jerseyContainerServlet: ""org.glassfish.jersey.containers:jersey-container-servlet:$versions.jersey"",
   jmhCore: ""org.openjdk.jmh:jmh-core:$versions.jmh"",
-  jmhGeneratorAnnProcess: ""org.openjdk.jmh:jmh-generator-annprocess:$versions.jmh"",
   jmhCoreBenchmarks: ""org.openjdk.jmh:jmh-core-benchmarks:$versions.jmh"",
+  jmhGeneratorAnnProcess: ""org.openjdk.jmh:jmh-generator-annprocess:$versions.jmh"",
+  joptSimple: ""net.sf.jopt-simple:jopt-simple:$versions.jopt"",
   junit: ""junit:junit:$versions.junit"",
+  kafkaStreams_0100: ""org.apache.kafka:kafka-streams:$versions.kafka_0100"",
+  kafkaStreams_0101: ""org.apache.kafka:kafka-streams:$versions.kafka_0101"",
+  kafkaStreams_0102: ""org.apache.kafka:kafka-streams:$versions.kafka_0102"",
+  kafkaStreams_0110: ""org.apache.kafka:kafka-streams:$versions.kafka_0110"",
+  kafkaStreams_10: ""org.apache.kafka:kafka-streams:$versions.kafka_10"",
   log4j: ""log4j:log4j:$versions.log4j"",
-  scalaLogging: ""com.typesafe.scala-logging:scala-logging_$versions.baseScala:$versions.scalaLogging"",
-  joptSimple: ""net.sf.jopt-simple:jopt-simple:$versions.jopt"",
   lz4: ""org.lz4:lz4-java:$versions.lz4"",
   metrics: ""com.yammer.metrics:metrics-core:$versions.metrics"",
   powermockJunit4: ""org.powermock:powermock-module-junit4:$versions.powermock"",
@@ -110,6 +119,7 @@ libs += [
   reflections: ""org.reflections:reflections:$versions.reflections"",
   rocksDBJni: ""org.rocksdb:rocksdbjni:$versions.rocksDB"",
   scalaLibrary: ""org.scala-lang:scala-library:$versions.scala"",
+  scalaLogging: ""com.typesafe.scala-logging:scala-logging_$versions.baseScala:$versions.scalaLogging"",
   scalaReflect: ""org.scala-lang:scala-reflect:$versions.scala"",
   scalatest: ""org.scalatest:scalatest_$versions.baseScala:$versions.scalatest"",
   scoveragePlugin: ""org.scoverage:scalac-scoverage-plugin_$versions.baseScala:$versions.scoverage"",
diff --git a/settings.gradle b/settings.gradle
index e599d01215c..03136849fd5 100644
--- a/settings.gradle
+++ b/settings.gradle
@@ -13,5 +13,7 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
-include 'core', 'examples', 'clients', 'tools', 'streams', 'streams:test-utils', 'streams:examples', 'log4j-appender',
+include 'core', 'examples', 'clients', 'tools', 'streams', 'streams:test-utils', 'streams:examples',
+        'streams:upgrade-system-tests-0100', 'streams:upgrade-system-tests-0101', 'streams:upgrade-system-tests-0102',
+        'streams:upgrade-system-tests-0110', 'streams:upgrade-system-tests-10', 'log4j-appender',
         'connect:api', 'connect:transforms', 'connect:runtime', 'connect:json', 'connect:file', 'jmh-benchmarks'
diff --git [file java] [file java]
index ec0a1a8385b..c1c50a64cbe 100644
--- [file java]
+++ [file java]
@@ -167,6 +167,11 @@
      */
     public static final String ADMIN_CLIENT_PREFIX = ""admin."";
+    /**
+     * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 0.10.0.x}.
+     */
+    public static final String UPGRADE_FROM_0100 = ""0.10.0"";
+
     /**
      * Config value for parameter {@link #PROCESSING_GUARANTEE_CONFIG ""processing.guarantee""} for at-least-once processing guarantees.
      */
@@ -326,6 +331,11 @@
     public static final String TIMESTAMP_EXTRACTOR_CLASS_CONFIG = ""timestamp.extractor"";
     private static final String TIMESTAMP_EXTRACTOR_CLASS_DOC = ""Timestamp extractor class that implements the <code>org.apache.kafka.streams.processor.TimestampExtractor</code> interface. This config is deprecated, use <code>"" + DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG + ""</code> instead"";
+    /** {@code upgrade.from} */
+    public static final String UPGRADE_FROM_CONFIG = ""upgrade.from"";
+    public static final String UPGRADE_FROM_DOC = ""Allows upgrading from version 0.10.0 to version 0.10.1 (or newer) in a backward compatible way. "" +
+        ""Default is null. Accepted values are \"""" + UPGRADE_FROM_0100 + ""\"" (for upgrading from 0.10.0.x)."";
+
     /**
      * {@code value.serde}
      * @deprecated Use {@link #DEFAULT_VALUE_SERDE_CLASS_CONFIG} instead.
@@ -548,6 +558,12 @@
                     10 * 60 * 1000,
                     Importance.LOW,
                     STATE_CLEANUP_DELAY_MS_DOC)
+            .define(UPGRADE_FROM_CONFIG,
+                    ConfigDef.Type.STRING,
+                    null,
+                    in(null, UPGRADE_FROM_0100),
+                    Importance.LOW,
+                    UPGRADE_FROM_DOC)
             .define(WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG,
                     Type.LONG,
                     24 * 60 * 60 * 1000,
@@ -779,6 +795,7 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map<String, Obje
         consumerProps.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId + ""-consumer"");
         // add configs required for stream partition assignor
+        consumerProps.put(UPGRADE_FROM_CONFIG, getString(UPGRADE_FROM_CONFIG));
         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));
         consumerProps.put(APPLICATION_SERVER_CONFIG, getString(APPLICATION_SERVER_CONFIG));
         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));
diff --git [file java] [file java]
index 2a08308a2fd..1bd03768153 100644
--- [file java]
+++ [file java]
@@ -174,6 +174,8 @@ public int compare(TopicPartition p1, TopicPartition p2) {
     private TaskManager taskManager;
     private PartitionGrouper partitionGrouper;
+    private int userMetadataVersion = SubscriptionInfo.CURRENT_VERSION;
+
     private InternalTopicManager internalTopicManager;
     private CopartitionedTopicsValidator copartitionedTopicsValidator;
@@ -192,6 +194,12 @@ public void configure(final Map<String, ?> configs) {
         final LogContext logContext = new LogContext(logPrefix);
         log = logContext.logger(getClass());
+        final String upgradeMode = (String) configs.get(StreamsConfig.UPGRADE_FROM_CONFIG);
+        if (StreamsConfig.UPGRADE_FROM_0100.equals(upgradeMode)) {
+            log.info(""Downgrading metadata version from 2 to 1 for upgrade from 0.10.0.x."");
+            userMetadataVersion = 1;
+        }
+
         final Object o = configs.get(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR);
         if (o == null) {
             KafkaException ex = new KafkaException(""TaskManager is not specified"");
@@ -249,7 +257,7 @@ public Subscription subscription(Set<String> topics) {
         final Set<TaskId> previousActiveTasks = taskManager.prevActiveTaskIds();
         final Set<TaskId> standbyTasks = taskManager.cachedTasksIds();
         standbyTasks.removeAll(previousActiveTasks);
-        final SubscriptionInfo data = new SubscriptionInfo(taskManager.processId(), previousActiveTasks, standbyTasks, this.userEndPoint);
+        final SubscriptionInfo data = new SubscriptionInfo(userMetadataVersion, taskManager.processId(), previousActiveTasks, standbyTasks, this.userEndPoint);
         taskManager.updateSubscriptionsFromMetadata(topics);
@@ -281,11 +289,16 @@ public Subscription subscription(Set<String> topics) {
         // construct the client metadata from the decoded subscription info
         Map<UUID, ClientMetadata> clientsMetadata = new HashMap<>();
+        int minUserMetadataVersion = SubscriptionInfo.CURRENT_VERSION;
         for (Map.Entry<String, Subscription> entry : subscriptions.entrySet()) {
             String consumerId = entry.getKey();
             Subscription subscription = entry.getValue();
             SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());
+            final int usedVersion = info.version;
+            if (usedVersion < minUserMetadataVersion) {
+                minUserMetadataVersion = usedVersion;
+            }
             // create the new client metadata if necessary
             ClientMetadata clientMetadata = clientsMetadata.get(info.processId);
@@ -546,7 +559,7 @@ public Subscription subscription(Set<String> topics) {
                 }
                 // finally, encode the assignment before sending back to coordinator
-                assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(active, standby, partitionsByHostState).encode()));
+                assignment.put(consumer, new Assignment(activePartitions, new AssignmentInfo(minUserMetadataVersion, active, standby, partitionsByHostState).encode()));
             }
         }
diff --git [file java] [file java]
index 8607472c281..42abb4cac0b 100644
--- [file java]
+++ [file java]
@@ -55,7 +55,7 @@ public AssignmentInfo(List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>>
         this(CURRENT_VERSION, activeTasks, standbyTasks, hostState);
     }
-    protected AssignmentInfo(int version, List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>> standbyTasks,
+    public AssignmentInfo(int version, List<TaskId> activeTasks, Map<TaskId, Set<TopicPartition>> standbyTasks,
                              Map<HostInfo, Set<TopicPartition>> hostState) {
         this.version = version;
         this.activeTasks = activeTasks;
@@ -153,8 +153,7 @@ public static AssignmentInfo decode(ByteBuffer data) {
                 }
             }
-            return new AssignmentInfo(activeTasks, standbyTasks, hostStateToTopicPartitions);
-
+            return new AssignmentInfo(version, activeTasks, standbyTasks, hostStateToTopicPartitions);
         } catch (IOException ex) {
             throw new TaskAssignmentException(""Failed to decode AssignmentInfo"", ex);
         }
diff --git [file java] [file java]
index f583dbafc94..00227e799b8 100644
--- [file java]
+++ [file java]
@@ -31,7 +31,7 @@
     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);
-    private static final int CURRENT_VERSION = 2;
+    public static final int CURRENT_VERSION = 2;
     public final int version;
     public final UUID processId;
@@ -43,7 +43,7 @@ public SubscriptionInfo(UUID processId, Set<TaskId> prevTasks, Set<TaskId> stand
         this(CURRENT_VERSION, processId, prevTasks, standbyTasks, userEndPoint);
     }
-    private SubscriptionInfo(int version, UUID processId, Set<TaskId> prevTasks, Set<TaskId> standbyTasks, String userEndPoint) {
+    public SubscriptionInfo(int version, UUID processId, Set<TaskId> prevTasks, Set<TaskId> standbyTasks, String userEndPoint) {
         this.version = version;
         this.processId = processId;
         this.prevTasks = prevTasks;
diff --git [file java] [file java]
index cc072d5508d..a873e4c4920 100644
--- [file java]
+++ [file java]
@@ -472,6 +472,7 @@ public void shouldNotOverrideUserConfigCommitIntervalMsIfExactlyOnceEnabled() {
         assertThat(streamsConfig.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG), equalTo(commitIntervalMs));
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldBeBackwardsCompatibleWithDeprecatedConfigs() {
         final Properties props = minimalStreamsConfig();
@@ -506,6 +507,7 @@ public void shouldUseCorrectDefaultsWhenNoneSpecified() {
         assertTrue(config.defaultTimestampExtractor() instanceof FailOnInvalidTimestamp);
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldSpecifyCorrectKeySerdeClassOnErrorUsingDeprecatedConfigs() {
         final Properties props = minimalStreamsConfig();
@@ -519,6 +521,7 @@ public void shouldSpecifyCorrectKeySerdeClassOnErrorUsingDeprecatedConfigs() {
         }
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldSpecifyCorrectKeySerdeClassOnError() {
         final Properties props = minimalStreamsConfig();
@@ -532,6 +535,7 @@ public void shouldSpecifyCorrectKeySerdeClassOnError() {
         }
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldSpecifyCorrectValueSerdeClassOnErrorUsingDeprecatedConfigs() {
         final Properties props = minimalStreamsConfig();
@@ -545,6 +549,7 @@ public void shouldSpecifyCorrectValueSerdeClassOnErrorUsingDeprecatedConfigs() {
         }
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldSpecifyCorrectValueSerdeClassOnError() {
         final Properties props = minimalStreamsConfig();
@@ -567,9 +572,7 @@ public void configure(final Map configs, final boolean isKey) {
         }
         @Override
-        public void close() {
-
-        }
+        public void close() {}
         @Override
         public Serializer serializer() {
diff --git [file java] [file java]
index 4c12bb93544..44e139a28bd 100644
--- [file java]
+++ [file java]
@@ -313,6 +313,4 @@ private void startStreams() {
     }
-
-
 }
diff --git [file java] [file java]
index 02ab803735a..e4b07ba3852 100644
--- [file java]
+++ [file java]
@@ -131,7 +131,7 @@ private void configurePartitionAssignor(final Map<String, Object> props) {
     private void mockTaskManager(final Set<TaskId> prevTasks,
                                  final Set<TaskId> cachedTasks,
                                  final UUID processId,
-                                 final InternalTopologyBuilder builder) throws NoSuchFieldException, IllegalAccessException {
+                                 final InternalTopologyBuilder builder) {
         EasyMock.expect(taskManager.builder()).andReturn(builder).anyTimes();
         EasyMock.expect(taskManager.prevActiveTaskIds()).andReturn(prevTasks).anyTimes();
         EasyMock.expect(taskManager.cachedTasksIds()).andReturn(cachedTasks).anyTimes();
@@ -167,7 +167,7 @@ public void shouldInterleaveTasksByGroupId() {
     }
     @Test
-    public void testSubscription() throws Exception {
+    public void testSubscription() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
@@ -195,7 +195,7 @@ public void testSubscription() throws Exception {
     }
     @Test
-    public void testAssignBasic() throws Exception {
+    public void testAssignBasic() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
@@ -235,11 +235,9 @@ public void testAssignBasic() throws Exception {
         // check assignment info
-        Set<TaskId> allActiveTasks = new HashSet<>();
-
         // the first consumer
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
+        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks);
         // the second consumer
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(""consumer11""));
@@ -259,7 +257,7 @@ public void testAssignBasic() throws Exception {
     }
     @Test
-    public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() throws Exception {
+    public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"");
@@ -327,7 +325,7 @@ public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() throws E
     }
     @Test
-    public void testAssignWithPartialTopology() throws Exception {
+    public void testAssignWithPartialTopology() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addProcessor(""processor1"", new MockProcessorSupplier(), ""source1"");
         builder.addStateStore(new MockStateStoreSupplier(""store1"", false), ""processor1"");
@@ -352,9 +350,8 @@ public void testAssignWithPartialTopology() throws Exception {
         Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
         // check assignment info
-        Set<TaskId> allActiveTasks = new HashSet<>();
         AssignmentInfo info10 = checkAssignment(Utils.mkSet(""topic1""), assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
+        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks);
         assertEquals(3, allActiveTasks.size());
         assertEquals(allTasks, new HashSet<>(allActiveTasks));
@@ -362,7 +359,7 @@ public void testAssignWithPartialTopology() throws Exception {
     @Test
-    public void testAssignEmptyMetadata() throws Exception {
+    public void testAssignEmptyMetadata() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
@@ -392,9 +389,8 @@ public void testAssignEmptyMetadata() throws Exception {
             new HashSet<>(assignments.get(""consumer10"").partitions()));
         // check assignment info
-        Set<TaskId> allActiveTasks = new HashSet<>();
         AssignmentInfo info10 = checkAssignment(Collections.<String>emptySet(), assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
+        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks);
         assertEquals(0, allActiveTasks.size());
         assertEquals(Collections.<TaskId>emptySet(), new HashSet<>(allActiveTasks));
@@ -417,7 +413,7 @@ public void testAssignEmptyMetadata() throws Exception {
     }
     @Test
-    public void testAssignWithNewTasks() throws Exception {
+    public void testAssignWithNewTasks() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addSource(null, ""source3"", null, null, null, ""topic3"");
@@ -450,13 +446,9 @@ public void testAssignWithNewTasks() throws Exception {
         // check assigned partitions: since there is no previous task for topic 3 it will be assigned randomly so we cannot check exact match
         // also note that previously assigned partitions / tasks may not stay on the previous host since we may assign the new task first and
         // then later ones will be re-assigned to other hosts due to load balancing
-        Set<TaskId> allActiveTasks = new HashSet<>();
-        Set<TopicPartition> allPartitions = new HashSet<>();
-        AssignmentInfo info;
-
-        info = AssignmentInfo.decode(assignments.get(""consumer10"").userData());
-        allActiveTasks.addAll(info.activeTasks);
-        allPartitions.addAll(assignments.get(""consumer10"").partitions());
+        AssignmentInfo info = AssignmentInfo.decode(assignments.get(""consumer10"").userData());
+        Set<TaskId> allActiveTasks = new HashSet<>(info.activeTasks);
+        Set<TopicPartition> allPartitions = new HashSet<>(assignments.get(""consumer10"").partitions());
         info = AssignmentInfo.decode(assignments.get(""consumer11"").userData());
         allActiveTasks.addAll(info.activeTasks);
@@ -471,7 +463,7 @@ public void testAssignWithNewTasks() throws Exception {
     }
     @Test
-    public void testAssignWithStates() throws Exception {
+    public void testAssignWithStates() {
         builder.setApplicationId(applicationId);
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
@@ -542,7 +534,10 @@ public void testAssignWithStates() throws Exception {
         assertEquals(Utils.mkSet(task10, task11, task12), tasksForState(applicationId, ""store3"", tasks, topicGroups));
     }
-    private Set<TaskId> tasksForState(String applicationId, String storeName, List<TaskId> tasks, Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups) {
+    private Set<TaskId> tasksForState(final String applicationId,
+                                      final String storeName,
+                                      final List<TaskId> tasks,
+                                      final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups) {
         final String changelogTopic = ProcessorStateManager.storeChangelogTopic(applicationId, storeName);
         Set<TaskId> ids = new HashSet<>();
@@ -560,7 +555,7 @@ public void testAssignWithStates() throws Exception {
     }
     @Test
-    public void testAssignWithStandbyReplicas() throws Exception {
+    public void testAssignWithStandbyReplicas() {
         Map<String, Object> props = configProps();
         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, ""1"");
         StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -598,13 +593,10 @@ public void testAssignWithStandbyReplicas() throws Exception {
         Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
-        Set<TaskId> allActiveTasks = new HashSet<>();
-        Set<TaskId> allStandbyTasks = new HashSet<>();
-
         // the first consumer
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks);
-        allStandbyTasks.addAll(info10.standbyTasks.keySet());
+        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks);
+        Set<TaskId> allStandbyTasks = new HashSet<>(info10.standbyTasks.keySet());
         // the second consumer
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(""consumer11""));
@@ -632,7 +624,7 @@ public void testAssignWithStandbyReplicas() throws Exception {
     }
     @Test
-    public void testOnAssignment() throws Exception {
+    public void testOnAssignment() {
         configurePartitionAssignor(Collections.<String, Object>emptyMap());
         final List<TaskId> activeTaskList = Utils.mkList(task0, task3);
@@ -667,7 +659,7 @@ public void testOnAssignment() throws Exception {
     }
     @Test
-    public void testAssignWithInternalTopics() throws Exception {
+    public void testAssignWithInternalTopics() {
         builder.setApplicationId(applicationId);
         builder.addInternalTopic(""topicX"");
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
@@ -697,7 +689,7 @@ public void testAssignWithInternalTopics() throws Exception {
     }
     @Test
-    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throws Exception {
+    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() {
         String applicationId = ""test"";
         builder.setApplicationId(applicationId);
         builder.addInternalTopic(""topicX"");
@@ -732,7 +724,7 @@ public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throw
     }
     @Test
-    public void shouldGenerateTasksForAllCreatedPartitions() throws Exception {
+    public void shouldGenerateTasksForAllCreatedPartitions() {
         final StreamsBuilder builder = new StreamsBuilder();
         final InternalTopologyBuilder internalTopologyBuilder = StreamsBuilderTest.internalTopologyBuilder(builder);
@@ -832,7 +824,7 @@ public Object apply(final Object value1, final Object value2) {
     }
     @Test
-    public void shouldAddUserDefinedEndPointToSubscription() throws Exception {
+    public void shouldAddUserDefinedEndPointToSubscription() {
         builder.setApplicationId(applicationId);
         builder.addSource(null, ""source"", null, null, null, ""input"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source"");
@@ -851,7 +843,56 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {
     }
     @Test
-    public void shouldMapUserEndPointToTopicPartitions() throws Exception {
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() {
+        final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
+        final Set<TaskId> emptyTasks = Collections.emptySet();
+        subscriptions.put(
+            ""consumer1"",
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(""topic1""),
+                new SubscriptionInfo(1, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+        subscriptions.put(
+            ""consumer2"",
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(""topic1""),
+                new SubscriptionInfo(2, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+
+        mockTaskManager(
+            emptyTasks,
+            emptyTasks,
+            UUID.randomUUID(),
+            builder);
+        configurePartitionAssignor(Collections.<String, Object>emptyMap());
+
+        final Map<String, PartitionAssignor.Assignment> assignment = partitionAssignor.assign(metadata, subscriptions);
+
+        assertEquals(2, assignment.size());
+        assertEquals(1, AssignmentInfo.decode(assignment.get(""consumer1"").userData()).version);
+        assertEquals(1, AssignmentInfo.decode(assignment.get(""consumer2"").userData()).version);
+    }
+
+    @Test
+    public void shouldDownGradeSubscription() {
+        final Set<TaskId> emptyTasks = Collections.emptySet();
+
+        mockTaskManager(
+            emptyTasks,
+            emptyTasks,
+            UUID.randomUUID(),
+            builder);
+        configurePartitionAssignor(Collections.singletonMap(StreamsConfig.UPGRADE_FROM_CONFIG, (Object) StreamsConfig.UPGRADE_FROM_0100));
+
+        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""topic1""));
+
+        assertEquals(1, SubscriptionInfo.decode(subscription.userData()).version);
+    }
+
+    @Test
+    public void shouldMapUserEndPointToTopicPartitions() {
         builder.setApplicationId(applicationId);
         builder.addSource(null, ""source"", null, null, null, ""topic1"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source"");
@@ -881,7 +922,7 @@ public void shouldMapUserEndPointToTopicPartitions() throws Exception {
     }
     @Test
-    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() throws Exception {
+    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() {
         builder.setApplicationId(applicationId);
         mockTaskManager(Collections.<TaskId>emptySet(), Collections.<TaskId>emptySet(), UUID.randomUUID(), builder);
@@ -908,7 +949,7 @@ public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() {
     }
     @Test
-    public void shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks() throws Exception {
+    public void shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks() {
         final StreamsBuilder builder = new StreamsBuilder();
         final InternalTopologyBuilder internalTopologyBuilder = StreamsBuilderTest.internalTopologyBuilder(builder);
@@ -1010,7 +1051,7 @@ public Object apply(final Object value1, final Object value2) {
     }
     @Test
-    public void shouldUpdateClusterMetadataAndHostInfoOnAssignment() throws Exception {
+    public void shouldUpdateClusterMetadataAndHostInfoOnAssignment() {
         final TopicPartition partitionOne = new TopicPartition(""topic"", 1);
         final TopicPartition partitionTwo = new TopicPartition(""topic"", 2);
         final Map<HostInfo, Set<TopicPartition>> hostState = Collections.singletonMap(
@@ -1028,7 +1069,7 @@ public void shouldUpdateClusterMetadataAndHostInfoOnAssignment() throws Exceptio
     }
     @Test
-    public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() throws Exception {
+    public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() {
         final StreamsBuilder builder = new StreamsBuilder();
         final InternalTopologyBuilder internalTopologyBuilder = StreamsBuilderTest.internalTopologyBuilder(builder);
diff --git [file java] [file java]
index ec94ad81acd..8032f7da865 100644
--- [file java]
+++ [file java]
@@ -64,10 +64,9 @@ public void shouldDecodePreviousVersion() throws IOException {
         assertEquals(oldVersion.activeTasks, decoded.activeTasks);
         assertEquals(oldVersion.standbyTasks, decoded.standbyTasks);
         assertEquals(0, decoded.partitionsByHost.size()); // should be empty as wasn't in V1
-        assertEquals(2, decoded.version); // automatically upgraded to v2 on decode;
+        assertEquals(1, decoded.version);
     }
-
     /**
      * This is a clone of what the V1 encoding did. The encode method has changed for V2
      * so it is impossible to test compatibility without having this
diff --git [file java] [file java]
index 727c4217595..9304cec5986 100644
--- [file java]
+++ [file java]
@@ -19,6 +19,7 @@
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.kafka.clients.producer.ProducerConfig;
 import org.apache.kafka.common.serialization.Serdes;
+import org.apache.kafka.common.utils.Bytes;
 import org.apache.kafka.streams.Consumed;
 import org.apache.kafka.streams.KafkaStreams;
 import org.apache.kafka.streams.StreamsBuilder;
@@ -30,10 +31,13 @@
 import org.apache.kafka.streams.kstream.KTable;
 import org.apache.kafka.streams.kstream.Materialized;
 import org.apache.kafka.streams.kstream.Predicate;
+import org.apache.kafka.streams.kstream.Produced;
 import org.apache.kafka.streams.kstream.Serialized;
 import org.apache.kafka.streams.kstream.TimeWindows;
 import org.apache.kafka.streams.kstream.ValueJoiner;
+import org.apache.kafka.streams.state.KeyValueStore;
 import org.apache.kafka.streams.state.Stores;
+import org.apache.kafka.streams.state.WindowStore;
 import java.io.File;
 import java.util.Properties;
@@ -47,7 +51,7 @@
     private Thread thread;
     private boolean uncaughtException = false;
-    public SmokeTestClient(File stateDir, String kafka) {
+    public SmokeTestClient(final File stateDir, final String kafka) {
         super();
         this.stateDir = stateDir;
         this.kafka = kafka;
@@ -57,7 +61,7 @@ public void start() {
         streams = createKafkaStreams(stateDir, kafka);
         streams.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
             @Override
-            public void uncaughtException(Thread t, Throwable e) {
+            public void uncaughtException(final Thread t, final Throwable e) {
                 System.out.println(""SMOKE-TEST-CLIENT-EXCEPTION"");
                 uncaughtException = true;
                 e.printStackTrace();
@@ -94,7 +98,7 @@ public void close() {
         }
     }
-    private static KafkaStreams createKafkaStreams(File stateDir, String kafka) {
+    private static Properties getStreamsConfig(final File stateDir, final String kafka) {
         final Properties props = new Properties();
         props.put(StreamsConfig.APPLICATION_ID_CONFIG, ""SmokeTest"");
         props.put(StreamsConfig.STATE_DIR_CONFIG, stateDir.toString());
@@ -109,25 +113,29 @@ private static KafkaStreams createKafkaStreams(File stateDir, String kafka) {
         props.put(ProducerConfig.ACKS_CONFIG, ""all"");
         //TODO remove this config or set to smaller value when KIP-91 is merged
         props.put(StreamsConfig.producerPrefix(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG), 80000);
+        return props;
+    }
-        StreamsBuilder builder = new StreamsBuilder();
-        Consumed<String, Integer> stringIntConsumed = Consumed.with(stringSerde, intSerde);
-        KStream<String, Integer> source = builder.stream(""data"", stringIntConsumed);
-        source.to(stringSerde, intSerde, ""echo"");
-        KStream<String, Integer> data = source.filter(new Predicate<String, Integer>() {
+    private static KafkaStreams createKafkaStreams(final File stateDir, final String kafka) {
+        final StreamsBuilder builder = new StreamsBuilder();
+        final Consumed<String, Integer> stringIntConsumed = Consumed.with(stringSerde, intSerde);
+        final KStream<String, Integer> source = builder.stream(""data"", stringIntConsumed);
+        source.to(""echo"", Produced.with(stringSerde, intSerde));
+        final KStream<String, Integer> data = source.filter(new Predicate<String, Integer>() {
             @Override
-            public boolean test(String key, Integer value) {
+            public boolean test(final String key, final Integer value) {
                 return value == null || value != END;
             }
         });
         data.process(SmokeTestUtil.printProcessorSupplier(""data""));
         // min
-        KGroupedStream<String, Integer>
-            groupedData =
+        final KGroupedStream<String, Integer> groupedData =
             data.groupByKey(Serialized.with(stringSerde, intSerde));
-        groupedData.aggregate(
+        groupedData
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(1)))
+            .aggregate(
                 new Initializer<Integer>() {
                     public Integer apply() {
                         return Integer.MAX_VALUE;
@@ -135,21 +143,24 @@ public Integer apply() {
                 },
                 new Aggregator<String, Integer, Integer>() {
                     @Override
-                    public Integer apply(String aggKey, Integer value, Integer aggregate) {
+                    public Integer apply(final String aggKey, final Integer value, final Integer aggregate) {
                         return (value < aggregate) ? value : aggregate;
                     }
                 },
-                TimeWindows.of(TimeUnit.DAYS.toMillis(1)),
-                intSerde, ""uwin-min""
-        ).toStream().map(
-                new Unwindow<String, Integer>()
-        ).to(stringSerde, intSerde, ""min"");
+                Materialized.<String, Integer, WindowStore<Bytes, byte>>as(""uwin-min"").withValueSerde(intSerde))
+            .toStream(new Unwindow<String, Integer>())
+            .to(""min"", Produced.with(stringSerde, intSerde));
-        KTable<String, Integer> minTable = builder.table(""min"", stringIntConsumed);
+        final KTable<String, Integer> minTable = builder.table(
+            ""min"",
+            Consumed.with(stringSerde, intSerde),
+            Materialized.<String, Integer, KeyValueStore<Bytes, byte>>as(""minStoreName""));
         minTable.toStream().process(SmokeTestUtil.printProcessorSupplier(""min""));
         // max
-        groupedData.aggregate(
+        groupedData
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))
+            .aggregate(
                 new Initializer<Integer>() {
                     public Integer apply() {
                         return Integer.MIN_VALUE;
@@ -157,21 +168,24 @@ public Integer apply() {
                 },
                 new Aggregator<String, Integer, Integer>() {
                     @Override
-                    public Integer apply(String aggKey, Integer value, Integer aggregate) {
+                    public Integer apply(final String aggKey, final Integer value, final Integer aggregate) {
                         return (value > aggregate) ? value : aggregate;
                     }
                 },
-                TimeWindows.of(TimeUnit.DAYS.toMillis(2)),
-                intSerde, ""uwin-max""
-        ).toStream().map(
-                new Unwindow<String, Integer>()
-        ).to(stringSerde, intSerde, ""max"");
+                Materialized.<String, Integer, WindowStore<Bytes, byte>>as(""uwin-max"").withValueSerde(intSerde))
+            .toStream(new Unwindow<String, Integer>())
+            .to(""max"", Produced.with(stringSerde, intSerde));
-        KTable<String, Integer> maxTable = builder.table(""max"", stringIntConsumed);
+        final KTable<String, Integer> maxTable = builder.table(
+            ""max"",
+            Consumed.with(stringSerde, intSerde),
+            Materialized.<String, Integer, KeyValueStore<Bytes, byte>>as(""maxStoreName""));
         maxTable.toStream().process(SmokeTestUtil.printProcessorSupplier(""max""));
         // sum
-        groupedData.aggregate(
+        groupedData
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))
+            .aggregate(
                 new Initializer<Long>() {
                     public Long apply() {
                         return 0L;
@@ -179,70 +193,74 @@ public Long apply() {
                 },
                 new Aggregator<String, Integer, Long>() {
                     @Override
-                    public Long apply(String aggKey, Integer value, Long aggregate) {
+                    public Long apply(final String aggKey, final Integer value, final Long aggregate) {
                         return (long) value + aggregate;
                     }
                 },
-                TimeWindows.of(TimeUnit.DAYS.toMillis(2)),
-                longSerde, ""win-sum""
-        ).toStream().map(
-                new Unwindow<String, Long>()
-        ).to(stringSerde, longSerde, ""sum"");
-
-        Consumed<String, Long> stringLongConsumed = Consumed.with(stringSerde, longSerde);
-        KTable<String, Long> sumTable = builder.table(""sum"", stringLongConsumed);
+                Materialized.<String, Long, WindowStore<Bytes, byte>>as(""win-sum"").withValueSerde(longSerde))
+            .toStream(new Unwindow<String, Long>())
+            .to(""sum"", Produced.with(stringSerde, longSerde));
+
+        final Consumed<String, Long> stringLongConsumed = Consumed.with(stringSerde, longSerde);
+        final KTable<String, Long> sumTable = builder.table(""sum"", stringLongConsumed);
         sumTable.toStream().process(SmokeTestUtil.printProcessorSupplier(""sum""));
+
         // cnt
-        groupedData.count(TimeWindows.of(TimeUnit.DAYS.toMillis(2)), ""uwin-cnt"")
-            .toStream().map(
-                new Unwindow<String, Long>()
-        ).to(stringSerde, longSerde, ""cnt"");
+        groupedData
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))
+            .count(Materialized.<String, Long, WindowStore<Bytes, byte>>as(""uwin-cnt""))
+            .toStream(new Unwindow<String, Long>())
+            .to(""cnt"", Produced.with(stringSerde, longSerde));
-        KTable<String, Long> cntTable = builder.table(""cnt"", stringLongConsumed);
+        final KTable<String, Long> cntTable = builder.table(
+            ""cnt"",
+            Consumed.with(stringSerde, longSerde),
+            Materialized.<String, Long, KeyValueStore<Bytes, byte>>as(""cntStoreName""));
         cntTable.toStream().process(SmokeTestUtil.printProcessorSupplier(""cnt""));
         // dif
-        maxTable.join(minTable,
+        maxTable
+            .join(
+                minTable,
                 new ValueJoiner<Integer, Integer, Integer>() {
-                    public Integer apply(Integer value1, Integer value2) {
+                    public Integer apply(final Integer value1, final Integer value2) {
                         return value1 - value2;
                     }
-                }
-        ).to(stringSerde, intSerde, ""dif"");
+                })
+            .toStream()
+            .to(""dif"", Produced.with(stringSerde, intSerde));
         // avg
-        sumTable.join(
+        sumTable
+            .join(
                 cntTable,
                 new ValueJoiner<Long, Long, Double>() {
-                    public Double apply(Long value1, Long value2) {
+                    public Double apply(final Long value1, final Long value2) {
                         return (double) value1 / (double) value2;
                     }
-                }
-        ).to(stringSerde, doubleSerde, ""avg"");
+                })
+            .toStream()
+            .to(""avg"", Produced.with(stringSerde, doubleSerde));
         // test repartition
-        Agg agg = new Agg();
-        cntTable.groupBy(agg.selector(),
-                         Serialized.with(stringSerde, longSerde)
-        ).aggregate(agg.init(),
-                    agg.adder(),
-                    agg.remover(),
-                    Materialized.<String, Long>as(Stores.inMemoryKeyValueStore(""cntByCnt""))
-                            .withKeySerde(Serdes.String())
-                            .withValueSerde(Serdes.Long())
-        ).to(stringSerde, longSerde, ""tagg"");
-
-        final KafkaStreams streamsClient = new KafkaStreams(builder.build(), props);
+        final Agg agg = new Agg();
+        cntTable.groupBy(agg.selector(), Serialized.with(stringSerde, longSerde))
+            .aggregate(agg.init(), agg.adder(), agg.remover(),
+                Materialized.<String, Long>as(Stores.inMemoryKeyValueStore(""cntByCnt""))
+                    .withKeySerde(Serdes.String())
+                    .withValueSerde(Serdes.Long()))
+            .toStream()
+            .to(""tagg"", Produced.with(stringSerde, longSerde));
+
+        final KafkaStreams streamsClient = new KafkaStreams(builder.build(), getStreamsConfig(stateDir, kafka));
         streamsClient.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
             @Override
-            public void uncaughtException(Thread t, Throwable e) {
+            public void uncaughtException(final Thread t, final Throwable e) {
                 System.out.println(""FATAL: An unexpected exception is encountered on thread "" + t + "": "" + e);
-                
                 streamsClient.close(30, TimeUnit.SECONDS);
             }
         });
         return streamsClient;
     }
-
 }
diff --git [file java] [file java]
index 882e9c0a585..cba5fba6bd8 100644
--- [file java]
+++ [file java]
@@ -130,53 +130,65 @@ public void run() {
         System.out.println(""shutdown"");
     }
-    public static Map<String, Set<Integer>> generate(String kafka, final int numKeys, final int maxRecordsPerKey) {
+    public static Map<String, Set<Integer>> generate(final String kafka,
+                                                     final int numKeys,
+                                                     final int maxRecordsPerKey) {
+        return generate(kafka, numKeys, maxRecordsPerKey, true);
+    }
+
+    public static Map<String, Set<Integer>> generate(final String kafka,
+                                                     final int numKeys,
+                                                     final int maxRecordsPerKey,
+                                                     final boolean autoTerminate) {
         final Properties producerProps = new Properties();
         producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, ""SmokeTest"");
         producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
         producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
-        // the next 4 config values make sure that all records are produced with no loss and
-        // no duplicates
+        // the next 2 config values make sure that all records are produced with no loss and no duplicates
         producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);
         producerProps.put(ProducerConfig.ACKS_CONFIG, ""all"");
         producerProps.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 80000);
-        KafkaProducer<byte, byte> producer = new KafkaProducer<>(producerProps);
+        final KafkaProducer<byte, byte> producer = new KafkaProducer<>(producerProps);
         int numRecordsProduced = 0;
-        Map<String, Set<Integer>> allData = new HashMap<>();
-        ValueList data = new ValueList;
+        final Map<String, Set<Integer>> allData = new HashMap<>();
+        final ValueList data = new ValueList;
         for (int i = 0; i < numKeys; i++) {
             data = new ValueList(i, i + maxRecordsPerKey - 1);
             allData.put(data.key, new HashSet<Integer>());
         }
-        Random rand = new Random();
+        final Random rand = new Random();
-        int remaining = data.length;
+        int remaining = 1; // dummy value must be positive if <autoTerminate> is false
+        if (autoTerminate) {
+            remaining = data.length;
+        }
         List<ProducerRecord<byte, byte>> needRetry = new ArrayList<>();
         while (remaining > 0) {
-            int index = rand.nextInt(remaining);
-            String key = data.key;
+            final int index = autoTerminate ? rand.nextInt(remaining) : rand.nextInt(numKeys);
+            final String key = data.key;
             int value = data.next();
-            if (value < 0) {
+            if (autoTerminate && value < 0) {
                 remaining--;
                 data = data;
             } else {
-                ProducerRecord<byte, byte> record =
-                        new ProducerRecord<>(""data"", stringSerde.serializer().serialize("""", key), intSerde.serializer().serialize("""", value));
+                final ProducerRecord<byte, byte> record =
+                    new ProducerRecord<>(""data"", stringSerde.serializer().serialize("""", key), intSerde.serializer().serialize("""", value));
                 producer.send(record, new TestCallback(record, needRetry));
                 numRecordsProduced++;
                 allData.get(key).add(value);
-                if (numRecordsProduced % 100 == 0)
+                if (numRecordsProduced % 100 == 0) {
                     System.out.println(numRecordsProduced + "" records produced"");
+                }
                 Utils.sleep(2);
             }
         }
diff --git [file java] [file java]
index dc4c91b4097..87ca82918a9 100644
--- [file java]
+++ [file java]
@@ -44,20 +44,15 @@
             public Processor<Object, Object> get() {
                 return new AbstractProcessor<Object, Object>() {
                     private int numRecordsProcessed = 0;
-                    private ProcessorContext context;
                     @Override
                     public void init(final ProcessorContext context) {
                         System.out.println(""initializing processor: topic="" + topic + "" taskId="" + context.taskId());
                         numRecordsProcessed = 0;
-                        this.context = context;
                     }
                     @Override
                     public void process(final Object key, final Object value) {
-                        if (printOffset) {
-                            System.out.println("">>> "" + context.offset());
-                        }
                         numRecordsProcessed++;
                         if (numRecordsProcessed % 100 == 0) {
                             System.out.println(System.currentTimeMillis());
@@ -66,19 +61,19 @@ public void process(final Object key, final Object value) {
                     }
                     @Override
-                    public void punctuate(final long timestamp) { }
+                    public void punctuate(final long timestamp) {}
                     @Override
-                    public void close() { }
+                    public void close() {}
                 };
             }
         };
     }
-    public static final class Unwindow<K, V> implements KeyValueMapper<Windowed<K>, V, KeyValue<K, V>> {
+    public static final class Unwindow<K, V> implements KeyValueMapper<Windowed<K>, V, K> {
         @Override
-        public KeyValue<K, V> apply(final Windowed<K> winKey, final V value) {
-            return new KeyValue<>(winKey.key(), value);
+        public K apply(final Windowed<K> winKey, final V value) {
+            return winKey.key();
         }
     }
diff --git [file java] [file java]
index 64597bdcda6..c0e345ff2b6 100644
--- [file java]
+++ [file java]
@@ -23,7 +23,7 @@
 public class StreamsSmokeTest {
     /**
-     *  args ::= command kafka zookeeper stateDir
+     *  args ::= command kafka zookeeper stateDir disableAutoTerminate
      *  command := ""run"" | ""process""
      *
      * @param args
@@ -32,11 +32,13 @@ public static void main(String args) throws InterruptedException {
         String kafka = args;
         String stateDir = args.length > 1 ? args : null;
         String command = args.length > 2 ? args : null;
+        boolean disableAutoTerminate = args.length > 3;
-        System.out.println(""StreamsTest instance started"");
+        System.out.println(""StreamsTest instance started (StreamsSmokeTest)"");
         System.out.println(""command="" + command);
         System.out.println(""kafka="" + kafka);
         System.out.println(""stateDir="" + stateDir);
+        System.out.println(""disableAutoTerminate="" + disableAutoTerminate);
         switch (command) {
             case ""standalone"":
@@ -46,8 +48,12 @@ public static void main(String args) throws InterruptedException {
                 // this starts the driver (data generation and result verification)
                 final int numKeys = 10;
                 final int maxRecordsPerKey = 500;
-                Map<String, Set<Integer>> allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
-                SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                if (disableAutoTerminate) {
+                    SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey, false);
+                } else {
+                    Map<String, Set<Integer>> allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
+                    SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                }
                 break;
             case ""process"":
                 // this starts a KafkaStreams client
diff --git [file java] [file java]
new file mode 100644
index 00000000000..5486374b62c
--- /dev/null
+++ [file java]
@@ -0,0 +1,72 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsBuilder;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 2) {
+            System.err.println(""StreamsUpgradeTest requires two argument (kafka-url, state-dir, ) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args : """"));
+        }
+        final String kafka = args;
+        final String stateDir = args;
+        final String upgradeFrom = args.length > 2 ? args : null;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest trunk)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""stateDir="" + stateDir);
+        System.out.println(""upgradeFrom="" + upgradeFrom);
+
+        final StreamsBuilder builder = new StreamsBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(SmokeTestUtil.printProcessorSupplier(""data""));
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        if (upgradeFrom != null) {
+            config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+        }
+
+
+        final KafkaStreams streams = new KafkaStreams(builder.build(), config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..72d7f5a7b04
--- /dev/null
+++ [file java]
@@ -0,0 +1,104 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 3) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args + "" "" : """")
+                + (args.length > 1 ? args : """"));
+        }
+        final String kafka = args;
+        final String zookeeper = args;
+        final String stateDir = args;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.10.0)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""zookeeper="" + zookeeper);
+        System.out.println(""stateDir="" + stateDir);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..eebd0fab83c
--- /dev/null
+++ [file java]
@@ -0,0 +1,114 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    /**
+     * This test cannot be run executed, as long as Kafka 0.10.1.2 is not released
+     */
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 3) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, state-dir, ) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args + "" "" : """")
+                + (args.length > 1 ? args : """"));
+        }
+        final String kafka = args;
+        final String zookeeper = args;
+        final String stateDir = args;
+        final String upgradeFrom = args.length > 3 ? args : null;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.10.1)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""zookeeper="" + zookeeper);
+        System.out.println(""stateDir="" + stateDir);
+        System.out.println(""upgradeFrom="" + upgradeFrom);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 0.10.1.2 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(""upgrade.from"", upgradeFrom);
+        }
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..18240f04ff1
--- /dev/null
+++ [file java]
@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    /**
+     * This test cannot be run executed, as long as Kafka 0.10.2.2 is not released
+     */
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 2) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, state-dir, ) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args : """"));
+        }
+        final String kafka = args;
+        final String stateDir = args;
+        final String upgradeFrom = args.length > 2 ? args : null;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.10.2)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""stateDir="" + stateDir);
+        System.out.println(""upgradeFrom="" + upgradeFrom);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 0.10.2.2 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(""upgrade.from"", upgradeFrom);
+        }
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..779021d8672
--- /dev/null
+++ [file java]
@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    /**
+     * This test cannot be run executed, as long as Kafka 0.11.0.3 is not released
+     */
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 2) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, state-dir, ) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args : """"));
+        }
+        final String kafka = args;
+        final String stateDir = args;
+        final String upgradeFrom = args.length > 2 ? args : null;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.11.0)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""stateDir="" + stateDir);
+        System.out.println(""upgradeFrom="" + upgradeFrom);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 0.11.0.3 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(""upgrade.from"", upgradeFrom);
+        }
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..4d008c25136
--- /dev/null
+++ [file java]
@@ -0,0 +1,108 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsBuilder;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    /**
+     * This test cannot be run executed, as long as Kafka 1.0.2 is not released
+     */
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) {
+        if (args.length < 2) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, state-dir, ) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args : """"));
+        }
+        final String kafka = args;
+        final String stateDir = args;
+        final String upgradeFrom = args.length > 2 ? args : null;
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v1.0)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""stateDir="" + stateDir);
+        System.out.println(""upgradeFrom="" + upgradeFrom);
+
+        final StreamsBuilder builder = new StreamsBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.STATE_DIR_CONFIG, stateDir);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        if (upgradeFrom != null) {
+            // TODO: because Kafka 1.0.2 is not released yet, thus `UPGRADE_FROM_CONFIG` is not available yet
+            //config.setProperty(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFrom);
+            config.setProperty(""upgrade.from"", upgradeFrom);
+        }
+
+        final KafkaStreams streams = new KafkaStreams(builder.build(), config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git a/tests/docker/Dockerfile b/tests/docker/Dockerfile
index 149e3911bc6..b6c499756a8 100644
--- a/tests/docker/Dockerfile
+++ b/tests/docker/Dockerfile
@@ -39,13 +39,15 @@ COPY ./ssh-config /root/.ssh/config
 RUN ssh-keygen -q -t rsa -N '' -f /root/.ssh/id_rsa && cp -f /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys
 # Install binary test dependencies.
+# we use the same versions as in vagrant/base.sh
 ARG KAFKA_MIRROR=""[link]""
-RUN mkdir -p ""/opt/kafka-0.8.2.2"" && curl -s ""$KAFKA_MIRROR/kafka_2.10-0.8.2.2.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.8.2.2""
+RUN mkdir -p ""/opt/kafka-0.8.2.2"" && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.8.2.2.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.8.2.2""
 RUN mkdir -p ""/opt/kafka-0.9.0.1"" && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.9.0.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.9.0.1""
 RUN mkdir -p ""/opt/kafka-0.10.0.1"" && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.10.0.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.0.1""
 RUN mkdir -p ""/opt/kafka-0.10.1.1"" && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.10.1.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.1.1""
 RUN mkdir -p ""/opt/kafka-0.10.2.1"" && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.10.2.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.2.1""
-RUN mkdir -p ""/opt/kafka-0.11.0.0"" && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.11.0.0.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.11.0.0""
+RUN mkdir -p ""/opt/kafka-0.11.0.2"" && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.11.0.2.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.11.0.2""
+RUN mkdir -p ""/opt/kafka-1.0.1"" && curl -s ""$KAFKA_MIRROR/kafka_2.11-1.0.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-1.0.1""
 # The version of Kibosh to use for testing.
 # If you update this, also update vagrant/base.sy
diff --git a/tests/kafkatest/services/streams.py b/tests/kafkatest/services/streams.py
index 54e26bfbc2b..0ed3a4290fb 100644
--- a/tests/kafkatest/services/streams.py
+++ b/tests/kafkatest/services/streams.py
@@ -20,6 +20,7 @@
 from ducktape.utils.util import wait_until
 from kafkatest.directory_layout.kafka_path import KafkaPathResolverMixin
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1
 class StreamsTestBaseService(KafkaPathResolverMixin, Service):
@@ -33,6 +34,8 @@ class StreamsTestBaseService(KafkaPathResolverMixin, Service):
     LOG4J_CONFIG_FILE = os.path.join(PERSISTENT_ROOT, ""tools-log4j.properties"")
     PID_FILE = os.path.join(PERSISTENT_ROOT, ""streams.pid"")
+    CLEAN_NODE_ENABLED = True
+
     logs = {
         ""streams_log"": {
             ""path"": LOG_FILE,
@@ -43,6 +46,114 @@ class StreamsTestBaseService(KafkaPathResolverMixin, Service):
         ""streams_stderr"": {
             ""path"": STDERR_FILE,
             ""collect_default"": True},
+        ""streams_log.0-1"": {
+            ""path"": LOG_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_stdout.0-1"": {
+            ""path"": STDOUT_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_stderr.0-1"": {
+            ""path"": STDERR_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_log.0-2"": {
+            ""path"": LOG_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_stdout.0-2"": {
+            ""path"": STDOUT_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_stderr.0-2"": {
+            ""path"": STDERR_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_log.0-3"": {
+            ""path"": LOG_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_stdout.0-3"": {
+            ""path"": STDOUT_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_stderr.0-3"": {
+            ""path"": STDERR_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_log.0-4"": {
+            ""path"": LOG_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_stdout.0-4"": {
+            ""path"": STDOUT_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_stderr.0-4"": {
+            ""path"": STDERR_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_log.0-5"": {
+            ""path"": LOG_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_stdout.0-5"": {
+            ""path"": STDOUT_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_stderr.0-5"": {
+            ""path"": STDERR_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_log.0-6"": {
+            ""path"": LOG_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_stdout.0-6"": {
+            ""path"": STDOUT_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_stderr.0-6"": {
+            ""path"": STDERR_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_log.1-1"": {
+            ""path"": LOG_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_stdout.1-1"": {
+            ""path"": STDOUT_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_stderr.1-1"": {
+            ""path"": STDERR_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_log.1-2"": {
+            ""path"": LOG_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_stdout.1-2"": {
+            ""path"": STDOUT_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_stderr.1-2"": {
+            ""path"": STDERR_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_log.1-3"": {
+            ""path"": LOG_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_stdout.1-3"": {
+            ""path"": STDOUT_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_stderr.1-3"": {
+            ""path"": STDERR_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_log.1-4"": {
+            ""path"": LOG_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_stdout.1-4"": {
+            ""path"": STDOUT_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_stderr.1-4"": {
+            ""path"": STDERR_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_log.1-5"": {
+            ""path"": LOG_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_stdout.1-5"": {
+            ""path"": STDOUT_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_stderr.1-5"": {
+            ""path"": STDERR_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_log.1-6"": {
+            ""path"": LOG_FILE + "".1-6"",
+            ""collect_default"": True},
+        ""streams_stdout.1-6"": {
+            ""path"": STDOUT_FILE + "".1-6"",
+            ""collect_default"": True},
+        ""streams_stderr.1-6"": {
+            ""path"": STDERR_FILE + "".1-6"",
+            ""collect_default"": True},
     }
     def __init__(self, test_context, kafka, streams_class_name, user_test_args, user_test_args1=None, user_test_args2=None, user_test_args3=None):
@@ -108,7 +219,8 @@ def wait_node(self, node, timeout_sec=None):
     def clean_node(self, node):
         node.account.kill_process(""streams"", clean_shutdown=False, allow_fail=True)
-        node.account.ssh(""rm -rf "" + self.PERSISTENT_ROOT, allow_fail=False)
+        if self.CLEAN_NODE_ENABLED:
+            node.account.ssh(""rm -rf "" + self.PERSISTENT_ROOT, allow_fail=False)
     def start_cmd(self, node):
         args = self.args.copy()
@@ -170,7 +282,28 @@ def clean_node(self, node):
 class StreamsSmokeTestDriverService(StreamsSmokeTestBaseService):
     def __init__(self, test_context, kafka):
         super(StreamsSmokeTestDriverService, self).__init__(test_context, kafka, ""run"")
+        self.DISABLE_AUTO_TERMINATE = """"
+
+    def disable_auto_terminate(self):
+        self.DISABLE_AUTO_TERMINATE = ""disableAutoTerminate""
+
+    def start_cmd(self, node):
+        args = self.args.copy()
+        args = self.kafka.bootstrap_servers()
+        args = self.PERSISTENT_ROOT
+        args = self.STDOUT_FILE
+        args = self.STDERR_FILE
+        args = self.PID_FILE
+        args = self.LOG4J_CONFIG_FILE
+        args = self.DISABLE_AUTO_TERMINATE
+        args = self.path.script(""kafka-run-class.sh"", node)
+        cmd = ""( export KAFKA_LOG4J_OPTS=\""-Dlog4j.configuration=file:%(log4j)s\""; "" \
+              ""INCLUDE_TEST_JARS=true %(kafka_run_class)s %(streams_class_name)s "" \
+              "" %(kafka)s %(state_dir)s %(user_test_args)s %(disable_auto_terminate)s"" \
+              "" & echo $! >&3 ) 1>> %(stdout)s 2>> %(stderr)s 3> %(pidfile)s"" % args
+
+        return cmd
 class StreamsSmokeTestJobRunnerService(StreamsSmokeTestBaseService):
     def __init__(self, test_context, kafka):
@@ -218,3 +351,40 @@ def __init__(self, test_context, kafka, configs):
                                                                  kafka,
                                                                  ""org.apache.kafka.streams.tests.StreamsBrokerDownResilienceTest"",
                                                                  configs)
+class StreamsUpgradeTestJobRunnerService(StreamsTestBaseService):
+    def __init__(self, test_context, kafka):
+        super(StreamsUpgradeTestJobRunnerService, self).__init__(test_context,
+                                                                 kafka,
+                                                                 ""org.apache.kafka.streams.tests.StreamsUpgradeTest"",
+                                                                 """")
+        self.UPGRADE_FROM = """"
+
+    def set_version(self, kafka_streams_version):
+        self.KAFKA_STREAMS_VERSION = kafka_streams_version
+
+    def set_upgrade_from(self, upgrade_from):
+        self.UPGRADE_FROM = upgrade_from
+
+    def start_cmd(self, node):
+        args = self.args.copy()
+        args = self.kafka.bootstrap_servers()
+        if self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_0) or self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_1):
+            args = self.kafka.zk.connect_setting()
+        else:
+            args = """"
+        args = self.PERSISTENT_ROOT
+        args = self.STDOUT_FILE
+        args = self.STDERR_FILE
+        args = self.PID_FILE
+        args = self.LOG4J_CONFIG_FILE
+        args = self.KAFKA_STREAMS_VERSION
+        args = self.UPGRADE_FROM
+        args = self.path.script(""kafka-run-class.sh"", node)
+
+        cmd = ""( export KAFKA_LOG4J_OPTS=\""-Dlog4j.configuration=file:%(log4j)s\""; "" \
+              ""INCLUDE_TEST_JARS=true UPGRADE_KAFKA_STREAMS_TEST_VERSION=%(version)s "" \
+              "" %(kafka_run_class)s %(streams_class_name)s "" \
+              "" %(kafka)s %(zk)s %(state_dir)s %(user_test_args)s %(upgrade_from)s"" \
+              "" & echo $! >&3 ) 1>> %(stdout)s 2>> %(stderr)s 3> %(pidfile)s"" % args
+
+        return cmd
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py
index 81b7ffe7047..77833a92be4 100644
--- a/tests/kafkatest/tests/streams/streams_upgrade_test.py
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py
@@ -15,21 +15,48 @@
 from ducktape.mark.resource import cluster
 from ducktape.tests.test import Test
-from ducktape.mark import parametrize, ignore
+from ducktape.mark import ignore, matrix, parametrize
 from kafkatest.services.kafka import KafkaService
 from kafkatest.services.zookeeper import ZookeeperService
-from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsSmokeTestJobRunnerService
-from kafkatest.version import LATEST_0_10_1, LATEST_0_10_2, LATEST_0_11_0, DEV_BRANCH, KafkaVersion
+from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsSmokeTestJobRunnerService, StreamsUpgradeTestJobRunnerService
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1, LATEST_0_10_2, LATEST_0_11_0, LATEST_1_0, DEV_BRANCH, DEV_VERSION, KafkaVersion
+import random
 import time
+broker_upgrade_versions = 
+simple_upgrade_versions_metadata_version_2 = 
 class StreamsUpgradeTest(Test):
     """"""
-    Tests rolling upgrades and downgrades of the Kafka Streams library.
+    Test upgrading Kafka Streams (all version combination)
+    If metadata was changes, upgrade is more difficult
+    Metadata version was bumped in 0.10.1.0
     """"""
     def __init__(self, test_context):
         super(StreamsUpgradeTest, self).__init__(test_context)
+        self.topics = {
+            'echo' : { 'partitions': 5 },
+            'data' : { 'partitions': 5 },
+        }
+
+    def perform_broker_upgrade(self, to_version):
+        self.logger.info(""First pass bounce - rolling broker upgrade"")
+        for node in self.kafka.nodes:
+            self.kafka.stop_node(node)
+            node.version = KafkaVersion(to_version)
+            self.kafka.start_node(node)
+
+    @cluster(num_nodes=6)
+    @matrix(from_version=broker_upgrade_versions, to_version=broker_upgrade_versions)
+    def test_upgrade_downgrade_brokers(self, from_version, to_version):
+        """"""
+        Start a smoke test client then perform rolling upgrades on the broker. 
+        """"""
+
+        if from_version == to_version:
+            return
+
         self.replication = 3
         self.partitions = 1
         self.isr = 2
@@ -55,45 +82,7 @@ def __init__(self, test_context):
             'tagg' : { 'partitions': self.partitions, 'replication-factor': self.replication,
                        'configs': {""min.insync.replicas"": self.isr} }
         }
-        
-
-    def perform_streams_upgrade(self, to_version):
-        self.logger.info(""First pass bounce - rolling streams upgrade"")
-
-        # get the node running the streams app
-        node = self.processor1.node
-        self.processor1.stop()
-
-        # change it's version. This will automatically make it pick up a different
-        # JAR when it starts again
-        node.version = KafkaVersion(to_version)
-        self.processor1.start()
-
-    def perform_broker_upgrade(self, to_version):
-        self.logger.info(""First pass bounce - rolling broker upgrade"")
-        for node in self.kafka.nodes:
-            self.kafka.stop_node(node)
-            node.version = KafkaVersion(to_version)
-            self.kafka.start_node(node)
-
-    @cluster(num_nodes=6)
-    @parametrize(from_version=str(LATEST_0_10_1), to_version=str(DEV_BRANCH))
-    @parametrize(from_version=str(LATEST_0_10_2), to_version=str(DEV_BRANCH))
-    @parametrize(from_version=str(LATEST_0_10_1), to_version=str(LATEST_0_11_0))
-    @parametrize(from_version=str(LATEST_0_10_2), to_version=str(LATEST_0_11_0))
-    @parametrize(from_version=str(LATEST_0_11_0), to_version=str(LATEST_0_10_2))
-    @parametrize(from_version=str(DEV_BRANCH), to_version=str(LATEST_0_10_2))
-    def test_upgrade_downgrade_streams(self, from_version, to_version):
-        """"""
-        Start a smoke test client, then abort (kill -9) and restart it a few times.
-        Ensure that all records are delivered.
-
-        Note, that just like tests/core/upgrade_test.py, a prerequisite for this test to succeed
-        if the inclusion of all parametrized versions of kafka in kafka/vagrant/base.sh 
-        (search for get_kafka()). For streams in particular, that means that someone has manually
-        copies the kafka-stream-$version-test.jar in the right S3 bucket as shown in base.sh.
-        """"""
         # Setup phase
         self.zk = ZookeeperService(self.test_context, num_nodes=1)
         self.zk.start()
@@ -108,13 +97,12 @@ def test_upgrade_downgrade_streams(self, from_version, to_version):
         self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)
         self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka)
-
         self.driver.start()
         self.processor1.start()
         time.sleep(15)
-        self.perform_streams_upgrade(to_version)
+        self.perform_broker_upgrade(to_version)
         time.sleep(15)
         self.driver.wait()
@@ -126,42 +114,241 @@ def test_upgrade_downgrade_streams(self, from_version, to_version):
         node.account.ssh(""grep ALL-RECORDS-DELIVERED %s"" % self.driver.STDOUT_FILE, allow_fail=False)
         self.processor1.node.account.ssh_capture(""grep SMOKE-TEST-CLIENT-CLOSED %s"" % self.processor1.STDOUT_FILE, allow_fail=False)
+    @matrix(from_version=simple_upgrade_versions_metadata_version_2, to_version=simple_upgrade_versions_metadata_version_2)
+    def test_simple_upgrade_downgrade(self, from_version, to_version):
+        """"""
+        Starts 3 KafkaStreams instances with <old_version>, and upgrades one-by-one to <new_version>
+        """"""
+        if from_version == to_version:
+            return
-    @cluster(num_nodes=6)
-    @parametrize(from_version=str(LATEST_0_10_2), to_version=str(DEV_BRANCH))
-    def test_upgrade_brokers(self, from_version, to_version):
+        self.zk = ZookeeperService(self.test_context, num_nodes=1)
+        self.zk.start()
+
+        self.kafka = KafkaService(self.test_context, num_nodes=1, zk=self.zk, topics=self.topics)
+        self.kafka.start()
+
+        self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)
+        self.driver.disable_auto_terminate()
+        self.processor1 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+        self.processor2 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+        self.processor3 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+
+        self.driver.start()
+        self.start_all_nodes_with(from_version)
+
+        self.processors = 
+
+        counter = 1
+        random.seed()
+
+        # upgrade one-by-one via rolling bounce
+        random.shuffle(self.processors)
+        for p in self.processors:
+            p.CLEAN_NODE_ENABLED = False
+            self.do_rolling_bounce(p, """", to_version, counter)
+            counter = counter + 1
+
+        # shutdown
+        self.driver.stop()
+        self.driver.wait()
+
+        random.shuffle(self.processors)
+        for p in self.processors:
+            node = p.node
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:
+                p.stop()
+                monitor.wait_until(""UPGRADE-TEST-CLIENT-CLOSED"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'UPGRADE-TEST-CLIENT-CLOSED' on"" + str(node.account))
+
+        self.driver.stop()
+
+    #@parametrize(new_version=str(LATEST_0_10_1)) we cannot run this test until Kafka 0.10.1.2 is released
+    #@parametrize(new_version=str(LATEST_0_10_2)) we cannot run this test until Kafka 0.10.2.2 is released
+    #@parametrize(new_version=str(LATEST_0_11_0)) we cannot run this test until Kafka 0.11.0.3 is released
+    #@parametrize(new_version=str(LATEST_1_0)) we cannot run this test until Kafka 1.0.2 is released
+    #@parametrize(new_version=str(LATEST_1_1)) we cannot run this test until Kafka 1.1.1 is released
+    @parametrize(new_version=str(DEV_VERSION))
+    def test_metadata_upgrade(self, new_version):
         """"""
-        Start a smoke test client then perform rolling upgrades on the broker. 
+        Starts 3 KafkaStreams instances with version 0.10.0, and upgrades one-by-one to <new_version>
         """"""
-        # Setup phase
+
         self.zk = ZookeeperService(self.test_context, num_nodes=1)
         self.zk.start()
-        # number of nodes needs to be >= 3 for the smoke test
-        self.kafka = KafkaService(self.test_context, num_nodes=3,
-                                  zk=self.zk, version=KafkaVersion(from_version), topics=self.topics)
+        self.kafka = KafkaService(self.test_context, num_nodes=1, zk=self.zk, topics=self.topics)
         self.kafka.start()
-        
-        # allow some time for topics to be created
-        time.sleep(10)
-        
+
         self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)
-        self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka)
+        self.driver.disable_auto_terminate()
+        self.processor1 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+        self.processor2 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+        self.processor3 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
-        
         self.driver.start()
-        self.processor1.start()
-        time.sleep(15)
+        self.start_all_nodes_with(str(LATEST_0_10_0))
-        self.perform_broker_upgrade(to_version)
+        self.processors = 
-        time.sleep(15)
+        counter = 1
+        random.seed()
+
+        # first rolling bounce
+        random.shuffle(self.processors)
+        for p in self.processors:
+            p.CLEAN_NODE_ENABLED = False
+            self.do_rolling_bounce(p, ""0.10.0"", new_version, counter)
+            counter = counter + 1
+
+        # second rolling bounce
+        random.shuffle(self.processors)
+        for p in self.processors:
+            self.do_rolling_bounce(p, """", new_version, counter)
+            counter = counter + 1
+
+        # shutdown
+        self.driver.stop()
         self.driver.wait()
+
+        random.shuffle(self.processors)
+        for p in self.processors:
+            node = p.node
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:
+                p.stop()
+                monitor.wait_until(""UPGRADE-TEST-CLIENT-CLOSED"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'UPGRADE-TEST-CLIENT-CLOSED' on"" + str(node.account))
+
         self.driver.stop()
-        self.processor1.stop()
+    def start_all_nodes_with(self, version):
+        # start first with <version>
+        self.prepare_for(self.processor1, version)
+        node1 = self.processor1.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as monitor:
+            with node1.account.monitor_log(self.processor1.LOG_FILE) as log_monitor:
+                self.processor1.start()
+                log_monitor.wait_until(""Kafka version : "" + version,
+                                       timeout_sec=60,
+                                       err_msg=""Could not detect Kafka Streams version "" + version + "" "" + str(node1.account))
+                monitor.wait_until(""processed 100 records from topic"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
-        node = self.driver.node
-        node.account.ssh(""grep ALL-RECORDS-DELIVERED %s"" % self.driver.STDOUT_FILE, allow_fail=False)
-        self.processor1.node.account.ssh_capture(""grep SMOKE-TEST-CLIENT-CLOSED %s"" % self.processor1.STDOUT_FILE, allow_fail=False)
+        # start second with <version>
+        self.prepare_for(self.processor2, version)
+        node2 = self.processor2.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:
+                with node2.account.monitor_log(self.processor2.LOG_FILE) as log_monitor:
+                    self.processor2.start()
+                    log_monitor.wait_until(""Kafka version : "" + version,
+                                           timeout_sec=60,
+                                           err_msg=""Could not detect Kafka Streams version "" + version + "" "" + str(node2.account))
+                    first_monitor.wait_until(""processed 100 records from topic"",
+                                             timeout_sec=60,
+                                             err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
+                    second_monitor.wait_until(""processed 100 records from topic"",
+                                              timeout_sec=60,
+                                              err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node2.account))
+
+        # start third with <version>
+        self.prepare_for(self.processor3, version)
+        node3 = self.processor3.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:
+                with node3.account.monitor_log(self.processor3.STDOUT_FILE) as third_monitor:
+                    with node3.account.monitor_log(self.processor3.LOG_FILE) as log_monitor:
+                        self.processor3.start()
+                        log_monitor.wait_until(""Kafka version : "" + version,
+                                               timeout_sec=60,
+                                               err_msg=""Could not detect Kafka Streams version "" + version + "" "" + str(node3.account))
+                        first_monitor.wait_until(""processed 100 records from topic"",
+                                                 timeout_sec=60,
+                                                 err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
+                        second_monitor.wait_until(""processed 100 records from topic"",
+                                                  timeout_sec=60,
+                                                  err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node2.account))
+                        third_monitor.wait_until(""processed 100 records from topic"",
+                                                  timeout_sec=60,
+                                                  err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node3.account))
+
+    @staticmethod
+    def prepare_for(processor, version):
+        processor.node.account.ssh(""rm -rf "" + processor.PERSISTENT_ROOT, allow_fail=False)
+        if version == str(DEV_VERSION):
+            processor.set_version("""")  # set to TRUNK
+        else:
+            processor.set_version(version)
+
+    def do_rolling_bounce(self, processor, upgrade_from, new_version, counter):
+        first_other_processor = None
+        second_other_processor = None
+        for p in self.processors:
+            if p != processor:
+                if first_other_processor is None:
+                    first_other_processor = p
+                else:
+                    second_other_processor = p
+
+        node = processor.node
+        first_other_node = first_other_processor.node
+        second_other_node = second_other_processor.node
+
+        # stop processor and wait for rebalance of others
+        with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:
+            with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:
+                processor.stop()
+                first_other_monitor.wait_until(""processed 100 records from topic"",
+                                               timeout_sec=60,
+                                               err_msg=""Never saw output 'processed 100 records from topic' on"" + str(first_other_node.account))
+                second_other_monitor.wait_until(""processed 100 records from topic"",
+                                                timeout_sec=60,
+                                                err_msg=""Never saw output 'processed 100 records from topic' on"" + str(second_other_node.account))
+        node.account.ssh_capture(""grep UPGRADE-TEST-CLIENT-CLOSED %s"" % processor.STDOUT_FILE, allow_fail=False)
+
+        if upgrade_from == """":  # upgrade disabled -- second round of rolling bounces
+            roll_counter = "".1-""  # second round of rolling bounces
+        else:
+            roll_counter = "".0-""  # first  round of rolling boundes
+
+        node.account.ssh(""mv "" + processor.STDOUT_FILE + "" "" + processor.STDOUT_FILE + roll_counter + str(counter), allow_fail=False)
+        node.account.ssh(""mv "" + processor.STDERR_FILE + "" "" + processor.STDERR_FILE + roll_counter + str(counter), allow_fail=False)
+        node.account.ssh(""mv "" + processor.LOG_FILE + "" "" + processor.LOG_FILE + roll_counter + str(counter), allow_fail=False)
+
+        if new_version == str(DEV_VERSION):
+            processor.set_version("""")  # set to TRUNK
+        else:
+            processor.set_version(new_version)
+        processor.set_upgrade_from(upgrade_from)
+
+        grep_metadata_error = ""grep \""org.apache.kafka.streams.errors.TaskAssignmentException: unable to decode subscription data: version=2\"" ""
+        with node.account.monitor_log(processor.STDOUT_FILE) as monitor:
+            with node.account.monitor_log(processor.LOG_FILE) as log_monitor:
+                with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:
+                    with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:
+                        processor.start()
+
+                        log_monitor.wait_until(""Kafka version : "" + new_version,
+                                               timeout_sec=60,
+                                               err_msg=""Could not detect Kafka Streams version "" + new_version + "" "" + str(node.account))
+                        first_other_monitor.wait_until(""processed 100 records from topic"",
+                                                       timeout_sec=60,
+                                                       err_msg=""Never saw output 'processed 100 records from topic' on"" + str(first_other_node.account))
+                        found = list(first_other_node.account.ssh_capture(grep_metadata_error + first_other_processor.STDERR_FILE, allow_fail=True))
+                        if len(found) > 0:
+                            raise Exception(""Kafka Streams failed with 'unable to decode subscription data: version=2'"")
+
+                        second_other_monitor.wait_until(""processed 100 records from topic"",
+                                                        timeout_sec=60,
+                                                        err_msg=""Never saw output 'processed 100 records from topic' on"" + str(second_other_node.account))
+                        found = list(second_other_node.account.ssh_capture(grep_metadata_error + second_other_processor.STDERR_FILE, allow_fail=True))
+                        if len(found) > 0:
+                            raise Exception(""Kafka Streams failed with 'unable to decode subscription data: version=2'"")
+
+                        monitor.wait_until(""processed 100 records from topic"",
+                                           timeout_sec=60,
+                                           err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node.account))
diff --git a/tests/kafkatest/version.py b/tests/kafkatest/version.py
index f63a7c17ecd..ee3f8b57ef0 100644
--- a/tests/kafkatest/version.py
+++ b/tests/kafkatest/version.py
@@ -61,6 +61,7 @@ def get_version(node=None):
         return DEV_BRANCH
 DEV_BRANCH = KafkaVersion(""dev"")
+DEV_VERSION = KafkaVersion(""1.1.1-SNAPSHOT"")
 # 0.8.2.X versions
 V_0_8_2_1 = KafkaVersion(""0.8.2.1"")
@@ -89,7 +90,14 @@ def get_version(node=None):
 LATEST_0_10 = LATEST_0_10_2
-# 0.11.0.0 versions
+# 0.11.0.x versions
 V_0_11_0_0 = KafkaVersion(""0.11.0.0"")
-LATEST_0_11_0 = V_0_11_0_0
+V_0_11_0_1 = KafkaVersion(""0.11.0.1"")
+V_0_11_0_2 = KafkaVersion(""0.11.0.2"")
+LATEST_0_11_0 = V_0_11_0_2
 LATEST_0_11 = LATEST_0_11_0
+
+# 1.0.x versions
+V_1_0_0 = KafkaVersion(""1.0.0"")
+V_1_0_1 = KafkaVersion(""1.0.1"")
+LATEST_1_0 = V_1_0_1
\ No newline at end of file
diff --git a/vagrant/base.sh b/vagrant/base.sh
index 4b5540652fa..c520d49971f 100755
--- a/vagrant/base.sh
+++ b/vagrant/base.sh
@@ -99,8 +99,10 @@ popd
 popd
 popd
-# Test multiple Scala versions
-get_kafka 0.8.2.2 2.10
+# Test multiple Kafka versions
+# we want to use the latest Scala version per Kafka version
+# however, we cannot pull in Scala 2.12 builds atm, because Scala 2.12 requires Java 8, but we use Java 7 to run the system tests
+get_kafka 0.8.2.2 2.11
 chmod a+rw /opt/kafka-0.8.2.2
 get_kafka 0.9.0.1 2.11
 chmod a+rw /opt/kafka-0.9.0.1
@@ -110,8 +112,10 @@ get_kafka 0.10.1.1 2.11
 chmod a+rw /opt/kafka-0.10.1.1
 get_kafka 0.10.2.1 2.11
 chmod a+rw /opt/kafka-0.10.2.1
-get_kafka 0.11.0.0 2.11
-chmod a+rw /opt/kafka-0.11.0.0
+get_kafka 0.11.0.2 2.11
+chmod a+rw /opt/kafka-0.11.0.2
+get_kafka 1.0.1 2.11
+chmod a+rw /opt/kafka-1.0.1
 # For EC2 nodes, we want to use /mnt, which should have the local disk. On local
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 17 **
mjsax closed pull request #4779: KAFKA-6054: Fix upgrade path from Kafka Streams v0.10.0
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git a/bin/kafka-run-class.sh b/bin/kafka-run-class.sh
index bb786da43ca..4dd092323b0 100755
--- a/bin/kafka-run-class.sh
+++ b/bin/kafka-run-class.sh
@@ -69,28 +69,50 @@ do
   fi
 done
-for file in ""$base_dir""/clients/build/libs/kafka-clients*.jar;
-do
-  if should_include_file ""$file""; then
-    CLASSPATH=""$CLASSPATH"":""$file""
-  fi
-done
+if ; then
+  clients_lib_dir=$(dirname $0)/../clients/build/libs
+  streams_lib_dir=$(dirname $0)/../streams/build/libs
+  rocksdb_lib_dir=$(dirname $0)/../streams/build/dependant-libs-${SCALA_VERSION}
+else
+  clients_lib_dir=/opt/kafka-$UPGRADE_KAFKA_STREAMS_TEST_VERSION/libs
+  streams_lib_dir=$clients_lib_dir
+  rocksdb_lib_dir=$streams_lib_dir
+fi
+
-for file in ""$base_dir""/streams/build/libs/kafka-streams*.jar;
+for file in ""$clients_lib_dir""/kafka-clients*.jar;
 do
   if should_include_file ""$file""; then
     CLASSPATH=""$CLASSPATH"":""$file""
   fi
 done
-for file in ""$base_dir""/streams/examples/build/libs/kafka-streams-examples*.jar;
+for file in ""$streams_lib_dir""/kafka-streams*.jar;
 do
   if should_include_file ""$file""; then
     CLASSPATH=""$CLASSPATH"":""$file""
   fi
 done
-for file in ""$base_dir""/streams/build/dependant-libs-${SCALA_VERSION}/rocksdb*.jar;
+if ; then
+  for file in ""$base_dir""/streams/examples/build/libs/kafka-streams-examples*.jar;
+  do
+    if should_include_file ""$file""; then
+      CLASSPATH=""$CLASSPATH"":""$file""
+    fi
+  done
+else
+  VERSION_NO_DOTS=`echo $UPGRADE_KAFKA_STREAMS_TEST_VERSION | sed 's/\.//g'`
+  SHORT_VERSION_NO_DOTS=${VERSION_NO_DOTS:0:((${#VERSION_NO_DOTS} - 1))} # remove last char, ie, bug-fix number
+  for file in ""$base_dir""/streams/upgrade-system-tests-$SHORT_VERSION_NO_DOTS/build/libs/kafka-streams-upgrade-system-tests*.jar;
+  do
+    if should_include_file ""$file""; then
+      CLASSPATH=""$CLASSPATH"":""$file""
+    fi
+  done
+fi
+
+for file in ""$rocksdb_lib_dir""/rocksdb*.jar;
 do
   CLASSPATH=""$CLASSPATH"":""$file""
 done
diff --git a/build.gradle b/build.gradle
index 33fa7a750f2..f8daf2fdddc 100644
--- a/build.gradle
+++ b/build.gradle
@@ -1027,6 +1027,66 @@ project(':streams:examples') {
   }
 }
+project(':streams:upgrade-system-tests-0100') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0100""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0100
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
+project(':streams:upgrade-system-tests-0101') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0101""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0101
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
+project(':streams:upgrade-system-tests-0102') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0102""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0102
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
+project(':streams:upgrade-system-tests-0110') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-0110""
+
+  dependencies {
+    testCompile libs.kafkaStreams_0110
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
+project(':streams:upgrade-system-tests-10') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-10""
+
+  dependencies {
+    testCompile libs.kafkaStreams_10
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
 project(':jmh-benchmarks') {
   apply plugin: 'com.github.johnrengelman.shadow'
diff --git a/checkstyle/suppressions.xml b/checkstyle/suppressions.xml
index e3bf15102bd..0fec810a95b 100644
--- a/checkstyle/suppressions.xml
+++ b/checkstyle/suppressions.xml
@@ -189,7 +189,7 @@
               files=""[file java]""/>
     <suppress checks=""NPathComplexity""
-              files=""[file java]""/>
+              files=""[file java]|[file java]""/>
     <suppress checks=""NPathComplexity""
               files=""[file java]""/>
diff --git a/docs/streams/upgrade-guide.html b/docs/streams/upgrade-guide.html
index d21d505671a..fdc0af1de1f 100644
--- a/docs/streams/upgrade-guide.html
+++ b/docs/streams/upgrade-guide.html
@@ -40,37 +40,64 @@ <h1>Upgrade Guide and API Changes</h1>
     </p>
     <p>
-        If you want to upgrade from 1.0.x to 1.1.0 and you have customized window store implementations on the <code>ReadOnlyWindowStore</code> interface
+        If you want to upgrade from 1.0.x to 1.2.0 and you have customized window store implementations on the <code>ReadOnlyWindowStore</code> interface
         you'd need to update your code to incorporate the newly added public APIs.
         Otherwise, if you are using Java 7 you don't need to make any code changes as the public API is fully backward compatible;
         but if you are using Java 8 method references in your Kafka Streams code you might need to update your code to resolve method ambiguities.
         Hot-swaping the jar-file only might not work for this case.
-        See <a href=""#streams_api_changes_110"">below</a> for a complete list of 1.1.0 API and semantic changes that allow you to advance your application and/or simplify your code base.
+        See below a complete list of <a href=""#streams_api_changes_120"">1.2.0</a> and <a href=""#streams_api_changes_110"">1.1.0</a>
+        API and semantic changes that allow you to advance your application and/or simplify your code base.
     </p>
     <p>
-        If you want to upgrade from 0.11.0.x to 1.0.0 you don't need to make any code changes as the public API is fully backward compatible.
+        If you want to upgrade from 0.10.2.x or 0.11.0.x to 1.2.x and you have customized window store implementations on the <code>ReadOnlyWindowStore</code> interface
+        you'd need to update your code to incorporate the newly added public APIs.
+        Otherwise, if you are using Java 7 you don't need to do any code changes as the public API is fully backward compatible;
+        but if you are using Java 8 method references in your Kafka Streams code you might need to update your code to resolve method ambiguities.
         However, some public APIs were deprecated and thus it is recommended to update your code eventually to allow for future upgrades.
-        See <a href=""#streams_api_changes_100"">below</a> for a complete list of 1.0.0 API and semantic changes that allow you to advance your application and/or simplify your code base.
-    </p>
-
-    <p>
-        If you want to upgrade from 0.10.2.x to 0.11.0 you don't need to make any code changes as the public API is fully backward compatible.
-        However, some configuration parameters were deprecated and thus it is recommended to update your code eventually to allow for future upgrades.
-        See <a href=""#streams_api_changes_0110"">below</a> for a complete list of 0.11.0 API and semantic changes that allow you to advance your application and/or simplify your code base.
+        See below a complete list of <a href=""#streams_api_changes_120"">1.2</a>, <a href=""#streams_api_changes_110"">1.1</a>,
+        <a href=""#streams_api_changes_100"">1.0</a>, and <a href=""#streams_api_changes_0110"">0.11.0</a> API
+        and semantic changes that allow you to advance your application and/or simplify your code base, including the usage of new features.
+        Additionally, Streams API 1.1.x requires broker on-disk message format version 0.10 or higher; thus, you need to make sure that the message
+        format is configured correctly before you upgrade your Kafka Streams application.
     </p>
     <p>
-        If you want to upgrade from 0.10.1.x to 0.10.2, see the <a href=""/{{version}}/documentation/#upgrade_1020_streams""><b>Upgrade Section for 0.10.2</b></a>.
-        It highlights incompatible changes you need to consider to upgrade your code and application.
-        See <a href=""#streams_api_changes_0102"">below</a> for a complete list of 0.10.2 API and semantic changes that allow you to advance your application and/or simplify your code base.
+        If you want to upgrade from 0.10.1.x to 1.2.x see the Upgrade Sections for <a href=""/{{version}}/documentation/#upgrade_1020_streams""><b>0.10.2</b></a>,
+        <a href=""/{{version}}/documentation/#upgrade_1100_streams""><b>0.11.0</b></a>,
+        <a href=""/{{version}}/documentation/#upgrade_100_streams""><b>1.0</b></a>,
+        <a href=""/{{version}}/documentation/#upgrade_100_streams""><b>1.0</b></a>, and
+        <a href=""/{{version}}/documentation/#upgrade_110_streams""><b>1.2</b></a>.
+        Note, that a brokers on-disk message format must be on version 0.10 or higher to run a Kafka Streams application version 1.2 or higher.
+        See below a complete list of <a href=""#streams_api_changes_0102"">0.10.2</a>, <a href=""#streams_api_changes_0110"">0.11.0</a>,
+        <a href=""#streams_api_changes_100"">1.0</a>, <a href=""#streams_api_changes_110"">1.1</a>, and <a href=""#streams_api_changes_120"">1.2</a>
+        API and semantical changes that allow you to advance your application and/or simplify your code base, including the usage of new features.
     </p>
     <p>
-        If you want to upgrade from 0.10.0.x to 0.10.1, see the <a href=""/{{version}}/documentation/#upgrade_1010_streams""><b>Upgrade Section for 0.10.1</b></a>.
-        It highlights incompatible changes you need to consider to upgrade your code and application.
-        See <a href=""#streams_api_changes_0101"">below</a> a complete list of 0.10.1 API changes that allow you to advance your application and/or simplify your code base, including the usage of new features.
+        Upgrading from 0.10.0.x to 1.2.0 directly is also possible.
+        Note, that a brokers must be on version 0.10.1 or higher and on-disk message format must be on version 0.10 or higher
+        to run a Kafka Streams application version 1.2 or higher.
+        See <a href=""#streams_api_changes_0101"">Streams API changes in 0.10.1</a>, <a href=""#streams_api_changes_0102"">Streams API changes in 0.10.2</a>,
+        <a href=""#streams_api_changes_0110"">Streams API changes in 0.11.0</a>, <a href=""#streams_api_changes_100"">Streams API changes in 1.0</a>, and
+        <a href=""#streams_api_changes_110"">Streams API changes in 1.1</a>, and <a href=""#streams_api_changes_120"">Streams API changes in 1.2</a>
+        for a complete list of API changes.
+        Upgrading to 1.2.0 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+        (cf. <a href=""[link]"">KIP-268</a>).
+        As an alternative, an offline upgrade is also possible.
     </p>
+    <ul>
+        <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 1.2.0</li>
+        <li> bounce each instance of your application once </li>
+        <li> prepare your newly deployed 1.2.0 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+        <li> bounce each instance of your application once more to complete the upgrade </li>
+    </ul>
+    <p> Upgrading from 0.10.0.x to 1.2.0 in offline mode: </p>
+    <ul>
+        <li> stop all old (0.10.0.x) application instances </li>
+        <li> update your code and swap old code and jar file with new code and new jar file </li>
+        <li> restart all new (1.2.0) application instances </li>
+    </ul>
     <!-- TODO: verify release verion and update `id` and `href` attributes (also at other places that link to this headline) -->
     <h3><a id=""streams_api_changes_120"" href=""#streams_api_changes_120"">Streams API changes in 1.2.0</a></h3>
diff --git a/docs/upgrade.html b/docs/upgrade.html
index 0c5f5fd3247..95f2c418c3b 100644
--- a/docs/upgrade.html
+++ b/docs/upgrade.html
@@ -68,6 +68,7 @@ <h5><a id=""upgrade_120_notable"" href=""#upgrade_120_notable"">Notable changes in 1
 <ul>
     <li><a href=""[link]"">KIP-186</a> increases the default offset retention time from 1 day to 7 days. This makes it less likely to ""lose"" offsets in an application that commits infrequently. It also increases the active set of offsets and therefore can increase memory usage on the broker. Note that the console consumer currently enables offset commit by default and can be the source of a large number of offsets which this change will now preserve for 7 days instead of 1. You can preserve the existing behavior by setting the broker config <code>offsets.retention.minutes</code> to 1440.</li>
     <li><a href=""[link]"">KAFKA-5674</a> extends the lower interval of <code>max.connections.per.ip minimum</code> to zero and therefore allows IP-based filtering of inbound connections.</li>
+    <li> New Kafka Streams configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from older version. </li>
 </ul>
 <h5><a id=""upgrade_120_new_protocols"" href=""#upgrade_120_new_protocols"">New Protocol Versions</a></h5>
@@ -76,7 +77,7 @@ <h5><a id=""upgrade_120_new_protocols"" href=""#upgrade_120_new_protocols"">New Prot
 <h5><a id=""upgrade_120_streams"" href=""#upgrade_120_streams"">Upgrading a 1.2.0 Kafka Streams Application</a></h5>
 <ul>
     <li> Upgrading your Streams application from 1.1.0 to 1.2.0 does not require a broker upgrade.
-        A Kafka Streams 1.2.0 application can connect to 1.2, 1.1, 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
+         A Kafka Streams 1.2.0 application can connect to 1.2, 1.1, 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
     <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_120"">Streams API changes in 1.2.0</a> for more details. </li>
 </ul>
@@ -125,6 +126,14 @@ <h4><a id=""upgrade_1_1_0"" href=""#upgrade_1_1_0"">Upgrading from 0.8.x, 0.9.x, 0.1
         Hot-swaping the jar-file only might not work.</li>
 </ol>
+<!-- TODO add if 1.1.1 gets release
+<h5><a id=""upgrade_111_notable"" href=""#upgrade_111_notable"">Notable changes in 1.1.1</a></h5>
+<ul>
+    <li> New Kafka Streams configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x </li>
+    <li> See the <a href=""/{{version}}/documentation/streams/upgrade-guide.html""><b>Kafka Streams upgrade guide</b></a> for details about this new config.
+</ul>
+-->
+
 <h5><a id=""upgrade_110_notable"" href=""#upgrade_110_notable"">Notable changes in 1.1.0</a></h5>
 <ul>
     <li>The kafka artifact in Maven no longer depends on log4j or slf4j-log4j12. Similarly to the kafka-clients artifact, users
@@ -196,6 +205,14 @@ <h4><a id=""upgrade_1_0_0"" href=""#upgrade_1_0_0"">Upgrading from 0.8.x, 0.9.x, 0.1
         Similarly for the message format version.</li>
 </ol>
+<!-- TODO add if 1.0.2 gets release
+<h5><a id=""upgrade_102_notable"" href=""#upgrade_102_notable"">Notable changes in 1.0.2</a></h5>
+<ul>
+    <li> New Kafka Streams configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x </li>
+    <li> See the <a href=""/{{version}}/documentation/streams/upgrade-guide.html""><b>Kafka Streams upgrade guide</b></a> for details about this new config.
+</ul>
+-->
+
 <h5><a id=""upgrade_101_notable"" href=""#upgrade_101_notable"">Notable changes in 1.0.1</a></h5>
 <ul>
     <li>Restored binary compatibility of AdminClient's Options classes (e.g. CreateTopicsOptions, DeleteTopicsOptions, etc.) with
@@ -261,17 +278,74 @@ <h5><a id=""upgrade_100_new_protocols"" href=""#upgrade_100_new_protocols"">New Prot
          be used if the SaslHandshake request version is greater than 0. </li>
 </ul>
-<h5><a id=""upgrade_100_streams"" href=""#upgrade_100_streams"">Upgrading a 1.0.0 Kafka Streams Application</a></h5>
+<h5><a id=""upgrade_100_streams"" href=""#upgrade_100_streams"">Upgrading a 0.11.0 Kafka Streams Application</a></h5>
 <ul>
     <li> Upgrading your Streams application from 0.11.0 to 1.0.0 does not require a broker upgrade.
-        A Kafka Streams 1.0.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).
-        However, Kafka Streams 1.0 requires 0.10 message format or newer and does not work with older message formats. </li>
+         A Kafka Streams 1.0.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though).
+         However, Kafka Streams 1.0 requires 0.10 message format or newer and does not work with older message formats. </li>
     <li> If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. </li>
     <li> There are a few public APIs including <code>ProcessorContext#schedule()</code>, <code>Processor#punctuate()</code> and <code>KStreamBuilder</code>, <code>TopologyBuilder</code> are being deprecated by new APIs.
-        We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.
     <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_100"">Streams API changes in 1.0.0</a> for more details. </li>
 </ul>
+<h5><a id=""upgrade_100_streams_from_0102"" href=""#upgrade_100_streams_from_0102"">Upgrading a 0.10.2 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.2 to 1.0 does not require a broker upgrade.
+         A Kafka Streams 1.0 application can connect to 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
+    <li> If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. </li>
+    <li> There are a few public APIs including <code>ProcessorContext#schedule()</code>, <code>Processor#punctuate()</code> and <code>KStreamBuilder</code>, <code>TopologyBuilder</code> are being deprecated by new APIs.
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.
+    <li> If you specify customized <code>key.serde</code>, <code>value.serde</code> and <code>timestamp.extractor</code> in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. </li>
+    <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0110"">Streams API changes in 0.11.0</a> for more details. </li>
+</ul>
+
+<h5><a id=""upgrade_100_streams_from_0101"" href=""#upgrade_1100_streams_from_0101"">Upgrading a 0.10.1 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.1 to 1.0 does not require a broker upgrade.
+         A Kafka Streams 1.0 application can connect to 1.0, 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
+    <li> You need to recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> If you are monitoring on streams metrics, you will need make some changes to the metrics names in your reporting and monitoring code, because the metrics sensor hierarchy was changed. </li>
+    <li> There are a few public APIs including <code>ProcessorContext#schedule()</code>, <code>Processor#punctuate()</code> and <code>KStreamBuilder</code>, <code>TopologyBuilder</code> are being deprecated by new APIs.
+         We recommend making corresponding code changes, which should be very minor since the new APIs look quite similar, when you upgrade.
+    <li> If you specify customized <code>key.serde</code>, <code>value.serde</code> and <code>timestamp.extractor</code> in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. </li>
+    <li> If you use a custom (i.e., user implemented) timestamp extractor, you will need to update this code, because the <code>TimestampExtractor</code> interface was changed. </li>
+    <li> If you register custom metrics, you will need to update this code, because the <code>StreamsMetric</code> interface was changed. </li>
+    <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_100"">Streams API changes in 1.0.0</a>,
+         <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0110"">Streams API changes in 0.11.0</a> and
+         <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0102"">Streams API changes in 0.10.2</a> for more details. </li>
+</ul>
+
+<h5><a id=""upgrade_100_streams_from_0100"" href=""#upgrade_100_streams_from_0100"">Upgrading a 0.10.0 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.0 to 1.0 does require a <a href=""#upgrade_10_1"">broker upgrade</a> because a Kafka Streams 1.0 application can only connect to 0.1, 0.11.0, 0.10.2, or 0.10.1 brokers. </li>
+    <li> There are couple of API changes, that are not backward compatible (cf. <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_100"">Streams API changes in 1.0.0</a>,
+         <a href=""/{{version}}/documentation/streams#streams_api_changes_0110"">Streams API changes in 0.11.0</a>,
+         <a href=""/{{version}}/documentation/streams#streams_api_changes_0102"">Streams API changes in 0.10.2</a>, and
+         <a href=""/{{version}}/documentation/streams#streams_api_changes_0101"">Streams API changes in 0.10.1</a> for more details).
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <!-- TODO add if 1.0.2 gets release
+    <li> Upgrading from 0.10.0.x to 1.0.2 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+        (cf. <a href=""[link]"">KIP-268</a>).
+        As an alternative, an offline upgrade is also possible.
+        <ul>
+            <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.11.0.3 </li>
+            <li> bounce each instance of your application once </li>
+            <li> prepare your newly deployed 1.0.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+            <li> bounce each instance of your application once more to complete the upgrade </li>
+        </ul>
+    </li>
+    -->
+    <li> Upgrading from 0.10.0.x to 1.0.0 or 1.0.1 requires an offline upgrade (rolling bounce upgrade is not supported)
+
+        <ul>
+            <li> stop all old (0.10.0.x) application instances </li>
+            <li> update your code and swap old code and jar file with new code and new jar file </li>
+            <li> restart all new (1.0.0 or 1.0.1) application instances </li>
+        </ul>
+    </li>
+</ul>
+
 <h4><a id=""upgrade_11_0_0"" href=""#upgrade_11_0_0"">Upgrading from 0.8.x, 0.9.x, 0.10.0.x, 0.10.1.x or 0.10.2.x to 0.11.0.0</a></h4>
 <p>Kafka 0.11.0.0 introduces a new message format version as well as wire protocol changes. By following the recommended rolling upgrade plan below,
   you guarantee no downtime during the upgrade. However, please review the <a href=""#upgrade_1100_notable"">notable changes in 0.11.0.0</a> before upgrading.
@@ -291,7 +365,7 @@ <h4><a id=""upgrade_11_0_0"" href=""#upgrade_11_0_0"">Upgrading from 0.8.x, 0.9.x, 0
         <ul>
             <li>inter.broker.protocol.version=CURRENT_KAFKA_VERSION (e.g. 0.8.2, 0.9.0, 0.10.0, 0.10.1 or 0.10.2).</li>
             <li>log.message.format.version=CURRENT_MESSAGE_FORMAT_VERSION  (See <a href=""#upgrade_10_performance_impact"">potential performance impact
-		following the upgrade</a> for the details on what this configuration does.)</li>
+        following the upgrade</a> for the details on what this configuration does.)</li>
         </ul>
     </li>
     <li> Upgrade the brokers one at a time: shut down the broker, update the code, and restart it. </li>
@@ -320,11 +394,59 @@ <h4><a id=""upgrade_11_0_0"" href=""#upgrade_11_0_0"">Upgrading from 0.8.x, 0.9.x, 0
 <h5><a id=""upgrade_1100_streams"" href=""#upgrade_1100_streams"">Upgrading a 0.10.2 Kafka Streams Application</a></h5>
 <ul>
     <li> Upgrading your Streams application from 0.10.2 to 0.11.0 does not require a broker upgrade.
-        A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
+         A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
     <li> If you specify customized <code>key.serde</code>, <code>value.serde</code> and <code>timestamp.extractor</code> in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. </li>
     <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0110"">Streams API changes in 0.11.0</a> for more details. </li>
 </ul>
+<h5><a id=""upgrade_1100_streams_from_0101"" href=""#upgrade_1100_streams_from_0101"">Upgrading a 0.10.1 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.1 to 0.11.0 does not require a broker upgrade.
+         A Kafka Streams 0.11.0 application can connect to 0.11.0, 0.10.2 and 0.10.1 brokers (it is not possible to connect to 0.10.0 brokers though). </li>
+    <li> You need to recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <li> If you specify customized <code>key.serde</code>, <code>value.serde</code> and <code>timestamp.extractor</code> in configs, it is recommended to use their replaced configure parameter as these configs are deprecated. </li>
+    <li> If you use a custom (i.e., user implemented) timestamp extractor, you will need to update this code, because the <code>TimestampExtractor</code> interface was changed. </li>
+    <li> If you register custom metrics, you will need to update this code, because the <code>StreamsMetric</code> interface was changed. </li>
+    <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0110"">Streams API changes in 0.11.0</a> and
+         <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0102"">Streams API changes in 0.10.2</a> for more details. </li>
+</ul>
+
+<h5><a id=""upgrade_1100_streams_from_0100"" href=""#upgrade_1100_streams_from_0100"">Upgrading a 0.10.0 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.0 to 0.11.0 does require a <a href=""#upgrade_10_1"">broker upgrade</a> because a Kafka Streams 0.11.0 application can only connect to 0.11.0, 0.10.2, or 0.10.1 brokers. </li>
+    <li> There are couple of API changes, that are not backward compatible (cf. <a href=""/{{version}}/documentation/streams#streams_api_changes_0110"">Streams API changes in 0.11.0</a>,
+         <a href=""/{{version}}/documentation/streams#streams_api_changes_0102"">Streams API changes in 0.10.2</a>, and
+         <a href=""/{{version}}/documentation/streams#streams_api_changes_0101"">Streams API changes in 0.10.1</a> for more details).
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <!-- TODO add if 0.11.0.3 gets release
+    <li> Upgrading from 0.10.0.x to 0.11.0.3 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+        (cf. <a href=""[link]"">KIP-268</a>).
+        As an alternative, an offline upgrade is also possible.
+        <ul>
+            <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.11.0.3 </li>
+            <li> bounce each instance of your application once </li>
+            <li> prepare your newly deployed 0.11.0.3 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+            <li> bounce each instance of your application once more to complete the upgrade </li>
+        </ul>
+    </li>
+    -->
+    <li> Upgrading from 0.10.0.x to 0.11.0.0, 0.11.0.1, or 0.11.0.2 requires an offline upgrade (rolling bounce upgrade is not supported)
+        <ul>
+            <li> stop all old (0.10.0.x) application instances </li>
+            <li> update your code and swap old code and jar file with new code and new jar file </li>
+            <li> restart all new (0.11.0.0 , 0.11.0.1, or 0.11.0.2) application instances </li>
+        </ul>
+    </li>
+</ul>
+
+<!-- TODO add if 0.11.0.3 gets release
+<h5><a id=""upgrade_1103_notable"" href=""#upgrade_1103_notable"">Notable changes in 0.11.0.3</a></h5>
+<ul>
+<li> New Kafka Streams configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x </li>
+<li> See the <a href=""/{{version}}/documentation/streams/upgrade-guide.html""><b>Kafka Streams upgrade guide</b></a> for details about this new config.
+</ul>
+-->
+
 <h5><a id=""upgrade_1100_notable"" href=""#upgrade_1100_notable"">Notable changes in 0.11.0.0</a></h5>
 <ul>
     <li>Unclean leader election is now disabled by default. The new default favors durability over availability. Users who wish to
@@ -475,6 +597,39 @@ <h5><a id=""upgrade_1020_streams"" href=""#upgrade_1020_streams"">Upgrading a 0.10.1
     <li> See <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0102"">Streams API changes in 0.10.2</a> for more details. </li>
 </ul>
+<h5><a id=""upgrade_1020_streams_from_0100"" href=""#upgrade_1020_streams_from_0100"">Upgrading a 0.10.0 Kafka Streams Application</a></h5>
+<ul>
+    <li> Upgrading your Streams application from 0.10.0 to 0.10.2 does require a <a href=""#upgrade_10_1"">broker upgrade</a> because a Kafka Streams 0.10.2 application can only connect to 0.10.2 or 0.10.1 brokers. </li>
+    <li> There are couple of API changes, that are not backward compatible (cf. <a href=""/{{version}}/documentation/streams#streams_api_changes_0102"">Streams API changes in 0.10.2</a> for more details).
+         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <!-- TODO add if 0.10.2.2 gets release
+    <li> Upgrading from 0.10.0.x to 0.10.2.2 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+         (cf. <a href=""[link]"">KIP-268</a>).
+         As an alternative, an offline upgrade is also possible.
+        <ul>
+            <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.10.2.2 </li>
+            <li> bounce each instance of your application once </li>
+            <li> prepare your newly deployed 0.10.2.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+            <li> bounce each instance of your application once more to complete the upgrade </li>
+        </ul>
+    </li>
+    -->
+    <li> Upgrading from 0.10.0.x to 0.10.2.0 or 0.10.2.1 requires an offline upgrade (rolling bounce upgrade is not supported)
+        <ul>
+            <li> stop all old (0.10.0.x) application instances </li>
+            <li> update your code and swap old code and jar file with new code and new jar file </li>
+            <li> restart all new (0.10.2.0 or 0.10.2.1) application instances </li>
+        </ul>
+    </li>
+</ul>
+
+<!-- TODO add if 0.10.2.2 gets release
+<h5><a id=""upgrade_10202_notable"" href=""#upgrade_10202_notable"">Notable changes in 0.10.2.2</a></h5>
+<ul>
+<li> New configuration parameter <code>upgrade.from</code> added that allows rolling bounce upgrade from version 0.10.0.x </li>
+</ul>
+-->
+
 <h5><a id=""upgrade_10201_notable"" href=""#upgrade_10201_notable"">Notable changes in 0.10.2.1</a></h5>
 <ul>
   <li> The default values for two configurations of the StreamsConfig class were changed to improve the resiliency of Kafka Streams applications. The internal Kafka Streams producer <code>retries</code> default value was changed from 0 to 10. The internal Kafka Streams consumer <code>max.poll.interval.ms</code>  default value was changed from 300000 to <code>Integer.MAX_VALUE</code>.
@@ -552,7 +707,26 @@ <h5><a id=""upgrade_1010_streams"" href=""#upgrade_1010_streams"">Upgrading a 0.10.0
 <ul>
     <li> Upgrading your Streams application from 0.10.0 to 0.10.1 does require a <a href=""#upgrade_10_1"">broker upgrade</a> because a Kafka Streams 0.10.1 application can only connect to 0.10.1 brokers. </li>
     <li> There are couple of API changes, that are not backward compatible (cf. <a href=""/{{version}}/documentation/streams/upgrade-guide#streams_api_changes_0101"">Streams API changes in 0.10.1</a> for more details).
-         Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+     Thus, you need to update and recompile your code. Just swapping the Kafka Streams library jar file will not work and will break your application. </li>
+    <!-- TODO add if 0.10.1.2 gets release
+        <li> Upgrading from 0.10.0.x to 0.10.1.2 requires two rolling bounces with config <code>upgrade.from=""0.10.0""</code> set for first upgrade phase
+         (cf. <a href=""[link]"">KIP-268</a>).
+         As an alternative, an offline upgrade is also possible.
+            <ul>
+                <li> prepare your application instances for a rolling bounce and make sure that config <code>upgrade.from</code> is set to <code>""0.10.0""</code> for new version 0.10.1.2 </li>
+                <li> bounce each instance of your application once </li>
+                <li> prepare your newly deployed 0.10.1.2 application instances for a second round of rolling bounces; make sure to remove the value for config <code>upgrade.mode</code> </li>
+                <li> bounce each instance of your application once more to complete the upgrade </li>
+            </ul>
+        </li>
+        -->
+    <li> Upgrading from 0.10.0.x to 0.10.1.0 or 0.10.1.1 requires an offline upgrade (rolling bounce upgrade is not supported)
+    <ul>
+        <li> stop all old (0.10.0.x) application instances </li>
+        <li> update your code and swap old code and jar file with new code and new jar file </li>
+        <li> restart all new (0.10.1.0 or 0.10.1.1) application instances </li>
+    </ul>
+    </li>
 </ul>
 <h5><a id=""upgrade_1010_notable"" href=""#upgrade_1010_notable"">Notable changes in 0.10.1.0</a></h5>
@@ -596,14 +770,17 @@ <h5><a id=""upgrade_1010_new_protocols"" href=""#upgrade_1010_new_protocols"">New Pr
 </ul>
 <h4><a id=""upgrade_10"" href=""#upgrade_10"">Upgrading from 0.8.x or 0.9.x to 0.10.0.0</a></h4>
+<p>
 0.10.0.0 has <a href=""#upgrade_10_breaking"">potential breaking changes</a> (please review before upgrading) and possible <a href=""#upgrade_10_performance_impact"">  performance impact following the upgrade</a>. By following the recommended rolling upgrade plan below, you guarantee no downtime and no performance impact during and following the upgrade.
 <br>
 Note: Because new protocols are introduced, it is important to upgrade your Kafka clusters before upgrading your clients.
-<p/>
+</p>
+<p>
 <b>Notes to clients with version 0.9.0.0: </b>Due to a bug introduced in 0.9.0.0,
 clients that depend on ZooKeeper (old Scala high-level Consumer and MirrorMaker if used with the old consumer) will not
 work with 0.10.0.x brokers. Therefore, 0.9.0.0 clients should be upgraded to 0.9.0.1 <b>before</b> brokers are upgraded to
 0.10.0.x. This step is not necessary for 0.8.X or 0.9.0.1 clients.
+</p>
 <p><b>For a rolling upgrade:</b></p>
diff --git a/gradle/dependencies.gradle b/gradle/dependencies.gradle
index 32c0040d0ff..effe763ac45 100644
--- a/gradle/dependencies.gradle
+++ b/gradle/dependencies.gradle
@@ -62,6 +62,11 @@ versions += [
   jaxb: ""2.3.0"",
   jopt: ""5.0.4"",
   junit: ""4.12"",
+  kafka_0100: ""0.10.0.1"",
+  kafka_0101: ""0.10.1.1"",
+  kafka_0102: ""0.10.2.1"",
+  kafka_0110: ""0.11.0.2"",
+  kafka_10: ""1.0.1"",
   lz4: ""1.4.1"",
   metrics: ""2.2.0"",
   // PowerMock 1.x doesn't support Java 9, so use PowerMock 2.0.0 beta
@@ -101,12 +106,16 @@ libs += [
   jettyServlets: ""org.eclipse.jetty:jetty-servlets:$versions.jetty"",
   jerseyContainerServlet: ""org.glassfish.jersey.containers:jersey-container-servlet:$versions.jersey"",
   jmhCore: ""org.openjdk.jmh:jmh-core:$versions.jmh"",
-  jmhGeneratorAnnProcess: ""org.openjdk.jmh:jmh-generator-annprocess:$versions.jmh"",
   jmhCoreBenchmarks: ""org.openjdk.jmh:jmh-core-benchmarks:$versions.jmh"",
+  jmhGeneratorAnnProcess: ""org.openjdk.jmh:jmh-generator-annprocess:$versions.jmh"",
+  joptSimple: ""net.sf.jopt-simple:jopt-simple:$versions.jopt"",
   junit: ""junit:junit:$versions.junit"",
+  kafkaStreams_0100: ""org.apache.kafka:kafka-streams:$versions.kafka_0100"",
+  kafkaStreams_0101: ""org.apache.kafka:kafka-streams:$versions.kafka_0101"",
+  kafkaStreams_0102: ""org.apache.kafka:kafka-streams:$versions.kafka_0102"",
+  kafkaStreams_0110: ""org.apache.kafka:kafka-streams:$versions.kafka_0110"",
+  kafkaStreams_10: ""org.apache.kafka:kafka-streams:$versions.kafka_10"",
   log4j: ""log4j:log4j:$versions.log4j"",
-  scalaLogging: ""com.typesafe.scala-logging:scala-logging_$versions.baseScala:$versions.scalaLogging"",
-  joptSimple: ""net.sf.jopt-simple:jopt-simple:$versions.jopt"",
   lz4: ""org.lz4:lz4-java:$versions.lz4"",
   metrics: ""com.yammer.metrics:metrics-core:$versions.metrics"",
   powermockJunit4: ""org.powermock:powermock-module-junit4:$versions.powermock"",
@@ -114,6 +123,7 @@ libs += [
   reflections: ""org.reflections:reflections:$versions.reflections"",
   rocksDBJni: ""org.rocksdb:rocksdbjni:$versions.rocksDB"",
   scalaLibrary: ""org.scala-lang:scala-library:$versions.scala"",
+  scalaLogging: ""com.typesafe.scala-logging:scala-logging_$versions.baseScala:$versions.scalaLogging"",
   scalaReflect: ""org.scala-lang:scala-reflect:$versions.scala"",
   scalatest: ""org.scalatest:scalatest_$versions.baseScala:$versions.scalatest"",
   scoveragePlugin: ""org.scoverage:scalac-scoverage-plugin_$versions.baseScala:$versions.scoverage"",
diff --git a/settings.gradle b/settings.gradle
index e599d01215c..03136849fd5 100644
--- a/settings.gradle
+++ b/settings.gradle
@@ -13,5 +13,7 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
-include 'core', 'examples', 'clients', 'tools', 'streams', 'streams:test-utils', 'streams:examples', 'log4j-appender',
+include 'core', 'examples', 'clients', 'tools', 'streams', 'streams:test-utils', 'streams:examples',
+        'streams:upgrade-system-tests-0100', 'streams:upgrade-system-tests-0101', 'streams:upgrade-system-tests-0102',
+        'streams:upgrade-system-tests-0110', 'streams:upgrade-system-tests-10', 'log4j-appender',
         'connect:api', 'connect:transforms', 'connect:runtime', 'connect:json', 'connect:file', 'jmh-benchmarks'
diff --git [file java] [file java]
index 0a525169fa6..819bebd43b6 100644
--- [file java]
+++ [file java]
@@ -167,6 +167,11 @@
      */
     public static final String ADMIN_CLIENT_PREFIX = ""admin."";
+    /**
+     * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 0.10.0.x}.
+     */
+    public static final String UPGRADE_FROM_0100 = ""0.10.0"";
+
     /**
      * Config value for parameter {@link #PROCESSING_GUARANTEE_CONFIG ""processing.guarantee""} for at-least-once processing guarantees.
      */
@@ -340,6 +345,11 @@
     public static final String TIMESTAMP_EXTRACTOR_CLASS_CONFIG = ""timestamp.extractor"";
     private static final String TIMESTAMP_EXTRACTOR_CLASS_DOC = ""Timestamp extractor class that implements the <code>org.apache.kafka.streams.processor.TimestampExtractor</code> interface. This config is deprecated, use <code>"" + DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG + ""</code> instead"";
+    /** {@code upgrade.from} */
+    public static final String UPGRADE_FROM_CONFIG = ""upgrade.from"";
+    public static final String UPGRADE_FROM_DOC = ""Allows upgrading from version 0.10.0 to version 0.10.1 (or newer) in a backward compatible way. "" +
+        ""Default is null. Accepted values are \"""" + UPGRADE_FROM_0100 + ""\"" (for upgrading from 0.10.0.x)."";
+
     /**
      * {@code value.serde}
      * @deprecated Use {@link #DEFAULT_VALUE_SERDE_CLASS_CONFIG} instead.
@@ -562,6 +572,12 @@
                     10 * 60 * 1000L,
                     Importance.LOW,
                     STATE_CLEANUP_DELAY_MS_DOC)
+            .define(UPGRADE_FROM_CONFIG,
+                    ConfigDef.Type.STRING,
+                    null,
+                    in(null, UPGRADE_FROM_0100),
+                    Importance.LOW,
+                    UPGRADE_FROM_DOC)
             .define(WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG,
                     Type.LONG,
                     24 * 60 * 60 * 1000L,
@@ -793,6 +809,7 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map<String, Obje
         consumerProps.put(CommonClientConfigs.CLIENT_ID_CONFIG, clientId + ""-consumer"");
         // add configs required for stream partition assignor
+        consumerProps.put(UPGRADE_FROM_CONFIG, getString(UPGRADE_FROM_CONFIG));
         consumerProps.put(REPLICATION_FACTOR_CONFIG, getInt(REPLICATION_FACTOR_CONFIG));
         consumerProps.put(APPLICATION_SERVER_CONFIG, getString(APPLICATION_SERVER_CONFIG));
         consumerProps.put(NUM_STANDBY_REPLICAS_CONFIG, getInt(NUM_STANDBY_REPLICAS_CONFIG));
diff --git [file java] [file java]
index 0edbe2f523d..97771e56879 100644
--- [file java]
+++ [file java]
@@ -179,6 +179,8 @@ public int compare(final TopicPartition p1,
     private TaskManager taskManager;
     private PartitionGrouper partitionGrouper;
+    private int userMetadataVersion = SubscriptionInfo.LATEST_SUPPORTED_VERSION;
+
     private InternalTopicManager internalTopicManager;
     private CopartitionedTopicsValidator copartitionedTopicsValidator;
@@ -197,6 +199,12 @@ public void configure(final Map<String, ?> configs) {
         final LogContext logContext = new LogContext(logPrefix);
         log = logContext.logger(getClass());
+        final String upgradeMode = (String) configs.get(StreamsConfig.UPGRADE_FROM_CONFIG);
+        if (StreamsConfig.UPGRADE_FROM_0100.equals(upgradeMode)) {
+            log.info(""Downgrading metadata version from 2 to 1 for upgrade from 0.10.0.x."");
+            userMetadataVersion = 1;
+        }
+
         final Object o = configs.get(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR);
         if (o == null) {
             final KafkaException fatalException = new KafkaException(""TaskManager is not specified"");
@@ -255,6 +263,7 @@ public Subscription subscription(final Set<String> topics) {
         final Set<TaskId> standbyTasks = taskManager.cachedTasksIds();
         standbyTasks.removeAll(previousActiveTasks);
         final SubscriptionInfo data = new SubscriptionInfo(
+            userMetadataVersion,
             taskManager.processId(),
             previousActiveTasks,
             standbyTasks,
diff --git [file java] [file java]
index 0309659042d..87f70759220 100644
--- [file java]
+++ [file java]
@@ -472,6 +472,7 @@ public void shouldNotOverrideUserConfigCommitIntervalMsIfExactlyOnceEnabled() {
         assertThat(streamsConfig.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG), equalTo(commitIntervalMs));
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldBeBackwardsCompatibleWithDeprecatedConfigs() {
         final Properties props = minimalStreamsConfig();
@@ -506,6 +507,7 @@ public void shouldUseCorrectDefaultsWhenNoneSpecified() {
         assertTrue(config.defaultTimestampExtractor() instanceof FailOnInvalidTimestamp);
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldSpecifyCorrectKeySerdeClassOnErrorUsingDeprecatedConfigs() {
         final Properties props = minimalStreamsConfig();
@@ -519,6 +521,7 @@ public void shouldSpecifyCorrectKeySerdeClassOnErrorUsingDeprecatedConfigs() {
         }
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldSpecifyCorrectKeySerdeClassOnError() {
         final Properties props = minimalStreamsConfig();
@@ -532,6 +535,7 @@ public void shouldSpecifyCorrectKeySerdeClassOnError() {
         }
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldSpecifyCorrectValueSerdeClassOnErrorUsingDeprecatedConfigs() {
         final Properties props = minimalStreamsConfig();
@@ -545,6 +549,7 @@ public void shouldSpecifyCorrectValueSerdeClassOnErrorUsingDeprecatedConfigs() {
         }
     }
+    @SuppressWarnings(""deprecation"")
     @Test
     public void shouldSpecifyCorrectValueSerdeClassOnError() {
         final Properties props = minimalStreamsConfig();
@@ -567,9 +572,7 @@ public void configure(final Map configs, final boolean isKey) {
         }
         @Override
-        public void close() {
-
-        }
+        public void close() {}
         @Override
         public Serializer serializer() {
diff --git [file java] [file java]
index 4c12bb93544..44e139a28bd 100644
--- [file java]
+++ [file java]
@@ -313,6 +313,4 @@ private void startStreams() {
     }
-
-
 }
diff --git [file java] [file java]
index b0c0d68287b..e9ed9682066 100644
--- [file java]
+++ [file java]
@@ -131,7 +131,7 @@ private void configurePartitionAssignor(final Map<String, Object> props) {
     private void mockTaskManager(final Set<TaskId> prevTasks,
                                  final Set<TaskId> cachedTasks,
                                  final UUID processId,
-                                 final InternalTopologyBuilder builder) throws NoSuchFieldException, IllegalAccessException {
+                                 final InternalTopologyBuilder builder) {
         EasyMock.expect(taskManager.builder()).andReturn(builder).anyTimes();
         EasyMock.expect(taskManager.prevActiveTaskIds()).andReturn(prevTasks).anyTimes();
         EasyMock.expect(taskManager.cachedTasksIds()).andReturn(cachedTasks).anyTimes();
@@ -167,7 +167,7 @@ public void shouldInterleaveTasksByGroupId() {
     }
     @Test
-    public void testSubscription() throws Exception {
+    public void testSubscription() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
@@ -195,7 +195,7 @@ public void testSubscription() throws Exception {
     }
     @Test
-    public void testAssignBasic() throws Exception {
+    public void testAssignBasic() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
@@ -235,11 +235,9 @@ public void testAssignBasic() throws Exception {
         // check assignment info
-        Set<TaskId> allActiveTasks = new HashSet<>();
-
         // the first consumer
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks());
+        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks());
         // the second consumer
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(""consumer11""));
@@ -259,7 +257,7 @@ public void testAssignBasic() throws Exception {
     }
     @Test
-    public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() throws Exception {
+    public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"");
@@ -327,7 +325,7 @@ public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() throws E
     }
     @Test
-    public void testAssignWithPartialTopology() throws Exception {
+    public void testAssignWithPartialTopology() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addProcessor(""processor1"", new MockProcessorSupplier(), ""source1"");
         builder.addStateStore(new MockStateStoreSupplier(""store1"", false), ""processor1"");
@@ -352,9 +350,8 @@ public void testAssignWithPartialTopology() throws Exception {
         Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
         // check assignment info
-        Set<TaskId> allActiveTasks = new HashSet<>();
         AssignmentInfo info10 = checkAssignment(Utils.mkSet(""topic1""), assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks());
+        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks());
         assertEquals(3, allActiveTasks.size());
         assertEquals(allTasks, new HashSet<>(allActiveTasks));
@@ -362,7 +359,7 @@ public void testAssignWithPartialTopology() throws Exception {
     @Test
-    public void testAssignEmptyMetadata() throws Exception {
+    public void testAssignEmptyMetadata() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
@@ -392,9 +389,8 @@ public void testAssignEmptyMetadata() throws Exception {
             new HashSet<>(assignments.get(""consumer10"").partitions()));
         // check assignment info
-        Set<TaskId> allActiveTasks = new HashSet<>();
         AssignmentInfo info10 = checkAssignment(Collections.<String>emptySet(), assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks());
+        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks());
         assertEquals(0, allActiveTasks.size());
         assertEquals(Collections.<TaskId>emptySet(), new HashSet<>(allActiveTasks));
@@ -417,7 +413,7 @@ public void testAssignEmptyMetadata() throws Exception {
     }
     @Test
-    public void testAssignWithNewTasks() throws Exception {
+    public void testAssignWithNewTasks() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addSource(null, ""source3"", null, null, null, ""topic3"");
@@ -450,13 +446,9 @@ public void testAssignWithNewTasks() throws Exception {
         // check assigned partitions: since there is no previous task for topic 3 it will be assigned randomly so we cannot check exact match
         // also note that previously assigned partitions / tasks may not stay on the previous host since we may assign the new task first and
         // then later ones will be re-assigned to other hosts due to load balancing
-        Set<TaskId> allActiveTasks = new HashSet<>();
-        Set<TopicPartition> allPartitions = new HashSet<>();
-        AssignmentInfo info;
-
-        info = AssignmentInfo.decode(assignments.get(""consumer10"").userData());
-        allActiveTasks.addAll(info.activeTasks());
-        allPartitions.addAll(assignments.get(""consumer10"").partitions());
+        AssignmentInfo info = AssignmentInfo.decode(assignments.get(""consumer10"").userData());
+        Set<TaskId> allActiveTasks = new HashSet<>(info.activeTasks());
+        Set<TopicPartition> allPartitions = new HashSet<>(assignments.get(""consumer10"").partitions());
         info = AssignmentInfo.decode(assignments.get(""consumer11"").userData());
         allActiveTasks.addAll(info.activeTasks());
@@ -471,7 +463,7 @@ public void testAssignWithNewTasks() throws Exception {
     }
     @Test
-    public void testAssignWithStates() throws Exception {
+    public void testAssignWithStates() {
         builder.setApplicationId(applicationId);
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
@@ -542,7 +534,10 @@ public void testAssignWithStates() throws Exception {
         assertEquals(Utils.mkSet(task10, task11, task12), tasksForState(applicationId, ""store3"", tasks, topicGroups));
     }
-    private Set<TaskId> tasksForState(String applicationId, String storeName, List<TaskId> tasks, Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups) {
+    private Set<TaskId> tasksForState(final String applicationId,
+                                      final String storeName,
+                                      final List<TaskId> tasks,
+                                      final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups) {
         final String changelogTopic = ProcessorStateManager.storeChangelogTopic(applicationId, storeName);
         Set<TaskId> ids = new HashSet<>();
@@ -560,7 +555,7 @@ public void testAssignWithStates() throws Exception {
     }
     @Test
-    public void testAssignWithStandbyReplicas() throws Exception {
+    public void testAssignWithStandbyReplicas() {
         Map<String, Object> props = configProps();
         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, ""1"");
         StreamsConfig streamsConfig = new StreamsConfig(props);
@@ -598,13 +593,10 @@ public void testAssignWithStandbyReplicas() throws Exception {
         Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
-        Set<TaskId> allActiveTasks = new HashSet<>();
-        Set<TaskId> allStandbyTasks = new HashSet<>();
-
         // the first consumer
         AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(""consumer10""));
-        allActiveTasks.addAll(info10.activeTasks());
-        allStandbyTasks.addAll(info10.standbyTasks().keySet());
+        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks());
+        Set<TaskId> allStandbyTasks = new HashSet<>(info10.standbyTasks().keySet());
         // the second consumer
         AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(""consumer11""));
@@ -632,7 +624,7 @@ public void testAssignWithStandbyReplicas() throws Exception {
     }
     @Test
-    public void testOnAssignment() throws Exception {
+    public void testOnAssignment() {
         configurePartitionAssignor(Collections.<String, Object>emptyMap());
         final List<TaskId> activeTaskList = Utils.mkList(task0, task3);
@@ -667,7 +659,7 @@ public void testOnAssignment() throws Exception {
     }
     @Test
-    public void testAssignWithInternalTopics() throws Exception {
+    public void testAssignWithInternalTopics() {
         builder.setApplicationId(applicationId);
         builder.addInternalTopic(""topicX"");
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
@@ -697,7 +689,7 @@ public void testAssignWithInternalTopics() throws Exception {
     }
     @Test
-    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throws Exception {
+    public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() {
         String applicationId = ""test"";
         builder.setApplicationId(applicationId);
         builder.addInternalTopic(""topicX"");
@@ -732,7 +724,7 @@ public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() throw
     }
     @Test
-    public void shouldGenerateTasksForAllCreatedPartitions() throws Exception {
+    public void shouldGenerateTasksForAllCreatedPartitions() {
         final StreamsBuilder builder = new StreamsBuilder();
         final InternalTopologyBuilder internalTopologyBuilder = StreamsBuilderTest.internalTopologyBuilder(builder);
@@ -832,7 +824,7 @@ public Object apply(final Object value1, final Object value2) {
     }
     @Test
-    public void shouldAddUserDefinedEndPointToSubscription() throws Exception {
+    public void shouldAddUserDefinedEndPointToSubscription() {
         builder.setApplicationId(applicationId);
         builder.addSource(null, ""source"", null, null, null, ""input"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source"");
@@ -851,7 +843,7 @@ public void shouldAddUserDefinedEndPointToSubscription() throws Exception {
     }
     @Test
-    public void shouldMapUserEndPointToTopicPartitions() throws Exception {
+    public void shouldMapUserEndPointToTopicPartitions() {
         builder.setApplicationId(applicationId);
         builder.addSource(null, ""source"", null, null, null, ""topic1"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source"");
@@ -881,7 +873,7 @@ public void shouldMapUserEndPointToTopicPartitions() throws Exception {
     }
     @Test
-    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() throws Exception {
+    public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() {
         builder.setApplicationId(applicationId);
         mockTaskManager(Collections.<TaskId>emptySet(), Collections.<TaskId>emptySet(), UUID.randomUUID(), builder);
@@ -908,7 +900,7 @@ public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() {
     }
     @Test
-    public void shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks() throws Exception {
+    public void shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks() {
         final StreamsBuilder builder = new StreamsBuilder();
         final InternalTopologyBuilder internalTopologyBuilder = StreamsBuilderTest.internalTopologyBuilder(builder);
@@ -1010,7 +1002,7 @@ public Object apply(final Object value1, final Object value2) {
     }
     @Test
-    public void shouldUpdateClusterMetadataAndHostInfoOnAssignment() throws Exception {
+    public void shouldUpdateClusterMetadataAndHostInfoOnAssignment() {
         final TopicPartition partitionOne = new TopicPartition(""topic"", 1);
         final TopicPartition partitionTwo = new TopicPartition(""topic"", 2);
         final Map<HostInfo, Set<TopicPartition>> hostState = Collections.singletonMap(
@@ -1028,7 +1020,7 @@ public void shouldUpdateClusterMetadataAndHostInfoOnAssignment() throws Exceptio
     }
     @Test
-    public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() throws Exception {
+    public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() {
         final StreamsBuilder builder = new StreamsBuilder();
         final InternalTopologyBuilder internalTopologyBuilder = StreamsBuilderTest.internalTopologyBuilder(builder);
@@ -1096,7 +1088,7 @@ public void shouldThrowKafkaExceptionIfStreamThreadConfigIsNotThreadDataProvider
     }
     @Test
-    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() throws Exception {
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() {
         final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
         final Set<TaskId> emptyTasks = Collections.emptySet();
         subscriptions.put(
@@ -1114,10 +1106,11 @@ public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions(
             )
         );
-        mockTaskManager(Collections.<TaskId>emptySet(),
-            Collections.<TaskId>emptySet(),
+        mockTaskManager(
+            emptyTasks,
+            emptyTasks,
             UUID.randomUUID(),
-            new InternalTopologyBuilder());
+            builder);
         partitionAssignor.configure(configProps());
         final Map<String, PartitionAssignor.Assignment> assignment = partitionAssignor.assign(metadata, subscriptions);
@@ -1126,6 +1119,22 @@ public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions(
         assertThat(AssignmentInfo.decode(assignment.get(""consumer2"").userData()).version(), equalTo(1));
     }
+    @Test
+    public void shouldDownGradeSubscription() {
+        final Set<TaskId> emptyTasks = Collections.emptySet();
+
+        mockTaskManager(
+            emptyTasks,
+            emptyTasks,
+            UUID.randomUUID(),
+            builder);
+        configurePartitionAssignor(Collections.singletonMap(StreamsConfig.UPGRADE_FROM_CONFIG, (Object) StreamsConfig.UPGRADE_FROM_0100));
+
+        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""topic1""));
+
+        assertThat(SubscriptionInfo.decode(subscription.userData()).version(), equalTo(1));
+    }
+
     private PartitionAssignor.Assignment createAssignment(final Map<HostInfo, Set<TopicPartition>> firstHostState) {
         final AssignmentInfo info = new AssignmentInfo(Collections.<TaskId>emptyList(),
                                                        Collections.<TaskId, Set<TopicPartition>>emptyMap(),
diff --git [file java] [file java]
index 726a5623cd5..c1020a98ba9 100644
--- [file java]
+++ [file java]
@@ -68,7 +68,6 @@ public void shouldDecodePreviousVersion() throws IOException {
         assertEquals(1, decoded.version());
     }
-
     /**
      * This is a clone of what the V1 encoding did. The encode method has changed for V2
      * so it is impossible to test compatibility without having this
diff --git [file java] [file java]
index e2493b2c5a0..2936c63d8f8 100644
--- [file java]
+++ [file java]
@@ -19,6 +19,7 @@
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 import org.apache.kafka.clients.producer.ProducerConfig;
 import org.apache.kafka.common.serialization.Serdes;
+import org.apache.kafka.common.utils.Bytes;
 import org.apache.kafka.streams.Consumed;
 import org.apache.kafka.streams.KafkaStreams;
 import org.apache.kafka.streams.StreamsBuilder;
@@ -30,10 +31,13 @@
 import org.apache.kafka.streams.kstream.KTable;
 import org.apache.kafka.streams.kstream.Materialized;
 import org.apache.kafka.streams.kstream.Predicate;
+import org.apache.kafka.streams.kstream.Produced;
 import org.apache.kafka.streams.kstream.Serialized;
 import org.apache.kafka.streams.kstream.TimeWindows;
 import org.apache.kafka.streams.kstream.ValueJoiner;
+import org.apache.kafka.streams.state.KeyValueStore;
 import org.apache.kafka.streams.state.Stores;
+import org.apache.kafka.streams.state.WindowStore;
 import java.util.Properties;
 import java.util.concurrent.TimeUnit;
@@ -56,7 +60,7 @@ public void start() {
         streams = createKafkaStreams(streamsProperties, kafka);
         streams.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
             @Override
-            public void uncaughtException(Thread t, Throwable e) {
+            public void uncaughtException(final Thread t, final Throwable e) {
                 System.out.println(""SMOKE-TEST-CLIENT-EXCEPTION"");
                 uncaughtException = true;
                 e.printStackTrace();
@@ -93,38 +97,45 @@ public void close() {
         }
     }
-    private static KafkaStreams createKafkaStreams(final Properties props, final String kafka) {
-        props.put(StreamsConfig.APPLICATION_ID_CONFIG, ""SmokeTest"");
-        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
-        props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 3);
-        props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);
-        props.put(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, 100);
-        props.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
-        props.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);
-        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");
-        props.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);
-        props.put(ProducerConfig.ACKS_CONFIG, ""all"");
+    private static Properties getStreamsConfig(final Properties props, final String kafka) {
+        final Properties config = new Properties(props);
+        config.put(StreamsConfig.APPLICATION_ID_CONFIG, ""SmokeTest"");
+        config.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 3);
+        config.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 2);
+        config.put(StreamsConfig.BUFFERED_RECORDS_PER_PARTITION_CONFIG, 100);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        config.put(StreamsConfig.REPLICATION_FACTOR_CONFIG, 3);
+        config.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, ""earliest"");
+        config.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);
+        config.put(ProducerConfig.ACKS_CONFIG, ""all"");
         //TODO remove this config or set to smaller value when KIP-91 is merged
-        props.put(StreamsConfig.producerPrefix(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG), 80000);
+        config.put(StreamsConfig.producerPrefix(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG), 80000);
+
+        config.putAll(props);
+        return config;
+    }
-        StreamsBuilder builder = new StreamsBuilder();
-        Consumed<String, Integer> stringIntConsumed = Consumed.with(stringSerde, intSerde);
-        KStream<String, Integer> source = builder.stream(""data"", stringIntConsumed);
-        source.to(stringSerde, intSerde, ""echo"");
-        KStream<String, Integer> data = source.filter(new Predicate<String, Integer>() {
+    private static KafkaStreams createKafkaStreams(final Properties props, final String kafka) {
+        final StreamsBuilder builder = new StreamsBuilder();
+        final Consumed<String, Integer> stringIntConsumed = Consumed.with(stringSerde, intSerde);
+        final KStream<String, Integer> source = builder.stream(""data"", stringIntConsumed);
+        source.to(""echo"", Produced.with(stringSerde, intSerde));
+        final KStream<String, Integer> data = source.filter(new Predicate<String, Integer>() {
             @Override
-            public boolean test(String key, Integer value) {
+            public boolean test(final String key, final Integer value) {
                 return value == null || value != END;
             }
         });
         data.process(SmokeTestUtil.printProcessorSupplier(""data""));
         // min
-        KGroupedStream<String, Integer>
-            groupedData =
+        final KGroupedStream<String, Integer> groupedData =
             data.groupByKey(Serialized.with(stringSerde, intSerde));
-        groupedData.aggregate(
+        groupedData
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(1)))
+            .aggregate(
                 new Initializer<Integer>() {
                     public Integer apply() {
                         return Integer.MAX_VALUE;
@@ -132,21 +143,24 @@ public Integer apply() {
                 },
                 new Aggregator<String, Integer, Integer>() {
                     @Override
-                    public Integer apply(String aggKey, Integer value, Integer aggregate) {
+                    public Integer apply(final String aggKey, final Integer value, final Integer aggregate) {
                         return (value < aggregate) ? value : aggregate;
                     }
                 },
-                TimeWindows.of(TimeUnit.DAYS.toMillis(1)),
-                intSerde, ""uwin-min""
-        ).toStream().map(
-                new Unwindow<String, Integer>()
-        ).to(stringSerde, intSerde, ""min"");
+                Materialized.<String, Integer, WindowStore<Bytes, byte>>as(""uwin-min"").withValueSerde(intSerde))
+            .toStream(new Unwindow<String, Integer>())
+            .to(""min"", Produced.with(stringSerde, intSerde));
-        KTable<String, Integer> minTable = builder.table(""min"", stringIntConsumed);
+        final KTable<String, Integer> minTable = builder.table(
+            ""min"",
+            Consumed.with(stringSerde, intSerde),
+            Materialized.<String, Integer, KeyValueStore<Bytes, byte>>as(""minStoreName""));
         minTable.toStream().process(SmokeTestUtil.printProcessorSupplier(""min""));
         // max
-        groupedData.aggregate(
+        groupedData
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))
+            .aggregate(
                 new Initializer<Integer>() {
                     public Integer apply() {
                         return Integer.MIN_VALUE;
@@ -154,21 +168,24 @@ public Integer apply() {
                 },
                 new Aggregator<String, Integer, Integer>() {
                     @Override
-                    public Integer apply(String aggKey, Integer value, Integer aggregate) {
+                    public Integer apply(final String aggKey, final Integer value, final Integer aggregate) {
                         return (value > aggregate) ? value : aggregate;
                     }
                 },
-                TimeWindows.of(TimeUnit.DAYS.toMillis(2)),
-                intSerde, ""uwin-max""
-        ).toStream().map(
-                new Unwindow<String, Integer>()
-        ).to(stringSerde, intSerde, ""max"");
+                Materialized.<String, Integer, WindowStore<Bytes, byte>>as(""uwin-max"").withValueSerde(intSerde))
+            .toStream(new Unwindow<String, Integer>())
+            .to(""max"", Produced.with(stringSerde, intSerde));
-        KTable<String, Integer> maxTable = builder.table(""max"", stringIntConsumed);
+        final KTable<String, Integer> maxTable = builder.table(
+            ""max"",
+            Consumed.with(stringSerde, intSerde),
+            Materialized.<String, Integer, KeyValueStore<Bytes, byte>>as(""maxStoreName""));
         maxTable.toStream().process(SmokeTestUtil.printProcessorSupplier(""max""));
         // sum
-        groupedData.aggregate(
+        groupedData
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))
+            .aggregate(
                 new Initializer<Long>() {
                     public Long apply() {
                         return 0L;
@@ -176,70 +193,74 @@ public Long apply() {
                 },
                 new Aggregator<String, Integer, Long>() {
                     @Override
-                    public Long apply(String aggKey, Integer value, Long aggregate) {
+                    public Long apply(final String aggKey, final Integer value, final Long aggregate) {
                         return (long) value + aggregate;
                     }
                 },
-                TimeWindows.of(TimeUnit.DAYS.toMillis(2)),
-                longSerde, ""win-sum""
-        ).toStream().map(
-                new Unwindow<String, Long>()
-        ).to(stringSerde, longSerde, ""sum"");
-
-        Consumed<String, Long> stringLongConsumed = Consumed.with(stringSerde, longSerde);
-        KTable<String, Long> sumTable = builder.table(""sum"", stringLongConsumed);
+                Materialized.<String, Long, WindowStore<Bytes, byte>>as(""win-sum"").withValueSerde(longSerde))
+            .toStream(new Unwindow<String, Long>())
+            .to(""sum"", Produced.with(stringSerde, longSerde));
+
+        final Consumed<String, Long> stringLongConsumed = Consumed.with(stringSerde, longSerde);
+        final KTable<String, Long> sumTable = builder.table(""sum"", stringLongConsumed);
         sumTable.toStream().process(SmokeTestUtil.printProcessorSupplier(""sum""));
+
         // cnt
-        groupedData.count(TimeWindows.of(TimeUnit.DAYS.toMillis(2)), ""uwin-cnt"")
-            .toStream().map(
-                new Unwindow<String, Long>()
-        ).to(stringSerde, longSerde, ""cnt"");
+        groupedData
+            .windowedBy(TimeWindows.of(TimeUnit.DAYS.toMillis(2)))
+            .count(Materialized.<String, Long, WindowStore<Bytes, byte>>as(""uwin-cnt""))
+            .toStream(new Unwindow<String, Long>())
+            .to(""cnt"", Produced.with(stringSerde, longSerde));
-        KTable<String, Long> cntTable = builder.table(""cnt"", stringLongConsumed);
+        final KTable<String, Long> cntTable = builder.table(
+            ""cnt"",
+            Consumed.with(stringSerde, longSerde),
+            Materialized.<String, Long, KeyValueStore<Bytes, byte>>as(""cntStoreName""));
         cntTable.toStream().process(SmokeTestUtil.printProcessorSupplier(""cnt""));
         // dif
-        maxTable.join(minTable,
+        maxTable
+            .join(
+                minTable,
                 new ValueJoiner<Integer, Integer, Integer>() {
-                    public Integer apply(Integer value1, Integer value2) {
+                    public Integer apply(final Integer value1, final Integer value2) {
                         return value1 - value2;
                     }
-                }
-        ).to(stringSerde, intSerde, ""dif"");
+                })
+            .toStream()
+            .to(""dif"", Produced.with(stringSerde, intSerde));
         // avg
-        sumTable.join(
+        sumTable
+            .join(
                 cntTable,
                 new ValueJoiner<Long, Long, Double>() {
-                    public Double apply(Long value1, Long value2) {
+                    public Double apply(final Long value1, final Long value2) {
                         return (double) value1 / (double) value2;
                     }
-                }
-        ).to(stringSerde, doubleSerde, ""avg"");
+                })
+            .toStream()
+            .to(""avg"", Produced.with(stringSerde, doubleSerde));
         // test repartition
-        Agg agg = new Agg();
-        cntTable.groupBy(agg.selector(),
-                         Serialized.with(stringSerde, longSerde)
-        ).aggregate(agg.init(),
-                    agg.adder(),
-                    agg.remover(),
-                    Materialized.<String, Long>as(Stores.inMemoryKeyValueStore(""cntByCnt""))
-                            .withKeySerde(Serdes.String())
-                            .withValueSerde(Serdes.Long())
-        ).to(stringSerde, longSerde, ""tagg"");
-
-        final KafkaStreams streamsClient = new KafkaStreams(builder.build(), props);
+        final Agg agg = new Agg();
+        cntTable.groupBy(agg.selector(), Serialized.with(stringSerde, longSerde))
+            .aggregate(agg.init(), agg.adder(), agg.remover(),
+                Materialized.<String, Long>as(Stores.inMemoryKeyValueStore(""cntByCnt""))
+                    .withKeySerde(Serdes.String())
+                    .withValueSerde(Serdes.Long()))
+            .toStream()
+            .to(""tagg"", Produced.with(stringSerde, longSerde));
+
+        final KafkaStreams streamsClient = new KafkaStreams(builder.build(), getStreamsConfig(props, kafka));
         streamsClient.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
             @Override
-            public void uncaughtException(Thread t, Throwable e) {
+            public void uncaughtException(final Thread t, final Throwable e) {
                 System.out.println(""FATAL: An unexpected exception is encountered on thread "" + t + "": "" + e);
-                
                 streamsClient.close(30, TimeUnit.SECONDS);
             }
         });
         return streamsClient;
     }
-
 }
diff --git [file java] [file java]
index a3f520a82db..fc7a26ee7a2 100644
--- [file java]
+++ [file java]
@@ -136,53 +136,65 @@ public void run() {
         System.out.println(""shutdown"");
     }
-    public static Map<String, Set<Integer>> generate(String kafka, final int numKeys, final int maxRecordsPerKey) {
+    public static Map<String, Set<Integer>> generate(final String kafka,
+                                                     final int numKeys,
+                                                     final int maxRecordsPerKey) {
+        return generate(kafka, numKeys, maxRecordsPerKey, true);
+    }
+
+    public static Map<String, Set<Integer>> generate(final String kafka,
+                                                     final int numKeys,
+                                                     final int maxRecordsPerKey,
+                                                     final boolean autoTerminate) {
         final Properties producerProps = new Properties();
         producerProps.put(ProducerConfig.CLIENT_ID_CONFIG, ""SmokeTest"");
         producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
         producerProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
         producerProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);
-        // the next 4 config values make sure that all records are produced with no loss and
-        // no duplicates
+        // the next 2 config values make sure that all records are produced with no loss and no duplicates
         producerProps.put(ProducerConfig.RETRIES_CONFIG, Integer.MAX_VALUE);
         producerProps.put(ProducerConfig.ACKS_CONFIG, ""all"");
         producerProps.put(ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, 80000);
-        KafkaProducer<byte, byte> producer = new KafkaProducer<>(producerProps);
+        final KafkaProducer<byte, byte> producer = new KafkaProducer<>(producerProps);
         int numRecordsProduced = 0;
-        Map<String, Set<Integer>> allData = new HashMap<>();
-        ValueList data = new ValueList;
+        final Map<String, Set<Integer>> allData = new HashMap<>();
+        final ValueList data = new ValueList;
         for (int i = 0; i < numKeys; i++) {
             data = new ValueList(i, i + maxRecordsPerKey - 1);
             allData.put(data.key, new HashSet<Integer>());
         }
-        Random rand = new Random();
+        final Random rand = new Random();
-        int remaining = data.length;
+        int remaining = 1; // dummy value must be positive if <autoTerminate> is false
+        if (autoTerminate) {
+            remaining = data.length;
+        }
         List<ProducerRecord<byte, byte>> needRetry = new ArrayList<>();
         while (remaining > 0) {
-            int index = rand.nextInt(remaining);
-            String key = data.key;
+            final int index = autoTerminate ? rand.nextInt(remaining) : rand.nextInt(numKeys);
+            final String key = data.key;
             int value = data.next();
-            if (value < 0) {
+            if (autoTerminate && value < 0) {
                 remaining--;
                 data = data;
             } else {
-                ProducerRecord<byte, byte> record =
-                        new ProducerRecord<>(""data"", stringSerde.serializer().serialize("""", key), intSerde.serializer().serialize("""", value));
+                final ProducerRecord<byte, byte> record =
+                    new ProducerRecord<>(""data"", stringSerde.serializer().serialize("""", key), intSerde.serializer().serialize("""", value));
                 producer.send(record, new TestCallback(record, needRetry));
                 numRecordsProduced++;
                 allData.get(key).add(value);
-                if (numRecordsProduced % 100 == 0)
+                if (numRecordsProduced % 100 == 0) {
                     System.out.println(numRecordsProduced + "" records produced"");
+                }
                 Utils.sleep(2);
             }
         }
diff --git [file java] [file java]
index dc4c91b4097..87ca82918a9 100644
--- [file java]
+++ [file java]
@@ -44,20 +44,15 @@
             public Processor<Object, Object> get() {
                 return new AbstractProcessor<Object, Object>() {
                     private int numRecordsProcessed = 0;
-                    private ProcessorContext context;
                     @Override
                     public void init(final ProcessorContext context) {
                         System.out.println(""initializing processor: topic="" + topic + "" taskId="" + context.taskId());
                         numRecordsProcessed = 0;
-                        this.context = context;
                     }
                     @Override
                     public void process(final Object key, final Object value) {
-                        if (printOffset) {
-                            System.out.println("">>> "" + context.offset());
-                        }
                         numRecordsProcessed++;
                         if (numRecordsProcessed % 100 == 0) {
                             System.out.println(System.currentTimeMillis());
@@ -66,19 +61,19 @@ public void process(final Object key, final Object value) {
                     }
                     @Override
-                    public void punctuate(final long timestamp) { }
+                    public void punctuate(final long timestamp) {}
                     @Override
-                    public void close() { }
+                    public void close() {}
                 };
             }
         };
     }
-    public static final class Unwindow<K, V> implements KeyValueMapper<Windowed<K>, V, KeyValue<K, V>> {
+    public static final class Unwindow<K, V> implements KeyValueMapper<Windowed<K>, V, K> {
         @Override
-        public KeyValue<K, V> apply(final Windowed<K> winKey, final V value) {
-            return new KeyValue<>(winKey.key(), value);
+        public K apply(final Windowed<K> winKey, final V value) {
+            return winKey.key();
         }
     }
diff --git [file java] [file java]
index 27aba29bba6..41c3f6c58f0 100644
--- [file java]
+++ [file java]
@@ -18,7 +18,6 @@
 import org.apache.kafka.common.utils.Utils;
-import java.io.IOException;
 import java.util.Map;
 import java.util.Properties;
 import java.util.Set;
@@ -26,22 +25,24 @@
 public class StreamsSmokeTest {
     /**
-     *  args ::= kafka propFileName command
+     *  args ::= kafka propFileName command disableAutoTerminate
      *  command := ""run"" | ""process""
      *
      * @param args
      */
-    public static void main(final String args) throws InterruptedException, IOException {
+    public static void main(final String args) throws Exception {
         final String kafka = args;
         final String propFileName = args.length > 1 ? args : null;
         final String command = args.length > 2 ? args : null;
+        final boolean disableAutoTerminate = args.length > 3;
         final Properties streamsProperties = Utils.loadProps(propFileName);
-        System.out.println(""StreamsTest instance started"");
+        System.out.println(""StreamsTest instance started (StreamsSmokeTest)"");
         System.out.println(""command="" + command);
         System.out.println(""kafka="" + kafka);
         System.out.println(""props="" + streamsProperties);
+        System.out.println(""disableAutoTerminate="" + disableAutoTerminate);
         switch (command) {
             case ""standalone"":
@@ -51,8 +52,12 @@ public static void main(final String args) throws InterruptedException, IOExce
                 // this starts the driver (data generation and result verification)
                 final int numKeys = 10;
                 final int maxRecordsPerKey = 500;
-                Map<String, Set<Integer>> allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
-                SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                if (disableAutoTerminate) {
+                    SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey, false);
+                } else {
+                    Map<String, Set<Integer>> allData = SmokeTestDriver.generate(kafka, numKeys, maxRecordsPerKey);
+                    SmokeTestDriver.verify(kafka, allData, maxRecordsPerKey);
+                }
                 break;
             case ""process"":
                 // this starts a KafkaStreams client
diff --git [file java] [file java]
new file mode 100644
index 00000000000..69eea0b37c0
--- /dev/null
+++ [file java]
@@ -0,0 +1,69 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.common.utils.Utils;
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsBuilder;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) throws Exception {
+        if (args.length < 2) {
+            System.err.println(""StreamsUpgradeTest requires two argument (kafka-url, properties-file) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args : """"));
+        }
+        final String kafka = args;
+        final String propFileName = args.length > 1 ? args : null;
+
+        final Properties streamsProperties = Utils.loadProps(propFileName);
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest trunk)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""props="" + streamsProperties);
+
+        final StreamsBuilder builder = new StreamsBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(SmokeTestUtil.printProcessorSupplier(""data""));
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        config.putAll(streamsProperties);
+
+        final KafkaStreams streams = new KafkaStreams(builder.build(), config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..32f96a01d99
--- /dev/null
+++ [file java]
@@ -0,0 +1,107 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.common.utils.Utils;
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) throws Exception {
+        if (args.length < 3) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, properties-file) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args + "" "" : """")
+                + (args.length > 1 ? args : """"));
+        }
+        final String kafka = args;
+        final String zookeeper = args;
+        final String propFileName = args.length > 2 ? args : null;
+
+        final Properties streamsProperties = Utils.loadProps(propFileName);
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.10.0)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""zookeeper="" + zookeeper);
+        System.out.println(""props="" + streamsProperties);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        config.putAll(streamsProperties);
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..dcb05ca5d72
--- /dev/null
+++ [file java]
@@ -0,0 +1,110 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.common.utils.Utils;
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    /**
+     * This test cannot be executed, as long as Kafka 0.10.1.2 is not released
+     */
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) throws Exception {
+        if (args.length < 3) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, zookeeper-url, properties-file) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args + "" "" : """")
+                + (args.length > 1 ? args : """"));
+        }
+        final String kafka = args;
+        final String zookeeper = args;
+        final String propFileName = args.length > 2 ? args : null;
+
+        final Properties streamsProperties = Utils.loadProps(propFileName);
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.10.1)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""zookeeper="" + zookeeper);
+        System.out.println(""props="" + streamsProperties);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.setProperty(StreamsConfig.ZOOKEEPER_CONNECT_CONFIG, zookeeper);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        config.putAll(streamsProperties);
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                System.out.println(""closing Kafka Streams instance"");
+                System.out.flush();
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..fb4a4091610
--- /dev/null
+++ [file java]
@@ -0,0 +1,104 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.common.utils.Utils;
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    /**
+     * This test cannot be executed, as long as Kafka 0.10.2.2 is not released
+     */
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) throws Exception {
+        if (args.length < 2) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, properties-file) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args : """"));
+        }
+        final String kafka = args;
+        final String propFileName = args.length > 1 ? args : null;
+
+        final Properties streamsProperties = Utils.loadProps(propFileName);
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.10.2)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""props="" + streamsProperties);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        config.putAll(streamsProperties);
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..b1aad5dea9d
--- /dev/null
+++ [file java]
@@ -0,0 +1,104 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.common.utils.Utils;
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.kstream.KStreamBuilder;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    /**
+     * This test cannot be executed, as long as Kafka 0.11.0.3 is not released
+     */
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) throws Exception {
+        if (args.length < 2) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, properties-file) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args : """"));
+        }
+        final String kafka = args;
+        final String propFileName = args.length > 1 ? args : null;
+
+        final Properties streamsProperties = Utils.loadProps(propFileName);
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v0.11.0)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""props="" + streamsProperties);
+
+        final KStreamBuilder builder = new KStreamBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        config.putAll(streamsProperties);
+
+        final KafkaStreams streams = new KafkaStreams(builder, config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git [file java] [file java]
new file mode 100644
index 00000000000..dc72f2dcc9d
--- /dev/null
+++ [file java]
@@ -0,0 +1,104 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.common.utils.Utils;
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsBuilder;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    /**
+     * This test cannot be executed, as long as Kafka 1.0.2 is not released
+     */
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) throws Exception {
+        if (args.length < 2) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, properties-file) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args : """"));
+        }
+        final String kafka = args;
+        final String propFileName = args.length > 1 ? args : null;
+
+        final Properties streamsProperties = Utils.loadProps(propFileName);
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v1.0)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""props="" + streamsProperties);
+
+        final StreamsBuilder builder = new StreamsBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        config.putAll(streamsProperties);
+
+        final KafkaStreams streams = new KafkaStreams(builder.build(), config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git a/tests/docker/Dockerfile b/tests/docker/Dockerfile
index 57ca2423e3f..25da8db59ea 100644
--- a/tests/docker/Dockerfile
+++ b/tests/docker/Dockerfile
@@ -39,24 +39,36 @@ COPY ./ssh-config /root/.ssh/config
 RUN ssh-keygen -q -t rsa -N '' -f /root/.ssh/id_rsa && cp -f /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys
 # Install binary test dependencies.
+# we use the same versions as in vagrant/base.sh
 ARG KAFKA_MIRROR=""[link]""
-RUN mkdir -p ""/opt/kafka-0.8.2.2"" && chmod a+rw /opt/kafka-0.8.2.2 && curl -s ""$KAFKA_MIRROR/kafka_2.10-0.8.2.2.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.8.2.2""
+RUN mkdir -p ""/opt/kafka-0.8.2.2"" && chmod a+rw /opt/kafka-0.8.2.2 && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.8.2.2.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.8.2.2""
 RUN mkdir -p ""/opt/kafka-0.9.0.1"" && chmod a+rw /opt/kafka-0.9.0.1 && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.9.0.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.9.0.1""
+RUN mkdir -p ""/opt/kafka-0.10.0.0"" && chmod a+rw /opt/kafka-0.10.0.0 && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.10.0.0.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.0.0""
 RUN mkdir -p ""/opt/kafka-0.10.0.1"" && chmod a+rw /opt/kafka-0.10.0.1 && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.10.0.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.0.1""
+RUN mkdir -p ""/opt/kafka-0.10.1.0"" && chmod a+rw /opt/kafka-0.10.1.0 && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.10.1.0.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.1.0""
 RUN mkdir -p ""/opt/kafka-0.10.1.1"" && chmod a+rw /opt/kafka-0.10.1.1 && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.10.1.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.1.1""
+RUN mkdir -p ""/opt/kafka-0.10.2.0"" && chmod a+rw /opt/kafka-0.10.2.0 && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.10.2.0.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.2.0""
 RUN mkdir -p ""/opt/kafka-0.10.2.1"" && chmod a+rw /opt/kafka-0.10.2.1 && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.10.2.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.10.2.1""
 RUN mkdir -p ""/opt/kafka-0.11.0.0"" && chmod a+rw /opt/kafka-0.11.0.0 && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.11.0.0.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.11.0.0""
+RUN mkdir -p ""/opt/kafka-0.11.0.1"" && chmod a+rw /opt/kafka-0.11.0.1 && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.11.0.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.11.0.1""
 RUN mkdir -p ""/opt/kafka-0.11.0.2"" && chmod a+rw /opt/kafka-0.11.0.2 && curl -s ""$KAFKA_MIRROR/kafka_2.11-0.11.0.2.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-0.11.0.2""
 RUN mkdir -p ""/opt/kafka-1.0.0"" && chmod a+rw /opt/kafka-1.0.0 && curl -s ""$KAFKA_MIRROR/kafka_2.11-1.0.0.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-1.0.0""
 RUN mkdir -p ""/opt/kafka-1.0.1"" && chmod a+rw /opt/kafka-1.0.1 && curl -s ""$KAFKA_MIRROR/kafka_2.11-1.0.1.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-1.0.1""
+RUN mkdir -p ""/opt/kafka-1.1.0"" && chmod a+rw /opt/kafka-1.1.0 && curl -s ""$KAFKA_MIRROR/kafka_2.11-1.1.0.tgz"" | tar xz --strip-components=1 -C ""/opt/kafka-1.1.0""
 # Streams test dependencies
-RUN curl -s ""$KAFKA_MIRROR/kafka-streams-0.10.1.1-test.jar"" -o /opt/kafka-0.10.1.1/libs/kafka-streams-0.10.1.1-test.jar && \
-    curl -s ""$KAFKA_MIRROR/kafka-streams-0.10.2.1-test.jar"" -o /opt/kafka-0.10.2.1/libs/kafka-streams-0.10.2.1-test.jar && \
-    curl -s ""$KAFKA_MIRROR/kafka-streams-0.11.0.0-test.jar"" -o /opt/kafka-0.11.0.0/libs/kafka-streams-0.11.0.0-test.jar && \
-    curl -s ""$KAFKA_MIRROR/kafka-streams-0.11.0.2-test.jar"" -o /opt/kafka-0.11.0.2/libs/kafka-streams-0.11.0.2-test.jar && \
-    curl -s ""$KAFKA_MIRROR/kafka-streams-1.0.0-test.jar"" -o /opt/kafka-1.0.0/libs/kafka-streams-1.0.0-test.jar && \
-    curl -s ""$KAFKA_MIRROR/kafka-streams-1.0.1-test.jar"" -o /opt/kafka-1.0.1/libs/kafka-streams-1.0.1-test.jar
+RUN curl -s ""$KAFKA_MIRROR/kafka-streams-0.10.0.0-test.jar"" -o /opt/kafka-0.10.0.0/libs/kafka-streams-0.10.0.0-test.jar
+RUN curl -s ""$KAFKA_MIRROR/kafka-streams-0.10.0.1-test.jar"" -o /opt/kafka-0.10.0.1/libs/kafka-streams-0.10.0.1-test.jar
+RUN curl -s ""$KAFKA_MIRROR/kafka-streams-0.10.1.0-test.jar"" -o /opt/kafka-0.10.1.0/libs/kafka-streams-0.10.1.0-test.jar
+RUN curl -s ""$KAFKA_MIRROR/kafka-streams-0.10.1.1-test.jar"" -o /opt/kafka-0.10.1.1/libs/kafka-streams-0.10.1.1-test.jar
+RUN curl -s ""$KAFKA_MIRROR/kafka-streams-0.10.2.0-test.jar"" -o /opt/kafka-0.10.2.0/libs/kafka-streams-0.10.2.0-test.jar
+RUN curl -s ""$KAFKA_MIRROR/kafka-streams-0.10.2.1-test.jar"" -o /opt/kafka-0.10.2.1/libs/kafka-streams-0.10.2.1-test.jar
+RUN curl -s ""$KAFKA_MIRROR/kafka-streams-0.11.0.0-test.jar"" -o /opt/kafka-0.11.0.0/libs/kafka-streams-0.11.0.0-test.jar
+RUN curl -s ""$KAFKA_MIRROR/kafka-streams-0.11.0.1-test.jar"" -o /opt/kafka-0.11.0.1/libs/kafka-streams-0.11.0.1-test.jar
+RUN curl -s ""$KAFKA_MIRROR/kafka-streams-0.11.0.2-test.jar"" -o /opt/kafka-0.11.0.2/libs/kafka-streams-0.11.0.2-test.jar
+RUN curl -s ""$KAFKA_MIRROR/kafka-streams-1.0.0-test.jar"" -o /opt/kafka-1.0.0/libs/kafka-streams-1.0.0-test.jar
+RUN curl -s ""$KAFKA_MIRROR/kafka-streams-1.0.1-test.jar"" -o /opt/kafka-1.0.1/libs/kafka-streams-1.0.1-test.jar
+RUN curl -s ""$KAFKA_MIRROR/kafka-streams-1.1.0-test.jar"" -o /opt/kafka-1.1.0/libs/kafka-streams-1.1.0-test.jar
 # The version of Kibosh to use for testing.
 # If you update this, also update vagrant/base.sy
diff --git a/tests/kafkatest/services/streams.py b/tests/kafkatest/services/streams.py
index d9b475e191b..a5be816c737 100644
--- a/tests/kafkatest/services/streams.py
+++ b/tests/kafkatest/services/streams.py
@@ -21,6 +21,7 @@
 from kafkatest.directory_layout.kafka_path import KafkaPathResolverMixin
 from kafkatest.services.monitor.jmx import JmxMixin
 from kafkatest.services.kafka import KafkaConfig
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1
 STATE_DIR = ""state.dir""
@@ -39,6 +40,8 @@ class StreamsTestBaseService(KafkaPathResolverMixin, JmxMixin, Service):
     LOG4J_CONFIG_FILE = os.path.join(PERSISTENT_ROOT, ""tools-log4j.properties"")
     PID_FILE = os.path.join(PERSISTENT_ROOT, ""streams.pid"")
+    CLEAN_NODE_ENABLED = True
+
     logs = {
         ""streams_log"": {
             ""path"": LOG_FILE,
@@ -49,6 +52,114 @@ class StreamsTestBaseService(KafkaPathResolverMixin, JmxMixin, Service):
         ""streams_stderr"": {
             ""path"": STDERR_FILE,
             ""collect_default"": True},
+        ""streams_log.0-1"": {
+            ""path"": LOG_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_stdout.0-1"": {
+            ""path"": STDOUT_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_stderr.0-1"": {
+            ""path"": STDERR_FILE + "".0-1"",
+            ""collect_default"": True},
+        ""streams_log.0-2"": {
+            ""path"": LOG_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_stdout.0-2"": {
+            ""path"": STDOUT_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_stderr.0-2"": {
+            ""path"": STDERR_FILE + "".0-2"",
+            ""collect_default"": True},
+        ""streams_log.0-3"": {
+            ""path"": LOG_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_stdout.0-3"": {
+            ""path"": STDOUT_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_stderr.0-3"": {
+            ""path"": STDERR_FILE + "".0-3"",
+            ""collect_default"": True},
+        ""streams_log.0-4"": {
+            ""path"": LOG_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_stdout.0-4"": {
+            ""path"": STDOUT_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_stderr.0-4"": {
+            ""path"": STDERR_FILE + "".0-4"",
+            ""collect_default"": True},
+        ""streams_log.0-5"": {
+            ""path"": LOG_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_stdout.0-5"": {
+            ""path"": STDOUT_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_stderr.0-5"": {
+            ""path"": STDERR_FILE + "".0-5"",
+            ""collect_default"": True},
+        ""streams_log.0-6"": {
+            ""path"": LOG_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_stdout.0-6"": {
+            ""path"": STDOUT_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_stderr.0-6"": {
+            ""path"": STDERR_FILE + "".0-6"",
+            ""collect_default"": True},
+        ""streams_log.1-1"": {
+            ""path"": LOG_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_stdout.1-1"": {
+            ""path"": STDOUT_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_stderr.1-1"": {
+            ""path"": STDERR_FILE + "".1-1"",
+            ""collect_default"": True},
+        ""streams_log.1-2"": {
+            ""path"": LOG_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_stdout.1-2"": {
+            ""path"": STDOUT_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_stderr.1-2"": {
+            ""path"": STDERR_FILE + "".1-2"",
+            ""collect_default"": True},
+        ""streams_log.1-3"": {
+            ""path"": LOG_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_stdout.1-3"": {
+            ""path"": STDOUT_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_stderr.1-3"": {
+            ""path"": STDERR_FILE + "".1-3"",
+            ""collect_default"": True},
+        ""streams_log.1-4"": {
+            ""path"": LOG_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_stdout.1-4"": {
+            ""path"": STDOUT_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_stderr.1-4"": {
+            ""path"": STDERR_FILE + "".1-4"",
+            ""collect_default"": True},
+        ""streams_log.1-5"": {
+            ""path"": LOG_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_stdout.1-5"": {
+            ""path"": STDOUT_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_stderr.1-5"": {
+            ""path"": STDERR_FILE + "".1-5"",
+            ""collect_default"": True},
+        ""streams_log.1-6"": {
+            ""path"": LOG_FILE + "".1-6"",
+            ""collect_default"": True},
+        ""streams_stdout.1-6"": {
+            ""path"": STDOUT_FILE + "".1-6"",
+            ""collect_default"": True},
+        ""streams_stderr.1-6"": {
+            ""path"": STDERR_FILE + "".1-6"",
+            ""collect_default"": True},
         ""jmx_log"": {
             ""path"": JMX_LOG_FILE,
             ""collect_default"": True},
@@ -120,7 +231,8 @@ def wait_node(self, node, timeout_sec=None):
     def clean_node(self, node):
         node.account.kill_process(""streams"", clean_shutdown=False, allow_fail=True)
-        node.account.ssh(""rm -rf "" + self.PERSISTENT_ROOT, allow_fail=False)
+        if self.CLEAN_NODE_ENABLED:
+            node.account.ssh(""rm -rf "" + self.PERSISTENT_ROOT, allow_fail=False)
     def start_cmd(self, node):
         args = self.args.copy()
@@ -141,13 +253,13 @@ def start_cmd(self, node):
         return cmd
-    def prop_file(self, node):
+    def prop_file(self):
         cfg = KafkaConfig(**{STATE_DIR: self.PERSISTENT_ROOT})
         return cfg.render()
     def start_node(self, node):
         node.account.mkdirs(self.PERSISTENT_ROOT)
-        prop_file = self.prop_file(node)
+        prop_file = self.prop_file()
         node.account.create_file(self.CONFIG_FILE, prop_file)
         node.account.create_file(self.LOG4J_CONFIG_FILE, self.render('tools_log4j.properties', log_file=self.LOG_FILE))
@@ -189,7 +301,28 @@ def clean_node(self, node):
 class StreamsSmokeTestDriverService(StreamsSmokeTestBaseService):
     def __init__(self, test_context, kafka):
         super(StreamsSmokeTestDriverService, self).__init__(test_context, kafka, ""run"")
+        self.DISABLE_AUTO_TERMINATE = """"
+    def disable_auto_terminate(self):
+        self.DISABLE_AUTO_TERMINATE = ""disableAutoTerminate""
+
+    def start_cmd(self, node):
+        args = self.args.copy()
+        args = self.kafka.bootstrap_servers()
+        args = self.CONFIG_FILE
+        args = self.STDOUT_FILE
+        args = self.STDERR_FILE
+        args = self.PID_FILE
+        args = self.LOG4J_CONFIG_FILE
+        args = self.DISABLE_AUTO_TERMINATE
+        args = self.path.script(""kafka-run-class.sh"", node)
+
+        cmd = ""( export KAFKA_LOG4J_OPTS=\""-Dlog4j.configuration=file:%(log4j)s\""; "" \
+              ""INCLUDE_TEST_JARS=true %(kafka_run_class)s %(streams_class_name)s "" \
+              "" %(kafka)s %(config_file)s %(user_test_args)s %(disable_auto_terminate)s"" \
+              "" & echo $! >&3 ) 1>> %(stdout)s 2>> %(stderr)s 3> %(pidfile)s"" % args
+
+        return cmd
 class StreamsSmokeTestJobRunnerService(StreamsSmokeTestBaseService):
     def __init__(self, test_context, kafka):
@@ -273,3 +406,48 @@ def __init__(self, test_context, kafka, configs):
                                                                         kafka,
                                                                         ""org.apache.kafka.streams.tests.StreamsRepeatingIntegerKeyProducer"",
                                                                         configs)
+class StreamsUpgradeTestJobRunnerService(StreamsTestBaseService):
+    def __init__(self, test_context, kafka):
+        super(StreamsUpgradeTestJobRunnerService, self).__init__(test_context,
+                                                                 kafka,
+                                                                 ""org.apache.kafka.streams.tests.StreamsUpgradeTest"",
+                                                                 """")
+        self.UPGRADE_FROM = None
+
+    def set_version(self, kafka_streams_version):
+        self.KAFKA_STREAMS_VERSION = kafka_streams_version
+
+    def set_upgrade_from(self, upgrade_from):
+        self.UPGRADE_FROM = upgrade_from
+
+    def prop_file(self):
+        properties = {STATE_DIR: self.PERSISTENT_ROOT}
+        if self.UPGRADE_FROM is not None:
+            properties = self.UPGRADE_FROM
+
+        cfg = KafkaConfig(**properties)
+        return cfg.render()
+
+    def start_cmd(self, node):
+        args = self.args.copy()
+        args = self.kafka.bootstrap_servers()
+        if self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_0) or self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_1):
+            args = self.kafka.zk.connect_setting()
+        else:
+            args = """"
+        args = self.CONFIG_FILE
+        args = self.STDOUT_FILE
+        args = self.STDERR_FILE
+        args = self.PID_FILE
+        args = self.LOG4J_CONFIG_FILE
+        args = self.KAFKA_STREAMS_VERSION
+        args = self.path.script(""kafka-run-class.sh"", node)
+
+        cmd = ""( export KAFKA_LOG4J_OPTS=\""-Dlog4j.configuration=file:%(log4j)s\""; "" \
+              ""INCLUDE_TEST_JARS=true UPGRADE_KAFKA_STREAMS_TEST_VERSION=%(version)s "" \
+              "" %(kafka_run_class)s %(streams_class_name)s  %(kafka)s %(zk)s %(config_file)s "" \
+              "" & echo $! >&3 ) 1>> %(stdout)s 2>> %(stderr)s 3> %(pidfile)s"" % args
+
+        self.logger.info(""Executing: "" + cmd)
+
+        return cmd
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py
index 3b38ff6f0f6..fa79d571f36 100644
--- a/tests/kafkatest/tests/streams/streams_upgrade_test.py
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py
@@ -13,25 +13,50 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import time
-from ducktape.mark import ignore
-from ducktape.mark import matrix
+from ducktape.mark import ignore, matrix, parametrize
 from ducktape.mark.resource import cluster
 from ducktape.tests.test import Test
 from kafkatest.services.kafka import KafkaService
-from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsSmokeTestJobRunnerService
+from kafkatest.services.streams import StreamsSmokeTestDriverService, StreamsSmokeTestJobRunnerService, StreamsUpgradeTestJobRunnerService
 from kafkatest.services.zookeeper import ZookeeperService
-from kafkatest.version import LATEST_0_10_2, LATEST_0_11, LATEST_1_0, DEV_BRANCH, KafkaVersion
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1, LATEST_0_10_2, LATEST_0_11_0, LATEST_1_0, LATEST_1_1, DEV_BRANCH, DEV_VERSION, KafkaVersion
+import random
+import time
-upgrade_versions = 
+broker_upgrade_versions = 
+simple_upgrade_versions_metadata_version_2 = 
 class StreamsUpgradeTest(Test):
     """"""
-    Tests rolling upgrades and downgrades of the Kafka Streams library.
+    Test upgrading Kafka Streams (all version combination)
+    If metadata was changes, upgrade is more difficult
+    Metadata version was bumped in 0.10.1.0
     """"""
     def __init__(self, test_context):
         super(StreamsUpgradeTest, self).__init__(test_context)
+        self.topics = {
+            'echo' : { 'partitions': 5 },
+            'data' : { 'partitions': 5 },
+        }
+
+    def perform_broker_upgrade(self, to_version):
+        self.logger.info(""First pass bounce - rolling broker upgrade"")
+        for node in self.kafka.nodes:
+            self.kafka.stop_node(node)
+            node.version = KafkaVersion(to_version)
+            self.kafka.start_node(node)
+
+    @cluster(num_nodes=6)
+    @matrix(from_version=broker_upgrade_versions, to_version=broker_upgrade_versions)
+    def test_upgrade_downgrade_brokers(self, from_version, to_version):
+        """"""
+        Start a smoke test client then perform rolling upgrades on the broker.
+        """"""
+
+        if from_version == to_version:
+            return
+
         self.replication = 3
         self.partitions = 1
         self.isr = 2
@@ -58,112 +83,272 @@ def __init__(self, test_context):
                        'configs': {""min.insync.replicas"": self.isr} }
         }
+        # Setup phase
+        self.zk = ZookeeperService(self.test_context, num_nodes=1)
+        self.zk.start()
-    def perform_streams_upgrade(self, to_version):
-        self.logger.info(""First pass bounce - rolling streams upgrade"")
+        # number of nodes needs to be >= 3 for the smoke test
+        self.kafka = KafkaService(self.test_context, num_nodes=3,
+                                  zk=self.zk, version=KafkaVersion(from_version), topics=self.topics)
+        self.kafka.start()
-        # get the node running the streams app
-        node = self.processor1.node
-        self.processor1.stop()
+        # allow some time for topics to be created
+        time.sleep(10)
-        # change it's version. This will automatically make it pick up a different
-        # JAR when it starts again
-        node.version = KafkaVersion(to_version)
+        self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)
+        self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka)
+        
+        self.driver.start()
         self.processor1.start()
+        time.sleep(15)
-    def perform_broker_upgrade(self, to_version):
-        self.logger.info(""First pass bounce - rolling broker upgrade"")
-        for node in self.kafka.nodes:
-            self.kafka.stop_node(node)
-            node.version = KafkaVersion(to_version)
-            self.kafka.start_node(node)
+        self.perform_broker_upgrade(to_version)
-    @ignore
-    @cluster(num_nodes=6)
-    @matrix(from_version=upgrade_versions, to_version=upgrade_versions)
-    def test_upgrade_downgrade_streams(self, from_version, to_version):
-        """"""
-        Start a smoke test client, then abort (kill -9) and restart it a few times.
-        Ensure that all records are delivered.
+        time.sleep(15)
+        self.driver.wait()
+        self.driver.stop()
+
+        self.processor1.stop()
-        Note, that just like tests/core/upgrade_test.py, a prerequisite for this test to succeed
-        if the inclusion of all parametrized versions of kafka in kafka/vagrant/base.sh 
-        (search for get_kafka()). For streams in particular, that means that someone has manually
-        copies the kafka-stream-$version-test.jar in the right S3 bucket as shown in base.sh.
+        node = self.driver.node
+        node.account.ssh(""grep ALL-RECORDS-DELIVERED %s"" % self.driver.STDOUT_FILE, allow_fail=False)
+        self.processor1.node.account.ssh_capture(""grep SMOKE-TEST-CLIENT-CLOSED %s"" % self.processor1.STDOUT_FILE, allow_fail=False)
+
+    @matrix(from_version=simple_upgrade_versions_metadata_version_2, to_version=simple_upgrade_versions_metadata_version_2)
+    def test_simple_upgrade_downgrade(self, from_version, to_version):
+        """"""
+        Starts 3 KafkaStreams instances with <old_version>, and upgrades one-by-one to <new_version>
         """"""
-        if from_version != to_version:
-            # Setup phase
-            self.zk = ZookeeperService(self.test_context, num_nodes=1)
-            self.zk.start()
-            # number of nodes needs to be >= 3 for the smoke test
-            self.kafka = KafkaService(self.test_context, num_nodes=3,
-                                      zk=self.zk, version=KafkaVersion(from_version), topics=self.topics)
-            self.kafka.start()
+        if from_version == to_version:
+            return
-            # allow some time for topics to be created
-            time.sleep(10)
+        self.zk = ZookeeperService(self.test_context, num_nodes=1)
+        self.zk.start()
-            self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)
-            self.driver.node.version = KafkaVersion(from_version)
-            self.driver.start()
+        self.kafka = KafkaService(self.test_context, num_nodes=1, zk=self.zk, topics=self.topics)
+        self.kafka.start()
-            self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka)
-            self.processor1.node.version = KafkaVersion(from_version)
-            self.processor1.start()
+        self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)
+        self.driver.disable_auto_terminate()
+        self.processor1 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+        self.processor2 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+        self.processor3 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
-            time.sleep(15)
+        self.driver.start()
+        self.start_all_nodes_with(from_version)
-            self.perform_streams_upgrade(to_version)
+        self.processors = 
-            time.sleep(15)
-            self.driver.wait()
-            self.driver.stop()
+        counter = 1
+        random.seed()
-            self.processor1.stop()
+        # upgrade one-by-one via rolling bounce
+        random.shuffle(self.processors)
+        for p in self.processors:
+            p.CLEAN_NODE_ENABLED = False
+            self.do_rolling_bounce(p, None, to_version, counter)
+            counter = counter + 1
-            self.driver.node.account.ssh(""grep ALL-RECORDS-DELIVERED %s"" % self.driver.STDOUT_FILE, allow_fail=False)
-            self.processor1.node.account.ssh_capture(""grep SMOKE-TEST-CLIENT-CLOSED %s"" % self.processor1.STDOUT_FILE, allow_fail=False)
+        # shutdown
+        self.driver.stop()
+        self.driver.wait()
+        random.shuffle(self.processors)
+        for p in self.processors:
+            node = p.node
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:
+                p.stop()
+                monitor.wait_until(""UPGRADE-TEST-CLIENT-CLOSED"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'UPGRADE-TEST-CLIENT-CLOSED' on"" + str(node.account))
-    @ignore
-    @cluster(num_nodes=6)
-    @matrix(from_version=upgrade_versions, to_version=upgrade_versions)
-    def test_upgrade_brokers(self, from_version, to_version):
+        self.driver.stop()
+
+    #@parametrize(new_version=str(LATEST_0_10_1)) we cannot run this test until Kafka 0.10.1.2 is released
+    #@parametrize(new_version=str(LATEST_0_10_2)) we cannot run this test until Kafka 0.10.2.2 is released
+    #@parametrize(new_version=str(LATEST_0_11_0)) we cannot run this test until Kafka 0.11.0.3 is released
+    #@parametrize(new_version=str(LATEST_1_0)) we cannot run this test until Kafka 1.0.2 is released
+    #@parametrize(new_version=str(LATEST_1_1)) we cannot run this test until Kafka 1.1.1 is released
+    @parametrize(new_version=str(DEV_VERSION))
+    def test_metadata_upgrade(self, new_version):
         """"""
-        Start a smoke test client then perform rolling upgrades on the broker.
+        Starts 3 KafkaStreams instances with version 0.10.0, and upgrades one-by-one to <new_version>
         """"""
-        if from_version != to_version:
-            # Setup phase
-            self.zk = ZookeeperService(self.test_context, num_nodes=1)
-            self.zk.start()
-            # number of nodes needs to be >= 3 for the smoke test
-            self.kafka = KafkaService(self.test_context, num_nodes=3,
-                                      zk=self.zk, version=KafkaVersion(from_version), topics=self.topics)
-            self.kafka.start()
+        self.zk = ZookeeperService(self.test_context, num_nodes=1)
+        self.zk.start()
+
+        self.kafka = KafkaService(self.test_context, num_nodes=1, zk=self.zk, topics=self.topics)
+        self.kafka.start()
+
+        self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)
+        self.driver.disable_auto_terminate()
+        self.processor1 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+        self.processor2 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+        self.processor3 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+
+        self.driver.start()
+        self.start_all_nodes_with(str(LATEST_0_10_0))
+
+        self.processors = 
+
+        counter = 1
+        random.seed()
+
+        # first rolling bounce
+        random.shuffle(self.processors)
+        for p in self.processors:
+            p.CLEAN_NODE_ENABLED = False
+            self.do_rolling_bounce(p, ""0.10.0"", new_version, counter)
+            counter = counter + 1
+
+        # second rolling bounce
+        random.shuffle(self.processors)
+        for p in self.processors:
+            self.do_rolling_bounce(p, None, new_version, counter)
+            counter = counter + 1
+
+        # shutdown
+        self.driver.stop()
+        self.driver.wait()
+
+        random.shuffle(self.processors)
+        for p in self.processors:
+            node = p.node
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:
+                p.stop()
+                monitor.wait_until(""UPGRADE-TEST-CLIENT-CLOSED"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'UPGRADE-TEST-CLIENT-CLOSED' on"" + str(node.account))
+
+        self.driver.stop()
+
+    def start_all_nodes_with(self, version):
+        # start first with <version>
+        self.prepare_for(self.processor1, version)
+        node1 = self.processor1.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as monitor:
+            with node1.account.monitor_log(self.processor1.LOG_FILE) as log_monitor:
+                self.processor1.start()
+                log_monitor.wait_until(""Kafka version : "" + version,
+                                       timeout_sec=60,
+                                       err_msg=""Could not detect Kafka Streams version "" + version + "" "" + str(node1.account))
+                monitor.wait_until(""processed 100 records from topic"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
+
+        # start second with <version>
+        self.prepare_for(self.processor2, version)
+        node2 = self.processor2.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:
+                with node2.account.monitor_log(self.processor2.LOG_FILE) as log_monitor:
+                    self.processor2.start()
+                    log_monitor.wait_until(""Kafka version : "" + version,
+                                           timeout_sec=60,
+                                           err_msg=""Could not detect Kafka Streams version "" + version + "" "" + str(node2.account))
+                    first_monitor.wait_until(""processed 100 records from topic"",
+                                             timeout_sec=60,
+                                             err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
+                    second_monitor.wait_until(""processed 100 records from topic"",
+                                              timeout_sec=60,
+                                              err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node2.account))
+
+        # start third with <version>
+        self.prepare_for(self.processor3, version)
+        node3 = self.processor3.node
+        with node1.account.monitor_log(self.processor1.STDOUT_FILE) as first_monitor:
+            with node2.account.monitor_log(self.processor2.STDOUT_FILE) as second_monitor:
+                with node3.account.monitor_log(self.processor3.STDOUT_FILE) as third_monitor:
+                    with node3.account.monitor_log(self.processor3.LOG_FILE) as log_monitor:
+                        self.processor3.start()
+                        log_monitor.wait_until(""Kafka version : "" + version,
+                                               timeout_sec=60,
+                                               err_msg=""Could not detect Kafka Streams version "" + version + "" "" + str(node3.account))
+                        first_monitor.wait_until(""processed 100 records from topic"",
+                                                 timeout_sec=60,
+                                                 err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node1.account))
+                        second_monitor.wait_until(""processed 100 records from topic"",
+                                                  timeout_sec=60,
+                                                  err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node2.account))
+                        third_monitor.wait_until(""processed 100 records from topic"",
+                                                  timeout_sec=60,
+                                                  err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node3.account))
+
+    @staticmethod
+    def prepare_for(processor, version):
+        processor.node.account.ssh(""rm -rf "" + processor.PERSISTENT_ROOT, allow_fail=False)
+        if version == str(DEV_VERSION):
+            processor.set_version("""")  # set to TRUNK
+        else:
+            processor.set_version(version)
+
+    def do_rolling_bounce(self, processor, upgrade_from, new_version, counter):
+        first_other_processor = None
+        second_other_processor = None
+        for p in self.processors:
+            if p != processor:
+                if first_other_processor is None:
+                    first_other_processor = p
+                else:
+                    second_other_processor = p
+
+        node = processor.node
+        first_other_node = first_other_processor.node
+        second_other_node = second_other_processor.node
-            # allow some time for topics to be created
-            time.sleep(10)
+        # stop processor and wait for rebalance of others
+        with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:
+            with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:
+                processor.stop()
+                first_other_monitor.wait_until(""processed 100 records from topic"",
+                                               timeout_sec=60,
+                                               err_msg=""Never saw output 'processed 100 records from topic' on"" + str(first_other_node.account))
+                second_other_monitor.wait_until(""processed 100 records from topic"",
+                                                timeout_sec=60,
+                                                err_msg=""Never saw output 'processed 100 records from topic' on"" + str(second_other_node.account))
+        node.account.ssh_capture(""grep UPGRADE-TEST-CLIENT-CLOSED %s"" % processor.STDOUT_FILE, allow_fail=False)
-            # use the current (dev) version driver
-            self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)
-            self.driver.node.version = KafkaVersion(from_version)
-            self.driver.start()
+        if upgrade_from is None:  # upgrade disabled -- second round of rolling bounces
+            roll_counter = "".1-""  # second round of rolling bounces
+        else:
+            roll_counter = "".0-""  # first  round of rolling boundes
-            self.processor1 = StreamsSmokeTestJobRunnerService(self.test_context, self.kafka)
-            self.processor1.node.version = KafkaVersion(from_version)
-            self.processor1.start()
+        node.account.ssh(""mv "" + processor.STDOUT_FILE + "" "" + processor.STDOUT_FILE + roll_counter + str(counter), allow_fail=False)
+        node.account.ssh(""mv "" + processor.STDERR_FILE + "" "" + processor.STDERR_FILE + roll_counter + str(counter), allow_fail=False)
+        node.account.ssh(""mv "" + processor.LOG_FILE + "" "" + processor.LOG_FILE + roll_counter + str(counter), allow_fail=False)
-            time.sleep(15)
+        if new_version == str(DEV_VERSION):
+            processor.set_version("""")  # set to TRUNK
+        else:
+            processor.set_version(new_version)
+        processor.set_upgrade_from(upgrade_from)
-            self.perform_broker_upgrade(to_version)
+        grep_metadata_error = ""grep \""org.apache.kafka.streams.errors.TaskAssignmentException: unable to decode subscription data: version=2\"" ""
+        with node.account.monitor_log(processor.STDOUT_FILE) as monitor:
+            with node.account.monitor_log(processor.LOG_FILE) as log_monitor:
+                with first_other_node.account.monitor_log(first_other_processor.STDOUT_FILE) as first_other_monitor:
+                    with second_other_node.account.monitor_log(second_other_processor.STDOUT_FILE) as second_other_monitor:
+                        processor.start()
-            time.sleep(15)
-            self.driver.wait()
-            self.driver.stop()
+                        log_monitor.wait_until(""Kafka version : "" + new_version,
+                                               timeout_sec=60,
+                                               err_msg=""Could not detect Kafka Streams version "" + new_version + "" "" + str(node.account))
+                        first_other_monitor.wait_until(""processed 100 records from topic"",
+                                                       timeout_sec=60,
+                                                       err_msg=""Never saw output 'processed 100 records from topic' on"" + str(first_other_node.account))
+                        found = list(first_other_node.account.ssh_capture(grep_metadata_error + first_other_processor.STDERR_FILE, allow_fail=True))
+                        if len(found) > 0:
+                            raise Exception(""Kafka Streams failed with 'unable to decode subscription data: version=2'"")
-            self.processor1.stop()
+                        second_other_monitor.wait_until(""processed 100 records from topic"",
+                                                        timeout_sec=60,
+                                                        err_msg=""Never saw output 'processed 100 records from topic' on"" + str(second_other_node.account))
+                        found = list(second_other_node.account.ssh_capture(grep_metadata_error + second_other_processor.STDERR_FILE, allow_fail=True))
+                        if len(found) > 0:
+                            raise Exception(""Kafka Streams failed with 'unable to decode subscription data: version=2'"")
-            self.driver.node.account.ssh(""grep ALL-RECORDS-DELIVERED %s"" % self.driver.STDOUT_FILE, allow_fail=False)
-            self.processor1.node.account.ssh_capture(""grep SMOKE-TEST-CLIENT-CLOSED %s"" % self.processor1.STDOUT_FILE, allow_fail=False)
+                        monitor.wait_until(""processed 100 records from topic"",
+                                           timeout_sec=60,
+                                           err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node.account))
diff --git a/tests/kafkatest/version.py b/tests/kafkatest/version.py
index b7071e79dae..66e5fcf18aa 100644
--- a/tests/kafkatest/version.py
+++ b/tests/kafkatest/version.py
@@ -61,6 +61,7 @@ def get_version(node=None):
         return DEV_BRANCH
 DEV_BRANCH = KafkaVersion(""dev"")
+DEV_VERSION = KafkaVersion(""1.2.0-SNAPSHOT"")
 # 0.8.2.X versions
 V_0_8_2_1 = KafkaVersion(""0.8.2.1"")
@@ -89,7 +90,7 @@ def get_version(node=None):
 LATEST_0_10 = LATEST_0_10_2
-# 0.11.0.0 versions
+# 0.11.0.x versions
 V_0_11_0_0 = KafkaVersion(""0.11.0.0"")
 V_0_11_0_1 = KafkaVersion(""0.11.0.1"")
 V_0_11_0_2 = KafkaVersion(""0.11.0.2"")
diff --git a/vagrant/base.sh b/vagrant/base.sh
index bfc34967351..f5c03cca4fd 100755
--- a/vagrant/base.sh
+++ b/vagrant/base.sh
@@ -99,8 +99,10 @@ popd
 popd
 popd
-# Test multiple Scala versions
-get_kafka 0.8.2.2 2.10
+# Test multiple Kafka versions
+# we want to use the latest Scala version per Kafka version
+# however, we cannot pull in Scala 2.12 builds atm, because Scala 2.12 requires Java 8, but we use Java 7 to run the system tests
+get_kafka 0.8.2.2 2.11
 chmod a+rw /opt/kafka-0.8.2.2
 get_kafka 0.9.0.1 2.11
 chmod a+rw /opt/kafka-0.9.0.1
@@ -112,10 +114,10 @@ get_kafka 0.10.2.1 2.11
 chmod a+rw /opt/kafka-0.10.2.1
 get_kafka 0.11.0.2 2.11
 chmod a+rw /opt/kafka-0.11.0.2
-get_kafka 1.0.0 2.11
-chmod a+rw /opt/kafka-1.0.0
 get_kafka 1.0.1 2.11
 chmod a+rw /opt/kafka-1.0.1
+get_kafka 1.1.0 2.11
+chmod a+rw /opt/kafka-1.1.0
 # For EC2 nodes, we want to use /mnt, which should have the local disk. On local
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 18 **
mjsax opened a new pull request #4880:  KAFKA-6054: Update Kafka Streams metadata to version 3
URL: [link]
   - adds Streams upgrade tests for 1.1 release
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 19 **
mjsax closed pull request #4880:  KAFKA-6054: Update Kafka Streams metadata to version 3
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git a/build.gradle b/build.gradle
index f8daf2fdddc..5b0e6496c2e 100644
--- a/build.gradle
+++ b/build.gradle
@@ -1087,6 +1087,18 @@ project(':streams:upgrade-system-tests-10') {
   }
 }
+project(':streams:upgrade-system-tests-11') {
+  archivesBaseName = ""kafka-streams-upgrade-system-tests-11""
+
+  dependencies {
+    testCompile libs.kafkaStreams_11
+  }
+
+  systemTestLibs {
+    dependsOn testJar
+  }
+}
+
 project(':jmh-benchmarks') {
   apply plugin: 'com.github.johnrengelman.shadow'
diff --git a/gradle/dependencies.gradle b/gradle/dependencies.gradle
index effe763ac45..a6ef5dddeec 100644
--- a/gradle/dependencies.gradle
+++ b/gradle/dependencies.gradle
@@ -67,6 +67,7 @@ versions += [
   kafka_0102: ""0.10.2.1"",
   kafka_0110: ""0.11.0.2"",
   kafka_10: ""1.0.1"",
+  kafka_11: ""1.1.0"",
   lz4: ""1.4.1"",
   metrics: ""2.2.0"",
   // PowerMock 1.x doesn't support Java 9, so use PowerMock 2.0.0 beta
@@ -115,6 +116,7 @@ libs += [
   kafkaStreams_0102: ""org.apache.kafka:kafka-streams:$versions.kafka_0102"",
   kafkaStreams_0110: ""org.apache.kafka:kafka-streams:$versions.kafka_0110"",
   kafkaStreams_10: ""org.apache.kafka:kafka-streams:$versions.kafka_10"",
+  kafkaStreams_11: ""org.apache.kafka:kafka-streams:$versions.kafka_11"",
   log4j: ""log4j:log4j:$versions.log4j"",
   lz4: ""org.lz4:lz4-java:$versions.lz4"",
   metrics: ""com.yammer.metrics:metrics-core:$versions.metrics"",
diff --git a/settings.gradle b/settings.gradle
index 03136849fd5..2a7977cfc93 100644
--- a/settings.gradle
+++ b/settings.gradle
@@ -15,5 +15,6 @@
 include 'core', 'examples', 'clients', 'tools', 'streams', 'streams:test-utils', 'streams:examples',
         'streams:upgrade-system-tests-0100', 'streams:upgrade-system-tests-0101', 'streams:upgrade-system-tests-0102',
-        'streams:upgrade-system-tests-0110', 'streams:upgrade-system-tests-10', 'log4j-appender',
-        'connect:api', 'connect:transforms', 'connect:runtime', 'connect:json', 'connect:file', 'jmh-benchmarks'
+        'streams:upgrade-system-tests-0110', 'streams:upgrade-system-tests-10', 'streams:upgrade-system-tests-11',
+        'log4j-appender', 'connect:api', 'connect:transforms', 'connect:runtime', 'connect:json', 'connect:file',
+        'jmh-benchmarks'
diff --git [file java] [file java]
index 819bebd43b6..65b1da6dede 100644
--- [file java]
+++ [file java]
@@ -172,6 +172,31 @@
      */
     public static final String UPGRADE_FROM_0100 = ""0.10.0"";
+    /**
+     * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 0.10.1.x}.
+     */
+    public static final String UPGRADE_FROM_0101 = ""0.10.1"";
+
+    /**
+     * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 0.10.2.x}.
+     */
+    public static final String UPGRADE_FROM_0102 = ""0.10.2"";
+
+    /**
+     * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 0.11.0.x}.
+     */
+    public static final String UPGRADE_FROM_0110 = ""0.11.0"";
+
+    /**
+     * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 1.0.x}.
+     */
+    public static final String UPGRADE_FROM_10 = ""1.0"";
+
+    /**
+     * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 1.1.x}.
+     */
+    public static final String UPGRADE_FROM_11 = ""1.1"";
+
     /**
      * Config value for parameter {@link #PROCESSING_GUARANTEE_CONFIG ""processing.guarantee""} for at-least-once processing guarantees.
      */
@@ -347,8 +372,9 @@
     /** {@code upgrade.from} */
     public static final String UPGRADE_FROM_CONFIG = ""upgrade.from"";
-    public static final String UPGRADE_FROM_DOC = ""Allows upgrading from version 0.10.0 to version 0.10.1 (or newer) in a backward compatible way. "" +
-        ""Default is null. Accepted values are \"""" + UPGRADE_FROM_0100 + ""\"" (for upgrading from 0.10.0.x)."";
+    public static final String UPGRADE_FROM_DOC = ""Allows upgrading from versions 0.10.0/0.10.1/0.10.2/0.11.0/1.0/1.1 to version 1.2 (or newer) in a backward compatible way. "" +
+        ""When upgrading from 1.2 to a newer version it is not required to specify this config."" +
+        ""Default is null. Accepted values are \"""" + UPGRADE_FROM_0100 + ""\"", \"""" + UPGRADE_FROM_0101 + ""\"", \"""" + UPGRADE_FROM_0102 + ""\"", \"""" + UPGRADE_FROM_0110 + ""\"", \"""" + UPGRADE_FROM_10 + ""\"", \"""" + UPGRADE_FROM_11 + ""\"" (for upgrading from the corresponding old version)."";
     /**
      * {@code value.serde}
@@ -364,7 +390,7 @@
     /**
      * {@code zookeeper.connect}
-     * @deprecated Kakfa Streams does not use Zookeeper anymore and this parameter will be ignored.
+     * @deprecated Kafka Streams does not use Zookeeper anymore and this parameter will be ignored.
      */
     @Deprecated
     public static final String ZOOKEEPER_CONNECT_CONFIG = ""zookeeper.connect"";
@@ -575,7 +601,7 @@
             .define(UPGRADE_FROM_CONFIG,
                     ConfigDef.Type.STRING,
                     null,
-                    in(null, UPGRADE_FROM_0100),
+                    in(null, UPGRADE_FROM_0100, UPGRADE_FROM_0101, UPGRADE_FROM_0102, UPGRADE_FROM_0110, UPGRADE_FROM_10, UPGRADE_FROM_11),
                     Importance.LOW,
                     UPGRADE_FROM_DOC)
             .define(WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG,
diff --git [file java] [file java]
index 97771e56879..c81105ef821 100644
--- [file java]
+++ [file java]
@@ -199,10 +199,24 @@ public void configure(final Map<String, ?> configs) {
         final LogContext logContext = new LogContext(logPrefix);
         log = logContext.logger(getClass());
-        final String upgradeMode = (String) configs.get(StreamsConfig.UPGRADE_FROM_CONFIG);
-        if (StreamsConfig.UPGRADE_FROM_0100.equals(upgradeMode)) {
-            log.info(""Downgrading metadata version from 2 to 1 for upgrade from 0.10.0.x."");
-            userMetadataVersion = 1;
+        final String upgradeFrom = streamsConfig.getString(StreamsConfig.UPGRADE_FROM_CONFIG);
+        if (upgradeFrom != null) {
+            switch (upgradeFrom) {
+                case StreamsConfig.UPGRADE_FROM_0100:
+                    log.info(""Downgrading metadata version from {} to 1 for upgrade from 0.10.0.x."", SubscriptionInfo.LATEST_SUPPORTED_VERSION);
+                    userMetadataVersion = 1;
+                    break;
+                case StreamsConfig.UPGRADE_FROM_0101:
+                case StreamsConfig.UPGRADE_FROM_0102:
+                case StreamsConfig.UPGRADE_FROM_0110:
+                case StreamsConfig.UPGRADE_FROM_10:
+                case StreamsConfig.UPGRADE_FROM_11:
+                    log.info(""Downgrading metadata version from {} to 2 for upgrade from "" + upgradeFrom + "".x."", SubscriptionInfo.LATEST_SUPPORTED_VERSION);
+                    userMetadataVersion = 2;
+                    break;
+                default:
+                    throw new IllegalArgumentException(""Unknown configuration value for parameter 'upgrade.from': "" + upgradeFrom);
+            }
         }
         final Object o = configs.get(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR);
@@ -512,7 +526,7 @@ public Subscription subscription(final Set<String> topics) {
         // construct the global partition assignment per host map
         final Map<HostInfo, Set<TopicPartition>> partitionsByHostState = new HashMap<>();
-        if (minUserMetadataVersion == 2) {
+        if (minUserMetadataVersion == 2 || minUserMetadataVersion == 3) {
             for (final Map.Entry<UUID, ClientMetadata> entry : clientsMetadata.entrySet()) {
                 final HostInfo hostInfo = entry.getValue().hostInfo;
@@ -631,6 +645,10 @@ public void onAssignment(final Assignment assignment) {
                 processVersionTwoAssignment(info, partitions, activeTasks, topicToPartitionInfo);
                 partitionsByHost = info.partitionsByHost();
                 break;
+            case 3:
+                processVersionThreeAssignment(info, partitions, activeTasks, topicToPartitionInfo);
+                partitionsByHost = info.partitionsByHost();
+                break;
             default:
                 throw new IllegalStateException(""Unknown metadata version: "" + usedVersion
                     + ""; latest supported version: "" + AssignmentInfo.LATEST_SUPPORTED_VERSION);
@@ -684,6 +702,13 @@ private void processVersionTwoAssignment(final AssignmentInfo info,
         }
     }
+    private void processVersionThreeAssignment(final AssignmentInfo info,
+                                               final List<TopicPartition> partitions,
+                                               final Map<TaskId, Set<TopicPartition>> activeTasks,
+                                               final Map<TopicPartition, PartitionInfo> topicToPartitionInfo) {
+        processVersionTwoAssignment(info, partitions, activeTasks, topicToPartitionInfo);
+    }
+
     /**
      * Internal helper function that creates a Kafka topic
      *
@@ -818,4 +843,5 @@ void validate(final Set<String> copartitionGroup,
     void setInternalTopicManager(final InternalTopicManager internalTopicManager) {
         this.internalTopicManager = internalTopicManager;
     }
+
 }
diff --git [file java] [file java]
index c8df7498755..3c5cee2bfc3 100644
--- [file java]
+++ [file java]
@@ -16,8 +16,8 @@
  */
 package org.apache.kafka.streams.processor.internals.assignment;
-import org.apache.kafka.common.utils.ByteBufferInputStream;
 import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.utils.ByteBufferInputStream;
 import org.apache.kafka.streams.errors.TaskAssignmentException;
 import org.apache.kafka.streams.processor.TaskId;
 import org.apache.kafka.streams.state.HostInfo;
@@ -30,6 +30,7 @@
 import java.io.IOException;
 import java.nio.ByteBuffer;
 import java.util.ArrayList;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
@@ -40,15 +41,20 @@
     private static final Logger log = LoggerFactory.getLogger(AssignmentInfo.class);
-    public static final int LATEST_SUPPORTED_VERSION = 2;
+    public static final int LATEST_SUPPORTED_VERSION = 3;
+    public static final int UNKNOWN = -1;
     private final int usedVersion;
+    private final int latestSupportedVersion;
     private List<TaskId> activeTasks;
     private Map<TaskId, Set<TopicPartition>> standbyTasks;
     private Map<HostInfo, Set<TopicPartition>> partitionsByHost;
-    private AssignmentInfo(final int version) {
+    // used for decoding; don't apply version checks
+    private AssignmentInfo(final int version,
+                           final int latestSupportedVersion) {
         this.usedVersion = version;
+        this.latestSupportedVersion = latestSupportedVersion;
     }
     public AssignmentInfo(final List<TaskId> activeTasks,
@@ -57,11 +63,33 @@ public AssignmentInfo(final List<TaskId> activeTasks,
         this(LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, hostState);
     }
+    public AssignmentInfo() {
+        this(LATEST_SUPPORTED_VERSION,
+            Collections.<TaskId>emptyList(),
+            Collections.<TaskId, Set<TopicPartition>>emptyMap(),
+            Collections.<HostInfo, Set<TopicPartition>>emptyMap());
+    }
+
     public AssignmentInfo(final int version,
                           final List<TaskId> activeTasks,
                           final Map<TaskId, Set<TopicPartition>> standbyTasks,
                           final Map<HostInfo, Set<TopicPartition>> hostState) {
+        this(version, LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, hostState);
+
+        if (version < 1 || version > LATEST_SUPPORTED_VERSION) {
+            throw new IllegalArgumentException(""version must be between 1 and "" + LATEST_SUPPORTED_VERSION
+                + ""; was: "" + version);
+        }
+    }
+
+    // for testing only; don't apply version checks
+    AssignmentInfo(final int version,
+                   final int latestSupportedVersion,
+                   final List<TaskId> activeTasks,
+                   final Map<TaskId, Set<TopicPartition>> standbyTasks,
+                   final Map<HostInfo, Set<TopicPartition>> hostState) {
         this.usedVersion = version;
+        this.latestSupportedVersion = latestSupportedVersion;
         this.activeTasks = activeTasks;
         this.standbyTasks = standbyTasks;
         this.partitionsByHost = hostState;
@@ -71,6 +99,10 @@ public int version() {
         return usedVersion;
     }
+    public int latestSupportedVersion() {
+        return latestSupportedVersion;
+    }
+
     public List<TaskId> activeTasks() {
         return activeTasks;
     }
@@ -98,6 +130,9 @@ public ByteBuffer encode() {
                 case 2:
                     encodeVersionTwo(out);
                     break;
+                case 3:
+                    encodeVersionThree(out);
+                    break;
                 default:
                     throw new IllegalStateException(""Unknown metadata version: "" + usedVersion
                         + ""; latest supported version: "" + LATEST_SUPPORTED_VERSION);
@@ -161,6 +196,13 @@ private void writeTopicPartitions(final DataOutputStream out,
         }
     }
+    private void encodeVersionThree(final DataOutputStream out) throws IOException {
+        out.writeInt(3);
+        out.writeInt(LATEST_SUPPORTED_VERSION);
+        encodeActiveAndStandbyTaskAssignment(out);
+        encodePartitionsByHost(out);
+    }
+
     /**
      * @throws TaskAssignmentException if method fails to decode the data or if the data version is unknown
      */
@@ -169,19 +211,25 @@ public static AssignmentInfo decode(final ByteBuffer data) {
         data.rewind();
         try (final DataInputStream in = new DataInputStream(new ByteBufferInputStream(data))) {
-            // decode used version
-            final int usedVersion = in.readInt();
-            final AssignmentInfo assignmentInfo = new AssignmentInfo(usedVersion);
+            final AssignmentInfo assignmentInfo;
+            final int usedVersion = in.readInt();
             switch (usedVersion) {
                 case 1:
+                    assignmentInfo = new AssignmentInfo(usedVersion, UNKNOWN);
                     decodeVersionOneData(assignmentInfo, in);
                     break;
                 case 2:
+                    assignmentInfo = new AssignmentInfo(usedVersion, UNKNOWN);
                     decodeVersionTwoData(assignmentInfo, in);
                     break;
+                case 3:
+                    final int latestSupportedVersion = in.readInt();
+                    assignmentInfo = new AssignmentInfo(usedVersion, latestSupportedVersion);
+                    decodeVersionThreeData(assignmentInfo, in);
+                    break;
                 default:
-                    TaskAssignmentException fatalException = new TaskAssignmentException(""Unable to decode subscription data: "" +
+                    TaskAssignmentException fatalException = new TaskAssignmentException(""Unable to decode assignment data: "" +
                         ""used version: "" + usedVersion + ""; latest supported version: "" + LATEST_SUPPORTED_VERSION);
                     log.error(fatalException.getMessage(), fatalException);
                     throw fatalException;
@@ -195,15 +243,23 @@ public static AssignmentInfo decode(final ByteBuffer data) {
     private static void decodeVersionOneData(final AssignmentInfo assignmentInfo,
                                              final DataInputStream in) throws IOException {
-        // decode active tasks
-        int count = in.readInt();
+        decodeActiveTasks(assignmentInfo, in);
+        decodeStandbyTasks(assignmentInfo, in);
+        assignmentInfo.partitionsByHost = new HashMap<>();
+    }
+
+    private static void decodeActiveTasks(final AssignmentInfo assignmentInfo,
+                                          final DataInputStream in) throws IOException {
+        final int count = in.readInt();
         assignmentInfo.activeTasks = new ArrayList<>(count);
         for (int i = 0; i < count; i++) {
             assignmentInfo.activeTasks.add(TaskId.readFrom(in));
         }
+    }
-        // decode standby tasks
-        count = in.readInt();
+    private static void decodeStandbyTasks(final AssignmentInfo assignmentInfo,
+                                           final DataInputStream in) throws IOException {
+        final int count = in.readInt();
         assignmentInfo.standbyTasks = new HashMap<>(count);
         for (int i = 0; i < count; i++) {
             TaskId id = TaskId.readFrom(in);
@@ -213,9 +269,13 @@ private static void decodeVersionOneData(final AssignmentInfo assignmentInfo,
     private static void decodeVersionTwoData(final AssignmentInfo assignmentInfo,
                                              final DataInputStream in) throws IOException {
-        decodeVersionOneData(assignmentInfo, in);
+        decodeActiveTasks(assignmentInfo, in);
+        decodeStandbyTasks(assignmentInfo, in);
+        decodeGlobalAssignmentData(assignmentInfo, in);
+    }
-        // decode partitions by host
+    private static void decodeGlobalAssignmentData(final AssignmentInfo assignmentInfo,
+                                                   final DataInputStream in) throws IOException {
         assignmentInfo.partitionsByHost = new HashMap<>();
         final int numEntries = in.readInt();
         for (int i = 0; i < numEntries; i++) {
@@ -233,19 +293,27 @@ private static void decodeVersionTwoData(final AssignmentInfo assignmentInfo,
         return partitions;
     }
+    private static void decodeVersionThreeData(final AssignmentInfo assignmentInfo,
+                                               final DataInputStream in) throws IOException {
+        decodeActiveTasks(assignmentInfo, in);
+        decodeStandbyTasks(assignmentInfo, in);
+        decodeGlobalAssignmentData(assignmentInfo, in);
+    }
+
     @Override
     public int hashCode() {
-        return usedVersion ^ activeTasks.hashCode() ^ standbyTasks.hashCode() ^ partitionsByHost.hashCode();
+        return usedVersion ^ latestSupportedVersion ^ activeTasks.hashCode() ^ standbyTasks.hashCode() ^ partitionsByHost.hashCode();
     }
     @Override
     public boolean equals(final Object o) {
         if (o instanceof AssignmentInfo) {
             final AssignmentInfo other = (AssignmentInfo) o;
-            return this.usedVersion == other.usedVersion &&
-                    this.activeTasks.equals(other.activeTasks) &&
-                    this.standbyTasks.equals(other.standbyTasks) &&
-                    this.partitionsByHost.equals(other.partitionsByHost);
+            return usedVersion == other.usedVersion &&
+                    latestSupportedVersion == other.latestSupportedVersion &&
+                    activeTasks.equals(other.activeTasks) &&
+                    standbyTasks.equals(other.standbyTasks) &&
+                    partitionsByHost.equals(other.partitionsByHost);
         } else {
             return false;
         }
@@ -253,7 +321,11 @@ public boolean equals(final Object o) {
     @Override
     public String toString() {
-        return """";
+        return ""[version="" + usedVersion
+            + "", supported version="" + latestSupportedVersion
+            + "", active tasks="" + activeTasks
+            + "", standby tasks="" + standbyTasks
+            + "", global assignment="" + partitionsByHost + ""]"";
     }
 }
diff --git [file java] [file java]
index 7fee90b5402..be709472441 100644
--- [file java]
+++ [file java]
@@ -23,6 +23,7 @@
 import java.nio.ByteBuffer;
 import java.nio.charset.Charset;
+import java.util.Collection;
 import java.util.HashSet;
 import java.util.Set;
 import java.util.UUID;
@@ -31,16 +32,21 @@
     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);
-    public static final int LATEST_SUPPORTED_VERSION = 2;
+    public static final int LATEST_SUPPORTED_VERSION = 3;
+    public static final int UNKNOWN = -1;
     private final int usedVersion;
+    private final int latestSupportedVersion;
     private UUID processId;
     private Set<TaskId> prevTasks;
     private Set<TaskId> standbyTasks;
     private String userEndPoint;
-    private SubscriptionInfo(final int version) {
+    // used for decoding; don't apply version checks
+    private SubscriptionInfo(final int version,
+                             final int latestSupportedVersion) {
         this.usedVersion = version;
+        this.latestSupportedVersion = latestSupportedVersion;
     }
     public SubscriptionInfo(final UUID processId,
@@ -55,7 +61,23 @@ public SubscriptionInfo(final int version,
                             final Set<TaskId> prevTasks,
                             final Set<TaskId> standbyTasks,
                             final String userEndPoint) {
+        this(version, LATEST_SUPPORTED_VERSION, processId, prevTasks, standbyTasks, userEndPoint);
+
+        if (version < 1 || version > LATEST_SUPPORTED_VERSION) {
+            throw new IllegalArgumentException(""version must be between 1 and "" + LATEST_SUPPORTED_VERSION
+                + ""; was: "" + version);
+        }
+    }
+
+    // for testing only; don't apply version checks
+    protected SubscriptionInfo(final int version,
+                               final int latestSupportedVersion,
+                               final UUID processId,
+                               final Set<TaskId> prevTasks,
+                               final Set<TaskId> standbyTasks,
+                               final String userEndPoint) {
         this.usedVersion = version;
+        this.latestSupportedVersion = latestSupportedVersion;
         this.processId = processId;
         this.prevTasks = prevTasks;
         this.standbyTasks = standbyTasks;
@@ -66,6 +88,10 @@ public int version() {
         return usedVersion;
     }
+    public int latestSupportedVersion() {
+        return latestSupportedVersion;
+    }
+
     public UUID processId() {
         return processId;
     }
@@ -93,7 +119,10 @@ public ByteBuffer encode() {
                 buf = encodeVersionOne();
                 break;
             case 2:
-                buf = encodeVersionTwo(prepareUserEndPoint());
+                buf = encodeVersionTwo();
+                break;
+            case 3:
+                buf = encodeVersionThree();
                 break;
             default:
                 throw new IllegalStateException(""Unknown metadata version: "" + usedVersion
@@ -108,7 +137,9 @@ private ByteBuffer encodeVersionOne() {
         final ByteBuffer buf = ByteBuffer.allocate(getVersionOneByteLength());
         buf.putInt(1); // version
-        encodeVersionOneData(buf);
+        encodeClientUUID(buf);
+        encodeTasks(buf, prevTasks);
+        encodeTasks(buf, standbyTasks);
         return buf;
     }
@@ -120,18 +151,15 @@ private int getVersionOneByteLength() {
                4 + standbyTasks.size() * 8; // length + standby tasks
     }
-    private void encodeVersionOneData(final ByteBuffer buf) {
-        // encode client UUID
+    private void encodeClientUUID(final ByteBuffer buf) {
         buf.putLong(processId.getMostSignificantBits());
         buf.putLong(processId.getLeastSignificantBits());
-        // encode ids of previously running tasks
-        buf.putInt(prevTasks.size());
-        for (TaskId id : prevTasks) {
-            id.writeTo(buf);
-        }
-        // encode ids of cached tasks
-        buf.putInt(standbyTasks.size());
-        for (TaskId id : standbyTasks) {
+    }
+
+    private void encodeTasks(final ByteBuffer buf,
+                             final Collection<TaskId> taskIds) {
+        buf.putInt(taskIds.size());
+        for (TaskId id : taskIds) {
             id.writeTo(buf);
         }
     }
@@ -144,52 +172,87 @@ private void encodeVersionOneData(final ByteBuffer buf) {
         }
     }
-    private ByteBuffer encodeVersionTwo(final byte endPointBytes) {
+    private ByteBuffer encodeVersionTwo() {
+        final byte endPointBytes = prepareUserEndPoint();
+
         final ByteBuffer buf = ByteBuffer.allocate(getVersionTwoByteLength(endPointBytes));
         buf.putInt(2); // version
-        encodeVersionTwoData(buf, endPointBytes);
+        encodeClientUUID(buf);
+        encodeTasks(buf, prevTasks);
+        encodeTasks(buf, standbyTasks);
+        encodeUserEndPoint(buf, endPointBytes);
         return buf;
     }
     private int getVersionTwoByteLength(final byte endPointBytes) {
-        return getVersionOneByteLength() +
+        return 4 + // version
+               16 + // client ID
+               4 + prevTasks.size() * 8 + // length + prev tasks
+               4 + standbyTasks.size() * 8 + // length + standby tasks
                4 + endPointBytes.length; // length + userEndPoint
     }
-    private void encodeVersionTwoData(final ByteBuffer buf,
-                                      final byte endPointBytes) {
-        encodeVersionOneData(buf);
+    private void encodeUserEndPoint(final ByteBuffer buf,
+                                    final byte endPointBytes) {
         if (endPointBytes != null) {
             buf.putInt(endPointBytes.length);
             buf.put(endPointBytes);
         }
     }
+    private ByteBuffer encodeVersionThree() {
+        final byte endPointBytes = prepareUserEndPoint();
+
+        final ByteBuffer buf = ByteBuffer.allocate(getVersionThreeByteLength(endPointBytes));
+
+        buf.putInt(3); // used version
+        buf.putInt(LATEST_SUPPORTED_VERSION); // supported version
+        encodeClientUUID(buf);
+        encodeTasks(buf, prevTasks);
+        encodeTasks(buf, standbyTasks);
+        encodeUserEndPoint(buf, endPointBytes);
+
+        return buf;
+    }
+
+    private int getVersionThreeByteLength(final byte endPointBytes) {
+        return 4 + // used version
+               4 + // latest supported version version
+               16 + // client ID
+               4 + prevTasks.size() * 8 + // length + prev tasks
+               4 + standbyTasks.size() * 8 + // length + standby tasks
+               4 + endPointBytes.length; // length + userEndPoint
+    }
+
     /**
      * @throws TaskAssignmentException if method fails to decode the data
      */
     public static SubscriptionInfo decode(final ByteBuffer data) {
+        final SubscriptionInfo subscriptionInfo;
+
         // ensure we are at the beginning of the ByteBuffer
         data.rewind();
-        // decode used version
         final int usedVersion = data.getInt();
-        final SubscriptionInfo subscriptionInfo = new SubscriptionInfo(usedVersion);
-
         switch (usedVersion) {
             case 1:
+                subscriptionInfo = new SubscriptionInfo(usedVersion, UNKNOWN);
                 decodeVersionOneData(subscriptionInfo, data);
                 break;
             case 2:
+                subscriptionInfo = new SubscriptionInfo(usedVersion, UNKNOWN);
                 decodeVersionTwoData(subscriptionInfo, data);
                 break;
+            case 3:
+                final int latestSupportedVersion = data.getInt();
+                subscriptionInfo = new SubscriptionInfo(usedVersion, latestSupportedVersion);
+                decodeVersionThreeData(subscriptionInfo, data);
+                break;
             default:
-                TaskAssignmentException fatalException = new TaskAssignmentException(""Unable to decode subscription data: "" +
-                    ""used version: "" + usedVersion + ""; latest supported version: "" + LATEST_SUPPORTED_VERSION);
-                log.error(fatalException.getMessage(), fatalException);
-                throw fatalException;
+                subscriptionInfo = new SubscriptionInfo(usedVersion, UNKNOWN);
+                log.info(""Unable to decode subscription data: used version: {}; latest supported version: {}"", usedVersion, LATEST_SUPPORTED_VERSION);
         }
         return subscriptionInfo;
@@ -197,30 +260,43 @@ public static SubscriptionInfo decode(final ByteBuffer data) {
     private static void decodeVersionOneData(final SubscriptionInfo subscriptionInfo,
                                              final ByteBuffer data) {
-        // decode client UUID
-        subscriptionInfo.processId = new UUID(data.getLong(), data.getLong());
+        decodeClientUUID(subscriptionInfo, data);
-        // decode previously active tasks
-        final int numPrevs = data.getInt();
         subscriptionInfo.prevTasks = new HashSet<>();
-        for (int i = 0; i < numPrevs; i++) {
-            TaskId id = TaskId.readFrom(data);
-            subscriptionInfo.prevTasks.add(id);
-        }
+        decodeTasks(subscriptionInfo.prevTasks, data);
-        // decode previously cached tasks
-        final int numCached = data.getInt();
         subscriptionInfo.standbyTasks = new HashSet<>();
-        for (int i = 0; i < numCached; i++) {
-            subscriptionInfo.standbyTasks.add(TaskId.readFrom(data));
+        decodeTasks(subscriptionInfo.standbyTasks, data);
+    }
+
+    private static void decodeClientUUID(final SubscriptionInfo subscriptionInfo,
+                                         final ByteBuffer data) {
+        subscriptionInfo.processId = new UUID(data.getLong(), data.getLong());
+    }
+
+    private static void decodeTasks(final Collection<TaskId> taskIds,
+                                    final ByteBuffer data) {
+        final int numPrevs = data.getInt();
+        for (int i = 0; i < numPrevs; i++) {
+            taskIds.add(TaskId.readFrom(data));
         }
     }
     private static void decodeVersionTwoData(final SubscriptionInfo subscriptionInfo,
                                              final ByteBuffer data) {
-        decodeVersionOneData(subscriptionInfo, data);
+        decodeClientUUID(subscriptionInfo, data);
+
+        subscriptionInfo.prevTasks = new HashSet<>();
+        decodeTasks(subscriptionInfo.prevTasks, data);
-        // decode user end point (can be null)
+        subscriptionInfo.standbyTasks = new HashSet<>();
+        decodeTasks(subscriptionInfo.standbyTasks, data);
+
+        decodeUserEndPoint(subscriptionInfo, data);
+    }
+
+    private static void decodeUserEndPoint(final SubscriptionInfo subscriptionInfo,
+                                           final ByteBuffer data) {
         int bytesLength = data.getInt();
         if (bytesLength != 0) {
             final byte bytes = new byte;
@@ -229,9 +305,21 @@ private static void decodeVersionTwoData(final SubscriptionInfo subscriptionInfo
         }
     }
-    @Override
+    private static void decodeVersionThreeData(final SubscriptionInfo subscriptionInfo,
+                                               final ByteBuffer data) {
+        decodeClientUUID(subscriptionInfo, data);
+
+        subscriptionInfo.prevTasks = new HashSet<>();
+        decodeTasks(subscriptionInfo.prevTasks, data);
+
+        subscriptionInfo.standbyTasks = new HashSet<>();
+        decodeTasks(subscriptionInfo.standbyTasks, data);
+
+        decodeUserEndPoint(subscriptionInfo, data);
+    }
+
     public int hashCode() {
-        final int hashCode = usedVersion ^ processId.hashCode() ^ prevTasks.hashCode() ^ standbyTasks.hashCode();
+        final int hashCode = usedVersion ^ latestSupportedVersion ^ processId.hashCode() ^ prevTasks.hashCode() ^ standbyTasks.hashCode();
         if (userEndPoint == null) {
             return hashCode;
         }
@@ -243,6 +331,7 @@ public boolean equals(final Object o) {
         if (o instanceof SubscriptionInfo) {
             final SubscriptionInfo other = (SubscriptionInfo) o;
             return this.usedVersion == other.usedVersion &&
+                    this.latestSupportedVersion == other.latestSupportedVersion &&
                     this.processId.equals(other.processId) &&
                     this.prevTasks.equals(other.prevTasks) &&
                     this.standbyTasks.equals(other.standbyTasks) &&
@@ -252,4 +341,13 @@ public boolean equals(final Object o) {
         }
     }
+    @Override
+    public String toString() {
+        return ""[version="" + usedVersion
+            + "", supported version="" + latestSupportedVersion
+            + "", process ID="" + processId
+            + "", prev tasks="" + prevTasks
+            + "", standby tasks="" + standbyTasks
+            + "", user endpoint="" + userEndPoint + ""]"";
+    }
 }
diff --git [file java] [file java]
index e9ed9682066..4e04b4985ed 100644
--- [file java]
+++ [file java]
@@ -46,7 +46,6 @@
 import org.apache.kafka.test.MockStateStoreSupplier;
 import org.easymock.Capture;
 import org.easymock.EasyMock;
-import org.junit.Assert;
 import org.junit.Test;
 import java.util.ArrayList;
@@ -64,6 +63,7 @@
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertNotEquals;
 import static org.junit.Assert.assertThat;
+import static org.junit.Assert.fail;
 public class StreamsPartitionAssignorTest {
@@ -867,9 +867,12 @@ public void shouldMapUserEndPointToTopicPartitions() {
         final PartitionAssignor.Assignment consumerAssignment = assignments.get(""consumer1"");
         final AssignmentInfo assignmentInfo = AssignmentInfo.decode(consumerAssignment.userData());
         final Set<TopicPartition> topicPartitions = assignmentInfo.partitionsByHost().get(new HostInfo(""localhost"", 8080));
-        assertEquals(Utils.mkSet(new TopicPartition(""topic1"", 0),
+        assertEquals(
+            Utils.mkSet(
+                new TopicPartition(""topic1"", 0),
                 new TopicPartition(""topic1"", 1),
-                new TopicPartition(""topic1"", 2)), topicPartitions);
+                new TopicPartition(""topic1"", 2)),
+            topicPartitions);
     }
     @Test
@@ -881,7 +884,7 @@ public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() {
         try {
             configurePartitionAssignor(Collections.singletonMap(StreamsConfig.APPLICATION_SERVER_CONFIG, (Object) ""localhost""));
-            Assert.fail(""expected to an exception due to invalid config"");
+            fail(""expected to an exception due to invalid config"");
         } catch (ConfigException e) {
             // pass
         }
@@ -893,7 +896,7 @@ public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() {
         try {
             configurePartitionAssignor(Collections.singletonMap(StreamsConfig.APPLICATION_SERVER_CONFIG, (Object) ""localhost:j87yhk""));
-            Assert.fail(""expected to an exception due to invalid config"");
+            fail(""expected to an exception due to invalid config"");
         } catch (ConfigException e) {
             // pass
         }
@@ -1088,21 +1091,36 @@ public void shouldThrowKafkaExceptionIfStreamThreadConfigIsNotThreadDataProvider
     }
     @Test
-    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions() {
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersionsV1V2() {
+        shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions(1, 2);
+    }
+
+    @Test
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersionsV1V3() {
+        shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions(1, 3);
+    }
+
+    @Test
+    public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersionsV2V3() {
+        shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions(2, 3);
+    }
+
+    private void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions(final int smallestVersion,
+                                                                                     final int otherVersion) {
         final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
         final Set<TaskId> emptyTasks = Collections.emptySet();
         subscriptions.put(
             ""consumer1"",
             new PartitionAssignor.Subscription(
                 Collections.singletonList(""topic1""),
-                new SubscriptionInfo(1, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+                new SubscriptionInfo(smallestVersion, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
             )
         );
         subscriptions.put(
             ""consumer2"",
             new PartitionAssignor.Subscription(
                 Collections.singletonList(""topic1""),
-                new SubscriptionInfo(2, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+                new SubscriptionInfo(otherVersion, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
             )
         );
@@ -1115,12 +1133,12 @@ public void shouldReturnLowestAssignmentVersionForDifferentSubscriptionVersions(
         final Map<String, PartitionAssignor.Assignment> assignment = partitionAssignor.assign(metadata, subscriptions);
         assertThat(assignment.size(), equalTo(2));
-        assertThat(AssignmentInfo.decode(assignment.get(""consumer1"").userData()).version(), equalTo(1));
-        assertThat(AssignmentInfo.decode(assignment.get(""consumer2"").userData()).version(), equalTo(1));
+        assertThat(AssignmentInfo.decode(assignment.get(""consumer1"").userData()).version(), equalTo(smallestVersion));
+        assertThat(AssignmentInfo.decode(assignment.get(""consumer2"").userData()).version(), equalTo(smallestVersion));
     }
     @Test
-    public void shouldDownGradeSubscription() {
+    public void shouldDownGradeSubscriptionToVersion1() {
         final Set<TaskId> emptyTasks = Collections.emptySet();
         mockTaskManager(
@@ -1135,6 +1153,46 @@ public void shouldDownGradeSubscription() {
         assertThat(SubscriptionInfo.decode(subscription.userData()).version(), equalTo(1));
     }
+    @Test
+    public void shouldDownGradeSubscriptionToVersion2For0101() {
+        shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_0101);
+    }
+
+    @Test
+    public void shouldDownGradeSubscriptionToVersion2For0102() {
+        shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_0102);
+    }
+
+    @Test
+    public void shouldDownGradeSubscriptionToVersion2For0110() {
+        shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_0110);
+    }
+
+    @Test
+    public void shouldDownGradeSubscriptionToVersion2For10() {
+        shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_10);
+    }
+
+    @Test
+    public void shouldDownGradeSubscriptionToVersion2For11() {
+        shouldDownGradeSubscriptionToVersion2(StreamsConfig.UPGRADE_FROM_11);
+    }
+
+    private void shouldDownGradeSubscriptionToVersion2(final Object upgradeFromValue) {
+        final Set<TaskId> emptyTasks = Collections.emptySet();
+
+        mockTaskManager(
+            emptyTasks,
+            emptyTasks,
+            UUID.randomUUID(),
+            builder);
+        configurePartitionAssignor(Collections.singletonMap(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFromValue));
+
+        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""topic1""));
+
+        assertThat(SubscriptionInfo.decode(subscription.userData()).version(), equalTo(2));
+    }
+
     private PartitionAssignor.Assignment createAssignment(final Map<HostInfo, Set<TopicPartition>> firstHostState) {
         final AssignmentInfo info = new AssignmentInfo(Collections.<TaskId>emptyList(),
                                                        Collections.<TaskId, Set<TopicPartition>>emptyMap(),
diff --git [file java] [file java]
index c1020a98ba9..c7382e7671c 100644
--- [file java]
+++ [file java]
@@ -22,85 +22,70 @@
 import org.apache.kafka.streams.state.HostInfo;
 import org.junit.Test;
-import java.io.ByteArrayOutputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.nio.ByteBuffer;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
 import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNull;
 public class AssignmentInfoTest {
+    private final List<TaskId> activeTasks = Arrays.asList(
+        new TaskId(0, 0),
+        new TaskId(0, 0),
+        new TaskId(0, 1), new TaskId(1, 0));
+    private final Map<TaskId, Set<TopicPartition>> standbyTasks = new HashMap<TaskId, Set<TopicPartition>>() {
+        {
+            put(new TaskId(1, 1),
+                Utils.mkSet(new TopicPartition(""t1"", 1), new TopicPartition(""t2"", 1)));
+            put(new TaskId(2, 0),
+                Utils.mkSet(new TopicPartition(""t3"", 0), new TopicPartition(""t3"", 0)));
+        }
+    };
+    private final Map<HostInfo, Set<TopicPartition>> globalAssignment = new HashMap<HostInfo, Set<TopicPartition>>() {
+        {
+            put(new HostInfo(""localhost"", 80),
+                Utils.mkSet(new TopicPartition(""t1"", 1), new TopicPartition(""t3"", 3)));
+        }
+    };
     @Test
-    public void testEncodeDecode() {
-        List<TaskId> activeTasks =
-                Arrays.asList(new TaskId(0, 0), new TaskId(0, 0), new TaskId(0, 1), new TaskId(1, 0));
-        Map<TaskId, Set<TopicPartition>> standbyTasks = new HashMap<>();
-
-        standbyTasks.put(new TaskId(1, 1), Utils.mkSet(new TopicPartition(""t1"", 1), new TopicPartition(""t2"", 1)));
-        standbyTasks.put(new TaskId(2, 0), Utils.mkSet(new TopicPartition(""t3"", 0), new TopicPartition(""t3"", 0)));
+    public void shouldUseLatestSupportedVersionByDefault() {
+        final AssignmentInfo info = new AssignmentInfo(activeTasks, standbyTasks, globalAssignment);
+        assertEquals(AssignmentInfo.LATEST_SUPPORTED_VERSION, info.version());
+    }
-        AssignmentInfo info = new AssignmentInfo(activeTasks, standbyTasks, new HashMap<HostInfo, Set<TopicPartition>>());
-        AssignmentInfo decoded = AssignmentInfo.decode(info.encode());
+    @Test(expected = IllegalArgumentException.class)
+    public void shouldThrowForUnknownVersion1() {
+        new AssignmentInfo(0, activeTasks, standbyTasks, globalAssignment);
+    }
-        assertEquals(info, decoded);
+    @Test(expected = IllegalArgumentException.class)
+    public void shouldThrowForUnknownVersion2() {
+        new AssignmentInfo(AssignmentInfo.LATEST_SUPPORTED_VERSION + 1, activeTasks, standbyTasks, globalAssignment);
     }
     @Test
-    public void shouldDecodePreviousVersion() throws IOException {
-        List<TaskId> activeTasks =
-                Arrays.asList(new TaskId(0, 0), new TaskId(0, 0), new TaskId(0, 1), new TaskId(1, 0));
-        Map<TaskId, Set<TopicPartition>> standbyTasks = new HashMap<>();
-
-        standbyTasks.put(new TaskId(1, 1), Utils.mkSet(new TopicPartition(""t1"", 1), new TopicPartition(""t2"", 1)));
-        standbyTasks.put(new TaskId(2, 0), Utils.mkSet(new TopicPartition(""t3"", 0), new TopicPartition(""t3"", 0)));
-        final AssignmentInfo oldVersion = new AssignmentInfo(1, activeTasks, standbyTasks, null);
-        final AssignmentInfo decoded = AssignmentInfo.decode(encodeV1(oldVersion));
-        assertEquals(oldVersion.activeTasks(), decoded.activeTasks());
-        assertEquals(oldVersion.standbyTasks(), decoded.standbyTasks());
-        assertNull(decoded.partitionsByHost()); // should be null as wasn't in V1
-        assertEquals(1, decoded.version());
+    public void shouldEncodeAndDecodeVersion1() {
+        final AssignmentInfo info = new AssignmentInfo(1, activeTasks, standbyTasks, globalAssignment);
+        final AssignmentInfo expectedInfo = new AssignmentInfo(1, AssignmentInfo.UNKNOWN, activeTasks, standbyTasks, Collections.<HostInfo, Set<TopicPartition>>emptyMap());
+        assertEquals(expectedInfo, AssignmentInfo.decode(info.encode()));
     }
-    /**
-     * This is a clone of what the V1 encoding did. The encode method has changed for V2
-     * so it is impossible to test compatibility without having this
-     */
-    private ByteBuffer encodeV1(AssignmentInfo oldVersion) throws IOException {
-        ByteArrayOutputStream baos = new ByteArrayOutputStream();
-        DataOutputStream out = new DataOutputStream(baos);
-        // Encode version
-        out.writeInt(oldVersion.version());
-        // Encode active tasks
-        out.writeInt(oldVersion.activeTasks().size());
-        for (TaskId id : oldVersion.activeTasks()) {
-            id.writeTo(out);
-        }
-        // Encode standby tasks
-        out.writeInt(oldVersion.standbyTasks().size());
-        for (Map.Entry<TaskId, Set<TopicPartition>> entry : oldVersion.standbyTasks().entrySet()) {
-            TaskId id = entry.getKey();
-            id.writeTo(out);
-
-            Set<TopicPartition> partitions = entry.getValue();
-            out.writeInt(partitions.size());
-            for (TopicPartition partition : partitions) {
-                out.writeUTF(partition.topic());
-                out.writeInt(partition.partition());
-            }
-        }
-
-        out.flush();
-        out.close();
-
-        return ByteBuffer.wrap(baos.toByteArray());
+    @Test
+    public void shouldEncodeAndDecodeVersion2() {
+        final AssignmentInfo info = new AssignmentInfo(2, activeTasks, standbyTasks, globalAssignment);
+        final AssignmentInfo expectedInfo = new AssignmentInfo(2, AssignmentInfo.UNKNOWN, activeTasks, standbyTasks, globalAssignment);
+        assertEquals(expectedInfo, AssignmentInfo.decode(info.encode()));
+    }
+    @Test
+    public void shouldEncodeAndDecodeVersion3() {
+        final AssignmentInfo info = new AssignmentInfo(3, activeTasks, standbyTasks, globalAssignment);
+        final AssignmentInfo expectedInfo = new AssignmentInfo(3, AssignmentInfo.LATEST_SUPPORTED_VERSION, activeTasks, standbyTasks, globalAssignment);
+        assertEquals(expectedInfo, AssignmentInfo.decode(info.encode()));
     }
 }
diff --git [file java] [file java]
index 633285a2b4d..e98b8ce0727 100644
--- [file java]
+++ [file java]
@@ -19,81 +19,60 @@
 import org.apache.kafka.streams.processor.TaskId;
 import org.junit.Test;
-import java.nio.ByteBuffer;
 import java.util.Arrays;
-import java.util.Collections;
 import java.util.HashSet;
 import java.util.Set;
 import java.util.UUID;
 import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNull;
 public class SubscriptionInfoTest {
+    private final UUID processId = UUID.randomUUID();
+    private final Set<TaskId> activeTasks = new HashSet<>(Arrays.asList(
+        new TaskId(0, 0),
+        new TaskId(0, 1),
+        new TaskId(1, 0)));
+    private final Set<TaskId> standbyTasks = new HashSet<>(Arrays.asList(
+        new TaskId(1, 1),
+        new TaskId(2, 0)));
-    @Test
-    public void testEncodeDecode() {
-        UUID processId = UUID.randomUUID();
+    private final static String IGNORED_USER_ENDPOINT = ""ignoredUserEndpoint:80"";
-        Set<TaskId> activeTasks =
-                new HashSet<>(Arrays.asList(new TaskId(0, 0), new TaskId(0, 1), new TaskId(1, 0)));
-        Set<TaskId> standbyTasks =
-                new HashSet<>(Arrays.asList(new TaskId(1, 1), new TaskId(2, 0)));
+    @Test
+    public void shouldUseLatestSupportedVersionByDefault() {
+        final SubscriptionInfo info = new SubscriptionInfo(processId, activeTasks, standbyTasks, ""localhost:80"");
+        assertEquals(SubscriptionInfo.LATEST_SUPPORTED_VERSION, info.version());
+    }
-        SubscriptionInfo info = new SubscriptionInfo(processId, activeTasks, standbyTasks, null);
-        SubscriptionInfo decoded = SubscriptionInfo.decode(info.encode());
+    @Test(expected = IllegalArgumentException.class)
+    public void shouldThrowForUnknownVersion1() {
+        new SubscriptionInfo(0, processId, activeTasks, standbyTasks, ""localhost:80"");
+    }
-        assertEquals(info, decoded);
+    @Test(expected = IllegalArgumentException.class)
+    public void shouldThrowForUnknownVersion2() {
+        new SubscriptionInfo(SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1, processId, activeTasks, standbyTasks, ""localhost:80"");
     }
     @Test
-    public void shouldEncodeDecodeWithUserEndPoint() {
-        SubscriptionInfo original = new SubscriptionInfo(UUID.randomUUID(),
-                Collections.singleton(new TaskId(0, 0)), Collections.<TaskId>emptySet(), ""localhost:80"");
-        SubscriptionInfo decoded = SubscriptionInfo.decode(original.encode());
-        assertEquals(original, decoded);
+    public void shouldEncodeAndDecodeVersion1() {
+        final SubscriptionInfo info = new SubscriptionInfo(1, processId, activeTasks, standbyTasks, IGNORED_USER_ENDPOINT);
+        final SubscriptionInfo expectedInfo = new SubscriptionInfo(1, SubscriptionInfo.UNKNOWN, processId, activeTasks, standbyTasks, null);
+        assertEquals(expectedInfo, SubscriptionInfo.decode(info.encode()));
     }
     @Test
-    public void shouldBeBackwardCompatible() {
-        UUID processId = UUID.randomUUID();
-
-        Set<TaskId> activeTasks =
-                new HashSet<>(Arrays.asList(new TaskId(0, 0), new TaskId(0, 1), new TaskId(1, 0)));
-        Set<TaskId> standbyTasks =
-                new HashSet<>(Arrays.asList(new TaskId(1, 1), new TaskId(2, 0)));
-
-        final ByteBuffer v1Encoding = encodePreviousVersion(processId, activeTasks, standbyTasks);
-        final SubscriptionInfo decode = SubscriptionInfo.decode(v1Encoding);
-        assertEquals(activeTasks, decode.prevTasks());
-        assertEquals(standbyTasks, decode.standbyTasks());
-        assertEquals(processId, decode.processId());
-        assertNull(decode.userEndPoint());
+    public void shouldEncodeAndDecodeVersion2() {
+        final SubscriptionInfo info = new SubscriptionInfo(2, processId, activeTasks, standbyTasks, ""localhost:80"");
+        final SubscriptionInfo expectedInfo = new SubscriptionInfo(2, SubscriptionInfo.UNKNOWN, processId, activeTasks, standbyTasks, ""localhost:80"");
+        assertEquals(expectedInfo, SubscriptionInfo.decode(info.encode()));
     }
-    /**
-     * This is a clone of what the V1 encoding did. The encode method has changed for V2
-     * so it is impossible to test compatibility without having this
-     */
-    private ByteBuffer encodePreviousVersion(UUID processId,  Set<TaskId> prevTasks, Set<TaskId> standbyTasks) {
-        ByteBuffer buf = ByteBuffer.allocate(4 /* version */ + 16 /* process id */ + 4 + prevTasks.size() * 8 + 4 + standbyTasks.size() * 8);
-        // version
-        buf.putInt(1);
-        // encode client UUID
-        buf.putLong(processId.getMostSignificantBits());
-        buf.putLong(processId.getLeastSignificantBits());
-        // encode ids of previously running tasks
-        buf.putInt(prevTasks.size());
-        for (TaskId id : prevTasks) {
-            id.writeTo(buf);
-        }
-        // encode ids of cached tasks
-        buf.putInt(standbyTasks.size());
-        for (TaskId id : standbyTasks) {
-            id.writeTo(buf);
-        }
-        buf.rewind();
-
-        return buf;
+    @Test
+    public void shouldEncodeAndDecodeVersion3() {
+        final SubscriptionInfo info = new SubscriptionInfo(3, processId, activeTasks, standbyTasks, ""localhost:80"");
+        final SubscriptionInfo expectedInfo = new SubscriptionInfo(3, SubscriptionInfo.LATEST_SUPPORTED_VERSION, processId, activeTasks, standbyTasks, ""localhost:80"");
+        assertEquals(expectedInfo, SubscriptionInfo.decode(info.encode()));
     }
+
 }
diff --git [file java] [file java]
new file mode 100644
index 00000000000..a8796cb056a
--- /dev/null
+++ [file java]
@@ -0,0 +1,104 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the ""License""); you may not use this file except in compliance with
+ * the License. You may obtain a copy of the License at
+ *
+ *    [link]
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.kafka.streams.tests;
+
+import org.apache.kafka.common.utils.Utils;
+import org.apache.kafka.streams.KafkaStreams;
+import org.apache.kafka.streams.StreamsBuilder;
+import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.processor.AbstractProcessor;
+import org.apache.kafka.streams.processor.Processor;
+import org.apache.kafka.streams.processor.ProcessorContext;
+import org.apache.kafka.streams.processor.ProcessorSupplier;
+
+import java.util.Properties;
+
+public class StreamsUpgradeTest {
+
+    /**
+     * This test cannot be executed, as long as Kafka 1.1.1 is not released
+     */
+    @SuppressWarnings(""unchecked"")
+    public static void main(final String args) throws Exception {
+        if (args.length < 2) {
+            System.err.println(""StreamsUpgradeTest requires three argument (kafka-url, properties-file) but only "" + args.length + "" provided: ""
+                + (args.length > 0 ? args : """"));
+        }
+        final String kafka = args;
+        final String propFileName = args.length > 1 ? args : null;
+
+        final Properties streamsProperties = Utils.loadProps(propFileName);
+
+        System.out.println(""StreamsTest instance started (StreamsUpgradeTest v1.1)"");
+        System.out.println(""kafka="" + kafka);
+        System.out.println(""props="" + streamsProperties);
+
+        final StreamsBuilder builder = new StreamsBuilder();
+        final KStream dataStream = builder.stream(""data"");
+        dataStream.process(printProcessorSupplier());
+        dataStream.to(""echo"");
+
+        final Properties config = new Properties();
+        config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
+        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
+        config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+        config.putAll(streamsProperties);
+
+        final KafkaStreams streams = new KafkaStreams(builder.build(), config);
+        streams.start();
+
+        Runtime.getRuntime().addShutdownHook(new Thread() {
+            @Override
+            public void run() {
+                streams.close();
+                System.out.println(""UPGRADE-TEST-CLIENT-CLOSED"");
+                System.out.flush();
+            }
+        });
+    }
+
+    private static <K, V> ProcessorSupplier<K, V> printProcessorSupplier() {
+        return new ProcessorSupplier<K, V>() {
+            public Processor<K, V> get() {
+                return new AbstractProcessor<K, V>() {
+                    private int numRecordsProcessed = 0;
+
+                    @Override
+                    public void init(final ProcessorContext context) {
+                        System.out.println(""initializing processor: topic=data taskId="" + context.taskId());
+                        numRecordsProcessed = 0;
+                    }
+
+                    @Override
+                    public void process(final K key, final V value) {
+                        numRecordsProcessed++;
+                        if (numRecordsProcessed % 100 == 0) {
+                            System.out.println(""processed "" + numRecordsProcessed + "" records from topic=data"");
+                        }
+                    }
+
+                    @Override
+                    public void punctuate(final long timestamp) {}
+
+                    @Override
+                    public void close() {}
+                };
+            }
+        };
+    }
+}
diff --git a/tests/kafkatest/services/streams.py b/tests/kafkatest/services/streams.py
index a5be816c737..e0e445de22a 100644
--- a/tests/kafkatest/services/streams.py
+++ b/tests/kafkatest/services/streams.py
@@ -413,6 +413,7 @@ def __init__(self, test_context, kafka):
                                                                  ""org.apache.kafka.streams.tests.StreamsUpgradeTest"",
                                                                  """")
         self.UPGRADE_FROM = None
+        self.UPGRADE_TO = None
     def set_version(self, kafka_streams_version):
         self.KAFKA_STREAMS_VERSION = kafka_streams_version
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py
index fa79d571f36..8b7d7712459 100644
--- a/tests/kafkatest/tests/streams/streams_upgrade_test.py
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py
@@ -23,8 +23,16 @@
 import random
 import time
+# broker 0.10.0 is not compatible with newer Kafka Streams versions
 broker_upgrade_versions = 
-simple_upgrade_versions_metadata_version_2 = 
+
+metadata_1_versions = 
+metadata_2_versions = 
+# we can add the following versions to `backward_compatible_metadata_2_versions` after the corresponding
+# bug-fix release 0.10.1.2, 0.10.2.2, 0.11.0.3, 1.0.2, and 1.1.1 are available:
+# str(LATEST_0_10_1), str(LATEST_0_10_2), str(LATEST_0_11_0), str(LATEST_1_0), str(LATEST_1_1)
+backward_compatible_metadata_2_versions = 
+metadata_3_versions = 
 class StreamsUpgradeTest(Test):
     """"""
@@ -39,6 +47,7 @@ def __init__(self, test_context):
             'echo' : { 'partitions': 5 },
             'data' : { 'partitions': 5 },
         }
+        self.leader = None
     def perform_broker_upgrade(self, to_version):
         self.logger.info(""First pass bounce - rolling broker upgrade"")
@@ -114,7 +123,7 @@ def test_upgrade_downgrade_brokers(self, from_version, to_version):
         node.account.ssh(""grep ALL-RECORDS-DELIVERED %s"" % self.driver.STDOUT_FILE, allow_fail=False)
         self.processor1.node.account.ssh_capture(""grep SMOKE-TEST-CLIENT-CLOSED %s"" % self.processor1.STDOUT_FILE, allow_fail=False)
-    @matrix(from_version=simple_upgrade_versions_metadata_version_2, to_version=simple_upgrade_versions_metadata_version_2)
+    @matrix(from_version=metadata_2_versions, to_version=metadata_2_versions)
     def test_simple_upgrade_downgrade(self, from_version, to_version):
         """"""
         Starts 3 KafkaStreams instances with <old_version>, and upgrades one-by-one to <new_version>
@@ -165,15 +174,12 @@ def test_simple_upgrade_downgrade(self, from_version, to_version):
         self.driver.stop()
-    #@parametrize(new_version=str(LATEST_0_10_1)) we cannot run this test until Kafka 0.10.1.2 is released
-    #@parametrize(new_version=str(LATEST_0_10_2)) we cannot run this test until Kafka 0.10.2.2 is released
-    #@parametrize(new_version=str(LATEST_0_11_0)) we cannot run this test until Kafka 0.11.0.3 is released
-    #@parametrize(new_version=str(LATEST_1_0)) we cannot run this test until Kafka 1.0.2 is released
-    #@parametrize(new_version=str(LATEST_1_1)) we cannot run this test until Kafka 1.1.1 is released
-    @parametrize(new_version=str(DEV_VERSION))
-    def test_metadata_upgrade(self, new_version):
+    #@matrix(from_version=metadata_1_versions, to_version=backward_compatible_metadata_2_versions)
+    @matrix(from_version=metadata_1_versions, to_version=metadata_3_versions)
+    @matrix(from_version=metadata_2_versions, to_version=metadata_3_versions)
+    def test_metadata_upgrade(self, from_version, to_version):
         """"""
-        Starts 3 KafkaStreams instances with version 0.10.0, and upgrades one-by-one to <new_version>
+        Starts 3 KafkaStreams instances with version <from_version> and upgrades one-by-one to <to_version>
         """"""
         self.zk = ZookeeperService(self.test_context, num_nodes=1)
@@ -189,7 +195,7 @@ def test_metadata_upgrade(self, new_version):
         self.processor3 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
         self.driver.start()
-        self.start_all_nodes_with(str(LATEST_0_10_0))
+        self.start_all_nodes_with(from_version)
         self.processors = 
@@ -200,13 +206,13 @@ def test_metadata_upgrade(self, new_version):
         random.shuffle(self.processors)
         for p in self.processors:
             p.CLEAN_NODE_ENABLED = False
-            self.do_rolling_bounce(p, ""0.10.0"", new_version, counter)
+            self.do_rolling_bounce(p, from_version, to_version, counter)
             counter = counter + 1
         # second rolling bounce
         random.shuffle(self.processors)
         for p in self.processors:
-            self.do_rolling_bounce(p, None, new_version, counter)
+            self.do_rolling_bounce(p, None, to_version, counter)
             counter = counter + 1
         # shutdown
diff --git a/tests/kafkatest/version.py b/tests/kafkatest/version.py
index 66e5fcf18aa..7823efac1d4 100644
--- a/tests/kafkatest/version.py
+++ b/tests/kafkatest/version.py
@@ -63,17 +63,17 @@ def get_version(node=None):
 DEV_BRANCH = KafkaVersion(""dev"")
 DEV_VERSION = KafkaVersion(""1.2.0-SNAPSHOT"")
-# 0.8.2.X versions
+# 0.8.2.x versions
 V_0_8_2_1 = KafkaVersion(""0.8.2.1"")
 V_0_8_2_2 = KafkaVersion(""0.8.2.2"")
 LATEST_0_8_2 = V_0_8_2_2
-# 0.9.0.X versions
+# 0.9.0.x versions
 V_0_9_0_0 = KafkaVersion(""0.9.0.0"")
 V_0_9_0_1 = KafkaVersion(""0.9.0.1"")
 LATEST_0_9 = V_0_9_0_1
-# 0.10.0.X versions
+# 0.10.0.x versions
 V_0_10_0_0 = KafkaVersion(""0.10.0.0"")
 V_0_10_0_1 = KafkaVersion(""0.10.0.1"")
 LATEST_0_10_0 = V_0_10_0_1
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 20 **
mjsax closed pull request #4636: KAFKA-6054: Add 'version probing' to Kafka Streams rebalance
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git [file java] [file java]
index 18dc891682d..bc549960de4 100644
--- [file java]
+++ [file java]
@@ -145,6 +145,7 @@
      */
     // TODO: currently we cannot get the full topic configurations and hence cannot allow topic configs without the prefix,
     //       this can be lifted once kafka.log.LogConfig is completely deprecated by org.apache.kafka.common.config.TopicConfig
+    @SuppressWarnings(""WeakerAccess"")
     public static final String TOPIC_PREFIX = ""topic."";
     /**
@@ -152,6 +153,7 @@
      * It is recommended to use {@link #consumerPrefix(String)} to add this prefix to {@link ConsumerConfig consumer
      * properties}.
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String CONSUMER_PREFIX = ""consumer."";
     /**
@@ -161,6 +163,7 @@
      * 2. consumer.
      * 3. 
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String MAIN_CONSUMER_PREFIX = ""main.consumer."";
     /**
@@ -170,6 +173,7 @@
      * 2. consumer.
      * 3. 
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String RESTORE_CONSUMER_PREFIX = ""restore.consumer."";
     /**
@@ -179,6 +183,7 @@
      * 2. consumer.
      * 3. 
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String GLOBAL_CONSUMER_PREFIX = ""global.consumer."";
     /**
@@ -186,6 +191,7 @@
      * It is recommended to use {@link #producerPrefix(String)} to add this prefix to {@link ProducerConfig producer
      * properties}.
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String PRODUCER_PREFIX = ""producer."";
     /**
@@ -193,202 +199,250 @@
      * It is recommended to use {@link #adminClientPrefix(String)} to add this prefix to {@link ProducerConfig producer
      * properties}.
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String ADMIN_CLIENT_PREFIX = ""admin."";
     /**
      * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 0.10.0.x}.
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String UPGRADE_FROM_0100 = ""0.10.0"";
     /**
      * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 0.10.1.x}.
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String UPGRADE_FROM_0101 = ""0.10.1"";
     /**
      * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 0.10.2.x}.
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String UPGRADE_FROM_0102 = ""0.10.2"";
     /**
      * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 0.11.0.x}.
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String UPGRADE_FROM_0110 = ""0.11.0"";
     /**
      * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 1.0.x}.
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String UPGRADE_FROM_10 = ""1.0"";
     /**
      * Config value for parameter {@link #UPGRADE_FROM_CONFIG ""upgrade.from""} for upgrading an application from version {@code 1.1.x}.
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String UPGRADE_FROM_11 = ""1.1"";
     /**
      * Config value for parameter {@link #PROCESSING_GUARANTEE_CONFIG ""processing.guarantee""} for at-least-once processing guarantees.
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String AT_LEAST_ONCE = ""at_least_once"";
     /**
      * Config value for parameter {@link #PROCESSING_GUARANTEE_CONFIG ""processing.guarantee""} for exactly-once processing guarantees.
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String EXACTLY_ONCE = ""exactly_once"";
     /** {@code application.id} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String APPLICATION_ID_CONFIG = ""application.id"";
     private static final String APPLICATION_ID_DOC = ""An identifier for the stream processing application. Must be unique within the Kafka cluster. It is used as 1) the default client-id prefix, 2) the group-id for membership management, 3) the changelog topic prefix."";
     /**{@code user.endpoint} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String APPLICATION_SERVER_CONFIG = ""application.server"";
     private static final String APPLICATION_SERVER_DOC = ""A host:port pair pointing to an embedded user defined endpoint that can be used for discovering the locations of state stores within a single KafkaStreams application"";
     /** {@code bootstrap.servers} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String BOOTSTRAP_SERVERS_CONFIG = CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG;
     /** {@code buffered.records.per.partition} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String BUFFERED_RECORDS_PER_PARTITION_CONFIG = ""buffered.records.per.partition"";
     private static final String BUFFERED_RECORDS_PER_PARTITION_DOC = ""The maximum number of records to buffer per partition."";
     /** {@code cache.max.bytes.buffering} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String CACHE_MAX_BYTES_BUFFERING_CONFIG = ""cache.max.bytes.buffering"";
     private static final String CACHE_MAX_BYTES_BUFFERING_DOC = ""Maximum number of memory bytes to be used for buffering across all threads"";
     /** {@code client.id} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String CLIENT_ID_CONFIG = CommonClientConfigs.CLIENT_ID_CONFIG;
     private static final String CLIENT_ID_DOC = ""An ID prefix string used for the client IDs of internal consumer, producer and restore-consumer,"" +
         "" with pattern '<client.id>-StreamThread-<threadSequenceNumber>-<consumer|producer|restore-consumer>'."";
     /** {@code commit.interval.ms} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String COMMIT_INTERVAL_MS_CONFIG = ""commit.interval.ms"";
     private static final String COMMIT_INTERVAL_MS_DOC = ""The frequency with which to save the position of the processor."" +
         "" (Note, if 'processing.guarantee' is set to '"" + EXACTLY_ONCE + ""', the default value is "" + EOS_DEFAULT_COMMIT_INTERVAL_MS + "","" +
         "" otherwise the default value is "" + DEFAULT_COMMIT_INTERVAL_MS + ""."";
     /** {@code connections.max.idle.ms} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String CONNECTIONS_MAX_IDLE_MS_CONFIG = CommonClientConfigs.CONNECTIONS_MAX_IDLE_MS_CONFIG;
     /**
      * {@code default.deserialization.exception.handler}
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG = ""default.deserialization.exception.handler"";
     private static final String DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_DOC = ""Exception handling class that implements the <code>org.apache.kafka.streams.errors.DeserializationExceptionHandler</code> interface."";
     /**
      * {@code default.production.exception.handler}
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG = ""default.production.exception.handler"";
     private static final String DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_DOC = ""Exception handling class that implements the <code>org.apache.kafka.streams.errors.ProductionExceptionHandler</code> interface."";
     /**
      * {@code default.windowed.key.serde.inner}
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS = ""default.windowed.key.serde.inner"";
     /**
      * {@code default.windowed.value.serde.inner}
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String DEFAULT_WINDOWED_VALUE_SERDE_INNER_CLASS = ""default.windowed.value.serde.inner"";
     /** {@code default key.serde} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String DEFAULT_KEY_SERDE_CLASS_CONFIG = ""default.key.serde"";
     private static final String DEFAULT_KEY_SERDE_CLASS_DOC = "" Default serializer / deserializer class for key that implements the <code>org.apache.kafka.common.serialization.Serde</code> interface. ""
             + ""Note when windowed serde class is used, one needs to set the inner serde class that implements the <code>org.apache.kafka.common.serialization.Serde</code> interface via '""
             + DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS + ""' or '"" + DEFAULT_WINDOWED_VALUE_SERDE_INNER_CLASS + ""' as well"";
     /** {@code default value.serde} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String DEFAULT_VALUE_SERDE_CLASS_CONFIG = ""default.value.serde"";
     private static final String DEFAULT_VALUE_SERDE_CLASS_DOC = ""Default serializer / deserializer class for value that implements the <code>org.apache.kafka.common.serialization.Serde</code> interface. ""
             + ""Note when windowed serde class is used, one needs to set the inner serde class that implements the <code>org.apache.kafka.common.serialization.Serde</code> interface via '""
             + DEFAULT_WINDOWED_KEY_SERDE_INNER_CLASS + ""' or '"" + DEFAULT_WINDOWED_VALUE_SERDE_INNER_CLASS + ""' as well"";
     /** {@code default.timestamp.extractor} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG = ""default.timestamp.extractor"";
     private static final String DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_DOC = ""Default timestamp extractor class that implements the <code>org.apache.kafka.streams.processor.TimestampExtractor</code> interface."";
     /** {@code metadata.max.age.ms} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String METADATA_MAX_AGE_CONFIG = CommonClientConfigs.METADATA_MAX_AGE_CONFIG;
     /** {@code metrics.num.samples} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String METRICS_NUM_SAMPLES_CONFIG = CommonClientConfigs.METRICS_NUM_SAMPLES_CONFIG;
     /** {@code metrics.record.level} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String METRICS_RECORDING_LEVEL_CONFIG = CommonClientConfigs.METRICS_RECORDING_LEVEL_CONFIG;
     /** {@code metric.reporters} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String METRIC_REPORTER_CLASSES_CONFIG = CommonClientConfigs.METRIC_REPORTER_CLASSES_CONFIG;
     /** {@code metrics.sample.window.ms} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String METRICS_SAMPLE_WINDOW_MS_CONFIG = CommonClientConfigs.METRICS_SAMPLE_WINDOW_MS_CONFIG;
     /** {@code num.standby.replicas} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String NUM_STANDBY_REPLICAS_CONFIG = ""num.standby.replicas"";
     private static final String NUM_STANDBY_REPLICAS_DOC = ""The number of standby replicas for each task."";
     /** {@code num.stream.threads} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String NUM_STREAM_THREADS_CONFIG = ""num.stream.threads"";
     private static final String NUM_STREAM_THREADS_DOC = ""The number of threads to execute stream processing."";
     /** {@code partition.grouper} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String PARTITION_GROUPER_CLASS_CONFIG = ""partition.grouper"";
     private static final String PARTITION_GROUPER_CLASS_DOC = ""Partition grouper class that implements the <code>org.apache.kafka.streams.processor.PartitionGrouper</code> interface."";
     /** {@code poll.ms} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String POLL_MS_CONFIG = ""poll.ms"";
     private static final String POLL_MS_DOC = ""The amount of time in milliseconds to block waiting for input."";
     /** {@code processing.guarantee} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String PROCESSING_GUARANTEE_CONFIG = ""processing.guarantee"";
     private static final String PROCESSING_GUARANTEE_DOC = ""The processing guarantee that should be used. Possible values are <code>"" + AT_LEAST_ONCE + ""</code> (default) and <code>"" + EXACTLY_ONCE + ""</code>. "" +
         ""Note that exactly-once processing requires a cluster of at least three brokers by default what is the recommended setting for production; for development you can change this, by adjusting broker setting `transaction.state.log.replication.factor`."";
     /** {@code receive.buffer.bytes} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String RECEIVE_BUFFER_CONFIG = CommonClientConfigs.RECEIVE_BUFFER_CONFIG;
     /** {@code reconnect.backoff.ms} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String RECONNECT_BACKOFF_MS_CONFIG = CommonClientConfigs.RECONNECT_BACKOFF_MS_CONFIG;
     /** {@code reconnect.backoff.max} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String RECONNECT_BACKOFF_MAX_MS_CONFIG = CommonClientConfigs.RECONNECT_BACKOFF_MAX_MS_CONFIG;
     /** {@code replication.factor} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String REPLICATION_FACTOR_CONFIG = ""replication.factor"";
     private static final String REPLICATION_FACTOR_DOC = ""The replication factor for change log topics and repartition topics created by the stream processing application."";
     /** {@code request.timeout.ms} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String REQUEST_TIMEOUT_MS_CONFIG = CommonClientConfigs.REQUEST_TIMEOUT_MS_CONFIG;
     /** {@code retries} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String RETRIES_CONFIG = CommonClientConfigs.RETRIES_CONFIG;
     /** {@code retry.backoff.ms} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String RETRY_BACKOFF_MS_CONFIG = CommonClientConfigs.RETRY_BACKOFF_MS_CONFIG;
     /** {@code rocksdb.config.setter} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String ROCKSDB_CONFIG_SETTER_CLASS_CONFIG = ""rocksdb.config.setter"";
     private static final String ROCKSDB_CONFIG_SETTER_CLASS_DOC = ""A Rocks DB config setter class or class name that implements the <code>org.apache.kafka.streams.state.RocksDBConfigSetter</code> interface"";
     /** {@code security.protocol} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String SECURITY_PROTOCOL_CONFIG = CommonClientConfigs.SECURITY_PROTOCOL_CONFIG;
     /** {@code send.buffer.bytes} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String SEND_BUFFER_CONFIG = CommonClientConfigs.SEND_BUFFER_CONFIG;
     /** {@code state.cleanup.delay} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String STATE_CLEANUP_DELAY_MS_CONFIG = ""state.cleanup.delay.ms"";
     private static final String STATE_CLEANUP_DELAY_MS_DOC = ""The amount of time in milliseconds to wait before deleting state when a partition has migrated. Only state directories that have not been modified for at least state.cleanup.delay.ms will be removed"";
     /** {@code state.dir} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String STATE_DIR_CONFIG = ""state.dir"";
     private static final String STATE_DIR_DOC = ""Directory location for state store."";
     /** {@code upgrade.from} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String UPGRADE_FROM_CONFIG = ""upgrade.from"";
-    public static final String UPGRADE_FROM_DOC = ""Allows upgrading from versions 0.10.0/0.10.1/0.10.2/0.11.0/1.0/1.1 to version 1.2 (or newer) in a backward compatible way. "" +
+    private static final String UPGRADE_FROM_DOC = ""Allows upgrading from versions 0.10.0/0.10.1/0.10.2/0.11.0/1.0/1.1 to version 1.2 (or newer) in a backward compatible way. "" +
         ""When upgrading from 1.2 to a newer version it is not required to specify this config."" +
         ""Default is null. Accepted values are \"""" + UPGRADE_FROM_0100 + ""\"", \"""" + UPGRADE_FROM_0101 + ""\"", \"""" + UPGRADE_FROM_0102 + ""\"", \"""" + UPGRADE_FROM_0110 + ""\"", \"""" + UPGRADE_FROM_10 + ""\"", \"""" + UPGRADE_FROM_11 + ""\"" (for upgrading from the corresponding old version)."";
     /** {@code windowstore.changelog.additional.retention.ms} */
+    @SuppressWarnings(""WeakerAccess"")
     public static final String WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_CONFIG = ""windowstore.changelog.additional.retention.ms"";
     private static final String WINDOW_STORE_CHANGE_LOG_ADDITIONAL_RETENTION_MS_DOC = ""Added to a windows maintainMs to ensure data is not deleted from the log prematurely. Allows for clock drift. Default is 1 day"";
@@ -653,6 +707,7 @@
     public static class InternalConfig {
         public static final String TASK_MANAGER_FOR_PARTITION_ASSIGNOR = ""__task.manager.instance__"";
+        public static final String VERSION_PROBING_FLAG = ""__version.probing.flag__"";
     }
     /**
@@ -662,6 +717,7 @@
      * @param consumerProp the consumer property to be masked
      * @return {@link #CONSUMER_PREFIX} + {@code consumerProp}
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static String consumerPrefix(final String consumerProp) {
         return CONSUMER_PREFIX + consumerProp;
     }
@@ -673,6 +729,7 @@ public static String consumerPrefix(final String consumerProp) {
      * @param consumerProp the consumer property to be masked
      * @return {@link #MAIN_CONSUMER_PREFIX} + {@code consumerProp}
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static String mainConsumerPrefix(final String consumerProp) {
         return MAIN_CONSUMER_PREFIX + consumerProp;
     }
@@ -684,6 +741,7 @@ public static String mainConsumerPrefix(final String consumerProp) {
      * @param consumerProp the consumer property to be masked
      * @return {@link #RESTORE_CONSUMER_PREFIX} + {@code consumerProp}
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static String restoreConsumerPrefix(final String consumerProp) {
         return RESTORE_CONSUMER_PREFIX + consumerProp;
     }
@@ -695,6 +753,7 @@ public static String restoreConsumerPrefix(final String consumerProp) {
      * @param consumerProp the consumer property to be masked
      * @return {@link #GLOBAL_CONSUMER_PREFIX} + {@code consumerProp}
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static String globalConsumerPrefix(final String consumerProp) {
         return GLOBAL_CONSUMER_PREFIX + consumerProp;
     }
@@ -706,6 +765,7 @@ public static String globalConsumerPrefix(final String consumerProp) {
      * @param producerProp the producer property to be masked
      * @return PRODUCER_PREFIX + {@code producerProp}
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static String producerPrefix(final String producerProp) {
         return PRODUCER_PREFIX + producerProp;
     }
@@ -717,6 +777,7 @@ public static String producerPrefix(final String producerProp) {
      * @param adminClientProp the admin client property to be masked
      * @return ADMIN_CLIENT_PREFIX + {@code adminClientProp}
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static String adminClientPrefix(final String adminClientProp) {
         return ADMIN_CLIENT_PREFIX + adminClientProp;
     }
@@ -728,6 +789,7 @@ public static String adminClientPrefix(final String adminClientProp) {
      * @param topicProp the topic property to be masked
      * @return TOPIC_PREFIX + {@code topicProp}
      */
+    @SuppressWarnings(""WeakerAccess"")
     public static String topicPrefix(final String topicProp) {
         return TOPIC_PREFIX + topicProp;
     }
@@ -737,6 +799,7 @@ public static String topicPrefix(final String topicProp) {
      *
      * @return a copy of the config definition
      */
+    @SuppressWarnings(""unused"")
     public static ConfigDef configDef() {
         return new ConfigDef(CONFIG);
     }
@@ -788,8 +851,8 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map<String, Obje
         // consumer/producer configurations, log a warning and remove the user defined value from the Map.
         // Thus the default values for these consumer/producer configurations that are suitable for
         // Streams will be used instead.
-        final Object maxInflightRequests = clientProvidedProps.get(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION);
-        if (eosEnabled && maxInflightRequests != null && 5 < (int) maxInflightRequests) {
+        final Object maxInFlightRequests = clientProvidedProps.get(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION);
+        if (eosEnabled && maxInFlightRequests != null && 5 < (int) maxInFlightRequests) {
             throw new ConfigException(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION + "" can't exceed 5 when using the idempotent producer"");
         }
         for (final String config: nonConfigurableConfigs) {
@@ -831,8 +894,9 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map<String, Obje
      * @param groupId      consumer groupId
      * @param clientId     clientId
      * @return Map of the consumer configuration.
-     * @Deprecated use {@link StreamsConfig#getMainConsumerConfigs(String, String)}
+     * @deprecated use {@link StreamsConfig#getMainConsumerConfigs(String, String)}
      */
+    @SuppressWarnings(""WeakerAccess"")
     @Deprecated
     public Map<String, Object> getConsumerConfigs(final String groupId,
                                                   final String clientId) {
@@ -853,13 +917,14 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map<String, Obje
      * @param clientId     clientId
      * @return Map of the consumer configuration.
      */
+    @SuppressWarnings(""WeakerAccess"")
     public Map<String, Object> getMainConsumerConfigs(final String groupId,
                                                       final String clientId) {
-        Map<String, Object> consumerProps = getCommonConsumerConfigs();
+        final Map<String, Object> consumerProps = getCommonConsumerConfigs();
         // Get main consumer override configs
-        Map<String, Object> mainConsumerProps = originalsWithPrefix(MAIN_CONSUMER_PREFIX);
-        for (Map.Entry<String, Object> entry: mainConsumerProps.entrySet()) {
+        final Map<String, Object> mainConsumerProps = originalsWithPrefix(MAIN_CONSUMER_PREFIX);
+        for (final Map.Entry<String, Object> entry: mainConsumerProps.entrySet()) {
             consumerProps.put(entry.getKey(), entry.getValue());
         }
@@ -919,12 +984,13 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map<String, Obje
      * @param clientId clientId
      * @return Map of the restore consumer configuration.
      */
+    @SuppressWarnings(""WeakerAccess"")
     public Map<String, Object> getRestoreConsumerConfigs(final String clientId) {
-        Map<String, Object> baseConsumerProps = getCommonConsumerConfigs();
+        final Map<String, Object> baseConsumerProps = getCommonConsumerConfigs();
         // Get restore consumer override configs
-        Map<String, Object> restoreConsumerProps = originalsWithPrefix(RESTORE_CONSUMER_PREFIX);
-        for (Map.Entry<String, Object> entry: restoreConsumerProps.entrySet()) {
+        final Map<String, Object> restoreConsumerProps = originalsWithPrefix(RESTORE_CONSUMER_PREFIX);
+        for (final Map.Entry<String, Object> entry: restoreConsumerProps.entrySet()) {
             baseConsumerProps.put(entry.getKey(), entry.getValue());
         }
@@ -950,12 +1016,13 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map<String, Obje
      * @param clientId clientId
      * @return Map of the global consumer configuration.
      */
+    @SuppressWarnings(""WeakerAccess"")
     public Map<String, Object> getGlobalConsumerConfigs(final String clientId) {
-        Map<String, Object> baseConsumerProps = getCommonConsumerConfigs();
+        final Map<String, Object> baseConsumerProps = getCommonConsumerConfigs();
         // Get global consumer override configs
-        Map<String, Object> globalConsumerProps = originalsWithPrefix(GLOBAL_CONSUMER_PREFIX);
-        for (Map.Entry<String, Object> entry: globalConsumerProps.entrySet()) {
+        final Map<String, Object> globalConsumerProps = originalsWithPrefix(GLOBAL_CONSUMER_PREFIX);
+        for (final Map.Entry<String, Object> entry: globalConsumerProps.entrySet()) {
             baseConsumerProps.put(entry.getKey(), entry.getValue());
         }
@@ -977,6 +1044,7 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map<String, Obje
      * @param clientId clientId
      * @return Map of the producer configuration.
      */
+    @SuppressWarnings(""WeakerAccess"")
     public Map<String, Object> getProducerConfigs(final String clientId) {
         final Map<String, Object> clientProvidedProps = getClientPropsWithPrefix(PRODUCER_PREFIX, ProducerConfig.configNames());
@@ -999,6 +1067,7 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map<String, Obje
      * @param clientId clientId
      * @return Map of the admin client configuration.
      */
+    @SuppressWarnings(""WeakerAccess"")
     public Map<String, Object> getAdminConfigs(final String clientId) {
         final Map<String, Object> clientProvidedProps = getClientPropsWithPrefix(ADMIN_CLIENT_PREFIX, AdminClientConfig.configNames());
@@ -1045,10 +1114,11 @@ private void checkIfUnexpectedUserSpecifiedConsumerConfig(final Map<String, Obje
      *
      * @return an configured instance of key Serde class
      */
+    @SuppressWarnings(""WeakerAccess"")
     public Serde defaultKeySerde() {
-        Object keySerdeConfigSetting = get(DEFAULT_KEY_SERDE_CLASS_CONFIG);
+        final Object keySerdeConfigSetting = get(DEFAULT_KEY_SERDE_CLASS_CONFIG);
         try {
-            Serde<?> serde = getConfiguredInstance(DEFAULT_KEY_SERDE_CLASS_CONFIG, Serde.class);
+            final Serde<?> serde = getConfiguredInstance(DEFAULT_KEY_SERDE_CLASS_CONFIG, Serde.class);
             serde.configure(originals(), true);
             return serde;
         } catch (final Exception e) {
@@ -1063,10 +1133,11 @@ public Serde defaultKeySerde() {
      *
      * @return an configured instance of value Serde class
      */
+    @SuppressWarnings(""WeakerAccess"")
     public Serde defaultValueSerde() {
-        Object valueSerdeConfigSetting = get(DEFAULT_VALUE_SERDE_CLASS_CONFIG);
+        final Object valueSerdeConfigSetting = get(DEFAULT_VALUE_SERDE_CLASS_CONFIG);
         try {
-            Serde<?> serde = getConfiguredInstance(DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serde.class);
+            final Serde<?> serde = getConfiguredInstance(DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serde.class);
             serde.configure(originals(), false);
             return serde;
         } catch (final Exception e) {
@@ -1075,14 +1146,17 @@ public Serde defaultValueSerde() {
         }
     }
+    @SuppressWarnings(""WeakerAccess"")
     public TimestampExtractor defaultTimestampExtractor() {
         return getConfiguredInstance(DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG, TimestampExtractor.class);
     }
+    @SuppressWarnings(""WeakerAccess"")
     public DeserializationExceptionHandler defaultDeserializationExceptionHandler() {
         return getConfiguredInstance(DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG, DeserializationExceptionHandler.class);
     }
+    @SuppressWarnings(""WeakerAccess"")
     public ProductionExceptionHandler defaultProductionExceptionHandler() {
         return getConfiguredInstance(DEFAULT_PRODUCTION_EXCEPTION_HANDLER_CLASS_CONFIG, ProductionExceptionHandler.class);
     }
diff --git [file java] [file java]
index 6f4f454bfc4..079d405cb50 100644
--- [file java]
+++ [file java]
@@ -40,15 +40,15 @@
     private final Logger log;
     private final String taskTypeName;
     private final TaskAction<T> commitAction;
-    private Map<TaskId, T> created = new HashMap<>();
-    private Map<TaskId, T> suspended = new HashMap<>();
-    private Map<TaskId, T> restoring = new HashMap<>();
-    private Set<TopicPartition> restoredPartitions = new HashSet<>();
-    private Set<TaskId> previousActiveTasks = new HashSet<>();
+    private final Map<TaskId, T> created = new HashMap<>();
+    private final Map<TaskId, T> suspended = new HashMap<>();
+    private final Map<TaskId, T> restoring = new HashMap<>();
+    private final Set<TopicPartition> restoredPartitions = new HashSet<>();
+    private final Set<TaskId> previousActiveTasks = new HashSet<>();
     // IQ may access this map.
-    Map<TaskId, T> running = new ConcurrentHashMap<>();
-    private Map<TopicPartition, T> runningByPartition = new HashMap<>();
-    Map<TopicPartition, T> restoringByPartition = new HashMap<>();
+    final Map<TaskId, T> running = new ConcurrentHashMap<>();
+    private final Map<TopicPartition, T> runningByPartition = new HashMap<>();
+    final Map<TopicPartition, T> restoringByPartition = new HashMap<>();
     AssignedTasks(final LogContext logContext,
                   final String taskTypeName) {
@@ -176,7 +176,7 @@ private RuntimeException closeNonRunningTasks(final Collection<T> tasks) {
     private RuntimeException suspendTasks(final Collection<T> tasks) {
         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);
-        for (Iterator<T> it = tasks.iterator(); it.hasNext(); ) {
+        for (final Iterator<T> it = tasks.iterator(); it.hasNext(); ) {
             final T task = it.next();
             try {
                 task.suspend();
@@ -249,10 +249,10 @@ boolean maybeResumeSuspendedTask(final TaskId taskId, final Set<TopicPartition>
     private void addToRestoring(final T task) {
         restoring.put(task.id(), task);
-        for (TopicPartition topicPartition : task.partitions()) {
+        for (final TopicPartition topicPartition : task.partitions()) {
             restoringByPartition.put(topicPartition, task);
         }
-        for (TopicPartition topicPartition : task.changelogPartitions()) {
+        for (final TopicPartition topicPartition : task.changelogPartitions()) {
             restoringByPartition.put(topicPartition, task);
         }
     }
@@ -264,10 +264,10 @@ private void transitionToRunning(final T task) {
         log.debug(""transitioning {} {} to running"", taskTypeName, task.id());
         running.put(task.id(), task);
         task.initializeTopology();
-        for (TopicPartition topicPartition : task.partitions()) {
+        for (final TopicPartition topicPartition : task.partitions()) {
             runningByPartition.put(topicPartition, task);
         }
-        for (TopicPartition topicPartition : task.changelogPartitions()) {
+        for (final TopicPartition topicPartition : task.changelogPartitions()) {
             runningByPartition.put(topicPartition, task);
         }
     }
@@ -356,7 +356,7 @@ int commit() {
     void applyToRunningTasks(final TaskAction<T> action) {
         RuntimeException firstException = null;
-        for (Iterator<T> it = running().iterator(); it.hasNext(); ) {
+        for (final Iterator<T> it = running().iterator(); it.hasNext(); ) {
             final T task = it.next();
             try {
                 action.apply(task);
diff --git [file java] [file java]
index 3080d2e1583..e72c4a5de94 100644
--- [file java]
+++ [file java]
@@ -62,6 +62,7 @@
 import java.util.Set;
 import java.util.UUID;
 import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 import static java.util.Collections.singleton;
@@ -69,7 +70,7 @@
 public class StreamThread extends Thread {
     private final static int UNLIMITED_RECORDS = -1;
-    private static final AtomicInteger STREAM_THREAD_ID_SEQUENCE = new AtomicInteger(1);
+    private final static AtomicInteger STREAM_THREAD_ID_SEQUENCE = new AtomicInteger(1);
     /**
      * Stream thread states are the possible states that a stream thread can be in.
@@ -264,7 +265,9 @@ public void onPartitionsAssigned(final Collection<TopicPartition> assignment) {
                 if (streamThread.setState(State.PARTITIONS_ASSIGNED) == null) {
                     return;
                 }
-                taskManager.createTasks(assignment);
+                if (!streamThread.versionProbingFlag.get()) {
+                    taskManager.createTasks(assignment);
+                }
             } catch (final Throwable t) {
                 log.error(
                     ""Error caught during partition assignment, "" +
@@ -298,7 +301,11 @@ public void onPartitionsRevoked(final Collection<TopicPartition> assignment) {
                 final long start = time.milliseconds();
                 try {
                     // suspend active tasks
-                    taskManager.suspendTasksAndState();
+                    if (streamThread.versionProbingFlag.get()) {
+                        streamThread.versionProbingFlag.set(false);
+                    } else {
+                        taskManager.suspendTasksAndState();
+                    }
                 } catch (final Throwable t) {
                     log.error(
                         ""Error caught during partition revocation, "" +
@@ -555,6 +562,7 @@ StandbyTask createTask(final Consumer<byte, byte> consumer,
     private final String logPrefix;
     private final TaskManager taskManager;
     private final StreamsMetricsThreadImpl streamsMetrics;
+    private final AtomicBoolean versionProbingFlag;
     private long lastCommitMs;
     private long timerStartedMs;
@@ -647,6 +655,8 @@ public static StreamThread create(final InternalTopologyBuilder builder,
         final String applicationId = config.getString(StreamsConfig.APPLICATION_ID_CONFIG);
         final Map<String, Object> consumerConfigs = config.getMainConsumerConfigs(applicationId, threadClientId);
         consumerConfigs.put(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR, taskManager);
+        final AtomicBoolean versionProbingFlag = new AtomicBoolean();
+        consumerConfigs.put(StreamsConfig.InternalConfig.VERSION_PROBING_FLAG, versionProbingFlag);
         String originalReset = null;
         if (builder.earliestResetTopicsPattern().pattern().equals("""")) {
             originalReset = (String) consumerConfigs.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);
@@ -666,7 +676,8 @@ public static StreamThread create(final InternalTopologyBuilder builder,
             streamsMetrics,
             builder,
             threadClientId,
-            logContext);
+            logContext,
+            versionProbingFlag);
     }
     public StreamThread(final Time time,
@@ -679,7 +690,8 @@ public StreamThread(final Time time,
                         final StreamsMetricsThreadImpl streamsMetrics,
                         final InternalTopologyBuilder builder,
                         final String threadClientId,
-                        final LogContext logContext) {
+                        final LogContext logContext,
+                        final AtomicBoolean versionProbingFlag) {
         super(threadClientId);
         this.stateLock = new Object();
@@ -696,6 +708,7 @@ public StreamThread(final Time time,
         this.restoreConsumer = restoreConsumer;
         this.consumer = consumer;
         this.originalReset = originalReset;
+        this.versionProbingFlag = versionProbingFlag;
         this.pollTimeMs = config.getLong(StreamsConfig.POLL_MS_CONFIG);
         this.commitTimeMs = config.getLong(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG);
@@ -750,19 +763,26 @@ private void runLoop() {
         while (isRunning()) {
             try {
                 recordsProcessedBeforeCommit = runOnce(recordsProcessedBeforeCommit);
+                if (versionProbingFlag.get()) {
+                    log.info(""Version probing detected. Triggering new rebalance."");
+                    enforceRebalance();
+                }
             } catch (final TaskMigratedException ignoreAndRejoinGroup) {
                 log.warn(""Detected task {} that got migrated to another thread. "" +
                         ""This implies that this thread missed a rebalance and dropped out of the consumer group. "" +
                         ""Will try to rejoin the consumer group. Below is the detailed description of the task:\n{}"",
                     ignoreAndRejoinGroup.migratedTask().id(), ignoreAndRejoinGroup.migratedTask().toString("">""));
-                // re-subscribe to enforce a rebalance in the next poll call
-                consumer.unsubscribe();
-                consumer.subscribe(builder.sourceTopicPattern(), rebalanceListener);
+                enforceRebalance();
             }
         }
     }
+    private void enforceRebalance() {
+        consumer.unsubscribe();
+        consumer.subscribe(builder.sourceTopicPattern(), rebalanceListener);
+    }
+
     /**
      * @throws IllegalStateException If store gets registered after initialized is already finished
      * @throws StreamsException      If the store's change log does not contain the partition
diff --git [file java] [file java]
index e1464e6b72c..db94ac0c852 100644
--- [file java]
+++ [file java]
@@ -51,6 +51,7 @@
 import java.util.Map;
 import java.util.Set;
 import java.util.UUID;
+import java.util.concurrent.atomic.AtomicBoolean;
 import static org.apache.kafka.common.utils.Utils.getHost;
 import static org.apache.kafka.common.utils.Utils.getPort;
@@ -59,6 +60,12 @@
     private final static int UNKNOWN = -1;
     public final static int NOT_AVAILABLE = -2;
+    private final static int VERSION_ONE = 1;
+    private final static int VERSION_TWO = 2;
+    private final static int VERSION_THREE = 3;
+    private final static int EARLIEST_PROBEABLE_VERSION = VERSION_THREE;
+    private int minReceivedMetadataVersion = UNKNOWN;
+    protected Set<Integer> supportedVersions = new HashSet<>();
     private Logger log;
     private String logPrefix;
@@ -159,7 +166,7 @@ public String toString() {
         }
     }
-    private static final Comparator<TopicPartition> PARTITION_COMPARATOR = new Comparator<TopicPartition>() {
+    protected static final Comparator<TopicPartition> PARTITION_COMPARATOR = new Comparator<TopicPartition>() {
         @Override
         public int compare(final TopicPartition p1,
                            final TopicPartition p2) {
@@ -178,12 +185,21 @@ public int compare(final TopicPartition p1,
     private TaskManager taskManager;
     private PartitionGrouper partitionGrouper;
+    private AtomicBoolean versionProbingFlag;
-    private int userMetadataVersion = SubscriptionInfo.LATEST_SUPPORTED_VERSION;
+    protected int usedSubscriptionMetadataVersion = SubscriptionInfo.LATEST_SUPPORTED_VERSION;
     private InternalTopicManager internalTopicManager;
     private CopartitionedTopicsValidator copartitionedTopicsValidator;
+    protected String userEndPoint() {
+        return userEndPoint;
+    }
+
+    protected TaskManager taskManger() {
+        return taskManager;
+    }
+
     /**
      * We need to have the PartitionAssignor and its StreamThread to be mutually accessible
      * since the former needs later's cached metadata while sending subscriptions,
@@ -204,15 +220,15 @@ public void configure(final Map<String, ?> configs) {
             switch (upgradeFrom) {
                 case StreamsConfig.UPGRADE_FROM_0100:
                     log.info(""Downgrading metadata version from {} to 1 for upgrade from 0.10.0.x."", SubscriptionInfo.LATEST_SUPPORTED_VERSION);
-                    userMetadataVersion = 1;
+                    usedSubscriptionMetadataVersion = VERSION_ONE;
                     break;
                 case StreamsConfig.UPGRADE_FROM_0101:
                 case StreamsConfig.UPGRADE_FROM_0102:
                 case StreamsConfig.UPGRADE_FROM_0110:
                 case StreamsConfig.UPGRADE_FROM_10:
                 case StreamsConfig.UPGRADE_FROM_11:
-                    log.info(""Downgrading metadata version from {} to 2 for upgrade from "" + upgradeFrom + "".x."", SubscriptionInfo.LATEST_SUPPORTED_VERSION);
-                    userMetadataVersion = 2;
+                    log.info(""Downgrading metadata version from {} to 2 for upgrade from {}.x."", SubscriptionInfo.LATEST_SUPPORTED_VERSION, upgradeFrom);
+                    usedSubscriptionMetadataVersion = VERSION_TWO;
                     break;
                 default:
                     throw new IllegalArgumentException(""Unknown configuration value for parameter 'upgrade.from': "" + upgradeFrom);
@@ -234,6 +250,21 @@ public void configure(final Map<String, ?> configs) {
         taskManager = (TaskManager) o;
+        final Object o2 = configs.get(StreamsConfig.InternalConfig.VERSION_PROBING_FLAG);
+        if (o2 == null) {
+            final KafkaException fatalException = new KafkaException(""VersionProbingFlag is not specified"");
+            log.error(fatalException.getMessage(), fatalException);
+            throw fatalException;
+        }
+
+        if (!(o2 instanceof AtomicBoolean)) {
+            final KafkaException fatalException = new KafkaException(String.format(""%s is not an instance of %s"", o2.getClass().getName(), AtomicBoolean.class.getName()));
+            log.error(fatalException.getMessage(), fatalException);
+            throw fatalException;
+        }
+
+        versionProbingFlag = (AtomicBoolean) o2;
+
         numStandbyReplicas = streamsConfig.getInt(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG);
         partitionGrouper = streamsConfig.getConfiguredInstance(StreamsConfig.PARTITION_GROUPER_CLASS_CONFIG, PartitionGrouper.class);
@@ -277,7 +308,7 @@ public Subscription subscription(final Set<String> topics) {
         final Set<TaskId> standbyTasks = taskManager.cachedTasksIds();
         standbyTasks.removeAll(previousActiveTasks);
         final SubscriptionInfo data = new SubscriptionInfo(
-            userMetadataVersion,
+            usedSubscriptionMetadataVersion,
             taskManager.processId(),
             previousActiveTasks,
             standbyTasks,
@@ -313,20 +344,25 @@ public Subscription subscription(final Set<String> topics) {
                                           final Map<String, Subscription> subscriptions) {
         // construct the client metadata from the decoded subscription info
         final Map<UUID, ClientMetadata> clientsMetadata = new HashMap<>();
+        final Set<String> futureConsumers = new HashSet<>();
-        int minUserMetadataVersion = SubscriptionInfo.LATEST_SUPPORTED_VERSION;
+        minReceivedMetadataVersion = SubscriptionInfo.LATEST_SUPPORTED_VERSION;
+        supportedVersions.clear();
+        int futureMetadataVersion = UNKNOWN;
         for (final Map.Entry<String, Subscription> entry : subscriptions.entrySet()) {
             final String consumerId = entry.getKey();
             final Subscription subscription = entry.getValue();
             final SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());
             final int usedVersion = info.version();
+            supportedVersions.add(info.latestSupportedVersion());
             if (usedVersion > SubscriptionInfo.LATEST_SUPPORTED_VERSION) {
-                throw new IllegalStateException(""Unknown metadata version: "" + usedVersion
-                    + ""; latest supported version: "" + SubscriptionInfo.LATEST_SUPPORTED_VERSION);
+                futureMetadataVersion = usedVersion;
+                futureConsumers.add(consumerId);
+                continue;
             }
-            if (usedVersion < minUserMetadataVersion) {
-                minUserMetadataVersion = usedVersion;
+            if (usedVersion < minReceivedMetadataVersion) {
+                minReceivedMetadataVersion = usedVersion;
             }
             // create the new client metadata if necessary
@@ -341,6 +377,27 @@ public Subscription subscription(final Set<String> topics) {
             clientMetadata.addConsumer(consumerId, info);
         }
+        final boolean versionProbing;
+        if (futureMetadataVersion != UNKNOWN) {
+            if (minReceivedMetadataVersion >= EARLIEST_PROBEABLE_VERSION) {
+                log.info(""Received a future (version probing) subscription (version: {}). Sending empty assignment back (with supported version {})."",
+                    futureMetadataVersion,
+                    SubscriptionInfo.LATEST_SUPPORTED_VERSION);
+                versionProbing = true;
+            } else {
+                throw new IllegalStateException(""Received a future (version probing) subscription (version: "" + futureMetadataVersion
+                    + "") and an incompatible pre Kafka 2.0 subscription (version: "" + minReceivedMetadataVersion + "") at the same time."");
+            }
+        } else {
+            versionProbing = false;
+        }
+
+        if (minReceivedMetadataVersion < SubscriptionInfo.LATEST_SUPPORTED_VERSION) {
+            log.info(""Downgrading metadata to version {}. Latest supported version is {}."",
+                minReceivedMetadataVersion,
+                SubscriptionInfo.LATEST_SUPPORTED_VERSION);
+        }
+
         log.debug(""Constructed client metadata {} from the member subscriptions."", clientsMetadata);
         // ---------------- Step Zero ---------------- //
@@ -457,12 +514,7 @@ public Subscription subscription(final Set<String> topics) {
             allAssignedPartitions.addAll(partitions);
             final TaskId id = entry.getKey();
-            Set<TaskId> ids = tasksByTopicGroup.get(id.topicGroupId);
-            if (ids == null) {
-                ids = new HashSet<>();
-                tasksByTopicGroup.put(id.topicGroupId, ids);
-            }
-            ids.add(id);
+            tasksByTopicGroup.computeIfAbsent(id.topicGroupId, k -> new HashSet<>()).add(id);
         }
         for (final String topic : allSourceTopics) {
             final List<PartitionInfo> partitionInfoList = fullMetadata.partitionsForTopic(topic);
@@ -530,7 +582,7 @@ public Subscription subscription(final Set<String> topics) {
         // construct the global partition assignment per host map
         final Map<HostInfo, Set<TopicPartition>> partitionsByHostState = new HashMap<>();
-        if (minUserMetadataVersion == 2 || minUserMetadataVersion == 3) {
+        if (minReceivedMetadataVersion == 2 || minReceivedMetadataVersion == 3) {
             for (final Map.Entry<UUID, ClientMetadata> entry : clientsMetadata.entrySet()) {
                 final HostInfo hostInfo = entry.getValue().hostInfo;
@@ -548,8 +600,23 @@ public Subscription subscription(final Set<String> topics) {
         }
         taskManager.setPartitionsByHostState(partitionsByHostState);
-        // within the client, distribute tasks to its owned consumers
+        final Map<String, Assignment> assignment;
+        if (versionProbing) {
+            assignment = versionProbingAssignment(clientsMetadata, partitionsForTask, partitionsByHostState, futureConsumers, minReceivedMetadataVersion);
+        } else {
+            assignment = computeNewAssignment(clientsMetadata, partitionsForTask, partitionsByHostState, minReceivedMetadataVersion);
+        }
+
+        return assignment;
+    }
+
+    private Map<String, Assignment> computeNewAssignment(final Map<UUID, ClientMetadata> clientsMetadata,
+                                                         final Map<TaskId, Set<TopicPartition>> partitionsForTask,
+                                                         final Map<HostInfo, Set<TopicPartition>> partitionsByHostState,
+                                                         final int minUserMetadataVersion) {
         final Map<String, Assignment> assignment = new HashMap<>();
+
+        // within the client, distribute tasks to its owned consumers
         for (final Map.Entry<UUID, ClientMetadata> entry : clientsMetadata.entrySet()) {
             final Set<String> consumers = entry.getValue().consumers;
             final ClientState state = entry.getValue().state;
@@ -574,12 +641,7 @@ public Subscription subscription(final Set<String> topics) {
                 if (!state.standbyTasks().isEmpty()) {
                     final List<TaskId> assignedStandbyList = interleavedStandby.get(consumerTaskIndex);
                     for (final TaskId taskId : assignedStandbyList) {
-                        Set<TopicPartition> standbyPartitions = standby.get(taskId);
-                        if (standbyPartitions == null) {
-                            standbyPartitions = new HashSet<>();
-                            standby.put(taskId, standbyPartitions);
-                        }
-                        standbyPartitions.addAll(partitionsForTask.get(taskId));
+                        standby.computeIfAbsent(taskId, k -> new HashSet<>()).addAll(partitionsForTask.get(taskId));
                     }
                 }
@@ -603,13 +665,63 @@ public Subscription subscription(final Set<String> topics) {
         return assignment;
     }
+    private Map<String, Assignment> versionProbingAssignment(final Map<UUID, ClientMetadata> clientsMetadata,
+                                                             final Map<TaskId, Set<TopicPartition>> partitionsForTask,
+                                                             final Map<HostInfo, Set<TopicPartition>> partitionsByHostState,
+                                                             final Set<String> futureConsumers,
+                                                             final int minUserMetadataVersion) {
+        final Map<String, Assignment> assignment = new HashMap<>();
+
+        // assign previously assigned tasks to ""old consumers""
+        for (final ClientMetadata clientMetadata : clientsMetadata.values()) {
+            for (final String consumerId : clientMetadata.consumers) {
+
+                if (futureConsumers.contains(consumerId)) {
+                    continue;
+                }
+
+                final List<TaskId> activeTasks = new ArrayList<>(clientMetadata.state.prevActiveTasks());
+
+                final List<TopicPartition> assignedPartitions = new ArrayList<>();
+                for (final TaskId taskId : activeTasks) {
+                    assignedPartitions.addAll(partitionsForTask.get(taskId));
+                }
+
+                final Map<TaskId, Set<TopicPartition>> standbyTasks = new HashMap<>();
+                for (final TaskId taskId : clientMetadata.state.prevStandbyTasks()) {
+                    standbyTasks.put(taskId, partitionsForTask.get(taskId));
+                }
+
+                assignment.put(consumerId, new Assignment(
+                    assignedPartitions,
+                    new AssignmentInfo(
+                        minUserMetadataVersion,
+                        activeTasks,
+                        standbyTasks,
+                        partitionsByHostState)
+                        .encode()
+                ));
+            }
+        }
+
+        // add empty assignment for ""future version"" clients (ie, empty version probing response)
+        for (final String consumerId : futureConsumers) {
+            assignment.put(consumerId, new Assignment(
+                Collections.emptyList(),
+                new AssignmentInfo().encode()
+            ));
+        }
+
+        return assignment;
+    }
+
     // visible for testing
     List<List<TaskId>> interleaveTasksByGroupId(final Collection<TaskId> taskIds, final int numberThreads) {
         final LinkedList<TaskId> sortedTasks = new LinkedList<>(taskIds);
         Collections.sort(sortedTasks);
         final List<List<TaskId>> taskIdsForConsumerAssignment = new ArrayList<>(numberThreads);
         for (int i = 0; i < numberThreads; i++) {
-            taskIdsForConsumerAssignment.add(new ArrayList<TaskId>());
+            taskIdsForConsumerAssignment.add(new ArrayList<>());
         }
         while (!sortedTasks.isEmpty()) {
             for (final List<TaskId> taskIdList : taskIdsForConsumerAssignment) {
@@ -632,7 +744,35 @@ public void onAssignment(final Assignment assignment) {
         Collections.sort(partitions, PARTITION_COMPARATOR);
         final AssignmentInfo info = AssignmentInfo.decode(assignment.userData());
-        final int usedVersion = info.version();
+        final int receivedAssignmentMetadataVersion = info.version();
+        final int leaderSupportedVersion = info.latestSupportedVersion();
+
+        if (receivedAssignmentMetadataVersion > usedSubscriptionMetadataVersion) {
+            throw new IllegalStateException(""Sent a version "" + usedSubscriptionMetadataVersion
+                + "" subscription but got an assignment with higher version "" + receivedAssignmentMetadataVersion + ""."");
+        }
+
+        if (receivedAssignmentMetadataVersion < usedSubscriptionMetadataVersion
+            && receivedAssignmentMetadataVersion >= EARLIEST_PROBEABLE_VERSION) {
+
+            if (receivedAssignmentMetadataVersion == leaderSupportedVersion) {
+                log.info(""Sent a version {} subscription and got version {} assignment back (successful version probing). "" +
+                        ""Downgrading subscription metadata to received version and trigger new rebalance."",
+                    usedSubscriptionMetadataVersion,
+                    receivedAssignmentMetadataVersion);
+                usedSubscriptionMetadataVersion = receivedAssignmentMetadataVersion;
+            } else {
+                log.info(""Sent a version {} subscription and got version {} assignment back (successful version probing). "" +
+                    ""Setting subscription metadata to leaders supported version {} and trigger new rebalance."",
+                    usedSubscriptionMetadataVersion,
+                    receivedAssignmentMetadataVersion,
+                    leaderSupportedVersion);
+                usedSubscriptionMetadataVersion = leaderSupportedVersion;
+            }
+
+            versionProbingFlag.set(true);
+            return;
+        }
         // version 1 field
         final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();
@@ -640,22 +780,29 @@ public void onAssignment(final Assignment assignment) {
         final Map<TopicPartition, PartitionInfo> topicToPartitionInfo = new HashMap<>();
         final Map<HostInfo, Set<TopicPartition>> partitionsByHost;
-        switch (usedVersion) {
-            case 1:
+        switch (receivedAssignmentMetadataVersion) {
+            case VERSION_ONE:
                 processVersionOneAssignment(info, partitions, activeTasks);
                 partitionsByHost = Collections.emptyMap();
                 break;
-            case 2:
+            case VERSION_TWO:
                 processVersionTwoAssignment(info, partitions, activeTasks, topicToPartitionInfo);
                 partitionsByHost = info.partitionsByHost();
                 break;
-            case 3:
+            case VERSION_THREE:
+                if (leaderSupportedVersion > usedSubscriptionMetadataVersion) {
+                    log.info(""Sent a version {} subscription and group leader's latest supported version is {}. "" +
+                        ""Upgrading subscription metadata version to {} for next rebalance."",
+                        usedSubscriptionMetadataVersion,
+                        leaderSupportedVersion,
+                        leaderSupportedVersion);
+                    usedSubscriptionMetadataVersion = leaderSupportedVersion;
+                }
                 processVersionThreeAssignment(info, partitions, activeTasks, topicToPartitionInfo);
                 partitionsByHost = info.partitionsByHost();
                 break;
             default:
-                throw new IllegalStateException(""Unknown metadata version: "" + usedVersion
-                    + ""; latest supported version: "" + AssignmentInfo.LATEST_SUPPORTED_VERSION);
+                throw new IllegalStateException(""This code should never be reached. Please file a bug report at [link]"");
         }
         taskManager.setClusterMetadata(Cluster.empty().withPartitions(topicToPartitionInfo));
@@ -679,13 +826,7 @@ private void processVersionOneAssignment(final AssignmentInfo info,
         for (int i = 0; i < partitions.size(); i++) {
             final TopicPartition partition = partitions.get(i);
             final TaskId id = info.activeTasks().get(i);
-
-            Set<TopicPartition> assignedPartitions = activeTasks.get(id);
-            if (assignedPartitions == null) {
-                assignedPartitions = new HashSet<>();
-                activeTasks.put(id, assignedPartitions);
-            }
-            assignedPartitions.add(partition);
+            activeTasks.computeIfAbsent(id, k -> new HashSet<>()).add(partition);
         }
     }
@@ -713,6 +854,14 @@ private void processVersionThreeAssignment(final AssignmentInfo info,
         processVersionTwoAssignment(info, partitions, activeTasks, topicToPartitionInfo);
     }
+    // for testing
+    protected void processLatestVersionAssignment(final AssignmentInfo info,
+                                                  final List<TopicPartition> partitions,
+                                                  final Map<TaskId, Set<TopicPartition>> activeTasks,
+                                                  final Map<TopicPartition, PartitionInfo> topicToPartitionInfo) {
+        processVersionThreeAssignment(info, partitions, activeTasks, topicToPartitionInfo);
+    }
+
     /**
      * Internal helper function that creates a Kafka topic
      *
diff --git [file java] [file java]
index 63224dbcde7..6e6e4ca7c5c 100644
--- [file java]
+++ [file java]
@@ -42,7 +42,7 @@
 import static java.util.Collections.singleton;
-class TaskManager {
+public class TaskManager {
     // initialize the task list
     // activeTasks needs to be concurrent as it can be accessed
     // by QueryableState
@@ -187,14 +187,14 @@ private void addStandbyTasks() {
         return standby.allAssignedTaskIds();
     }
-    Set<TaskId> prevActiveTaskIds() {
+    public Set<TaskId> prevActiveTaskIds() {
         return active.previousTaskIds();
     }
     /**
      * Returns ids of tasks whose states are kept on the local storage.
      */
-    Set<TaskId> cachedTasksIds() {
+    public Set<TaskId> cachedTasksIds() {
         // A client could contain some inactive tasks whose states are still kept on the local storage in the following scenarios:
         // 1) the client is actively maintaining standby tasks by maintaining their states from the change log.
         // 2) the client has just got some tasks migrated out of itself to other clients while these task states
@@ -221,7 +221,7 @@ private void addStandbyTasks() {
         return tasks;
     }
-    UUID processId() {
+    public UUID processId() {
         return processId;
     }
@@ -356,21 +356,21 @@ private void assignStandbyPartitions() {
         }
     }
-    void setClusterMetadata(final Cluster cluster) {
+    public void setClusterMetadata(final Cluster cluster) {
         this.cluster = cluster;
     }
-    void setPartitionsByHostState(final Map<HostInfo, Set<TopicPartition>> partitionsByHostState) {
+    public void setPartitionsByHostState(final Map<HostInfo, Set<TopicPartition>> partitionsByHostState) {
         this.streamsMetadataState.onChange(partitionsByHostState, cluster);
     }
-    void setAssignmentMetadata(final Map<TaskId, Set<TopicPartition>> activeTasks,
+    public void setAssignmentMetadata(final Map<TaskId, Set<TopicPartition>> activeTasks,
                                final Map<TaskId, Set<TopicPartition>> standbyTasks) {
         this.assignedActiveTasks = activeTasks;
         this.assignedStandbyTasks = standbyTasks;
     }
-    void updateSubscriptionsFromAssignment(List<TopicPartition> partitions) {
+    public void updateSubscriptionsFromAssignment(List<TopicPartition> partitions) {
         if (builder().sourceTopicPattern() != null) {
             final Set<String> assignedTopics = new HashSet<>();
             for (final TopicPartition topicPartition : partitions) {
@@ -385,7 +385,7 @@ void updateSubscriptionsFromAssignment(List<TopicPartition> partitions) {
         }
     }
-    void updateSubscriptionsFromMetadata(Set<String> topics) {
+    public void updateSubscriptionsFromMetadata(Set<String> topics) {
         if (builder().sourceTopicPattern() != null) {
             final Collection<String> existingTopics = builder().subscriptionUpdates().getUpdates();
             if (!existingTopics.equals(topics)) {
diff --git [file java] [file java]
index 3c5cee2bfc3..c577830e3e2 100644
--- [file java]
+++ [file java]
@@ -42,7 +42,7 @@
     private static final Logger log = LoggerFactory.getLogger(AssignmentInfo.class);
     public static final int LATEST_SUPPORTED_VERSION = 3;
-    public static final int UNKNOWN = -1;
+    static final int UNKNOWN = -1;
     private final int usedVersion;
     private final int latestSupportedVersion;
@@ -65,9 +65,9 @@ public AssignmentInfo(final List<TaskId> activeTasks,
     public AssignmentInfo() {
         this(LATEST_SUPPORTED_VERSION,
-            Collections.<TaskId>emptyList(),
-            Collections.<TaskId, Set<TopicPartition>>emptyMap(),
-            Collections.<HostInfo, Set<TopicPartition>>emptyMap());
+            Collections.emptyList(),
+            Collections.emptyMap(),
+            Collections.emptyMap());
     }
     public AssignmentInfo(final int version,
@@ -229,7 +229,7 @@ public static AssignmentInfo decode(final ByteBuffer data) {
                     decodeVersionThreeData(assignmentInfo, in);
                     break;
                 default:
-                    TaskAssignmentException fatalException = new TaskAssignmentException(""Unable to decode assignment data: "" +
+                    final TaskAssignmentException fatalException = new TaskAssignmentException(""Unable to decode assignment data: "" +
                         ""used version: "" + usedVersion + ""; latest supported version: "" + LATEST_SUPPORTED_VERSION);
                     log.error(fatalException.getMessage(), fatalException);
                     throw fatalException;
@@ -262,7 +262,7 @@ private static void decodeStandbyTasks(final AssignmentInfo assignmentInfo,
         final int count = in.readInt();
         assignmentInfo.standbyTasks = new HashMap<>(count);
         for (int i = 0; i < count; i++) {
-            TaskId id = TaskId.readFrom(in);
+            final TaskId id = TaskId.readFrom(in);
             assignmentInfo.standbyTasks.put(id, readTopicPartitions(in));
         }
     }
diff --git [file java] [file java]
index 15ee849bffc..66e655fa837 100644
--- [file java]
+++ [file java]
@@ -26,6 +26,7 @@
     private final Set<TaskId> standbyTasks;
     private final Set<TaskId> assignedTasks;
     private final Set<TaskId> prevActiveTasks;
+    private final Set<TaskId> prevStandbyTasks;
     private final Set<TaskId> prevAssignedTasks;
     private int capacity;
@@ -36,21 +37,34 @@ public ClientState() {
     }
     ClientState(final int capacity) {
-        this(new HashSet<TaskId>(), new HashSet<TaskId>(), new HashSet<TaskId>(), new HashSet<TaskId>(), new HashSet<TaskId>(), capacity);
+        this(new HashSet<TaskId>(), new HashSet<TaskId>(), new HashSet<TaskId>(), new HashSet<TaskId>(), new HashSet<TaskId>(), new HashSet<TaskId>(), capacity);
     }
-    private ClientState(Set<TaskId> activeTasks, Set<TaskId> standbyTasks, Set<TaskId> assignedTasks, Set<TaskId> prevActiveTasks, Set<TaskId> prevAssignedTasks, int capacity) {
+    private ClientState(final Set<TaskId> activeTasks,
+                        final Set<TaskId> standbyTasks,
+                        final Set<TaskId> assignedTasks,
+                        final Set<TaskId> prevActiveTasks,
+                        final Set<TaskId> prevStandbyTasks,
+                        final Set<TaskId> prevAssignedTasks,
+                        final int capacity) {
         this.activeTasks = activeTasks;
         this.standbyTasks = standbyTasks;
         this.assignedTasks = assignedTasks;
         this.prevActiveTasks = prevActiveTasks;
+        this.prevStandbyTasks = prevStandbyTasks;
         this.prevAssignedTasks = prevAssignedTasks;
         this.capacity = capacity;
     }
     public ClientState copy() {
-        return new ClientState(new HashSet<>(activeTasks), new HashSet<>(standbyTasks), new HashSet<>(assignedTasks),
-                new HashSet<>(prevActiveTasks), new HashSet<>(prevAssignedTasks), capacity);
+        return new ClientState(
+            new HashSet<>(activeTasks),
+            new HashSet<>(standbyTasks),
+            new HashSet<>(assignedTasks),
+            new HashSet<>(prevActiveTasks),
+            new HashSet<>(prevStandbyTasks),
+            new HashSet<>(prevAssignedTasks),
+            capacity);
     }
     public void assign(final TaskId taskId, final boolean active) {
@@ -71,6 +85,14 @@ public void assign(final TaskId taskId, final boolean active) {
         return standbyTasks;
     }
+    public Set<TaskId> prevActiveTasks() {
+        return prevActiveTasks;
+    }
+
+    public Set<TaskId> prevStandbyTasks() {
+        return prevStandbyTasks;
+    }
+
     public int assignedTaskCount() {
         return assignedTasks.size();
     }
@@ -89,6 +111,7 @@ public void addPreviousActiveTasks(final Set<TaskId> prevTasks) {
     }
     public void addPreviousStandbyTasks(final Set<TaskId> standbyTasks) {
+        prevStandbyTasks.addAll(standbyTasks);
         prevAssignedTasks.addAll(standbyTasks);
     }
@@ -98,6 +121,7 @@ public String toString() {
                 "") standbyTasks: ("" + standbyTasks +
                 "") assignedTasks: ("" + assignedTasks +
                 "") prevActiveTasks: ("" + prevActiveTasks +
+                "") prevStandbyTasks: ("" + prevStandbyTasks +
                 "") prevAssignedTasks: ("" + prevAssignedTasks +
                 "") capacity: "" + capacity +
                 ""]"";
diff --git [file java] [file java]
index be709472441..4ebc95674b0 100644
--- [file java]
+++ [file java]
@@ -33,7 +33,7 @@
     private static final Logger log = LoggerFactory.getLogger(SubscriptionInfo.class);
     public static final int LATEST_SUPPORTED_VERSION = 3;
-    public static final int UNKNOWN = -1;
+    static final int UNKNOWN = -1;
     private final int usedVersion;
     private final int latestSupportedVersion;
@@ -151,20 +151,20 @@ private int getVersionOneByteLength() {
                4 + standbyTasks.size() * 8; // length + standby tasks
     }
-    private void encodeClientUUID(final ByteBuffer buf) {
+    protected void encodeClientUUID(final ByteBuffer buf) {
         buf.putLong(processId.getMostSignificantBits());
         buf.putLong(processId.getLeastSignificantBits());
     }
-    private void encodeTasks(final ByteBuffer buf,
-                             final Collection<TaskId> taskIds) {
+    protected void encodeTasks(final ByteBuffer buf,
+                               final Collection<TaskId> taskIds) {
         buf.putInt(taskIds.size());
-        for (TaskId id : taskIds) {
+        for (final TaskId id : taskIds) {
             id.writeTo(buf);
         }
     }
-    private byte prepareUserEndPoint() {
+    protected byte prepareUserEndPoint() {
         if (userEndPoint == null) {
             return new byte;
         } else {
@@ -194,8 +194,8 @@ private int getVersionTwoByteLength(final byte endPointBytes) {
                4 + endPointBytes.length; // length + userEndPoint
     }
-    private void encodeUserEndPoint(final ByteBuffer buf,
-                                    final byte endPointBytes) {
+    protected void encodeUserEndPoint(final ByteBuffer buf,
+                                      final byte endPointBytes) {
         if (endPointBytes != null) {
             buf.putInt(endPointBytes.length);
             buf.put(endPointBytes);
@@ -217,7 +217,7 @@ private ByteBuffer encodeVersionThree() {
         return buf;
     }
-    private int getVersionThreeByteLength(final byte endPointBytes) {
+    protected int getVersionThreeByteLength(final byte endPointBytes) {
         return 4 + // used version
                4 + // latest supported version version
                16 + // client ID
@@ -236,6 +236,7 @@ public static SubscriptionInfo decode(final ByteBuffer data) {
         data.rewind();
         final int usedVersion = data.getInt();
+        final int latestSupportedVersion;
         switch (usedVersion) {
             case 1:
                 subscriptionInfo = new SubscriptionInfo(usedVersion, UNKNOWN);
@@ -246,12 +247,13 @@ public static SubscriptionInfo decode(final ByteBuffer data) {
                 decodeVersionTwoData(subscriptionInfo, data);
                 break;
             case 3:
-                final int latestSupportedVersion = data.getInt();
+                latestSupportedVersion = data.getInt();
                 subscriptionInfo = new SubscriptionInfo(usedVersion, latestSupportedVersion);
                 decodeVersionThreeData(subscriptionInfo, data);
                 break;
             default:
-                subscriptionInfo = new SubscriptionInfo(usedVersion, UNKNOWN);
+                latestSupportedVersion = data.getInt();
+                subscriptionInfo = new SubscriptionInfo(usedVersion, latestSupportedVersion);
                 log.info(""Unable to decode subscription data: used version: {}; latest supported version: {}"", usedVersion, LATEST_SUPPORTED_VERSION);
         }
@@ -261,12 +263,7 @@ public static SubscriptionInfo decode(final ByteBuffer data) {
     private static void decodeVersionOneData(final SubscriptionInfo subscriptionInfo,
                                              final ByteBuffer data) {
         decodeClientUUID(subscriptionInfo, data);
-
-        subscriptionInfo.prevTasks = new HashSet<>();
-        decodeTasks(subscriptionInfo.prevTasks, data);
-
-        subscriptionInfo.standbyTasks = new HashSet<>();
-        decodeTasks(subscriptionInfo.standbyTasks, data);
+        decodeTasks(subscriptionInfo, data);
     }
     private static void decodeClientUUID(final SubscriptionInfo subscriptionInfo,
@@ -274,30 +271,31 @@ private static void decodeClientUUID(final SubscriptionInfo subscriptionInfo,
         subscriptionInfo.processId = new UUID(data.getLong(), data.getLong());
     }
-    private static void decodeTasks(final Collection<TaskId> taskIds,
+    private static void decodeTasks(final SubscriptionInfo subscriptionInfo,
                                     final ByteBuffer data) {
-        final int numPrevs = data.getInt();
-        for (int i = 0; i < numPrevs; i++) {
-            taskIds.add(TaskId.readFrom(data));
+        subscriptionInfo.prevTasks = new HashSet<>();
+        final int numPrevTasks = data.getInt();
+        for (int i = 0; i < numPrevTasks; i++) {
+            subscriptionInfo.prevTasks.add(TaskId.readFrom(data));
+        }
+
+        subscriptionInfo.standbyTasks = new HashSet<>();
+        final int numStandbyTasks = data.getInt();
+        for (int i = 0; i < numStandbyTasks; i++) {
+            subscriptionInfo.standbyTasks.add(TaskId.readFrom(data));
         }
     }
     private static void decodeVersionTwoData(final SubscriptionInfo subscriptionInfo,
                                              final ByteBuffer data) {
         decodeClientUUID(subscriptionInfo, data);
-
-        subscriptionInfo.prevTasks = new HashSet<>();
-        decodeTasks(subscriptionInfo.prevTasks, data);
-
-        subscriptionInfo.standbyTasks = new HashSet<>();
-        decodeTasks(subscriptionInfo.standbyTasks, data);
-
+        decodeTasks(subscriptionInfo, data);
         decodeUserEndPoint(subscriptionInfo, data);
     }
     private static void decodeUserEndPoint(final SubscriptionInfo subscriptionInfo,
                                            final ByteBuffer data) {
-        int bytesLength = data.getInt();
+        final int bytesLength = data.getInt();
         if (bytesLength != 0) {
             final byte bytes = new byte;
             data.get(bytes);
@@ -308,16 +306,11 @@ private static void decodeUserEndPoint(final SubscriptionInfo subscriptionInfo,
     private static void decodeVersionThreeData(final SubscriptionInfo subscriptionInfo,
                                                final ByteBuffer data) {
         decodeClientUUID(subscriptionInfo, data);
-
-        subscriptionInfo.prevTasks = new HashSet<>();
-        decodeTasks(subscriptionInfo.prevTasks, data);
-
-        subscriptionInfo.standbyTasks = new HashSet<>();
-        decodeTasks(subscriptionInfo.standbyTasks, data);
-
+        decodeTasks(subscriptionInfo, data);
         decodeUserEndPoint(subscriptionInfo, data);
     }
+    @Override
     public int hashCode() {
         final int hashCode = usedVersion ^ latestSupportedVersion ^ processId.hashCode() ^ prevTasks.hashCode() ^ standbyTasks.hashCode();
         if (userEndPoint == null) {
diff --git [file java] [file java]
index 749d618bac5..f3dce521998 100644
--- [file java]
+++ [file java]
@@ -78,6 +78,7 @@
 import java.util.Properties;
 import java.util.Set;
 import java.util.UUID;
+import java.util.concurrent.atomic.AtomicBoolean;
 import static java.util.Collections.singletonList;
 import static org.apache.kafka.common.utils.Utils.mkEntry;
@@ -298,7 +299,8 @@ public void shouldNotCommitBeforeTheCommitInterval() {
             streamsMetrics,
             internalTopologyBuilder,
             clientId,
-            new LogContext("""")
+            new LogContext(""""),
+            new AtomicBoolean()
         );
         thread.maybeCommit(mockTime.milliseconds());
         mockTime.sleep(commitInterval - 10L);
@@ -331,7 +333,9 @@ public void shouldNotCauseExceptionIfNothingCommitted() {
             streamsMetrics,
             internalTopologyBuilder,
             clientId,
-            new LogContext(""""));
+            new LogContext(""""),
+            new AtomicBoolean()
+        );
         thread.maybeCommit(mockTime.milliseconds());
         mockTime.sleep(commitInterval - 10L);
         thread.maybeCommit(mockTime.milliseconds());
@@ -364,7 +368,9 @@ public void shouldCommitAfterTheCommitInterval() {
             streamsMetrics,
             internalTopologyBuilder,
             clientId,
-            new LogContext(""""));
+            new LogContext(""""),
+            new AtomicBoolean()
+        );
         thread.maybeCommit(mockTime.milliseconds());
         mockTime.sleep(commitInterval + 1);
         thread.maybeCommit(mockTime.milliseconds());
@@ -511,7 +517,8 @@ public void shouldShutdownTaskManagerOnClose() {
             streamsMetrics,
             internalTopologyBuilder,
             clientId,
-            new LogContext("""")
+            new LogContext(""""),
+            new AtomicBoolean()
         );
         thread.setStateListener(
             new StreamThread.StateListener() {
@@ -547,7 +554,8 @@ public void shouldShutdownTaskManagerOnCloseWithoutStart() {
             streamsMetrics,
             internalTopologyBuilder,
             clientId,
-            new LogContext("""")
+            new LogContext(""""),
+            new AtomicBoolean()
         );
         thread.shutdown();
         EasyMock.verify(taskManager);
@@ -574,7 +582,9 @@ public void shouldOnlyShutdownOnce() {
             streamsMetrics,
             internalTopologyBuilder,
             clientId,
-            new LogContext(""""));
+            new LogContext(""""),
+            new AtomicBoolean()
+        );
         thread.shutdown();
         // Execute the run method. Verification of the mock will check that shutdown was only done once
         thread.run();
@@ -1255,7 +1265,7 @@ private void assertThreadMetadataHasEmptyTasksWithState(final ThreadMetadata met
     @Test
     // TODO: Need to add a test case covering EOS when we create a mock taskManager class
     public void producerMetricsVerificationWithoutEOS() {
-        final MockProducer<byte, byte> producer = new MockProducer();
+        final MockProducer<byte, byte> producer = new MockProducer<>();
         final Consumer<byte, byte> consumer = EasyMock.createNiceMock(Consumer.class);
         final TaskManager taskManager = mockTaskManagerCommit(consumer, 1, 0);
@@ -1271,7 +1281,8 @@ public void producerMetricsVerificationWithoutEOS() {
                 streamsMetrics,
                 internalTopologyBuilder,
                 clientId,
-                new LogContext(""""));
+                new LogContext(""""),
+                new AtomicBoolean());
         final MetricName testMetricName = new MetricName(""test_metric"", """", """", new HashMap<String, String>());
         final Metric testMetric = new KafkaMetric(
                 new Object(),
diff --git [file java] [file java]
index 37b03fa3418..a32d193a171 100644
--- [file java]
+++ [file java]
@@ -48,6 +48,7 @@
 import org.easymock.EasyMock;
 import org.junit.Test;
+import java.nio.ByteBuffer;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
@@ -57,6 +58,7 @@
 import java.util.Map;
 import java.util.Set;
 import java.util.UUID;
+import java.util.concurrent.atomic.AtomicBoolean;
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.not;
@@ -98,7 +100,8 @@
     private final Cluster metadata = new Cluster(
         ""cluster"",
         Collections.singletonList(Node.noNode()),
-        infos, Collections.<String>emptySet(),
+        infos,
+        Collections.<String>emptySet(),
         Collections.<String>emptySet());
     private final TaskId task0 = new TaskId(0, 0);
@@ -115,15 +118,16 @@
     private final TaskManager taskManager = EasyMock.createNiceMock(TaskManager.class);
     private Map<String, Object> configProps() {
-        Map<String, Object> configurationMap = new HashMap<>();
+        final Map<String, Object> configurationMap = new HashMap<>();
         configurationMap.put(StreamsConfig.APPLICATION_ID_CONFIG, applicationId);
         configurationMap.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, userEndPoint);
         configurationMap.put(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR, taskManager);
+        configurationMap.put(StreamsConfig.InternalConfig.VERSION_PROBING_FLAG, new AtomicBoolean());
         return configurationMap;
     }
     private void configurePartitionAssignor(final Map<String, Object> props) {
-        Map<String, Object> configurationMap = configProps();
+        final Map<String, Object> configurationMap = configProps();
         configurationMap.putAll(props);
         partitionAssignor.configure(configurationMap);
     }
@@ -158,7 +162,7 @@ public void shouldInterleaveTasksByGroupId() {
         final List<TaskId> expectedSubList3 = Arrays.asList(taskIdA2, taskIdB1, taskIdC1);
         final List<List<TaskId>> embeddedList = Arrays.asList(expectedSubList1, expectedSubList2, expectedSubList3);
-        List<TaskId> tasks = Arrays.asList(taskIdC0, taskIdC1, taskIdB0, taskIdB1, taskIdB2, taskIdA0, taskIdA1, taskIdA2, taskIdA3);
+        final List<TaskId> tasks = Arrays.asList(taskIdC0, taskIdC1, taskIdB0, taskIdB1, taskIdB2, taskIdA0, taskIdA1, taskIdA2, taskIdA3);
         Collections.shuffle(tasks);
         final List<List<TaskId>> interleavedTaskIds = partitionAssignor.interleaveTasksByGroupId(tasks, 3);
@@ -182,15 +186,15 @@ public void testSubscription() {
         mockTaskManager(prevTasks, cachedTasks, processId, builder);
         configurePartitionAssignor(Collections.<String, Object>emptyMap());
-        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""topic1"", ""topic2""));
+        final PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""topic1"", ""topic2""));
         Collections.sort(subscription.topics());
         assertEquals(Utils.mkList(""topic1"", ""topic2""), subscription.topics());
-        Set<TaskId> standbyTasks = new HashSet<>(cachedTasks);
+        final Set<TaskId> standbyTasks = new HashSet<>(cachedTasks);
         standbyTasks.removeAll(prevTasks);
-        SubscriptionInfo info = new SubscriptionInfo(processId, prevTasks, standbyTasks, null);
+        final SubscriptionInfo info = new SubscriptionInfo(processId, prevTasks, standbyTasks, null);
         assertEquals(info.encode(), subscription.userData());
     }
@@ -199,8 +203,8 @@ public void testAssignBasic() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
-        List<String> topics = Utils.mkList(""topic1"", ""topic2"");
-        Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);
+        final List<String> topics = Utils.mkList(""topic1"", ""topic2"");
+        final Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);
         final Set<TaskId> prevTasks10 = Utils.mkSet(task0);
         final Set<TaskId> prevTasks11 = Utils.mkSet(task1);
@@ -209,15 +213,15 @@ public void testAssignBasic() {
         final Set<TaskId> standbyTasks11 = Utils.mkSet(task2);
         final Set<TaskId> standbyTasks20 = Utils.mkSet(task0);
-        UUID uuid1 = UUID.randomUUID();
-        UUID uuid2 = UUID.randomUUID();
+        final UUID uuid1 = UUID.randomUUID();
+        final UUID uuid2 = UUID.randomUUID();
         mockTaskManager(prevTasks10, standbyTasks10, uuid1, builder);
         configurePartitionAssignor(Collections.<String, Object>emptyMap());
         partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));
-        Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
+        final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
         subscriptions.put(""consumer10"",
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, prevTasks10, standbyTasks10, userEndPoint).encode()));
         subscriptions.put(""consumer11"",
@@ -226,7 +230,7 @@ public void testAssignBasic() {
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid2, prevTasks20, standbyTasks20, userEndPoint).encode()));
-        Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
+        final Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
         // check assigned partitions
         assertEquals(Utils.mkSet(Utils.mkSet(t1p0, t2p0), Utils.mkSet(t1p1, t2p1)),
@@ -236,17 +240,17 @@ public void testAssignBasic() {
         // check assignment info
         // the first consumer
-        AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(""consumer10""));
-        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks());
+        final AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(""consumer10""));
+        final Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks());
         // the second consumer
-        AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(""consumer11""));
+        final AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(""consumer11""));
         allActiveTasks.addAll(info11.activeTasks());
         assertEquals(Utils.mkSet(task0, task1), allActiveTasks);
         // the third consumer
-        AssignmentInfo info20 = checkAssignment(allTopics, assignments.get(""consumer20""));
+        final AssignmentInfo info20 = checkAssignment(allTopics, assignments.get(""consumer20""));
         allActiveTasks.addAll(info20.activeTasks());
         assertEquals(3, allActiveTasks.size());
@@ -277,7 +281,8 @@ public void shouldAssignEvenlyAcrossConsumersOneClientMultipleThreads() {
         final Cluster localMetadata = new Cluster(
             ""cluster"",
             Collections.singletonList(Node.noNode()),
-            localInfos, Collections.<String>emptySet(),
+            localInfos,
+            Collections.<String>emptySet(),
             Collections.<String>emptySet());
         final List<String> topics = Utils.mkList(""topic1"", ""topic2"");
@@ -332,26 +337,26 @@ public void testAssignWithPartialTopology() {
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addProcessor(""processor2"", new MockProcessorSupplier(), ""source2"");
         builder.addStateStore(new MockStoreBuilder(""store2"", false), ""processor2"");
-        List<String> topics = Utils.mkList(""topic1"", ""topic2"");
-        Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);
+        final List<String> topics = Utils.mkList(""topic1"", ""topic2"");
+        final Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);
-        UUID uuid1 = UUID.randomUUID();
+        final UUID uuid1 = UUID.randomUUID();
         mockTaskManager(Collections.<TaskId>emptySet(), Collections.<TaskId>emptySet(), uuid1, builder);
         configurePartitionAssignor(Collections.singletonMap(StreamsConfig.PARTITION_GROUPER_CLASS_CONFIG, (Object) SingleGroupPartitionGrouperStub.class));
         partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));
-        Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
+        final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
         subscriptions.put(""consumer10"",
             new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, Collections.<TaskId>emptySet(), Collections.<TaskId>emptySet(), userEndPoint).encode()));
         // will throw exception if it fails
-        Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
+        final Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
         // check assignment info
-        AssignmentInfo info10 = checkAssignment(Utils.mkSet(""topic1""), assignments.get(""consumer10""));
-        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks());
+        final AssignmentInfo info10 = checkAssignment(Utils.mkSet(""topic1""), assignments.get(""consumer10""));
+        final Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks());
         assertEquals(3, allActiveTasks.size());
         assertEquals(allTasks, new HashSet<>(allActiveTasks));
@@ -363,8 +368,8 @@ public void testAssignEmptyMetadata() {
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
-        List<String> topics = Utils.mkList(""topic1"", ""topic2"");
-        Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);
+        final List<String> topics = Utils.mkList(""topic1"", ""topic2"");
+        final Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);
         final Set<TaskId> prevTasks10 = Utils.mkSet(task0);
         final Set<TaskId> standbyTasks10 = Utils.mkSet(task1);
@@ -372,12 +377,12 @@ public void testAssignEmptyMetadata() {
             Collections.<PartitionInfo>emptySet(),
             Collections.<String>emptySet(),
             Collections.<String>emptySet());
-        UUID uuid1 = UUID.randomUUID();
+        final UUID uuid1 = UUID.randomUUID();
         mockTaskManager(prevTasks10, standbyTasks10, uuid1, builder);
         configurePartitionAssignor(Collections.<String, Object>emptyMap());
-        Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
+        final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
         subscriptions.put(""consumer10"",
             new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, prevTasks10, standbyTasks10, userEndPoint).encode()));
@@ -390,7 +395,7 @@ public void testAssignEmptyMetadata() {
         // check assignment info
         AssignmentInfo info10 = checkAssignment(Collections.<String>emptySet(), assignments.get(""consumer10""));
-        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks());
+        final Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks());
         assertEquals(0, allActiveTasks.size());
         assertEquals(Collections.<TaskId>emptySet(), new HashSet<>(allActiveTasks));
@@ -418,22 +423,22 @@ public void testAssignWithNewTasks() {
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addSource(null, ""source3"", null, null, null, ""topic3"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"", ""source3"");
-        List<String> topics = Utils.mkList(""topic1"", ""topic2"", ""topic3"");
-        Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2, task3);
+        final List<String> topics = Utils.mkList(""topic1"", ""topic2"", ""topic3"");
+        final Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2, task3);
         // assuming that previous tasks do not have topic3
         final Set<TaskId> prevTasks10 = Utils.mkSet(task0);
         final Set<TaskId> prevTasks11 = Utils.mkSet(task1);
         final Set<TaskId> prevTasks20 = Utils.mkSet(task2);
-        UUID uuid1 = UUID.randomUUID();
-        UUID uuid2 = UUID.randomUUID();
+        final UUID uuid1 = UUID.randomUUID();
+        final UUID uuid2 = UUID.randomUUID();
         mockTaskManager(prevTasks10, Collections.<TaskId>emptySet(), uuid1, builder);
         configurePartitionAssignor(Collections.<String, Object>emptyMap());
         partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));
-        Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
+        final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
         subscriptions.put(""consumer10"",
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, prevTasks10, Collections.<TaskId>emptySet(), userEndPoint).encode()));
         subscriptions.put(""consumer11"",
@@ -441,14 +446,14 @@ public void testAssignWithNewTasks() {
         subscriptions.put(""consumer20"",
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid2, prevTasks20, Collections.<TaskId>emptySet(), userEndPoint).encode()));
-        Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
+        final Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
         // check assigned partitions: since there is no previous task for topic 3 it will be assigned randomly so we cannot check exact match
         // also note that previously assigned partitions / tasks may not stay on the previous host since we may assign the new task first and
         // then later ones will be re-assigned to other hosts due to load balancing
         AssignmentInfo info = AssignmentInfo.decode(assignments.get(""consumer10"").userData());
-        Set<TaskId> allActiveTasks = new HashSet<>(info.activeTasks());
-        Set<TopicPartition> allPartitions = new HashSet<>(assignments.get(""consumer10"").partitions());
+        final Set<TaskId> allActiveTasks = new HashSet<>(info.activeTasks());
+        final Set<TopicPartition> allPartitions = new HashSet<>(assignments.get(""consumer10"").partitions());
         info = AssignmentInfo.decode(assignments.get(""consumer11"").userData());
         allActiveTasks.addAll(info.activeTasks());
@@ -475,18 +480,18 @@ public void testAssignWithStates() {
         builder.addStateStore(new MockStoreBuilder(""store2"", false), ""processor-2"");
         builder.addStateStore(new MockStoreBuilder(""store3"", false), ""processor-2"");
-        List<String> topics = Utils.mkList(""topic1"", ""topic2"");
+        final List<String> topics = Utils.mkList(""topic1"", ""topic2"");
-        TaskId task00 = new TaskId(0, 0);
-        TaskId task01 = new TaskId(0, 1);
-        TaskId task02 = new TaskId(0, 2);
-        TaskId task10 = new TaskId(1, 0);
-        TaskId task11 = new TaskId(1, 1);
-        TaskId task12 = new TaskId(1, 2);
-        List<TaskId> tasks = Utils.mkList(task00, task01, task02, task10, task11, task12);
+        final TaskId task00 = new TaskId(0, 0);
+        final TaskId task01 = new TaskId(0, 1);
+        final TaskId task02 = new TaskId(0, 2);
+        final TaskId task10 = new TaskId(1, 0);
+        final TaskId task11 = new TaskId(1, 1);
+        final TaskId task12 = new TaskId(1, 2);
+        final List<TaskId> tasks = Utils.mkList(task00, task01, task02, task10, task11, task12);
-        UUID uuid1 = UUID.randomUUID();
-        UUID uuid2 = UUID.randomUUID();
+        final UUID uuid1 = UUID.randomUUID();
+        final UUID uuid2 = UUID.randomUUID();
         mockTaskManager(
             Collections.<TaskId>emptySet(),
@@ -497,7 +502,7 @@ public void testAssignWithStates() {
         partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));
-        Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
+        final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
         subscriptions.put(""consumer10"",
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, Collections.<TaskId>emptySet(), Collections.<TaskId>emptySet(), userEndPoint).encode()));
         subscriptions.put(""consumer11"",
@@ -505,47 +510,46 @@ public void testAssignWithStates() {
         subscriptions.put(""consumer20"",
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid2, Collections.<TaskId>emptySet(), Collections.<TaskId>emptySet(), userEndPoint).encode()));
-        Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
+        final Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
         // check assigned partition size: since there is no previous task and there are two sub-topologies the assignment is random so we cannot check exact match
         assertEquals(2, assignments.get(""consumer10"").partitions().size());
         assertEquals(2, assignments.get(""consumer11"").partitions().size());
         assertEquals(2, assignments.get(""consumer20"").partitions().size());
-        AssignmentInfo info10 = AssignmentInfo.decode(assignments.get(""consumer10"").userData());
-        AssignmentInfo info11 = AssignmentInfo.decode(assignments.get(""consumer11"").userData());
-        AssignmentInfo info20 = AssignmentInfo.decode(assignments.get(""consumer20"").userData());
+        final AssignmentInfo info10 = AssignmentInfo.decode(assignments.get(""consumer10"").userData());
+        final AssignmentInfo info11 = AssignmentInfo.decode(assignments.get(""consumer11"").userData());
+        final AssignmentInfo info20 = AssignmentInfo.decode(assignments.get(""consumer20"").userData());
         assertEquals(2, info10.activeTasks().size());
         assertEquals(2, info11.activeTasks().size());
         assertEquals(2, info20.activeTasks().size());
-        Set<TaskId> allTasks = new HashSet<>();
+        final Set<TaskId> allTasks = new HashSet<>();
         allTasks.addAll(info10.activeTasks());
         allTasks.addAll(info11.activeTasks());
         allTasks.addAll(info20.activeTasks());
         assertEquals(new HashSet<>(tasks), allTasks);
         // check tasks for state topics
-        Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();
+        final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();
-        assertEquals(Utils.mkSet(task00, task01, task02), tasksForState(applicationId, ""store1"", tasks, topicGroups));
-        assertEquals(Utils.mkSet(task10, task11, task12), tasksForState(applicationId, ""store2"", tasks, topicGroups));
-        assertEquals(Utils.mkSet(task10, task11, task12), tasksForState(applicationId, ""store3"", tasks, topicGroups));
+        assertEquals(Utils.mkSet(task00, task01, task02), tasksForState(""store1"", tasks, topicGroups));
+        assertEquals(Utils.mkSet(task10, task11, task12), tasksForState(""store2"", tasks, topicGroups));
+        assertEquals(Utils.mkSet(task10, task11, task12), tasksForState(""store3"", tasks, topicGroups));
     }
-    private Set<TaskId> tasksForState(final String applicationId,
-                                      final String storeName,
+    private Set<TaskId> tasksForState(final String storeName,
                                       final List<TaskId> tasks,
                                       final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups) {
         final String changelogTopic = ProcessorStateManager.storeChangelogTopic(applicationId, storeName);
-        Set<TaskId> ids = new HashSet<>();
-        for (Map.Entry<Integer, InternalTopologyBuilder.TopicsInfo> entry : topicGroups.entrySet()) {
-            Set<String> stateChangelogTopics = entry.getValue().stateChangelogTopics.keySet();
+        final Set<TaskId> ids = new HashSet<>();
+        for (final Map.Entry<Integer, InternalTopologyBuilder.TopicsInfo> entry : topicGroups.entrySet()) {
+            final Set<String> stateChangelogTopics = entry.getValue().stateChangelogTopics.keySet();
             if (stateChangelogTopics.contains(changelogTopic)) {
-                for (TaskId id : tasks) {
+                for (final TaskId id : tasks) {
                     if (id.topicGroupId == entry.getKey())
                         ids.add(id);
                 }
@@ -556,15 +560,15 @@ public void testAssignWithStates() {
     @Test
     public void testAssignWithStandbyReplicas() {
-        Map<String, Object> props = configProps();
+        final Map<String, Object> props = configProps();
         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, ""1"");
-        StreamsConfig streamsConfig = new StreamsConfig(props);
+        final StreamsConfig streamsConfig = new StreamsConfig(props);
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
         builder.addSource(null, ""source2"", null, null, null, ""topic2"");
         builder.addProcessor(""processor"", new MockProcessorSupplier(), ""source1"", ""source2"");
-        List<String> topics = Utils.mkList(""topic1"", ""topic2"");
-        Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);
+        final List<String> topics = Utils.mkList(""topic1"", ""topic2"");
+        final Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);
         final Set<TaskId> prevTasks00 = Utils.mkSet(task0);
@@ -574,8 +578,8 @@ public void testAssignWithStandbyReplicas() {
         final Set<TaskId> standbyTasks02 = Utils.mkSet(task2);
         final Set<TaskId> standbyTasks00 = Utils.mkSet(task0);
-        UUID uuid1 = UUID.randomUUID();
-        UUID uuid2 = UUID.randomUUID();
+        final UUID uuid1 = UUID.randomUUID();
+        final UUID uuid2 = UUID.randomUUID();
         mockTaskManager(prevTasks00, standbyTasks01, uuid1, builder);
@@ -583,7 +587,7 @@ public void testAssignWithStandbyReplicas() {
         partitionAssignor.setInternalTopicManager(new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer));
-        Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
+        final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
         subscriptions.put(""consumer10"",
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, prevTasks00, standbyTasks01, userEndPoint).encode()));
         subscriptions.put(""consumer11"",
@@ -591,15 +595,15 @@ public void testAssignWithStandbyReplicas() {
         subscriptions.put(""consumer20"",
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid2, prevTasks02, standbyTasks00, ""any:9097"").encode()));
-        Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
+        final Map<String, PartitionAssignor.Assignment> assignments = partitionAssignor.assign(metadata, subscriptions);
         // the first consumer
-        AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(""consumer10""));
-        Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks());
-        Set<TaskId> allStandbyTasks = new HashSet<>(info10.standbyTasks().keySet());
+        final AssignmentInfo info10 = checkAssignment(allTopics, assignments.get(""consumer10""));
+        final Set<TaskId> allActiveTasks = new HashSet<>(info10.activeTasks());
+        final Set<TaskId> allStandbyTasks = new HashSet<>(info10.standbyTasks().keySet());
         // the second consumer
-        AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(""consumer11""));
+        final AssignmentInfo info11 = checkAssignment(allTopics, assignments.get(""consumer11""));
         allActiveTasks.addAll(info11.activeTasks());
         allStandbyTasks.addAll(info11.standbyTasks().keySet());
@@ -610,7 +614,7 @@ public void testAssignWithStandbyReplicas() {
         assertEquals(Utils.mkSet(task2), new HashSet<>(allStandbyTasks));
         // the third consumer
-        AssignmentInfo info20 = checkAssignment(allTopics, assignments.get(""consumer20""));
+        final AssignmentInfo info20 = checkAssignment(allTopics, assignments.get(""consumer20""));
         allActiveTasks.addAll(info20.activeTasks());
         allStandbyTasks.addAll(info20.standbyTasks().keySet());
@@ -641,7 +645,7 @@ public void testOnAssignment() {
         final AssignmentInfo info = new AssignmentInfo(activeTaskList, standbyTasks, hostState);
         final PartitionAssignor.Assignment assignment = new PartitionAssignor.Assignment(Utils.mkList(t3p0, t3p3), info.encode());
-        Capture<Cluster> capturedCluster = EasyMock.newCapture();
+        final Capture<Cluster> capturedCluster = EasyMock.newCapture();
         taskManager.setPartitionsByHostState(hostState);
         EasyMock.expectLastCall();
         taskManager.setAssignmentMetadata(activeTasks, standbyTasks);
@@ -667,17 +671,17 @@ public void testAssignWithInternalTopics() {
         builder.addSink(""sink1"", ""topicX"", null, null, null, ""processor1"");
         builder.addSource(null, ""source2"", null, null, null, ""topicX"");
         builder.addProcessor(""processor2"", new MockProcessorSupplier(), ""source2"");
-        List<String> topics = Utils.mkList(""topic1"", applicationId + ""-topicX"");
-        Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);
+        final List<String> topics = Utils.mkList(""topic1"", applicationId + ""-topicX"");
+        final Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);
-        UUID uuid1 = UUID.randomUUID();
+        final UUID uuid1 = UUID.randomUUID();
         mockTaskManager(Collections.<TaskId>emptySet(), Collections.<TaskId>emptySet(), uuid1, builder);
         configurePartitionAssignor(Collections.<String, Object>emptyMap());
-        MockInternalTopicManager internalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);
+        final MockInternalTopicManager internalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);
         partitionAssignor.setInternalTopicManager(internalTopicManager);
-        Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
-        Set<TaskId> emptyTasks = Collections.emptySet();
+        final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
+        final Set<TaskId> emptyTasks = Collections.emptySet();
         subscriptions.put(""consumer10"",
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, emptyTasks, emptyTasks, userEndPoint).encode()));
@@ -690,7 +694,7 @@ public void testAssignWithInternalTopics() {
     @Test
     public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() {
-        String applicationId = ""test"";
+        final String applicationId = ""test"";
         builder.setApplicationId(applicationId);
         builder.addInternalTopic(""topicX"");
         builder.addSource(null, ""source1"", null, null, null, ""topic1"");
@@ -701,18 +705,18 @@ public void testAssignWithInternalTopicThatsSourceIsAnotherInternalTopic() {
         builder.addProcessor(""processor2"", new MockProcessorSupplier(), ""source2"");
         builder.addSink(""sink2"", ""topicZ"", null, null, null, ""processor2"");
         builder.addSource(null, ""source3"", null, null, null, ""topicZ"");
-        List<String> topics = Utils.mkList(""topic1"", ""test-topicX"", ""test-topicZ"");
-        Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);
+        final List<String> topics = Utils.mkList(""topic1"", ""test-topicX"", ""test-topicZ"");
+        final Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);
-        UUID uuid1 = UUID.randomUUID();
+        final UUID uuid1 = UUID.randomUUID();
         mockTaskManager(Collections.<TaskId>emptySet(), Collections.<TaskId>emptySet(), uuid1, builder);
         configurePartitionAssignor(Collections.<String, Object>emptyMap());
-        MockInternalTopicManager internalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);
+        final MockInternalTopicManager internalTopicManager = new MockInternalTopicManager(streamsConfig, mockClientSupplier.restoreConsumer);
         partitionAssignor.setInternalTopicManager(internalTopicManager);
-        Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
-        Set<TaskId> emptyTasks = Collections.emptySet();
+        final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
+        final Set<TaskId> emptyTasks = Collections.emptySet();
         subscriptions.put(""consumer10"",
                 new PartitionAssignor.Subscription(topics, new SubscriptionInfo(uuid1, emptyTasks, emptyTasks, userEndPoint).encode()));
@@ -731,7 +735,7 @@ public void shouldGenerateTasksForAllCreatedPartitions() {
         internalTopologyBuilder.setApplicationId(applicationId);
         // KStream with 3 partitions
-        KStream<Object, Object> stream1 = builder
+        final KStream<Object, Object> stream1 = builder
             .stream(""topic1"")
             // force creation of internal repartition topic
             .map(new KeyValueMapper<Object, Object, KeyValue<Object, Object>>() {
@@ -742,7 +746,7 @@ public void shouldGenerateTasksForAllCreatedPartitions() {
             });
         // KTable with 4 partitions
-        KTable<Object, Long> table1 = builder
+        final KTable<Object, Long> table1 = builder
             .table(""topic3"")
             // force creation of internal repartition topic
             .groupBy(new KeyValueMapper<Object, Object, KeyValue<Object, Object>>() {
@@ -884,7 +888,7 @@ public void shouldThrowExceptionIfApplicationServerConfigIsNotHostPortPair() {
         try {
             configurePartitionAssignor(Collections.singletonMap(StreamsConfig.APPLICATION_SERVER_CONFIG, (Object) ""localhost""));
             fail(""expected to an exception due to invalid config"");
-        } catch (ConfigException e) {
+        } catch (final ConfigException e) {
             // pass
         }
     }
@@ -896,7 +900,7 @@ public void shouldThrowExceptionIfApplicationServerConfigPortIsNotAnInteger() {
         try {
             configurePartitionAssignor(Collections.singletonMap(StreamsConfig.APPLICATION_SERVER_CONFIG, (Object) ""localhost:j87yhk""));
             fail(""expected to an exception due to invalid config"");
-        } catch (ConfigException e) {
+        } catch (final ConfigException e) {
             // pass
         }
     }
@@ -908,7 +912,7 @@ public void shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTas
         final InternalTopologyBuilder internalTopologyBuilder = TopologyWrapper.getInternalTopologyBuilder(builder.build());
         internalTopologyBuilder.setApplicationId(applicationId);
-        KStream<Object, Object> stream1 = builder
+        final KStream<Object, Object> stream1 = builder
             // Task 1 (should get created):
             .stream(""topic1"")
@@ -964,10 +968,11 @@ public Object apply(final Object value1, final Object value2) {
         final UUID uuid = UUID.randomUUID();
         final String client = ""client1"";
-        mockTaskManager(Collections.<TaskId>emptySet(),
-                               Collections.<TaskId>emptySet(),
-                               UUID.randomUUID(),
-                internalTopologyBuilder);
+        mockTaskManager(
+            Collections.<TaskId>emptySet(),
+            Collections.<TaskId>emptySet(),
+            UUID.randomUUID(),
+            internalTopologyBuilder);
         configurePartitionAssignor(Collections.<String, Object>emptyMap());
         final MockInternalTopicManager mockInternalTopicManager = new MockInternalTopicManager(
@@ -1037,7 +1042,7 @@ public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() {
             uuid,
             internalTopologyBuilder);
-        Map<String, Object> props = new HashMap<>();
+        final Map<String, Object> props = new HashMap<>();
         props.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);
         props.put(StreamsConfig.APPLICATION_SERVER_CONFIG, userEndPoint);
         configurePartitionAssignor(props);
@@ -1075,18 +1080,58 @@ public void shouldNotAddStandbyTaskPartitionsToPartitionsForHost() {
         assertThat(allAssignedPartitions, equalTo(allPartitions));
     }
-    @Test(expected = KafkaException.class)
-    public void shouldThrowKafkaExceptionIfStreamThreadNotConfigured() {
-        partitionAssignor.configure(Collections.singletonMap(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1));
+    @Test
+    public void shouldThrowKafkaExceptionIfTaskMangerNotConfigured() {
+        final Map<String, Object> config = configProps();
+        config.remove(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR);
+
+        try {
+            partitionAssignor.configure(config);
+            fail(""Should have thrown KafkaException"");
+        } catch (final KafkaException expected) {
+            assertThat(expected.getMessage(), equalTo(""TaskManager is not specified""));
+        }
     }
-    @Test(expected = KafkaException.class)
-    public void shouldThrowKafkaExceptionIfStreamThreadConfigIsNotThreadDataProviderInstance() {
-        final Map<String, Object> config = new HashMap<>();
-        config.put(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, 1);
-        config.put(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR, ""i am not a stream thread"");
+    @Test
+    public void shouldThrowKafkaExceptionIfTaskMangerConfigIsNotTaskManagerInstance() {
+        final Map<String, Object> config = configProps();
+        config.put(StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR, ""i am not a task manager"");
-        partitionAssignor.configure(config);
+        try {
+            partitionAssignor.configure(config);
+            fail(""Should have thrown KafkaException"");
+        } catch (final KafkaException expected) {
+            assertThat(expected.getMessage(),
+                equalTo(""java.lang.String is not an instance of org.apache.kafka.streams.processor.internals.TaskManager""));
+        }
+    }
+
+    @Test
+    public void shouldThrowKafkaExceptionVersionProbingFlagNotConfigured() {
+        final Map<String, Object> config = configProps();
+        config.remove(StreamsConfig.InternalConfig.VERSION_PROBING_FLAG);
+
+        try {
+            partitionAssignor.configure(config);
+            fail(""Should have thrown KafkaException"");
+        } catch (final KafkaException expected) {
+            assertThat(expected.getMessage(), equalTo(""VersionProbingFlag is not specified""));
+        }
+    }
+
+    @Test
+    public void shouldThrowKafkaExceptionIfVersionProbingFlagConfigIsNotAtomicBoolean() {
+        final Map<String, Object> config = configProps();
+        config.put(StreamsConfig.InternalConfig.VERSION_PROBING_FLAG, ""i am not an AtomicBoolean"");
+
+        try {
+            partitionAssignor.configure(config);
+            fail(""Should have thrown KafkaException"");
+        } catch (final KafkaException expected) {
+            assertThat(expected.getMessage(),
+                equalTo(""java.lang.String is not an instance of java.util.concurrent.atomic.AtomicBoolean""));
+        }
     }
     @Test
@@ -1147,7 +1192,7 @@ public void shouldDownGradeSubscriptionToVersion1() {
             builder);
         configurePartitionAssignor(Collections.singletonMap(StreamsConfig.UPGRADE_FROM_CONFIG, (Object) StreamsConfig.UPGRADE_FROM_0100));
-        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""topic1""));
+        final PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""topic1""));
         assertThat(SubscriptionInfo.decode(subscription.userData()).version(), equalTo(1));
     }
@@ -1187,11 +1232,114 @@ private void shouldDownGradeSubscriptionToVersion2(final Object upgradeFromValue
             builder);
         configurePartitionAssignor(Collections.singletonMap(StreamsConfig.UPGRADE_FROM_CONFIG, upgradeFromValue));
-        PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""topic1""));
+        final PartitionAssignor.Subscription subscription = partitionAssignor.subscription(Utils.mkSet(""topic1""));
         assertThat(SubscriptionInfo.decode(subscription.userData()).version(), equalTo(2));
     }
+    @Test
+    public void shouldReturnUnchangedAssignmentForOldInstancesAndEmptyAssignmentForFutureInstances() {
+        builder.addSource(null, ""source1"", null, null, null, ""topic1"");
+
+        final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
+        final Set<TaskId> allTasks = Utils.mkSet(task0, task1, task2);
+
+        final Set<TaskId> activeTasks = Utils.mkSet(task0, task1);
+        final Set<TaskId> standbyTasks = Utils.mkSet(task2);
+        final Map<TaskId, Set<TopicPartition>> standbyTaskMap = new HashMap<TaskId, Set<TopicPartition>>() {
+            {
+                put(task2, Collections.singleton(t1p2));
+            }
+        };
+
+        subscriptions.put(
+            ""consumer1"",
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(""topic1""),
+                new SubscriptionInfo(UUID.randomUUID(), activeTasks, standbyTasks, null).encode()
+            )
+        );
+        subscriptions.put(
+            ""future-consumer"",
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(""topic1""),
+                encodeFutureSubscription()
+            )
+        );
+
+        mockTaskManager(
+            allTasks,
+            allTasks,
+            UUID.randomUUID(),
+            builder);
+        partitionAssignor.configure(configProps());
+        final Map<String, PartitionAssignor.Assignment> assignment = partitionAssignor.assign(metadata, subscriptions);
+
+        assertThat(assignment.size(), equalTo(2));
+        assertThat(
+            AssignmentInfo.decode(assignment.get(""consumer1"").userData()),
+            equalTo(new AssignmentInfo(
+                new ArrayList<>(activeTasks),
+                standbyTaskMap,
+                Collections.<HostInfo, Set<TopicPartition>>emptyMap()
+            )));
+        assertThat(assignment.get(""consumer1"").partitions(), equalTo(Utils.mkList(t1p0, t1p1)));
+
+        assertThat(AssignmentInfo.decode(assignment.get(""future-consumer"").userData()), equalTo(new AssignmentInfo()));
+        assertThat(assignment.get(""future-consumer"").partitions().size(), equalTo(0));
+    }
+
+    @Test
+    public void shouldThrowIfV1SubscriptionAndFutureSubscriptionIsMixed() {
+        shouldThrowIfPreVersionProbingSubscriptionAndFutureSubscriptionIsMixed(1);
+    }
+
+    @Test
+    public void shouldThrowIfV2SubscriptionAndFutureSubscriptionIsMixed() {
+        shouldThrowIfPreVersionProbingSubscriptionAndFutureSubscriptionIsMixed(2);
+    }
+
+    private ByteBuffer encodeFutureSubscription() {
+        final ByteBuffer buf = ByteBuffer.allocate(4 /* used version */
+                                                   + 4 /* supported version */);
+        buf.putInt(SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1);
+        buf.putInt(SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1);
+        return buf;
+    }
+
+    private void shouldThrowIfPreVersionProbingSubscriptionAndFutureSubscriptionIsMixed(final int oldVersion) {
+        final Map<String, PartitionAssignor.Subscription> subscriptions = new HashMap<>();
+        final Set<TaskId> emptyTasks = Collections.emptySet();
+        subscriptions.put(
+            ""consumer1"",
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(""topic1""),
+                new SubscriptionInfo(oldVersion, UUID.randomUUID(), emptyTasks, emptyTasks, null).encode()
+            )
+        );
+        subscriptions.put(
+            ""future-consumer"",
+            new PartitionAssignor.Subscription(
+                Collections.singletonList(""topic1""),
+                encodeFutureSubscription()
+            )
+        );
+
+        mockTaskManager(
+            emptyTasks,
+            emptyTasks,
+            UUID.randomUUID(),
+            builder);
+        partitionAssignor.configure(configProps());
+
+        try {
+            partitionAssignor.assign(metadata, subscriptions);
+            fail(""Should have thrown IllegalStateException"");
+        } catch (final IllegalStateException expected) {
+            // pass
+        }
+    }
+
     private PartitionAssignor.Assignment createAssignment(final Map<HostInfo, Set<TopicPartition>> firstHostState) {
         final AssignmentInfo info = new AssignmentInfo(Collections.<TaskId>emptyList(),
                                                        Collections.<TaskId, Set<TopicPartition>>emptyMap(),
@@ -1201,19 +1349,20 @@ private void shouldDownGradeSubscriptionToVersion2(final Object upgradeFromValue
                 Collections.<TopicPartition>emptyList(), info.encode());
     }
-    private AssignmentInfo checkAssignment(Set<String> expectedTopics, PartitionAssignor.Assignment assignment) {
+    private AssignmentInfo checkAssignment(final Set<String> expectedTopics,
+                                           final PartitionAssignor.Assignment assignment) {
         // This assumed 1) DefaultPartitionGrouper is used, and 2) there is an only one topic group.
-        AssignmentInfo info = AssignmentInfo.decode(assignment.userData());
+        final AssignmentInfo info = AssignmentInfo.decode(assignment.userData());
         // check if the number of assigned partitions == the size of active task id list
         assertEquals(assignment.partitions().size(), info.activeTasks().size());
         // check if active tasks are consistent
-        List<TaskId> activeTasks = new ArrayList<>();
-        Set<String> activeTopics = new HashSet<>();
-        for (TopicPartition partition : assignment.partitions()) {
+        final List<TaskId> activeTasks = new ArrayList<>();
+        final Set<String> activeTopics = new HashSet<>();
+        for (final TopicPartition partition : assignment.partitions()) {
             // since default grouper, taskid.partition == partition.partition()
             activeTasks.add(new TaskId(0, partition.partition()));
             activeTopics.add(partition.topic());
@@ -1224,11 +1373,11 @@ private AssignmentInfo checkAssignment(Set<String> expectedTopics, PartitionAssi
         assertEquals(expectedTopics, activeTopics);
         // check if standby tasks are consistent
-        Set<String> standbyTopics = new HashSet<>();
-        for (Map.Entry<TaskId, Set<TopicPartition>> entry : info.standbyTasks().entrySet()) {
-            TaskId id = entry.getKey();
-            Set<TopicPartition> partitions = entry.getValue();
-            for (TopicPartition partition : partitions) {
+        final Set<String> standbyTopics = new HashSet<>();
+        for (final Map.Entry<TaskId, Set<TopicPartition>> entry : info.standbyTasks().entrySet()) {
+            final TaskId id = entry.getKey();
+            final Set<TopicPartition> partitions = entry.getValue();
+            for (final TopicPartition partition : partitions) {
                 // since default grouper, taskid.partition == partition.partition()
                 assertEquals(id.partition, partition.partition());
diff --git [file java] [file java]
index e98b8ce0727..0611bfc4d5c 100644
--- [file java]
+++ [file java]
@@ -19,6 +19,7 @@
 import org.apache.kafka.streams.processor.TaskId;
 import org.junit.Test;
+import java.nio.ByteBuffer;
 import java.util.Arrays;
 import java.util.HashSet;
 import java.util.Set;
@@ -75,4 +76,19 @@ public void shouldEncodeAndDecodeVersion3() {
         assertEquals(expectedInfo, SubscriptionInfo.decode(info.encode()));
     }
+    @Test
+    public void shouldAllowToDecodeFutureSupportedVersion() {
+        final SubscriptionInfo info = SubscriptionInfo.decode(encodeFutureVersion());
+        assertEquals(SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1, info.version());
+        assertEquals(SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1, info.latestSupportedVersion());
+    }
+
+    private ByteBuffer encodeFutureVersion() {
+        final ByteBuffer buf = ByteBuffer.allocate(4 /* used version */
+                                                   + 4 /* supported version */);
+        buf.putInt(SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1);
+        buf.putInt(SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1);
+        return buf;
+    }
+
 }
diff --git [file java] [file java]
index 2409bd59643..8c807800869 100644
--- [file java]
+++ [file java]
@@ -40,7 +40,7 @@ public static void main(final String args) throws InterruptedException, IOExce
         final String propFileName = args;
         final String command = args;
-        final boolean disableAutoTerminate = args.length > 3;
+        final boolean disableAutoTerminate = args.length > 2;
         final Properties streamsProperties = Utils.loadProps(propFileName);
         final String kafka = streamsProperties.getProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG);
diff --git [file java] [file java]
index 69eea0b37c0..1b01a7300a1 100644
--- [file java]
+++ [file java]
@@ -16,29 +16,57 @@
  */
 package org.apache.kafka.streams.tests;
+import org.apache.kafka.clients.consumer.Consumer;
+import org.apache.kafka.clients.consumer.ConsumerConfig;
+import org.apache.kafka.clients.consumer.KafkaConsumer;
+import org.apache.kafka.clients.consumer.internals.PartitionAssignor;
+import org.apache.kafka.common.Cluster;
+import org.apache.kafka.common.PartitionInfo;
+import org.apache.kafka.common.TopicPartition;
+import org.apache.kafka.common.serialization.ByteArrayDeserializer;
+import org.apache.kafka.common.utils.ByteBufferInputStream;
 import org.apache.kafka.common.utils.Utils;
+import org.apache.kafka.streams.KafkaClientSupplier;
 import org.apache.kafka.streams.KafkaStreams;
 import org.apache.kafka.streams.StreamsBuilder;
 import org.apache.kafka.streams.StreamsConfig;
+import org.apache.kafka.streams.errors.TaskAssignmentException;
 import org.apache.kafka.streams.kstream.KStream;
+import org.apache.kafka.streams.processor.TaskId;
+import org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier;
+import org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor;
+import org.apache.kafka.streams.processor.internals.TaskManager;
+import org.apache.kafka.streams.processor.internals.assignment.AssignmentInfo;
+import org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo;
+import org.apache.kafka.streams.state.HostInfo;
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.nio.BufferUnderflowException;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
 import java.util.Properties;
+import java.util.Set;
+import java.util.UUID;
 public class StreamsUpgradeTest {
     @SuppressWarnings(""unchecked"")
     public static void main(final String args) throws Exception {
-        if (args.length < 2) {
-            System.err.println(""StreamsUpgradeTest requires two argument (kafka-url, properties-file) but only "" + args.length + "" provided: ""
-                + (args.length > 0 ? args : """"));
+        if (args.length < 1) {
+            System.err.println(""StreamsUpgradeTest requires one argument (properties-file) but no provided: "");
         }
-        final String kafka = args;
-        final String propFileName = args.length > 1 ? args : null;
+        final String propFileName = args.length > 0 ? args : null;
         final Properties streamsProperties = Utils.loadProps(propFileName);
         System.out.println(""StreamsTest instance started (StreamsUpgradeTest trunk)"");
-        System.out.println(""kafka="" + kafka);
         System.out.println(""props="" + streamsProperties);
         final StreamsBuilder builder = new StreamsBuilder();
@@ -48,11 +76,18 @@ public static void main(final String args) throws Exception {
         final Properties config = new Properties();
         config.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, ""StreamsUpgradeTest"");
-        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, kafka);
         config.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, 1000);
+
+        final KafkaClientSupplier kafkaClientSupplier;
+        if (streamsProperties.containsKey(""test.future.metadata"")) {
+            streamsProperties.remove(""test.future.metadata"");
+            kafkaClientSupplier = new FutureKafkaClientSupplier();
+        } else {
+            kafkaClientSupplier = new DefaultKafkaClientSupplier();
+        }
         config.putAll(streamsProperties);
-        final KafkaStreams streams = new KafkaStreams(builder.build(), config);
+        final KafkaStreams streams = new KafkaStreams(builder.build(), config, kafkaClientSupplier);
         streams.start();
         Runtime.getRuntime().addShutdownHook(new Thread() {
@@ -66,4 +101,237 @@ public void run() {
             }
         });
     }
+
+    private static class FutureKafkaClientSupplier extends DefaultKafkaClientSupplier {
+        @Override
+        public Consumer<byte, byte> getConsumer(final Map<String, Object> config) {
+            config.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, FutureStreamsPartitionAssignor.class.getName());
+            return new KafkaConsumer<>(config, new ByteArrayDeserializer(), new ByteArrayDeserializer());
+        }
+    }
+
+    public static class FutureStreamsPartitionAssignor extends StreamsPartitionAssignor {
+
+        public FutureStreamsPartitionAssignor() {
+            usedSubscriptionMetadataVersion = SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1;
+        }
+
+        @Override
+        public Subscription subscription(final Set<String> topics) {
+            // Adds the following information to subscription
+            // 1. Client UUID (a unique id assigned to an instance of KafkaStreams)
+            // 2. Task ids of previously running tasks
+            // 3. Task ids of valid local states on the client's state directory.
+
+            final TaskManager taskManager = taskManger();
+            final Set<TaskId> previousActiveTasks = taskManager.prevActiveTaskIds();
+            final Set<TaskId> standbyTasks = taskManager.cachedTasksIds();
+            standbyTasks.removeAll(previousActiveTasks);
+            final FutureSubscriptionInfo data = new FutureSubscriptionInfo(
+                usedSubscriptionMetadataVersion,
+                SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1,
+                taskManager.processId(),
+                previousActiveTasks,
+                standbyTasks,
+                userEndPoint());
+
+            taskManager.updateSubscriptionsFromMetadata(topics);
+
+            return new Subscription(new ArrayList<>(topics), data.encode());
+        }
+
+        @Override
+        public void onAssignment(final PartitionAssignor.Assignment assignment) {
+            try {
+                super.onAssignment(assignment);
+                return;
+            } catch (final TaskAssignmentException cannotProcessFutureVersion) {
+                // continue
+            }
+
+            final ByteBuffer data = assignment.userData();
+            data.rewind();
+
+            final int usedVersion;
+            try (final DataInputStream in = new DataInputStream(new ByteBufferInputStream(data))) {
+                usedVersion = in.readInt();
+            } catch (final IOException ex) {
+                throw new TaskAssignmentException(""Failed to decode AssignmentInfo"", ex);
+            }
+
+            if (usedVersion > AssignmentInfo.LATEST_SUPPORTED_VERSION + 1) {
+                throw new IllegalStateException(""Unknown metadata version: "" + usedVersion
+                    + ""; latest supported version: "" + AssignmentInfo.LATEST_SUPPORTED_VERSION + 1);
+            }
+
+            final AssignmentInfo info = AssignmentInfo.decode(
+                assignment.userData().putInt(0, AssignmentInfo.LATEST_SUPPORTED_VERSION));
+
+            final List<TopicPartition> partitions = new ArrayList<>(assignment.partitions());
+            Collections.sort(partitions, PARTITION_COMPARATOR);
+
+            // version 1 field
+            final Map<TaskId, Set<TopicPartition>> activeTasks = new HashMap<>();
+            // version 2 fields
+            final Map<TopicPartition, PartitionInfo> topicToPartitionInfo = new HashMap<>();
+            final Map<HostInfo, Set<TopicPartition>> partitionsByHost;
+
+            processLatestVersionAssignment(info, partitions, activeTasks, topicToPartitionInfo);
+            partitionsByHost = info.partitionsByHost();
+
+            final TaskManager taskManager = taskManger();
+            taskManager.setClusterMetadata(Cluster.empty().withPartitions(topicToPartitionInfo));
+            taskManager.setPartitionsByHostState(partitionsByHost);
+            taskManager.setAssignmentMetadata(activeTasks, info.standbyTasks());
+            taskManager.updateSubscriptionsFromAssignment(partitions);
+        }
+
+        @Override
+        public Map<String, Assignment> assign(final Cluster metadata,
+                                              final Map<String, Subscription> subscriptions) {
+            Map<String, Assignment> assignment = null;
+
+            final Map<String, Subscription> downgradedSubscriptions = new HashMap<>();
+            for (final Subscription subscription : subscriptions.values()) {
+                final SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());
+                if (info.version() < SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1) {
+                    assignment = super.assign(metadata, subscriptions);
+                    break;
+                }
+            }
+
+            boolean bumpUsedVersion = false;
+            final boolean bumpSupportedVersion;
+            if (assignment != null) {
+                bumpSupportedVersion = supportedVersions.size() == 1 && supportedVersions.iterator().next() == SubscriptionInfo.LATEST_SUPPORTED_VERSION + 1;
+            } else {
+                for (final Map.Entry<String, Subscription> entry : subscriptions.entrySet()) {
+                    final Subscription subscription = entry.getValue();
+
+                    final SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData()
+                        .putInt(0, SubscriptionInfo.LATEST_SUPPORTED_VERSION)
+                        .putInt(4, SubscriptionInfo.LATEST_SUPPORTED_VERSION));
+
+                    downgradedSubscriptions.put(
+                        entry.getKey(),
+                        new Subscription(
+                            subscription.topics(),
+                            new SubscriptionInfo(
+                                info.processId(),
+                                info.prevTasks(),
+                                info.standbyTasks(),
+                                info.userEndPoint())
+                                .encode()));
+                }
+                assignment = super.assign(metadata, downgradedSubscriptions);
+                bumpUsedVersion = true;
+                bumpSupportedVersion = true;
+            }
+
+            final Map<String, Assignment> newAssignment = new HashMap<>();
+            for (final Map.Entry<String, Assignment> entry : assignment.entrySet()) {
+                final Assignment singleAssignment = entry.getValue();
+                newAssignment.put(
+                    entry.getKey(),
+                    new Assignment(
+                        singleAssignment.partitions(),
+                        new FutureAssignmentInfo(
+                            bumpUsedVersion,
+                            bumpSupportedVersion,
+                            singleAssignment.userData())
+                            .encode()));
+            }
+
+            return newAssignment;
+        }
+    }
+
+    private static class FutureSubscriptionInfo extends SubscriptionInfo {
+        // for testing only; don't apply version checks
+        FutureSubscriptionInfo(final int version,
+                               final int latestSupportedVersion,
+                               final UUID processId,
+                               final Set<TaskId> prevTasks,
+                               final Set<TaskId> standbyTasks,
+                               final String userEndPoint) {
+            super(version, latestSupportedVersion, processId, prevTasks, standbyTasks, userEndPoint);
+        }
+
+        public ByteBuffer encode() {
+            if (version() <= SubscriptionInfo.LATEST_SUPPORTED_VERSION) {
+                final ByteBuffer buf = super.encode();
+                // super.encode() always encodes `LATEST_SUPPORTED_VERSION` as ""latest supported version""
+                // need to update to future version
+                buf.putInt(4, latestSupportedVersion());
+                return buf;
+            }
+
+            final ByteBuffer buf = encodeFutureVersion();
+            buf.rewind();
+            return buf;
+        }
+
+        private ByteBuffer encodeFutureVersion() {
+            final byte endPointBytes = prepareUserEndPoint();
+
+            final ByteBuffer buf = ByteBuffer.allocate(getVersionThreeByteLength(endPointBytes));
+
+            buf.putInt(LATEST_SUPPORTED_VERSION + 1); // used version
+            buf.putInt(LATEST_SUPPORTED_VERSION + 1); // supported version
+            encodeClientUUID(buf);
+            encodeTasks(buf, prevTasks());
+            encodeTasks(buf, standbyTasks());
+            encodeUserEndPoint(buf, endPointBytes);
+
+            return buf;
+        }
+
+    }
+
+    private static class FutureAssignmentInfo extends AssignmentInfo {
+        private final boolean bumpUsedVersion;
+        private final boolean bumpSupportedVersion;
+        final ByteBuffer originalUserMetadata;
+
+        private FutureAssignmentInfo(final boolean bumpUsedVersion,
+                                     final boolean bumpSupportedVersion,
+                                     final ByteBuffer bytes) {
+            this.bumpUsedVersion = bumpUsedVersion;
+            this.bumpSupportedVersion = bumpSupportedVersion;
+            originalUserMetadata = bytes;
+        }
+
+        @Override
+        public ByteBuffer encode() {
+            final ByteArrayOutputStream baos = new ByteArrayOutputStream();
+
+            originalUserMetadata.rewind();
+
+            try (final DataOutputStream out = new DataOutputStream(baos)) {
+                if (bumpUsedVersion) {
+                    originalUserMetadata.getInt(); // discard original used version
+                    out.writeInt(AssignmentInfo.LATEST_SUPPORTED_VERSION + 1);
+                } else {
+                    out.writeInt(originalUserMetadata.getInt());
+                }
+                if (bumpSupportedVersion) {
+                    originalUserMetadata.getInt(); // discard original supported version
+                    out.writeInt(AssignmentInfo.LATEST_SUPPORTED_VERSION + 1);
+                }
+
+                try {
+                    while (true) {
+                        out.write(originalUserMetadata.get());
+                    }
+                } catch (final BufferUnderflowException expectedWhenAllDataCopied) { }
+
+                out.flush();
+                out.close();
+
+                return ByteBuffer.wrap(baos.toByteArray());
+            } catch (final IOException ex) {
+                throw new TaskAssignmentException(""Failed to encode AssignmentInfo"", ex);
+            }
+        }
+    }
 }
diff --git a/tests/kafkatest/services/streams.py b/tests/kafkatest/services/streams.py
index f268ab8de59..1d8ed270cc5 100644
--- a/tests/kafkatest/services/streams.py
+++ b/tests/kafkatest/services/streams.py
@@ -21,7 +21,7 @@
 from kafkatest.directory_layout.kafka_path import KafkaPathResolverMixin
 from kafkatest.services.kafka import KafkaConfig
 from kafkatest.services.monitor.jmx import JmxMixin
-from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1
+from kafkatest.version import LATEST_0_10_0, LATEST_0_10_1, LATEST_0_10_2, LATEST_0_11_0, LATEST_1_0, LATEST_1_1
 STATE_DIR = ""state.dir""
@@ -52,6 +52,33 @@ class StreamsTestBaseService(KafkaPathResolverMixin, JmxMixin, Service):
         ""streams_stderr"": {
             ""path"": STDERR_FILE,
             ""collect_default"": True},
+        ""streams_log.1"": {
+            ""path"": LOG_FILE + "".1"",
+            ""collect_default"": True},
+        ""streams_stdout.1"": {
+            ""path"": STDOUT_FILE + "".1"",
+            ""collect_default"": True},
+        ""streams_stderr.1"": {
+            ""path"": STDERR_FILE + "".1"",
+            ""collect_default"": True},
+        ""streams_log.2"": {
+            ""path"": LOG_FILE + "".2"",
+            ""collect_default"": True},
+        ""streams_stdout.2"": {
+            ""path"": STDOUT_FILE + "".2"",
+            ""collect_default"": True},
+        ""streams_stderr.2"": {
+            ""path"": STDERR_FILE + "".2"",
+            ""collect_default"": True},
+        ""streams_log.3"": {
+            ""path"": LOG_FILE + "".3"",
+            ""collect_default"": True},
+        ""streams_stdout.3"": {
+            ""path"": STDOUT_FILE + "".3"",
+            ""collect_default"": True},
+        ""streams_stderr.3"": {
+            ""path"": STDERR_FILE + "".3"",
+            ""collect_default"": True},
         ""streams_log.0-1"": {
             ""path"": LOG_FILE + "".0-1"",
             ""collect_default"": True},
@@ -412,17 +439,26 @@ def set_version(self, kafka_streams_version):
     def set_upgrade_from(self, upgrade_from):
         self.UPGRADE_FROM = upgrade_from
+    def set_upgrade_to(self, upgrade_to):
+        self.UPGRADE_TO = upgrade_to
+
     def prop_file(self):
-        properties = {STATE_DIR: self.PERSISTENT_ROOT}
+        properties = {streams_property.STATE_DIR: self.PERSISTENT_ROOT,
+                      streams_property.KAFKA_SERVERS: self.kafka.bootstrap_servers()}
         if self.UPGRADE_FROM is not None:
             properties = self.UPGRADE_FROM
+        if self.UPGRADE_TO == ""future_version"":
+            properties = ""any_value""
         cfg = KafkaConfig(**properties)
         return cfg.render()
     def start_cmd(self, node):
         args = self.args.copy()
-        args = self.kafka.bootstrap_servers()
+        if self.KAFKA_STREAMS_VERSION in :
+            args = self.kafka.bootstrap_servers()
+        else:
+            args = """"
         if self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_0) or self.KAFKA_STREAMS_VERSION == str(LATEST_0_10_1):
             args = self.kafka.zk.connect_setting()
         else:
@@ -437,7 +473,7 @@ def start_cmd(self, node):
         cmd = ""( export KAFKA_LOG4J_OPTS=\""-Dlog4j.configuration=file:%(log4j)s\""; "" \
               ""INCLUDE_TEST_JARS=true UPGRADE_KAFKA_STREAMS_TEST_VERSION=%(version)s "" \
-              "" %(kafka_run_class)s %(streams_class_name)s  %(kafka)s %(zk)s %(config_file)s "" \
+              "" %(kafka_run_class)s %(streams_class_name)s %(kafka)s %(zk)s %(config_file)s "" \
               "" & echo $! >&3 ) 1>> %(stdout)s 2>> %(stderr)s 3> %(pidfile)s"" % args
         self.logger.info(""Executing: "" + cmd)
diff --git a/tests/kafkatest/tests/streams/streams_upgrade_test.py b/tests/kafkatest/tests/streams/streams_upgrade_test.py
index debe85fd7e2..41134672e98 100644
--- a/tests/kafkatest/tests/streams/streams_upgrade_test.py
+++ b/tests/kafkatest/tests/streams/streams_upgrade_test.py
@@ -48,6 +48,7 @@ def __init__(self, test_context):
             'data' : { 'partitions': 5 },
         }
         self.leader = None
+        self.leader_counter = {}
     def perform_broker_upgrade(self, to_version):
         self.logger.info(""First pass bounce - rolling broker upgrade"")
@@ -158,7 +159,7 @@ def test_simple_upgrade_downgrade(self, from_version, to_version):
         random.shuffle(self.processors)
         for p in self.processors:
             p.CLEAN_NODE_ENABLED = False
-            self.do_rolling_bounce(p, None, to_version, counter)
+            self.do_stop_start_bounce(p, None, to_version, counter)
             counter = counter + 1
         # shutdown
@@ -176,8 +177,7 @@ def test_simple_upgrade_downgrade(self, from_version, to_version):
         self.driver.stop()
-    #@matrix(from_version=metadata_1_versions, to_version=backward_compatible_metadata_2_versions)
-    @ignore
+    @matrix(from_version=metadata_1_versions, to_version=backward_compatible_metadata_2_versions)
     @matrix(from_version=metadata_1_versions, to_version=metadata_3_versions)
     @matrix(from_version=metadata_2_versions, to_version=metadata_3_versions)
     def test_metadata_upgrade(self, from_version, to_version):
@@ -209,13 +209,70 @@ def test_metadata_upgrade(self, from_version, to_version):
         random.shuffle(self.processors)
         for p in self.processors:
             p.CLEAN_NODE_ENABLED = False
-            self.do_rolling_bounce(p, from_version, to_version, counter)
+            self.do_stop_start_bounce(p, from_version, to_version, counter)
             counter = counter + 1
         # second rolling bounce
         random.shuffle(self.processors)
         for p in self.processors:
-            self.do_rolling_bounce(p, None, to_version, counter)
+            self.do_stop_start_bounce(p, None, to_version, counter)
+            counter = counter + 1
+
+        # shutdown
+        self.driver.stop()
+        self.driver.wait()
+
+        random.shuffle(self.processors)
+        for p in self.processors:
+            node = p.node
+            with node.account.monitor_log(p.STDOUT_FILE) as monitor:
+                p.stop()
+                monitor.wait_until(""UPGRADE-TEST-CLIENT-CLOSED"",
+                                   timeout_sec=60,
+                                   err_msg=""Never saw output 'UPGRADE-TEST-CLIENT-CLOSED' on"" + str(node.account))
+
+        self.driver.stop()
+
+    def test_version_probing_upgrade(self):
+        """"""
+        Starts 3 KafkaStreams instances, and upgrades one-by-one to ""future version""
+        """"""
+
+        self.zk = ZookeeperService(self.test_context, num_nodes=1)
+        self.zk.start()
+
+        self.kafka = KafkaService(self.test_context, num_nodes=1, zk=self.zk, topics=self.topics)
+        self.kafka.start()
+
+        self.driver = StreamsSmokeTestDriverService(self.test_context, self.kafka)
+        self.driver.disable_auto_terminate()
+        self.processor1 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+        self.processor2 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+        self.processor3 = StreamsUpgradeTestJobRunnerService(self.test_context, self.kafka)
+
+        self.driver.start()
+        self.start_all_nodes_with("""") # run with TRUNK
+
+        self.processors = 
+        self.old_processors = 
+        self.upgraded_processors = 
+        for p in self.processors:
+            self.leader_counter = 2
+
+        self.update_leader()
+        for p in self.processors:
+            self.leader_counter = 0
+        self.leader_counter = 3
+
+        counter = 1
+        current_generation = 3
+
+        random.seed()
+        random.shuffle(self.processors)
+
+        for p in self.processors:
+            p.CLEAN_NODE_ENABLED = False
+            current_generation = self.do_rolling_bounce(p, counter, current_generation)
             counter = counter + 1
         # shutdown
@@ -233,6 +290,27 @@ def test_metadata_upgrade(self, from_version, to_version):
         self.driver.stop()
+    def update_leader(self):
+        self.leader = None
+        retries = 10
+        while retries > 0:
+            for p in self.processors:
+                found = list(p.node.account.ssh_capture(""grep \""Finished assignment for group\"" %s"" % p.LOG_FILE, allow_fail=True))
+                if len(found) == self.leader_counter + 1:
+                    if self.leader is not None:
+                        raise Exception(""Could not uniquely identify leader"")
+                    self.leader = p
+                    self.leader_counter = self.leader_counter + 1
+
+            if self.leader is None:
+                retries = retries - 1
+                time.sleep(5)
+            else:
+                break
+
+        if self.leader is None:
+            raise Exception(""Could not identify leader"")
+
     def start_all_nodes_with(self, version):
         # start first with <version>
         self.prepare_for(self.processor1, version)
@@ -293,7 +371,7 @@ def prepare_for(processor, version):
         else:
             processor.set_version(version)
-    def do_rolling_bounce(self, processor, upgrade_from, new_version, counter):
+    def do_stop_start_bounce(self, processor, upgrade_from, new_version, counter):
         first_other_processor = None
         second_other_processor = None
         for p in self.processors:
@@ -361,3 +439,120 @@ def do_rolling_bounce(self, processor, upgrade_from, new_version, counter):
                         monitor.wait_until(""processed 100 records from topic"",
                                            timeout_sec=60,
                                            err_msg=""Never saw output 'processed 100 records from topic' on"" + str(node.account))
+
+    def do_rolling_bounce(self, processor, counter, current_generation):
+        first_other_processor = None
+        second_other_processor = None
+        for p in self.processors:
+            if p != processor:
+                if first_other_processor is None:
+                    first_other_processor = p
+                else:
+                    second_other_processor = p
+
+        node = processor.node
+        first_other_node = first_other_processor.node
+        second_other_node = second_other_processor.node
+
+        with first_other_node.account.monitor_log(first_other_processor.LOG_FILE) as first_other_monitor:
+            with second_other_node.account.monitor_log(second_other_processor.LOG_FILE) as second_other_monitor:
+                # stop processor
+                processor.stop()
+                node.account.ssh_capture(""grep UPGRADE-TEST-CLIENT-CLOSED %s"" % processor.STDOUT_FILE, allow_fail=False)
+
+                node.account.ssh(""mv "" + processor.STDOUT_FILE + "" "" + processor.STDOUT_FILE + ""."" + str(counter), allow_fail=False)
+                node.account.ssh(""mv "" + processor.STDERR_FILE + "" "" + processor.STDERR_FILE + ""."" + str(counter), allow_fail=False)
+                node.account.ssh(""mv "" + processor.LOG_FILE + "" "" + processor.LOG_FILE + ""."" + str(counter), allow_fail=False)
+                self.leader_counter = 0
+
+                with node.account.monitor_log(processor.LOG_FILE) as log_monitor:
+                    processor.set_upgrade_to(""future_version"")
+                    processor.start()
+                    self.old_processors.remove(processor)
+                    self.upgraded_processors.append(processor)
+
+                    current_generation = current_generation + 1
+
+                    log_monitor.wait_until(""Kafka version : "" + str(DEV_VERSION),
+                                           timeout_sec=60,
+                                           err_msg=""Could not detect Kafka Streams version "" + str(DEV_VERSION) + "" in "" + str(node.account))
+                    log_monitor.offset = 5
+                    log_monitor.wait_until(""partition\.assignment\.strategy = \"",
+                                           timeout_sec=60,
+                                           err_msg=""Could not detect FutureStreamsPartitionAssignor in "" + str(node.account))
+
+                    log_monitor.wait_until(""Successfully joined group with generation "" + str(current_generation),
+                                           timeout_sec=60,
+                                           err_msg=""Never saw output 'Successfully joined group with generation "" + str(current_generation) + ""' on"" + str(node.account))
+                    first_other_monitor.wait_until(""Successfully joined group with generation "" + str(current_generation),
+                                                   timeout_sec=60,
+                                                   err_msg=""Never saw output 'Successfully joined group with generation "" + str(current_generation) + ""' on"" + str(first_other_node.account))
+                    second_other_monitor.wait_until(""Successfully joined group with generation "" + str(current_generation),
+                                                    timeout_sec=60,
+                                                    err_msg=""Never saw output 'Successfully joined group with generation "" + str(current_generation) + ""' on"" + str(second_other_node.account))
+
+                    if processor == self.leader:
+                        self.update_leader()
+                    else:
+                        self.leader_counter = self.leader_counter + 1
+
+                    if processor == self.leader:
+                        leader_monitor = log_monitor
+                    elif first_other_processor == self.leader:
+                        leader_monitor = first_other_monitor
+                    elif second_other_processor == self.leader:
+                        leader_monitor = second_other_monitor
+                    else:
+                        raise Exception(""Could not identify leader."")
+
+                    monitors = {}
+                    monitors = log_monitor
+                    monitors = first_other_monitor
+                    monitors = second_other_monitor
+
+                    leader_monitor.wait_until(""Received a future (version probing) subscription (version: 4). Sending empty assignment back (with supported version 3)."",
+                                              timeout_sec=60,
+                                              err_msg=""Could not detect 'version probing' attempt at leader "" + str(self.leader.node.account))
+
+                    if len(self.old_processors) > 0:
+                        log_monitor.wait_until(""Sent a version 4 subscription and got version 3 assignment back (successful version probing). Downgrading subscription metadata to received version and trigger new rebalance."",
+                                               timeout_sec=60,
+                                               err_msg=""Could not detect 'successful version probing' at upgrading node "" + str(node.account))
+                    else:
+                        log_monitor.wait_until(""Sent a version 4 subscription and got version 3 assignment back (successful version probing). Setting subscription metadata to leaders supported version 4 and trigger new rebalance."",
+                                               timeout_sec=60,
+                                               err_msg=""Could not detect 'successful version probing with upgraded leader' at upgrading node "" + str(node.account))
+                        first_other_monitor.wait_until(""Sent a version 3 subscription and group leader.s latest supported version is 4. Upgrading subscription metadata version to 4 for next rebalance."",
+                                                       timeout_sec=60,
+                                                       err_msg=""Never saw output 'Upgrade metadata to version 4' on"" + str(first_other_node.account))
+                        second_other_monitor.wait_until(""Sent a version 3 subscription and group leader.s latest supported version is 4. Upgrading subscription metadata version to 4 for next rebalance."",
+                                                        timeout_sec=60,
+                                                        err_msg=""Never saw output 'Upgrade metadata to version 4' on"" + str(second_other_node.account))
+
+                    log_monitor.wait_until(""Version probing detected. Triggering new rebalance."",
+                                           timeout_sec=60,
+                                           err_msg=""Could not detect 'Triggering new rebalance' at upgrading node "" + str(node.account))
+
+                    # version probing should trigger second rebalance
+                    current_generation = current_generation + 1
+
+                    for p in self.processors:
+                        monitors.wait_until(""Successfully joined group with generation "" + str(current_generation),
+                                               timeout_sec=60,
+                                               err_msg=""Never saw output 'Successfully joined group with generation "" + str(current_generation) + ""' on"" + str(p.node.account))
+
+                    if processor == self.leader:
+                        self.update_leader()
+                    else:
+                        self.leader_counter = self.leader_counter + 1
+
+                    if self.leader in self.old_processors or len(self.old_processors) > 0:
+                        self.verify_metadata_no_upgraded_yet()
+
+        return current_generation
+
+    def verify_metadata_no_upgraded_yet(self):
+        for p in self.processors:
+            found = list(p.node.account.ssh_capture(""grep \""Sent a version 3 subscription and group leader.s latest supported version is 4. Upgrading subscription metadata version to 4 for next rebalance.\"" "" + p.LOG_FILE, allow_fail=True))
+            if len(found) > 0:
+                raise Exception(""Kafka Streams failed with 'group member upgraded to metadata 4 too early'"")
diff --git a/tests/kafkatest/version.py b/tests/kafkatest/version.py
index 7823efac1d4..0ed29a34968 100644
--- a/tests/kafkatest/version.py
+++ b/tests/kafkatest/version.py
@@ -61,7 +61,7 @@ def get_version(node=None):
         return DEV_BRANCH
 DEV_BRANCH = KafkaVersion(""dev"")
-DEV_VERSION = KafkaVersion(""1.2.0-SNAPSHOT"")
+DEV_VERSION = KafkaVersion(""2.0.0-SNAPSHOT"")
 # 0.8.2.x versions
 V_0_8_2_1 = KafkaVersion(""0.8.2.1"")
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 21 **
Hi all,we got error *Kafka Streams error “TaskAssignmentException: unable to decode subscription data: version=4”*During deployment with only changed Kafka-Streams version from {{1.1.1}} to {{2.x.x}} (without changing _{{application.id}}_), we got exceptions on app node with older Kafka-Streams version and, as a result, Kafka streams changed state to error and closed, meanwhile app node with new Kafka-Streams version consumes messages fine.If we upgrade from {{1.1.1}} to {{2.0.0}}, got error _{{unable to decode subscription data: version=3}}_; if from {{1.1.1}} to {{2.3.0}}: _{{unable to decode subscription data: version=4}}_. It might be really painful during canary deployment, e.g. we have 3 app nodes with previous Kafka-Streams version, and when we add one more node with a new version, all existing 3 nodes will be in error state. Issue is reproducible in 100% cases and not depend on the number of app instances with previous Kafka Streams version (an error occurred for both cases with either one or three app nodes having Kafka Streams 1.1.1, during deployment time the first app node with new Kafka Streams version).Error stack trace:TaskAssignmentException: unable to decode subscription data: version=4    at org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo.decode([file java]:128)    at org.apache.kafka.streams.processor.internals.StreamPartitionAssignor.assign([file java]:297)    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.performAssignment([file java]:358)    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.onJoinLeader([file java]:520)    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.access$1100([file java]:93)    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle([file java]:472)    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle([file java]:455)    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess([file java]:822)    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess([file java]:802)    at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess([file java]:204)    at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess([file java]:167)    at org.apache.kafka.clients.consumer.internals.RequestFuture.complete([file java]:127)    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion([file java]:563)    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests([file java]:390)    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll([file java]:293)    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll([file java]:233)    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll([file java]:193)    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded([file java]:364)    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup([file java]:316)    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll([file java]:290)    at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce([file java]:1149)    at org.apache.kafka.clients.consumer.KafkaConsumer.poll([file java]:1115)    at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests([file java]:831)    at org.apache.kafka.streams.processor.internals.StreamThread.runOnce([file java]:788)    at org.apache.kafka.streams.processor.internals.StreamThread.runLoop([file java]:749)    at org.apache.kafka.streams.processor.internals.StreamThread.run([file java]:719){code}Issue is reproducible on both Kafka broker versions {{1.1.0}} and {{2.1.1}}, even with the simple Kafka-Streams DSL example: Properties props = new Properties(); props.put(""bootstrap.servers"", ""localhost:9092""); props.put(""default.key.serde"", ""org.apache.kafka.common.serialization.Serdes$StringSerde""); props.put(""default.value.serde"", ""org.apache.kafka.common.serialization.Serdes$StringSerde""); props.put(""application.id"", ""xxx"");StreamsBuilder streamsBuilder = new StreamsBuilder(); streamsBuilder.<String, String>stream(""source"") .mapValues(value -> value + value) .to(""destination""); KafkaStreams kafkaStreams = new KafkaStreams(streamsBuilder.build(), props);{code}Seems it's a bug of Kafka Streams.As I see, the problem is inside the implementation of Kafka Streams {{1.1.1}} _{{org.apache.kafka.streams.processor.internals.assignment.SubscriptionInfo}}_ . With new version {{2.3.0}}, {{decode}} implementation already take into consideration that we might receive newer _{{latestSupportedVersion}}_, but still seems new version will not decode appropriately.
"
KAFKA-6696,https://issues.apache.org/jira/browse/KAFKA-6696,https://github.com/apache/kafka/blob/2.0.0/tools/src/main/java/org/apache/kafka/trogdor/coordinator/NodeManager.java,Trogdor should support destroying tasks,NO,Trogdor should support destroying tasks.  This will make it more practical to have very long running Trogdor instances.,"** Comment 1 **
cmccabe opened a new pull request #4759: KAFKA-6696 Trogdor should support destroying tasks
URL: [link]
   KAFKA-6696 Trogdor should support destroying tasks   Implement destroying tasks and workers.  This means erasing all record of them on the Coordinator and the Agent.   Workers should be identified by unique 64-bit worker IDs, rather than by the names of the tasks they are implementing.  This ensures that when a task is destroyed and re-created with the same task ID, the old workers will be not be treated as part of the new task instance.       Fix some return results from RPCs.  In some cases RPCs were returning values that were never used.  Attempting to re-create the same task ID with different arguments should fail.  Add RequestConflictException to represent HTTP error code 409 (CONFLICT) for this scenario.   If only one worker in a task stops, don't stop all the other workers for that task, unless the worker that stopped had an error.
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
rajinisivaram closed pull request #4759: KAFKA-6696 Trogdor should support destroying tasks
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git a/checkstyle/suppressions.xml b/checkstyle/suppressions.xml
index 2767132886d..64258bf7b07 100644
--- a/checkstyle/suppressions.xml
+++ b/checkstyle/suppressions.xml
@@ -45,7 +45,7 @@
     <suppress checks=""ClassDataAbstractionCoupling""
               files=""(KafkaConsumer|ConsumerCoordinator|Fetcher|KafkaProducer|AbstractRequest|AbstractResponse|TransactionManager|KafkaAdminClient).java""/>
     <suppress checks=""ClassDataAbstractionCoupling""
-              files=""(Errors|SaslAuthenticatorTest|AgentTest).java""/>
+              files=""(Errors|SaslAuthenticatorTest|AgentTest|CoordinatorTest).java""/>
     <suppress checks=""BooleanExpressionComplexity""
               files=""(Utils|Topic|KafkaLZ4BlockOutputStream|AclData).java""/>
diff --git [file java] [file java]
index 3b5b21e68d8..0324d2d2dba 100644
--- [file java]
+++ [file java]
@@ -27,10 +27,9 @@
 import org.apache.kafka.trogdor.common.Platform;
 import org.apache.kafka.trogdor.rest.AgentStatusResponse;
 import org.apache.kafka.trogdor.rest.CreateWorkerRequest;
-import org.apache.kafka.trogdor.rest.CreateWorkerResponse;
+import org.apache.kafka.trogdor.rest.DestroyWorkerRequest;
 import org.apache.kafka.trogdor.rest.JsonRestServer;
 import org.apache.kafka.trogdor.rest.StopWorkerRequest;
-import org.apache.kafka.trogdor.rest.StopWorkerResponse;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -95,13 +94,16 @@ public AgentStatusResponse status() throws Exception {
         return new AgentStatusResponse(serverStartMs, workerManager.workerStates());
     }
-    public CreateWorkerResponse createWorker(CreateWorkerRequest req) throws Exception {
-        workerManager.createWorker(req.id(), req.spec());
-        return new CreateWorkerResponse(req.spec());
+    public void createWorker(CreateWorkerRequest req) throws Throwable {
+        workerManager.createWorker(req.workerId(), req.taskId(), req.spec());
     }
-    public StopWorkerResponse stopWorker(StopWorkerRequest req) throws Exception {
-        return new StopWorkerResponse(workerManager.stopWorker(req.id()));
+    public void stopWorker(StopWorkerRequest req) throws Throwable {
+        workerManager.stopWorker(req.workerId(), false);
+    }
+
+    public void destroyWorker(DestroyWorkerRequest req) throws Throwable {
+        workerManager.stopWorker(req.workerId(), true);
     }
     public static void main(String args) throws Exception {
diff --git [file java] [file java]
index 08769a0971d..c89011b8650 100644
--- [file java]
+++ [file java]
@@ -27,15 +27,16 @@
 import org.apache.kafka.trogdor.common.JsonUtil;
 import org.apache.kafka.trogdor.rest.AgentStatusResponse;
 import org.apache.kafka.trogdor.rest.CreateWorkerRequest;
-import org.apache.kafka.trogdor.rest.CreateWorkerResponse;
+import org.apache.kafka.trogdor.rest.DestroyWorkerRequest;
 import org.apache.kafka.trogdor.rest.Empty;
 import org.apache.kafka.trogdor.rest.JsonRestServer;
 import org.apache.kafka.trogdor.rest.JsonRestServer.HttpResponse;
 import org.apache.kafka.trogdor.rest.StopWorkerRequest;
-import org.apache.kafka.trogdor.rest.StopWorkerResponse;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import javax.ws.rs.core.UriBuilder;
+
 import static net.sourceforge.argparse4j.impl.Arguments.store;
 import static net.sourceforge.argparse4j.impl.Arguments.storeTrue;
@@ -116,20 +117,29 @@ public AgentStatusResponse status() throws Exception {
         return resp.body();
     }
-    public CreateWorkerResponse createWorker(CreateWorkerRequest request) throws Exception {
-        HttpResponse<CreateWorkerResponse> resp =
-            JsonRestServer.<CreateWorkerResponse>httpRequest(
+    public void createWorker(CreateWorkerRequest request) throws Exception {
+        HttpResponse<Empty> resp =
+            JsonRestServer.<Empty>httpRequest(
                 url(""/agent/worker/create""), ""POST"",
-                request, new TypeReference<CreateWorkerResponse>() { }, maxTries);
-        return resp.body();
+                request, new TypeReference<Empty>() { }, maxTries);
+        resp.body();
     }
-    public StopWorkerResponse stopWorker(StopWorkerRequest request) throws Exception {
-        HttpResponse<StopWorkerResponse> resp =
-            JsonRestServer.<StopWorkerResponse>httpRequest(url(
+    public void stopWorker(StopWorkerRequest request) throws Exception {
+        HttpResponse<Empty> resp =
+            JsonRestServer.<Empty>httpRequest(url(
                 ""/agent/worker/stop""), ""PUT"",
-                request, new TypeReference<StopWorkerResponse>() { }, maxTries);
-        return resp.body();
+                request, new TypeReference<Empty>() { }, maxTries);
+        resp.body();
+    }
+
+    public void destroyWorker(DestroyWorkerRequest request) throws Exception {
+        UriBuilder uriBuilder = UriBuilder.fromPath(url(""/agent/worker""));
+        uriBuilder.queryParam(""workerId"", request.workerId());
+        HttpResponse<Empty> resp =
+            JsonRestServer.<Empty>httpRequest(uriBuilder.build().toString(), ""DELETE"",
+                null, new TypeReference<Empty>() { }, maxTries);
+        resp.body();
     }
     public void invokeShutdown() throws Exception {
@@ -166,10 +176,16 @@ public static void main(String args) throws Exception {
             .help(""Create a new fault."");
         actions.addArgument(""--stop-worker"")
             .action(store())
-            .type(String.class)
+            .type(Long.class)
             .dest(""stop_worker"")
-            .metavar(""SPEC_JSON"")
-            .help(""Create a new fault."");
+            .metavar(""WORKER_ID"")
+            .help(""Stop a worker ID."");
+        actions.addArgument(""--destroy-worker"")
+            .action(store())
+            .type(Long.class)
+            .dest(""destroy_worker"")
+            .metavar(""WORKER_ID"")
+            .help(""Destroy a worker ID."");
         actions.addArgument(""--shutdown"")
             .action(storeTrue())
             .type(Boolean.class)
@@ -197,13 +213,21 @@ public static void main(String args) throws Exception {
             System.out.println(""Got agent status: "" +
                 JsonUtil.toPrettyJsonString(client.status()));
         } else if (res.getString(""create_worker"") != null) {
-            client.createWorker(JsonUtil.JSON_SERDE.
-                readValue(res.getString(""create_worker""),
-                    CreateWorkerRequest.class));
-            System.out.println(""Created fault."");
+            CreateWorkerRequest req = JsonUtil.JSON_SERDE.
+                readValue(res.getString(""create_worker""), CreateWorkerRequest.class);
+            client.createWorker(req);
+            System.out.printf(""Sent CreateWorkerRequest for worker %d%n."", req.workerId());
+        } else if (res.getString(""stop_worker"") != null) {
+            long workerId = res.getLong(""stop_worker"");
+            client.stopWorker(new StopWorkerRequest(workerId));
+            System.out.printf(""Sent StopWorkerRequest for worker %d%n."", workerId);
+        } else if (res.getString(""destroy_worker"") != null) {
+            long workerId = res.getLong(""stop_worker"");
+            client.destroyWorker(new DestroyWorkerRequest(workerId));
+            System.out.printf(""Sent DestroyWorkerRequest for worker %d%n."", workerId);
         } else if (res.getBoolean(""shutdown"")) {
             client.invokeShutdown();
-            System.out.println(""Sent shutdown request."");
+            System.out.println(""Sent ShutdownRequest."");
         } else {
             System.out.println(""You must choose an action. Type --help for help."");
             Exit.exit(1);
diff --git [file java] [file java]
index 773c580fa15..1f2ad49d2fe 100644
--- [file java]
+++ [file java]
@@ -18,22 +18,34 @@
 import org.apache.kafka.trogdor.rest.AgentStatusResponse;
 import org.apache.kafka.trogdor.rest.CreateWorkerRequest;
-import org.apache.kafka.trogdor.rest.CreateWorkerResponse;
+import org.apache.kafka.trogdor.rest.DestroyWorkerRequest;
 import org.apache.kafka.trogdor.rest.Empty;
 import org.apache.kafka.trogdor.rest.StopWorkerRequest;
-import org.apache.kafka.trogdor.rest.StopWorkerResponse;
 import javax.servlet.ServletContext;
 import javax.ws.rs.Consumes;
+import javax.ws.rs.DELETE;
+import javax.ws.rs.DefaultValue;
 import javax.ws.rs.GET;
 import javax.ws.rs.POST;
 import javax.ws.rs.PUT;
 import javax.ws.rs.Path;
 import javax.ws.rs.Produces;
+import javax.ws.rs.QueryParam;
 import javax.ws.rs.core.MediaType;
 import java.util.concurrent.atomic.AtomicReference;
-
+/**
+ * The REST resource for the Agent. This describes the RPCs which the agent can accept.
+ *
+ * RPCs should be idempotent.  This is important because if the server's response is
+ * lost, the client will simply retransmit the same request. The server's response must
+ * be the same the second time around.
+ *
+ * We return the empty JSON object {} rather than void for RPCs that have no results.
+ * This ensures that if we want to add more return results later, we can do so in a
+ * compatible way.
+ */
 @Path(""/agent"")
 @Produces(MediaType.APPLICATION_JSON)
 @Consumes(MediaType.APPLICATION_JSON)
@@ -55,14 +67,23 @@ public AgentStatusResponse getStatus() throws Throwable {
     @POST
     @Path(""/worker/create"")
-    public CreateWorkerResponse createWorker(CreateWorkerRequest req) throws Throwable {
-        return agent().createWorker(req);
+    public Empty createWorker(CreateWorkerRequest req) throws Throwable {
+        agent().createWorker(req);
+        return Empty.INSTANCE;
     }
     @PUT
     @Path(""/worker/stop"")
-    public StopWorkerResponse stopWorker(StopWorkerRequest req) throws Throwable {
-        return agent().stopWorker(req);
+    public Empty stopWorker(StopWorkerRequest req) throws Throwable {
+        agent().stopWorker(req);
+        return Empty.INSTANCE;
+    }
+
+    @DELETE
+    @Path(""/worker"")
+    public Empty destroyWorker(@DefaultValue(""0"") @QueryParam(""workerId"") long workerId) throws Throwable {
+        agent().destroyWorker(new DestroyWorkerRequest(workerId));
+        return Empty.INSTANCE;
     }
     @PUT
diff --git [file java] [file java]
index 7c8de6d3f22..59d34c90ab6 100644
--- [file java]
+++ [file java]
@@ -25,6 +25,7 @@
 import org.apache.kafka.common.utils.Utils;
 import org.apache.kafka.trogdor.common.Platform;
 import org.apache.kafka.trogdor.common.ThreadUtils;
+import org.apache.kafka.trogdor.rest.RequestConflictException;
 import org.apache.kafka.trogdor.rest.WorkerDone;
 import org.apache.kafka.trogdor.rest.WorkerRunning;
 import org.apache.kafka.trogdor.rest.WorkerStarting;
@@ -36,10 +37,12 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.TreeMap;
 import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutionException;
 import java.util.concurrent.Executors;
 import java.util.concurrent.Future;
 import java.util.concurrent.ScheduledExecutorService;
@@ -72,7 +75,7 @@
     /**
      * A map of task IDs to Work objects.
      */
-    private final Map<String, Worker> workers;
+    private final Map<Long, Worker> workers;
     /**
      * An ExecutorService used to schedule events in the future.
@@ -137,12 +140,15 @@ synchronized boolean shutdown() {
                 return false;
             }
             shutdown = true;
+            if (refCount == 0) {
+                this.notifyAll();
+            }
             return true;
         }
         synchronized void waitForQuiescence() throws InterruptedException {
             while ((!shutdown) || (refCount > 0)) {
-                wait();
+                this.wait();
             }
         }
     }
@@ -173,10 +179,15 @@ synchronized void waitForQuiescence() throws InterruptedException {
      * A worker which is being tracked.
      */
     class Worker {
+        /**
+         * The worker ID.
+         */
+        private final long workerId;
+
         /**
          * The task ID.
          */
-        private final String id;
+        private final String taskId;
         /**
          * The task specification.
@@ -217,7 +228,7 @@ synchronized void waitForQuiescence() throws InterruptedException {
          * If there is a task timeout scheduled, this is a future which can
          * be used to cancel it.
          */
-        private Future<TaskSpec> timeoutFuture = null;
+        private Future<Void> timeoutFuture = null;
         /**
          * A shutdown manager reference which will keep the WorkerManager
@@ -225,16 +236,26 @@ synchronized void waitForQuiescence() throws InterruptedException {
          */
         private ShutdownManager.Reference reference;
-        Worker(String id, TaskSpec spec, long now) {
-            this.id = id;
+        /**
+         * Whether we should destroy the records of this worker once it stops.
+         */
+        private boolean mustDestroy = false;
+
+        Worker(long workerId, String taskId, TaskSpec spec, long now) {
+            this.workerId = workerId;
+            this.taskId = taskId;
             this.spec = spec;
-            this.taskWorker = spec.newTaskWorker(id);
+            this.taskWorker = spec.newTaskWorker(taskId);
             this.startedMs = now;
             this.reference = shutdownManager.takeReference();
         }
-        String id() {
-            return id;
+        long workerId() {
+            return workerId;
+        }
+
+        String taskId() {
+            return taskId;
         }
         TaskSpec spec() {
@@ -244,14 +265,14 @@ TaskSpec spec() {
         WorkerState state() {
             switch (state) {
                 case STARTING:
-                    return new WorkerStarting(spec);
+                    return new WorkerStarting(taskId, spec);
                 case RUNNING:
-                    return new WorkerRunning(spec, startedMs, status.get());
+                    return new WorkerRunning(taskId, spec, startedMs, status.get());
                 case CANCELLING:
                 case STOPPING:
-                    return new WorkerStopping(spec, startedMs, status.get());
+                    return new WorkerStopping(taskId, spec, startedMs, status.get());
                 case DONE:
-                    return new WorkerDone(spec, startedMs, doneMs, status.get(), error);
+                    return new WorkerDone(taskId, spec, startedMs, doneMs, status.get(), error);
             }
             throw new RuntimeException(""unreachable"");
         }
@@ -259,7 +280,7 @@ WorkerState state() {
         void transitionToRunning() {
             state = State.RUNNING;
             timeoutFuture = scheduler.schedule(stateChangeExecutor,
-                new StopWorker(id), spec.durationMs());
+                new StopWorker(workerId, false), spec.durationMs());
         }
         void transitionToStopping() {
@@ -268,7 +289,7 @@ void transitionToStopping() {
                 timeoutFuture.cancel(false);
                 timeoutFuture = null;
             }
-            workerCleanupExecutor.submit(new CleanupWorker(this));
+            workerCleanupExecutor.submit(new HaltWorker(this));
         }
         void transitionToDone() {
@@ -279,15 +300,20 @@ void transitionToDone() {
                 reference = null;
             }
         }
+
+        @Override
+        public String toString() {
+            return String.format(""%s_%d"", taskId, workerId);
+        }
     }
-    public void createWorker(final String id, TaskSpec spec) throws Exception {
+    public void createWorker(long workerId, String taskId, TaskSpec spec) throws Throwable {
         try (ShutdownManager.Reference ref = shutdownManager.takeReference()) {
             final Worker worker = stateChangeExecutor.
-                submit(new CreateWorker(id, spec, time.milliseconds())).get();
+                submit(new CreateWorker(workerId, taskId, spec, time.milliseconds())).get();
             if (worker == null) {
                 log.info(""{}: Ignoring request to create worker {}, because there is already "" +
-                    ""a worker with that id."", nodeName, id);
+                    ""a worker with that id."", nodeName, workerId);
                 return;
             }
             KafkaFutureImpl<String> haltFuture = new KafkaFutureImpl<>();
@@ -297,9 +323,10 @@ public Void apply(String errorString) {
                     if (errorString == null)
                         errorString = """";
                     if (errorString.isEmpty()) {
-                        log.info(""{}: Worker {} is halting."", nodeName, id);
+                        log.info(""{}: Worker {} is halting."", nodeName, worker);
                     } else {
-                        log.info(""{}: Worker {} is halting with error {}"", nodeName, id, errorString);
+                        log.info(""{}: Worker {} is halting with error {}"",
+                            nodeName, worker, errorString);
                     }
                     stateChangeExecutor.submit(
                         new HandleWorkerHalting(worker, errorString, false));
@@ -309,11 +336,20 @@ public Void apply(String errorString) {
             try {
                 worker.taskWorker.start(platform, worker.status, haltFuture);
             } catch (Exception e) {
-                log.info(""{}: Worker {} start() exception"", nodeName, id, e);
+                log.info(""{}: Worker {} start() exception"", nodeName, worker, e);
                 stateChangeExecutor.submit(new HandleWorkerHalting(worker,
                     ""worker.start() exception: "" + Utils.stackTrace(e), true));
             }
             stateChangeExecutor.submit(new FinishCreatingWorker(worker));
+        } catch (ExecutionException e) {
+            if (e.getCause() instanceof RequestConflictException) {
+                log.info(""{}: request conflict while creating worker {} for task {} with spec {}."",
+                    nodeName, workerId, taskId, spec);
+            } else {
+                log.info(""{}: Error creating worker {} for task {} with spec {}"",
+                    nodeName, workerId, taskId, spec, e);
+            }
+            throw e.getCause();
         }
     }
@@ -321,27 +357,42 @@ public Void apply(String errorString) {
      * Handles a request to create a new worker.  Processed by the state change thread.
      */
     class CreateWorker implements Callable<Worker> {
-        private final String id;
+        private final long workerId;
+        private final String taskId;
         private final TaskSpec spec;
         private final long now;
-        CreateWorker(String id, TaskSpec spec, long now) {
-            this.id = id;
+        CreateWorker(long workerId, String taskId, TaskSpec spec, long now) {
+            this.workerId = workerId;
+            this.taskId = taskId;
             this.spec = spec;
             this.now = now;
         }
         @Override
         public Worker call() throws Exception {
-            Worker worker = workers.get(id);
-            if (worker != null) {
-                log.info(""{}: Task ID {} is already in use."", nodeName, id);
-                return null;
+            try {
+                Worker worker = workers.get(workerId);
+                if (worker != null) {
+                    if (!worker.taskId().equals(taskId)) {
+                        throw new RequestConflictException(""There is already a worker ID "" + workerId +
+                            "" with a different task ID."");
+                    } else if (!worker.spec().equals(spec)) {
+                        throw new RequestConflictException(""There is already a worker ID "" + workerId +
+                            "" with a different task spec."");
+                    } else {
+                        return null;
+                    }
+                }
+                worker = new Worker(workerId, taskId, spec, now);
+                workers.put(workerId, worker);
+                log.info(""{}: Created worker {} with spec {}"", nodeName, worker, spec);
+                return worker;
+            } catch (Exception e) {
+                log.info(""{}: unable to create worker {} for task {}, with spec {}"",
+                    nodeName, workerId, taskId, spec, e);
+                throw e;
             }
-            worker = new Worker(id, spec, now);
-            workers.put(id, worker);
-            log.info(""{}: Created a new worker for task {} with spec {}"", nodeName, id, spec);
-            return worker;
         }
     }
@@ -360,12 +411,12 @@ public Void call() throws Exception {
             switch (worker.state) {
                 case CANCELLING:
                     log.info(""{}: Worker {} was cancelled while it was starting up.  "" +
-                        ""Transitioning to STOPPING."", nodeName, worker.id);
+                        ""Transitioning to STOPPING."", nodeName, worker);
                     worker.transitionToStopping();
                     break;
                 case STARTING:
                     log.info(""{}: Worker {} is now RUNNING.  Scheduled to stop in {} ms."",
-                        nodeName, worker.id, worker.spec.durationMs());
+                        nodeName, worker, worker.spec.durationMs());
                     worker.transitionToRunning();
                     break;
                 default:
@@ -400,29 +451,29 @@ public Void call() throws Exception {
                 case STARTING:
                     if (startupHalt) {
                         log.info(""{}: Worker {} {} during startup.  Transitioning to DONE."",
-                            nodeName, worker.id, verb);
+                            nodeName, worker, verb);
                         worker.transitionToDone();
                     } else {
                         log.info(""{}: Worker {} {} during startup.  Transitioning to CANCELLING."",
-                            nodeName, worker.id, verb);
+                            nodeName, worker, verb);
                         worker.state = State.CANCELLING;
                     }
                     break;
                 case CANCELLING:
                     log.info(""{}: Cancelling worker {} {}.  "",
-                            nodeName, worker.id, verb);
+                            nodeName, worker, verb);
                     break;
                 case RUNNING:
                     log.info(""{}: Running worker {} {}.  Transitioning to STOPPING."",
-                        nodeName, worker.id, verb);
+                        nodeName, worker, verb);
                     worker.transitionToStopping();
                     break;
                 case STOPPING:
-                    log.info(""{}: Stopping worker {} {}."", nodeName, worker.id, verb);
+                    log.info(""{}: Stopping worker {} {}."", nodeName, worker, verb);
                     break;
                 case DONE:
                     log.info(""{}: Can't halt worker {} because it is already DONE."",
-                        nodeName, worker.id);
+                        nodeName, worker);
                     break;
             }
             return null;
@@ -432,7 +483,7 @@ public Void call() throws Exception {
     /**
      * Transitions a worker to WorkerDone.  Processed by the state change thread.
      */
-    static class CompleteWorker implements Callable<Void> {
+    class CompleteWorker implements Callable<Void> {
         private final Worker worker;
         private final String failure;
@@ -448,60 +499,79 @@ public Void call() throws Exception {
                 worker.error = failure;
             }
             worker.transitionToDone();
+            if (worker.mustDestroy) {
+                log.info(""{}: destroying worker {} with error {}"",
+                    nodeName, worker, worker.error);
+                workers.remove(worker.workerId);
+            } else {
+                log.info(""{}: completed worker {} with error {}"",
+                    nodeName, worker, worker.error);
+            }
             return null;
         }
     }
-    public TaskSpec stopWorker(String id) throws Exception {
+    public void stopWorker(long workerId, boolean mustDestroy) throws Throwable {
         try (ShutdownManager.Reference ref = shutdownManager.takeReference()) {
-            TaskSpec taskSpec = stateChangeExecutor.submit(new StopWorker(id)).get();
-            if (taskSpec == null) {
-                throw new KafkaException(""No task found with id "" + id);
-            }
-            return taskSpec;
+            stateChangeExecutor.submit(new StopWorker(workerId, mustDestroy)).get();
+        } catch (ExecutionException e) {
+            throw e.getCause();
         }
     }
     /**
      * Stops a worker.  Processed by the state change thread.
      */
-    class StopWorker implements Callable<TaskSpec> {
-        private final String id;
+    class StopWorker implements Callable<Void> {
+        private final long workerId;
+        private final boolean mustDestroy;
-        StopWorker(String id) {
-            this.id = id;
+        StopWorker(long workerId, boolean mustDestroy) {
+            this.workerId = workerId;
+            this.mustDestroy = mustDestroy;
         }
         @Override
-        public TaskSpec call() throws Exception {
-            Worker worker = workers.get(id);
+        public Void call() throws Exception {
+            Worker worker = workers.get(workerId);
             if (worker == null) {
+                log.info(""{}: Can't stop worker {} because there is no worker with that ID."",
+                    nodeName, workerId);
                 return null;
             }
+            if (mustDestroy) {
+                worker.mustDestroy = true;
+            }
             switch (worker.state) {
                 case STARTING:
                     log.info(""{}: Cancelling worker {} during its startup process."",
-                        nodeName, id);
+                        nodeName, worker);
                     worker.state = State.CANCELLING;
                     break;
                 case CANCELLING:
                     log.info(""{}: Can't stop worker {}, because it is already being "" +
-                        ""cancelled."", nodeName, id);
+                        ""cancelled."", nodeName, worker);
                     break;
                 case RUNNING:
-                    log.info(""{}: Stopping running worker {}."", nodeName, id);
+                    log.info(""{}: Stopping running worker {}."", nodeName, worker);
                     worker.transitionToStopping();
                     break;
                 case STOPPING:
                     log.info(""{}: Can't stop worker {}, because it is already "" +
-                            ""stopping."", nodeName, id);
+                            ""stopping."", nodeName, worker);
                     break;
                 case DONE:
-                    log.debug(""{}: Can't stop worker {}, because it is already done."",
-                        nodeName, id);
+                    if (worker.mustDestroy) {
+                        log.info(""{}: destroying worker {} with error {}"",
+                            nodeName, worker, worker.error);
+                        workers.remove(worker.workerId);
+                    } else {
+                        log.debug(""{}: Can't stop worker {}, because it is already done."",
+                            nodeName, worker);
+                    }
                     break;
             }
-            return worker.spec();
+            return null;
         }
     }
@@ -509,10 +579,10 @@ public TaskSpec call() throws Exception {
      * Cleans up the resources associated with a worker.  Processed by the worker
      * cleanup thread pool.
      */
-    class CleanupWorker implements Callable<Void> {
+    class HaltWorker implements Callable<Void> {
         private final Worker worker;
-        CleanupWorker(Worker worker) {
+        HaltWorker(Worker worker) {
             this.worker = worker;
         }
@@ -530,18 +600,18 @@ public Void call() throws Exception {
         }
     }
-    public TreeMap<String, WorkerState> workerStates() throws Exception {
+    public TreeMap<Long, WorkerState> workerStates() throws Exception {
         try (ShutdownManager.Reference ref = shutdownManager.takeReference()) {
             return stateChangeExecutor.submit(new GetWorkerStates()).get();
         }
     }
-    class GetWorkerStates implements Callable<TreeMap<String, WorkerState>> {
+    class GetWorkerStates implements Callable<TreeMap<Long, WorkerState>> {
         @Override
-        public TreeMap<String, WorkerState> call() throws Exception {
-            TreeMap<String, WorkerState> workerMap = new TreeMap<>();
+        public TreeMap<Long, WorkerState> call() throws Exception {
+            TreeMap<Long, WorkerState> workerMap = new TreeMap<>();
             for (Worker worker : workers.values()) {
-                workerMap.put(worker.id(), worker.state());
+                workerMap.put(worker.workerId(), worker.state());
             }
             return workerMap;
         }
@@ -562,17 +632,53 @@ public void waitForShutdown() throws Exception {
     class Shutdown implements Callable<Void> {
         @Override
         public Void call() throws Exception {
-            log.info(""{}: Shutting down WorkerManager."", platform.curNode().name());
-            for (Worker worker : workers.values()) {
-                stateChangeExecutor.submit(new StopWorker(worker.id));
+            log.info(""{}: Shutting down WorkerManager."", nodeName);
+            try {
+                stateChangeExecutor.submit(new DestroyAllWorkers()).get();
+                log.info(""{}: Waiting for shutdownManager quiescence..."", nodeName);
+                shutdownManager.waitForQuiescence();
+                workerCleanupExecutor.shutdownNow();
+                stateChangeExecutor.shutdownNow();
+                log.info(""{}: Waiting for workerCleanupExecutor to terminate..."", nodeName);
+                workerCleanupExecutor.awaitTermination(1, TimeUnit.DAYS);
+                log.info(""{}: Waiting for stateChangeExecutor to terminate..."", nodeName);
+                stateChangeExecutor.awaitTermination(1, TimeUnit.DAYS);
+                log.info(""{}: Shutting down shutdownExecutor."", nodeName);
+                shutdownExecutor.shutdown();
+            } catch (Exception e) {
+                log.info(""{}: Caught exception while shutting down WorkerManager"", nodeName, e);
+                throw e;
+            }
+            return null;
+        }
+    }
+
+    /**
+     * Begins the process of destroying all workers.  Processed by the state change thread.
+     */
+    class DestroyAllWorkers implements Callable<Void> {
+        @Override
+        public Void call() throws Exception {
+            log.info(""{}: Destroying all workers."", nodeName);
+
+            // StopWorker may remove elements from the set of worker IDs.  That might generate
+            // a ConcurrentModificationException if we were iterating over the worker ID
+            // set directly.  Therefore, we make a copy of the worker IDs here and iterate
+            // over that instead.
+            //
+            // Note that there is no possible way that more worker IDs can be added while this
+            // callable is running, because the state change executor is single-threaded.
+            ArrayList<Long> workerIds = new ArrayList<>(workers.keySet());
+
+            for (long workerId : workerIds) {
+                try {
+                    new StopWorker(workerId, true).call();
+                } catch (Exception e) {
+                    log.error(""Failed to stop worker {}"", workerId, e);
+                }
             }
-            shutdownManager.waitForQuiescence();
-            workerCleanupExecutor.shutdownNow();
-            stateChangeExecutor.shutdownNow();
-            workerCleanupExecutor.awaitTermination(1, TimeUnit.DAYS);
-            stateChangeExecutor.awaitTermination(1, TimeUnit.DAYS);
-            shutdownExecutor.shutdown();
             return null;
         }
     }
+
 }
diff --git [file java] [file java]
index 717d7c7047a..23f3ceb91b0 100644
--- [file java]
+++ [file java]
@@ -27,15 +27,16 @@
 import org.apache.kafka.trogdor.common.Platform;
 import org.apache.kafka.trogdor.rest.CoordinatorStatusResponse;
 import org.apache.kafka.trogdor.rest.CreateTaskRequest;
-import org.apache.kafka.trogdor.rest.CreateTaskResponse;
+import org.apache.kafka.trogdor.rest.DestroyTaskRequest;
 import org.apache.kafka.trogdor.rest.JsonRestServer;
 import org.apache.kafka.trogdor.rest.StopTaskRequest;
-import org.apache.kafka.trogdor.rest.StopTaskResponse;
 import org.apache.kafka.trogdor.rest.TasksRequest;
 import org.apache.kafka.trogdor.rest.TasksResponse;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import java.util.concurrent.ThreadLocalRandom;
+
 import static net.sourceforge.argparse4j.impl.Arguments.store;
 /**
@@ -72,9 +73,9 @@
      * @param resource      The AgentRestResoure to use.
      */
     public Coordinator(Platform platform, Scheduler scheduler, JsonRestServer restServer,
-                       CoordinatorRestResource resource) {
+                       CoordinatorRestResource resource, long firstWorkerId) {
         this.startTimeMs = scheduler.time().milliseconds();
-        this.taskManager = new TaskManager(platform, scheduler);
+        this.taskManager = new TaskManager(platform, scheduler, firstWorkerId);
         this.restServer = restServer;
         resource.setCoordinator(this);
     }
@@ -87,12 +88,16 @@ public CoordinatorStatusResponse status() throws Exception {
         return new CoordinatorStatusResponse(startTimeMs);
     }
-    public CreateTaskResponse createTask(CreateTaskRequest request) throws Exception {
-        return new CreateTaskResponse(taskManager.createTask(request.id(), request.spec()));
+    public void createTask(CreateTaskRequest request) throws Throwable {
+        taskManager.createTask(request.id(), request.spec());
+    }
+
+    public void stopTask(StopTaskRequest request) throws Throwable {
+        taskManager.stopTask(request.id());
     }
-    public StopTaskResponse stopTask(StopTaskRequest request) throws Exception {
-        return new StopTaskResponse(taskManager.stopTask(request.id()));
+    public void destroyTask(DestroyTaskRequest request) throws Throwable {
+        taskManager.destroyTask(request.id());
     }
     public TasksResponse tasks(TasksRequest request) throws Exception {
@@ -149,7 +154,7 @@ public static void main(String args) throws Exception {
         CoordinatorRestResource resource = new CoordinatorRestResource();
         log.info(""Starting coordinator process."");
         final Coordinator coordinator = new Coordinator(platform, Scheduler.SYSTEM,
-            restServer, resource);
+            restServer, resource, ThreadLocalRandom.current().nextLong(0, Long.MAX_VALUE / 2));
         restServer.start(resource);
         Runtime.getRuntime().addShutdownHook(new Thread() {
             @Override
diff --git [file java] [file java]
index 0677296ee3c..780ae737e0e 100644
--- [file java]
+++ [file java]
@@ -27,12 +27,11 @@
 import org.apache.kafka.trogdor.common.JsonUtil;
 import org.apache.kafka.trogdor.rest.CoordinatorStatusResponse;
 import org.apache.kafka.trogdor.rest.CreateTaskRequest;
-import org.apache.kafka.trogdor.rest.CreateTaskResponse;
+import org.apache.kafka.trogdor.rest.DestroyTaskRequest;
 import org.apache.kafka.trogdor.rest.Empty;
 import org.apache.kafka.trogdor.rest.JsonRestServer;
 import org.apache.kafka.trogdor.rest.JsonRestServer.HttpResponse;
 import org.apache.kafka.trogdor.rest.StopTaskRequest;
-import org.apache.kafka.trogdor.rest.StopTaskResponse;
 import org.apache.kafka.trogdor.rest.TasksRequest;
 import org.apache.kafka.trogdor.rest.TasksResponse;
 import org.slf4j.Logger;
@@ -116,36 +115,45 @@ public CoordinatorStatusResponse status() throws Exception {
         return resp.body();
     }
-    public CreateTaskResponse createTask(CreateTaskRequest request) throws Exception {
-        HttpResponse<CreateTaskResponse> resp =
-            JsonRestServer.<CreateTaskResponse>httpRequest(log, url(""/coordinator/task/create""), ""POST"",
-                request, new TypeReference<CreateTaskResponse>() { }, maxTries);
-        return resp.body();
+    public void createTask(CreateTaskRequest request) throws Exception {
+        HttpResponse<Empty> resp =
+            JsonRestServer.httpRequest(log, url(""/coordinator/task/create""), ""POST"",
+                request, new TypeReference<Empty>() { }, maxTries);
+        resp.body();
     }
-    public StopTaskResponse stopTask(StopTaskRequest request) throws Exception {
-        HttpResponse<StopTaskResponse> resp =
-            JsonRestServer.<StopTaskResponse>httpRequest(log, url(""/coordinator/task/stop""), ""PUT"",
-                request, new TypeReference<StopTaskResponse>() { }, maxTries);
-        return resp.body();
+    public void stopTask(StopTaskRequest request) throws Exception {
+        HttpResponse<Empty> resp =
+            JsonRestServer.httpRequest(log, url(""/coordinator/task/stop""), ""PUT"",
+                request, new TypeReference<Empty>() { }, maxTries);
+        resp.body();
+    }
+
+    public void destroyTask(DestroyTaskRequest request) throws Exception {
+        UriBuilder uriBuilder = UriBuilder.fromPath(url(""/coordinator/tasks""));
+        uriBuilder.queryParam(""taskId"", request.id());
+        HttpResponse<Empty> resp =
+            JsonRestServer.httpRequest(log, uriBuilder.build().toString(), ""DELETE"",
+                null, new TypeReference<Empty>() { }, maxTries);
+        resp.body();
     }
     public TasksResponse tasks(TasksRequest request) throws Exception {
         UriBuilder uriBuilder = UriBuilder.fromPath(url(""/coordinator/tasks""));
-        uriBuilder.queryParam(""taskId"", request.taskIds().toArray(new String));
+        uriBuilder.queryParam(""taskId"", (Object) request.taskIds().toArray(new String));
         uriBuilder.queryParam(""firstStartMs"", request.firstStartMs());
         uriBuilder.queryParam(""lastStartMs"", request.lastStartMs());
         uriBuilder.queryParam(""firstEndMs"", request.firstEndMs());
         uriBuilder.queryParam(""lastEndMs"", request.lastEndMs());
         HttpResponse<TasksResponse> resp =
-            JsonRestServer.<TasksResponse>httpRequest(log, uriBuilder.build().toString(), ""GET"",
+            JsonRestServer.httpRequest(log, uriBuilder.build().toString(), ""GET"",
                 null, new TypeReference<TasksResponse>() { }, maxTries);
         return resp.body();
     }
     public void shutdown() throws Exception {
         HttpResponse<Empty> resp =
-            JsonRestServer.<Empty>httpRequest(log, url(""/coordinator/shutdown""), ""PUT"",
+            JsonRestServer.httpRequest(log, url(""/coordinator/shutdown""), ""PUT"",
                 null, new TypeReference<Empty>() { }, maxTries);
         resp.body();
     }
@@ -185,6 +193,12 @@ public static void main(String args) throws Exception {
             .dest(""stop_task"")
             .metavar(""TASK_ID"")
             .help(""Stop a task."");
+        actions.addArgument(""--destroy-task"")
+            .action(store())
+            .type(String.class)
+            .dest(""destroy_task"")
+            .metavar(""TASK_ID"")
+            .help(""Destroy a task."");
         actions.addArgument(""--shutdown"")
             .action(storeTrue())
             .type(Boolean.class)
@@ -216,15 +230,21 @@ public static void main(String args) throws Exception {
                 JsonUtil.toPrettyJsonString(client.tasks(
                     new TasksRequest(null, 0, 0, 0, 0))));
         } else if (res.getString(""create_task"") != null) {
-            client.createTask(JsonUtil.JSON_SERDE.readValue(res.getString(""create_task""),
-                CreateTaskRequest.class));
-            System.out.println(""Created task."");
+            CreateTaskRequest req = JsonUtil.JSON_SERDE.
+                readValue(res.getString(""create_task""), CreateTaskRequest.class);
+            client.createTask(req);
+            System.out.printf(""Sent CreateTaskRequest for task %s."", req.id());
         } else if (res.getString(""stop_task"") != null) {
-            client.stopTask(new StopTaskRequest(res.getString(""stop_task"")));
-            System.out.println(""Created task."");
+            String taskId = res.getString(""stop_task"");
+            client.stopTask(new StopTaskRequest(taskId));
+            System.out.printf(""Sent StopTaskRequest for task %s.%n"", taskId);
+        } else if (res.getString(""destroy_task"") != null) {
+            String taskId = res.getString(""destroy_task"");
+            client.destroyTask(new DestroyTaskRequest(taskId));
+            System.out.printf(""Sent DestroyTaskRequest for task %s.%n"", taskId);
         } else if (res.getBoolean(""shutdown"")) {
             client.shutdown();
-            System.out.println(""Sent shutdown request."");
+            System.out.println(""Sent ShutdownRequest."");
         } else {
             System.out.println(""You must choose an action. Type --help for help."");
             Exit.exit(1);
diff --git [file java] [file java]
index b8663ec4cc3..cbfbddd7eda 100644
--- [file java]
+++ [file java]
@@ -19,15 +19,15 @@
 import org.apache.kafka.trogdor.rest.CoordinatorShutdownRequest;
 import org.apache.kafka.trogdor.rest.CoordinatorStatusResponse;
 import org.apache.kafka.trogdor.rest.CreateTaskRequest;
-import org.apache.kafka.trogdor.rest.CreateTaskResponse;
+import org.apache.kafka.trogdor.rest.DestroyTaskRequest;
 import org.apache.kafka.trogdor.rest.Empty;
 import org.apache.kafka.trogdor.rest.StopTaskRequest;
-import org.apache.kafka.trogdor.rest.StopTaskResponse;
 import org.apache.kafka.trogdor.rest.TasksRequest;
 import org.apache.kafka.trogdor.rest.TasksResponse;
 import javax.servlet.ServletContext;
 import javax.ws.rs.Consumes;
+import javax.ws.rs.DELETE;
 import javax.ws.rs.DefaultValue;
 import javax.ws.rs.GET;
 import javax.ws.rs.POST;
@@ -39,7 +39,18 @@
 import java.util.List;
 import java.util.concurrent.atomic.AtomicReference;
-
+/**
+ * The REST resource for the Coordinator. This describes the RPCs which the coordinator
+ * can accept.
+ *
+ * RPCs should be idempotent.  This is important because if the server's response is
+ * lost, the client will simply retransmit the same request. The server's response must
+ * be the same the second time around.
+ *
+ * We return the empty JSON object {} rather than void for RPCs that have no results.
+ * This ensures that if we want to add more return results later, we can do so in a
+ * compatible way.
+ */
 @Path(""/coordinator"")
 @Produces(MediaType.APPLICATION_JSON)
 @Consumes(MediaType.APPLICATION_JSON)
@@ -61,14 +72,23 @@ public CoordinatorStatusResponse status() throws Throwable {
     @POST
     @Path(""/task/create"")
-    public CreateTaskResponse createTask(CreateTaskRequest request) throws Throwable {
-        return coordinator().createTask(request);
+    public Empty createTask(CreateTaskRequest request) throws Throwable {
+        coordinator().createTask(request);
+        return Empty.INSTANCE;
     }
     @PUT
     @Path(""/task/stop"")
-    public StopTaskResponse stopTask(StopTaskRequest request) throws Throwable {
-        return coordinator().stopTask(request);
+    public Empty stopTask(StopTaskRequest request) throws Throwable {
+        coordinator().stopTask(request);
+        return Empty.INSTANCE;
+    }
+
+    @DELETE
+    @Path(""/tasks"")
+    public Empty destroyTask(@DefaultValue("""") @QueryParam(""taskId"") String taskId) throws Throwable {
+        coordinator().destroyTask(new DestroyTaskRequest(taskId));
+        return Empty.INSTANCE;
     }
     @GET
diff --git [file java] [file java]
index 91ef9c2928a..3f0075e598a 100644
--- [file java]
+++ [file java]
@@ -79,13 +79,16 @@
     private static final long HEARTBEAT_DELAY_MS = 1000L;
     class ManagedWorker {
-        private final String id;
+        private final long workerId;
+        private final String taskId;
         private final TaskSpec spec;
         private boolean shouldRun;
         private WorkerState state;
-        ManagedWorker(String id, TaskSpec spec, boolean shouldRun, WorkerState state) {
-            this.id = id;
+        ManagedWorker(long workerId, String taskId, TaskSpec spec,
+                      boolean shouldRun, WorkerState state) {
+            this.workerId = workerId;
+            this.taskId = taskId;
             this.spec = spec;
             this.shouldRun = shouldRun;
             this.state = state;
@@ -93,19 +96,24 @@
         void tryCreate() {
             try {
-                client.createWorker(new CreateWorkerRequest(id, spec));
+                client.createWorker(new CreateWorkerRequest(workerId, taskId, spec));
             } catch (Throwable e) {
-                log.error(""{}: error creating worker {}."", node.name(), id, e);
+                log.error(""{}: error creating worker {}."", node.name(), this, e);
             }
         }
         void tryStop() {
             try {
-                client.stopWorker(new StopWorkerRequest(id));
+                client.stopWorker(new StopWorkerRequest(workerId));
             } catch (Throwable e) {
-                log.error(""{}: error stopping worker {}."", node.name(), id, e);
+                log.error(""{}: error stopping worker {}."", node.name(), this, e);
             }
         }
+
+        @Override
+        public String toString() {
+            return String.format(""%s_%d"", taskId, workerId);
+        }
     }
     /**
@@ -126,7 +134,7 @@ void tryStop() {
     /**
      * Maps task IDs to worker structures.
      */
-    private final Map<String, ManagedWorker> workers;
+    private final Map<Long, ManagedWorker> workers;
     /**
      * An executor service which manages the thread dedicated to this node.
@@ -196,24 +204,25 @@ public void run() {
                 }
                 // Identify workers which we think should be running, but which do not appear
                 // in the agent's response.  We need to send startWorker requests for these.
-                for (Map.Entry<String, ManagedWorker> entry : workers.entrySet()) {
-                    String id = entry.getKey();
-                    if (!agentStatus.workers().containsKey(id)) {
+                for (Map.Entry<Long, ManagedWorker> entry : workers.entrySet()) {
+                    Long workerId = entry.getKey();
+                    if (!agentStatus.workers().containsKey(workerId)) {
                         ManagedWorker worker = entry.getValue();
                         if (worker.shouldRun) {
                             worker.tryCreate();
                         }
                     }
                 }
-                for (Map.Entry<String, WorkerState> entry : agentStatus.workers().entrySet()) {
-                    String id = entry.getKey();
+                for (Map.Entry<Long, WorkerState> entry : agentStatus.workers().entrySet()) {
+                    long workerId = entry.getKey();
                     WorkerState state = entry.getValue();
-                    ManagedWorker worker = workers.get(id);
+                    ManagedWorker worker = workers.get(workerId);
                     if (worker == null) {
                         // Identify tasks which are running, but which we don't know about.
                         // Add these to the NodeManager as tasks that should not be running.
-                        log.warn(""{}: scheduling unknown worker {} for stopping."", node.name(), id);
-                        workers.put(id, new ManagedWorker(id, state.spec(), false, state));
+                        log.warn(""{}: scheduling unknown worker with ID {} for stopping."", node.name(), workerId);
+                        workers.put(workerId, new ManagedWorker(workerId, state.taskId(),
+                            state.spec(), false, state));
                     } else {
                         // Handle workers which need to be stopped.
                         if (state instanceof WorkerStarting || state instanceof WorkerRunning) {
@@ -227,7 +236,7 @@ public void run() {
                         } else {
                             log.info(""{}: worker state changed from {} to {}"", node.name(), worker.state, state);
                             worker.state = state;
-                            taskManager.updateWorkerState(node.name(), worker.id, state);
+                            taskManager.updateWorkerState(node.name(), worker.workerId, state);
                         }
                     }
                 }
@@ -240,34 +249,39 @@ public void run() {
     /**
      * Create a new worker.
      *
-     * @param id                    The new worker id.
+     * @param workerId              The new worker id.
+     * @param taskId                The new task id.
      * @param spec                  The task specification to use with the new worker.
      */
-    public void createWorker(String id, TaskSpec spec) {
-        executor.submit(new CreateWorker(id, spec));
+    public void createWorker(long workerId, String taskId, TaskSpec spec) {
+        executor.submit(new CreateWorker(workerId, taskId, spec));
     }
     /**
      * Starts a worker.
      */
     class CreateWorker implements Callable<Void> {
-        private final String id;
+        private final long workerId;
+        private final String taskId;
         private final TaskSpec spec;
-        CreateWorker(String id, TaskSpec spec) {
-            this.id = id;
+        CreateWorker(long workerId, String taskId, TaskSpec spec) {
+            this.workerId = workerId;
+            this.taskId = taskId;
             this.spec = spec;
         }
         @Override
         public Void call() throws Exception {
-            ManagedWorker worker = workers.get(id);
+            ManagedWorker worker = workers.get(workerId);
             if (worker != null) {
-                log.error(""{}: there is already a worker for task {}."", node.name(), id);
+                log.error(""{}: there is already a worker {} with ID {}."",
+                    node.name(), worker, workerId);
                 return null;
             }
-            log.info(""{}: scheduling worker {} to start."", node.name(), id);
-            workers.put(id, new ManagedWorker(id, spec, true, new WorkerReceiving(spec)));
+            worker = new ManagedWorker(workerId, taskId, spec, true, new WorkerReceiving(taskId, spec));
+            log.info(""{}: scheduling worker {} to start."", node.name(), worker);
+            workers.put(workerId, worker);
             rescheduleNextHeartbeat(0);
             return null;
         }
@@ -276,41 +290,72 @@ public Void call() throws Exception {
     /**
      * Stop a worker.
      *
-     * @param id                    The id of the worker to stop.
+     * @param workerId              The id of the worker to stop.
      */
-    public void stopWorker(String id) {
-        executor.submit(new StopWorker(id));
+    public void stopWorker(long workerId) {
+        executor.submit(new StopWorker(workerId));
     }
     /**
      * Stops a worker.
      */
     class StopWorker implements Callable<Void> {
-        private final String id;
+        private final long workerId;
-        StopWorker(String id) {
-            this.id = id;
+        StopWorker(long workerId) {
+            this.workerId = workerId;
         }
         @Override
         public Void call() throws Exception {
-            ManagedWorker worker = workers.get(id);
+            ManagedWorker worker = workers.get(workerId);
             if (worker == null) {
-                log.error(""{}: can't stop non-existent worker {}."", node.name(), id);
+                log.error(""{}: unable to locate worker to stop with ID {}."", node.name(), workerId);
                 return null;
             }
             if (!worker.shouldRun) {
-                log.error(""{}: The worker for task {} is already scheduled to stop."",
-                    node.name(), id);
+                log.error(""{}: Worker {} is already scheduled to stop."",
+                    node.name(), worker);
                 return null;
             }
-            log.info(""{}: scheduling worker {} on {} to stop."", node.name(), id);
+            log.info(""{}: scheduling worker {} to stop."", node.name(), worker);
             worker.shouldRun = false;
             rescheduleNextHeartbeat(0);
             return null;
         }
     }
+    /**
+     * Destroy a worker.
+     *
+     * @param workerId              The id of the worker to destroy.
+     */
+    public void destroyWorker(long workerId) {
+        executor.submit(new DestroyWorker(workerId));
+    }
+
+    /**
+     * Destroys a worker.
+     */
+    class DestroyWorker implements Callable<Void> {
+        private final long workerId;
+
+        DestroyWorker(long workerId) {
+            this.workerId = workerId;
+        }
+
+        @Override
+        public Void call() throws Exception {
+            ManagedWorker worker = workers.remove(workerId);
+            if (worker == null) {
+                log.error(""{}: unable to locate worker to destroy with ID {}."", node.name(), workerId);
+                return null;
+            }
+            rescheduleNextHeartbeat(0);
+            return null;
+        }
+    }
+
     public void beginShutdown(boolean stopNode) {
         executor.shutdownNow();
         if (stopNode) {
diff --git [file java] [file java]
index 7e19c8b34ae..74082bdb60a 100644
--- [file java]
+++ [file java]
@@ -21,6 +21,7 @@
 import com.fasterxml.jackson.databind.node.JsonNodeFactory;
 import com.fasterxml.jackson.databind.node.ObjectNode;
 import org.apache.kafka.common.KafkaException;
+import org.apache.kafka.common.errors.InvalidRequestException;
 import org.apache.kafka.common.utils.Scheduler;
 import org.apache.kafka.common.utils.Time;
 import org.apache.kafka.common.utils.Utils;
@@ -28,6 +29,7 @@
 import org.apache.kafka.trogdor.common.Node;
 import org.apache.kafka.trogdor.common.Platform;
 import org.apache.kafka.trogdor.common.ThreadUtils;
+import org.apache.kafka.trogdor.rest.RequestConflictException;
 import org.apache.kafka.trogdor.rest.TaskDone;
 import org.apache.kafka.trogdor.rest.TaskPending;
 import org.apache.kafka.trogdor.rest.TaskRunning;
@@ -106,12 +108,22 @@
      */
     private final Map<String, NodeManager> nodeManagers;
+    /**
+     * The states of all workers.
+     */
+    private final Map<Long, WorkerState> workerStates = new HashMap<>();
+
     /**
      * True if the TaskManager is shut down.
      */
     private AtomicBoolean shutdown = new AtomicBoolean(false);
-    TaskManager(Platform platform, Scheduler scheduler) {
+    /**
+     * The ID to use for the next worker.  Only accessed by the state change thread.
+     */
+    private long nextWorkerId;
+
+    TaskManager(Platform platform, Scheduler scheduler, long firstWorkerId) {
         this.platform = platform;
         this.scheduler = scheduler;
         this.time = scheduler.time();
@@ -119,6 +131,7 @@
         this.executor = Executors.newSingleThreadScheduledExecutor(
             ThreadUtils.createThreadFactory(""TaskManagerStateThread"", false));
         this.nodeManagers = new HashMap<>();
+        this.nextWorkerId = firstWorkerId;
         for (Node node : platform.topology().nodes().values()) {
             if (Node.Util.getTrogdorAgentPort(node) > 0) {
                 this.nodeManagers.put(node.name(), new NodeManager(node, this));
@@ -178,9 +191,9 @@
         private Future<?> startFuture = null;
         /**
-         * The states of the workers involved with this task.
+         * Maps node names to worker IDs.
          */
-        public Map<String, WorkerState> workerStates = new TreeMap<>();
+        public TreeMap<String, Long> workerIds = new TreeMap<>();
         /**
          * If this is non-empty, a message describing how this task failed.
@@ -240,38 +253,42 @@ TaskState taskState() {
                 case PENDING:
                     return new TaskPending(spec);
                 case RUNNING:
-                    return new TaskRunning(spec, startedMs, getCombinedStatus(workerStates));
+                    return new TaskRunning(spec, startedMs, getCombinedStatus());
                 case STOPPING:
-                    return new TaskStopping(spec, startedMs, getCombinedStatus(workerStates));
+                    return new TaskStopping(spec, startedMs, getCombinedStatus());
                 case DONE:
-                    return new TaskDone(spec, startedMs, doneMs, error, cancelled, getCombinedStatus(workerStates));
+                    return new TaskDone(spec, startedMs, doneMs, error, cancelled, getCombinedStatus());
             }
             throw new RuntimeException(""unreachable"");
         }
-        TreeSet<String> activeWorkers() {
-            TreeSet<String> workerNames = new TreeSet<>();
-            for (Map.Entry<String, WorkerState> entry : workerStates.entrySet()) {
-                if (!entry.getValue().done()) {
-                    workerNames.add(entry.getKey());
+        private JsonNode getCombinedStatus() {
+            if (workerIds.size() == 1) {
+                return workerStates.get(workerIds.values().iterator().next()).status();
+            } else {
+                ObjectNode objectNode = new ObjectNode(JsonNodeFactory.instance);
+                for (Map.Entry<String, Long> entry : workerIds.entrySet()) {
+                    String nodeName = entry.getKey();
+                    Long workerId = entry.getValue();
+                    WorkerState state = workerStates.get(workerId);
+                    JsonNode node = state.status();
+                    if (node != null) {
+                        objectNode.set(nodeName, node);
+                    }
                 }
+                return objectNode;
             }
-            return workerNames;
         }
-    }
-    private static final JsonNode getCombinedStatus(Map<String, WorkerState> states) {
-        if (states.size() == 1) {
-            return states.values().iterator().next().status();
-        } else {
-            ObjectNode objectNode = new ObjectNode(JsonNodeFactory.instance);
-            for (Map.Entry<String, WorkerState> entry : states.entrySet()) {
-                JsonNode node = entry.getValue().status();
-                if (node != null) {
-                    objectNode.set(entry.getKey(), node);
+        TreeMap<String, Long> activeWorkerIds() {
+            TreeMap<String, Long> activeWorkerIds = new TreeMap<>();
+            for (Map.Entry<String, Long> entry : workerIds.entrySet()) {
+                WorkerState workerState = workerStates.get(entry.getValue());
+                if (!workerState.done()) {
+                    activeWorkerIds.put(entry.getKey(), entry.getValue());
                 }
             }
-            return objectNode;
+            return activeWorkerIds;
         }
     }
@@ -280,27 +297,21 @@ private static final JsonNode getCombinedStatus(Map<String, WorkerState> states)
      *
      * @param id                    The ID of the task to create.
      * @param spec                  The specification of the task to create.
-     *
-     * @return                      The specification of the task with the given ID.
-     *                              Note that if there was already a task with the given ID,
-     *                              this may be different from the specification that was
-     *                              requested.
      */
-    public TaskSpec createTask(final String id, TaskSpec spec)
-            throws ExecutionException, InterruptedException {
-        final TaskSpec existingSpec = executor.submit(new CreateTask(id, spec)).get();
-        if (existingSpec != null) {
-            log.info(""Ignoring request to create task {}, because there is already "" +
-                ""a task with that id."", id);
-            return existingSpec;
+    public void createTask(final String id, TaskSpec spec)
+            throws Throwable {
+        try {
+            executor.submit(new CreateTask(id, spec)).get();
+        } catch (ExecutionException e) {
+            log.info(""createTask(id={}, spec={}) error"", id, spec, e);
+            throw e.getCause();
         }
-        return spec;
     }
     /**
      * Handles a request to create a new task.  Processed by the state change thread.
      */
-    class CreateTask implements Callable<TaskSpec> {
+    class CreateTask implements Callable<Void> {
         private final String id;
         private final TaskSpec spec;
@@ -310,11 +321,18 @@ public TaskSpec createTask(final String id, TaskSpec spec)
         }
         @Override
-        public TaskSpec call() throws Exception {
+        public Void call() throws Exception {
+            if (id.isEmpty()) {
+                throw new InvalidRequestException(""Invalid empty ID in createTask request."");
+            }
             ManagedTask task = tasks.get(id);
             if (task != null) {
-                log.info(""Task ID {} is already in use."", id);
-                return task.spec;
+                if (!task.spec.equals(spec)) {
+                    throw new RequestConflictException(""Task ID "" + id + "" already "" +
+                        ""exists, and has a different spec "" + task.spec);
+                }
+                log.info(""Task {} already exists with spec {}"", id, spec);
+                return null;
             }
             TaskController controller = null;
             String failure = null;
@@ -374,8 +392,10 @@ public Void call() throws Exception {
             task.state = ManagedTaskState.RUNNING;
             task.startedMs = time.milliseconds();
             for (String workerName : nodeNames) {
-                task.workerStates.put(workerName, new WorkerReceiving(task.spec));
-                nodeManagers.get(workerName).createWorker(task.id, task.spec);
+                long workerId = nextWorkerId++;
+                task.workerIds.put(workerName, workerId);
+                workerStates.put(workerId, new WorkerReceiving(task.id, task.spec));
+                nodeManagers.get(workerName).createWorker(workerId, task.id, task.spec);
             }
             return null;
         }
@@ -385,18 +405,20 @@ public Void call() throws Exception {
      * Stop a task.
      *
      * @param id                    The ID of the task to stop.
-     * @return                      The specification of the task which was stopped, or null if there
-     *                              was no task found with the given ID.
      */
-    public TaskSpec stopTask(final String id) throws ExecutionException, InterruptedException {
-        final TaskSpec spec = executor.submit(new CancelTask(id)).get();
-        return spec;
+    public void stopTask(final String id) throws Throwable {
+        try {
+            executor.submit(new CancelTask(id)).get();
+        } catch (ExecutionException e) {
+            log.info(""stopTask(id={}) error"", id, e);
+            throw e.getCause();
+        }
     }
     /**
      * Handles cancelling a task.  Processed by the state change thread.
      */
-    class CancelTask implements Callable<TaskSpec> {
+    class CancelTask implements Callable<Void> {
         private final String id;
         CancelTask(String id) {
@@ -404,7 +426,10 @@ public TaskSpec stopTask(final String id) throws ExecutionException, Interrupted
         }
         @Override
-        public TaskSpec call() throws Exception {
+        public Void call() throws Exception {
+            if (id.isEmpty()) {
+                throw new InvalidRequestException(""Invalid empty ID in stopTask request."");
+            }
             ManagedTask task = tasks.get(id);
             if (task == null) {
                 log.info(""Can't cancel non-existent task {}."", id);
@@ -420,16 +445,21 @@ public TaskSpec call() throws Exception {
                     break;
                 case RUNNING:
                     task.cancelled = true;
-                    TreeSet<String> activeWorkers = task.activeWorkers();
-                    if (activeWorkers.isEmpty()) {
-                        log.info(""Task {} is now complete with error: {}"", id, task.error);
+                    TreeMap<String, Long> activeWorkerIds = task.activeWorkerIds();
+                    if (activeWorkerIds.isEmpty()) {
+                        if (task.error.isEmpty()) {
+                            log.info(""Task {} is now complete with no errors."", id);
+                        } else {
+                            log.info(""Task {} is now complete with error: {}"", id, task.error);
+                        }
                         task.doneMs = time.milliseconds();
                         task.state = ManagedTaskState.DONE;
                     } else {
-                        for (String workerName : activeWorkers) {
-                            nodeManagers.get(workerName).stopWorker(id);
+                        for (Map.Entry<String, Long> entry : activeWorkerIds.entrySet()) {
+                            nodeManagers.get(entry.getKey()).stopWorker(entry.getValue());
                         }
-                        log.info(""Cancelling task {} on worker(s): {}"", id, Utils.join(activeWorkers, "", ""));
+                        log.info(""Cancelling task {} with worker(s) {}"",
+                            id, Utils.mkString(activeWorkerIds, """", """", "" = "", "", ""));
                         task.state = ManagedTaskState.STOPPING;
                     }
                     break;
@@ -440,7 +470,48 @@ public TaskSpec call() throws Exception {
                     log.info(""Can't cancel task {} because it is already done."", id);
                     break;
             }
-            return task.spec;
+            return null;
+        }
+    }
+
+    public void destroyTask(String id) throws Throwable {
+        try {
+            executor.submit(new DestroyTask(id)).get();
+        } catch (ExecutionException e) {
+            log.info(""destroyTask(id={}) error"", id, e);
+            throw e.getCause();
+        }
+    }
+
+    /**
+     * Handles destroying a task.  Processed by the state change thread.
+     */
+    class DestroyTask implements Callable<Void> {
+        private final String id;
+
+        DestroyTask(String id) {
+            this.id = id;
+        }
+
+        @Override
+        public Void call() throws Exception {
+            if (id.isEmpty()) {
+                throw new InvalidRequestException(""Invalid empty ID in destroyTask request."");
+            }
+            ManagedTask task = tasks.remove(id);
+            if (task == null) {
+                log.info(""Can't destroy task {}: no such task found."", id);
+                return null;
+            }
+            log.info(""Destroying task {}."", id);
+            task.clearStartFuture();
+            for (Map.Entry<String, Long> entry : task.workerIds.entrySet()) {
+                long workerId = entry.getValue();
+                workerStates.remove(workerId);
+                String nodeName = entry.getKey();
+                nodeManagers.get(nodeName).destroyWorker(workerId);
+            }
+            return null;
         }
     }
@@ -448,38 +519,48 @@ public TaskSpec call() throws Exception {
      * Update the state of a particular agent's worker.
      *
      * @param nodeName      The node where the agent is running.
-     * @param id            The worker name.
+     * @param workerId      The worker ID.
      * @param state         The worker state.
      */
-    public void updateWorkerState(String nodeName, String id, WorkerState state) {
-        executor.submit(new UpdateWorkerState(nodeName, id, state));
+    public void updateWorkerState(String nodeName, long workerId, WorkerState state) {
+        executor.submit(new UpdateWorkerState(nodeName, workerId, state));
     }
+    /**
+     * Updates the state of a worker.  Process by the state change thread.
+     */
     class UpdateWorkerState implements Callable<Void> {
         private final String nodeName;
-        private final String id;
-        private final WorkerState state;
+        private final long workerId;
+        private final WorkerState nextState;
-        UpdateWorkerState(String nodeName, String id, WorkerState state) {
+        UpdateWorkerState(String nodeName, long workerId, WorkerState nextState) {
             this.nodeName = nodeName;
-            this.id = id;
-            this.state = state;
+            this.workerId = workerId;
+            this.nextState = nextState;
         }
         @Override
         public Void call() throws Exception {
-            ManagedTask task = tasks.get(id);
-            if (task == null) {
-                log.error(""Can't update worker state unknown worker {} on node {}"",
-                    id, nodeName);
-                return null;
-            }
-            WorkerState prevState = task.workerStates.get(nodeName);
-            log.debug(""Task {}: Updating worker state for {} from {} to {}."",
-                id, nodeName, prevState, state);
-            task.workerStates.put(nodeName, state);
-            if (state.done() && (!prevState.done())) {
-                handleWorkerCompletion(task, nodeName, (WorkerDone) state);
+            try {
+                WorkerState prevState = workerStates.get(workerId);
+                if (prevState == null) {
+                    throw new RuntimeException(""Unable to find workerId "" + workerId);
+                }
+                ManagedTask task = tasks.get(prevState.taskId());
+                if (task == null) {
+                    throw new RuntimeException(""Unable to find taskId "" + prevState.taskId());
+                }
+                log.debug(""Task {}: Updating worker state for {} on {} from {} to {}."",
+                    task.id, workerId, nodeName, prevState, nextState);
+                workerStates.put(workerId, nextState);
+                if (nextState.done() && (!prevState.done())) {
+                    handleWorkerCompletion(task, nodeName, (WorkerDone) nextState);
+                }
+            } catch (Exception e) {
+                log.error(""Error updating worker state for {} on {}.  Stopping worker."",
+                    workerId, nodeName, e);
+                nodeManagers.get(nodeName).stopWorker(workerId);
             }
             return null;
         }
@@ -501,19 +582,19 @@ private void handleWorkerCompletion(ManagedTask task, String nodeName, WorkerDon
                 nodeName, task.id, state.error(), JsonUtil.toJsonString(state.status()));
             task.maybeSetError(state.error());
         }
-        if (task.activeWorkers().isEmpty()) {
+        TreeMap<String, Long> activeWorkerIds = task.activeWorkerIds();
+        if (activeWorkerIds.isEmpty()) {
             task.doneMs = time.milliseconds();
             task.state = ManagedTaskState.DONE;
             log.info(""{}: Task {} is now complete on {} with error: {}"",
-                nodeName, task.id, Utils.join(task.workerStates.keySet(), "", ""),
+                nodeName, task.id, Utils.join(task.workerIds.keySet(), "", ""),
                 task.error.isEmpty() ? ""(none)"" : task.error);
         } else if ((task.state == ManagedTaskState.RUNNING) && (!task.error.isEmpty())) {
-            TreeSet<String> activeWorkers = task.activeWorkers();
             log.info(""{}: task {} stopped with error {}.  Stopping worker(s): {}"",
-                nodeName, task.id, task.error, Utils.join(activeWorkers, "", ""));
+                nodeName, task.id, task.error, Utils.mkString(activeWorkerIds, ""{"", ""}"", "": "", "", ""));
             task.state = ManagedTaskState.STOPPING;
-            for (String workerName : activeWorkers) {
-                nodeManagers.get(workerName).stopWorker(task.id);
+            for (Map.Entry<String, Long> entry : activeWorkerIds.entrySet()) {
+                nodeManagers.get(entry.getKey()).stopWorker(entry.getValue());
             }
         }
     }
@@ -525,6 +606,9 @@ public TasksResponse tasks(TasksRequest request) throws ExecutionException, Inte
         return executor.submit(new GetTasksResponse(request)).get();
     }
+    /**
+     * Gets information about the tasks being managed.  Processed by the state change thread.
+     */
     class GetTasksResponse implements Callable<TasksResponse> {
         private final TasksRequest request;
diff --git [file java] [file java]
index c505e75e3ff..d41a54b2881 100644
--- [file java]
+++ [file java]
@@ -27,13 +27,13 @@
  */
 public class AgentStatusResponse extends Message {
     private final long serverStartMs;
-    private final TreeMap<String, WorkerState> workers;
+    private final TreeMap<Long, WorkerState> workers;
     @JsonCreator
     public AgentStatusResponse(@JsonProperty(""serverStartMs"") long serverStartMs,
-            @JsonProperty(""workers"") TreeMap<String, WorkerState> workers) {
+            @JsonProperty(""workers"") TreeMap<Long, WorkerState> workers) {
         this.serverStartMs = serverStartMs;
-        this.workers = workers == null ? new TreeMap<String, WorkerState>() : workers;
+        this.workers = workers == null ? new TreeMap<Long, WorkerState>() : workers;
     }
     @JsonProperty
@@ -42,7 +42,7 @@ public long serverStartMs() {
     }
     @JsonProperty
-    public TreeMap<String, WorkerState> workers() {
+    public TreeMap<Long, WorkerState> workers() {
         return workers;
     }
 }
diff --git [file java] [file java]
index 9f6e8dcf0d2..4acc943251e 100644
--- [file java]
+++ [file java]
@@ -25,19 +25,27 @@
  * A request to the Trogdor agent to create a worker.
  */
 public class CreateWorkerRequest extends Message {
-    private final String id;
+    private final long workerId;
+    private final String taskId;
     private final TaskSpec spec;
     @JsonCreator
-    public CreateWorkerRequest(@JsonProperty(""id"") String id,
+    public CreateWorkerRequest(@JsonProperty(""workerId"") long workerId,
+            @JsonProperty(""taskId"") String taskId,
             @JsonProperty(""spec"") TaskSpec spec) {
-        this.id = id;
+        this.workerId = workerId;
+        this.taskId = taskId;
         this.spec = spec;
     }
     @JsonProperty
-    public String id() {
-        return id;
+    public long workerId() {
+        return workerId;
+    }
+
+    @JsonProperty
+    public String taskId() {
+        return taskId;
     }
     @JsonProperty
diff --git [file java] [file java]
deleted file mode 100644
index 9e068eccd7a..00000000000
--- [file java]
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the ""License""); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- *    [link]
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an ""AS IS"" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.kafka.trogdor.rest;
-
-import com.fasterxml.jackson.annotation.JsonCreator;
-import com.fasterxml.jackson.annotation.JsonProperty;
-import org.apache.kafka.trogdor.task.TaskSpec;
-
-/**
- * A response from the Trogdor agent about creating a worker.
- */
-public class CreateWorkerResponse extends Message {
-    private final TaskSpec spec;
-
-    @JsonCreator
-    public CreateWorkerResponse(@JsonProperty(""spec"") TaskSpec spec) {
-        this.spec = spec;
-    }
-
-    @JsonProperty
-    public TaskSpec spec() {
-        return spec;
-    }
-}
diff --git [file java] [file java]
similarity index 74%
rename from [file java]
rename to [file java]
index 7d5b4687db3..d782d5d1cfb 100644
--- [file java]
+++ [file java]
@@ -19,21 +19,20 @@
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import org.apache.kafka.trogdor.task.TaskSpec;
 /**
- * A response from the Trogdor agent about stopping a worker.
+ * A request to the Trogdor coordinator to delete all memory of a task.
  */
-public class StopWorkerResponse extends Message {
-    private final TaskSpec spec;
+public class DestroyTaskRequest extends Message {
+    private final String id;
     @JsonCreator
-    public StopWorkerResponse(@JsonProperty(""spec"") TaskSpec spec) {
-        this.spec = spec;
+    public DestroyTaskRequest(@JsonProperty(""id"") String id) {
+        this.id = id;
     }
     @JsonProperty
-    public TaskSpec spec() {
-        return spec;
+    public String id() {
+        return id;
     }
 }
diff --git [file java] [file java]
similarity index 75%
rename from [file java]
rename to [file java]
index f344dc9666a..e5a8969d4cd 100644
--- [file java]
+++ [file java]
@@ -19,21 +19,20 @@
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import org.apache.kafka.trogdor.task.TaskSpec;
 /**
- * A response from the Trogdor coordinator about stopping a task.
+ * A request to the Trogdor agent to delete all memory of a task.
  */
-public class StopTaskResponse extends Message {
-    private final TaskSpec spec;
+public class DestroyWorkerRequest extends Message {
+    private final long workerId;
     @JsonCreator
-    public StopTaskResponse(@JsonProperty(""spec"") TaskSpec spec) {
-        this.spec = spec;
+    public DestroyWorkerRequest(@JsonProperty(""workerId"") long workerId) {
+        this.workerId = workerId;
     }
     @JsonProperty
-    public TaskSpec spec() {
-        return spec;
+    public long workerId() {
+        return workerId;
     }
 }
diff --git [file java] [file java]
similarity index 64%
rename from [file java]
rename to [file java]
index 54ea0f23c97..2701f6af8f9 100644
--- [file java]
+++ [file java]
@@ -17,23 +17,17 @@
 package org.apache.kafka.trogdor.rest;
-import com.fasterxml.jackson.annotation.JsonCreator;
-import com.fasterxml.jackson.annotation.JsonProperty;
-import org.apache.kafka.trogdor.task.TaskSpec;
-
 /**
- * A response from the Trogdor coordinator about creating a task.
+ * Indicates that a given request got an HTTP error 409: CONFLICT.
  */
-public class CreateTaskResponse extends Message {
-    private final TaskSpec spec;
+public class RequestConflictException extends RuntimeException {
+    private static final long serialVersionUID = 1L;
-    @JsonCreator
-    public CreateTaskResponse(@JsonProperty(""spec"") TaskSpec spec) {
-        this.spec = spec;
+    public RequestConflictException(String message) {
+        super(message);
     }
-    @JsonProperty
-    public TaskSpec spec() {
-        return spec;
+    public RequestConflictException() {
+        super();
     }
 }
diff --git [file java] [file java]
index f62a775fad9..57c54ec04d8 100644
--- [file java]
+++ [file java]
@@ -18,6 +18,7 @@
 import com.fasterxml.jackson.databind.JsonMappingException;
 import com.fasterxml.jackson.databind.exc.InvalidTypeIdException;
+import org.apache.kafka.common.errors.InvalidRequestException;
 import org.apache.kafka.common.errors.SerializationException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -38,6 +39,8 @@ public Response toResponse(Throwable e) {
         }
         if (e instanceof NotFoundException) {
             return buildResponse(Response.Status.NOT_FOUND, e);
+        } else if (e instanceof InvalidRequestException) {
+            return buildResponse(Response.Status.BAD_REQUEST, e);
         } else if (e instanceof InvalidTypeIdException) {
             return buildResponse(Response.Status.NOT_IMPLEMENTED, e);
         } else if (e instanceof JsonMappingException) {
@@ -46,6 +49,8 @@ public Response toResponse(Throwable e) {
             return buildResponse(Response.Status.NOT_IMPLEMENTED, e);
         } else if (e instanceof SerializationException) {
             return buildResponse(Response.Status.BAD_REQUEST, e);
+        } else if (e instanceof RequestConflictException) {
+            return buildResponse(Response.Status.CONFLICT, e);
         } else {
             return buildResponse(Response.Status.INTERNAL_SERVER_ERROR, e);
         }
@@ -57,7 +62,9 @@ public static Exception toException(int code, String msg) throws Exception {
         } else if (code == Response.Status.NOT_IMPLEMENTED.getStatusCode()) {
             throw new ClassNotFoundException(msg);
         } else if (code == Response.Status.BAD_REQUEST.getStatusCode()) {
-            throw new SerializationException(msg);
+            throw new InvalidRequestException(msg);
+        } else if (code == Response.Status.CONFLICT.getStatusCode()) {
+            throw new RequestConflictException(msg);
         } else {
             throw new RuntimeException(msg);
         }
diff --git [file java] [file java]
index 3287801d303..704a961f99e 100644
--- [file java]
+++ [file java]
@@ -28,7 +28,7 @@
     @JsonCreator
     public StopTaskRequest(@JsonProperty(""id"") String id) {
-        this.id = id;
+        this.id = (id == null) ? """" : id;
     }
     @JsonProperty
diff --git [file java] [file java]
index 54c689adfcd..c1dcff363c8 100644
--- [file java]
+++ [file java]
@@ -24,15 +24,15 @@
  * A request to the Trogdor agent to stop a worker.
  */
 public class StopWorkerRequest extends Message {
-    private final String id;
+    private final long workerId;
     @JsonCreator
-    public StopWorkerRequest(@JsonProperty(""id"") String id) {
-        this.id = id;
+    public StopWorkerRequest(@JsonProperty(""workerId"") long workerId) {
+        this.workerId = workerId;
     }
     @JsonProperty
-    public String id() {
-        return id;
+    public long workerId() {
+        return workerId;
     }
 }
diff --git [file java] [file java]
index 500d3c6a0c2..5f773bba5c4 100644
--- [file java]
+++ [file java]
@@ -49,12 +49,13 @@
     private final String error;
     @JsonCreator
-    public WorkerDone(@JsonProperty(""spec"") TaskSpec spec,
+    public WorkerDone(@JsonProperty(""taskId"") String taskId,
+            @JsonProperty(""spec"") TaskSpec spec,
             @JsonProperty(""startedMs"") long startedMs,
             @JsonProperty(""doneMs"") long doneMs,
             @JsonProperty(""status"") JsonNode status,
             @JsonProperty(""error"") String error) {
-        super(spec);
+        super(taskId, spec);
         this.startedMs = startedMs;
         this.doneMs = doneMs;
         this.status = status == null ? NullNode.instance : status;
diff --git [file java] [file java]
index 70687743f74..1babcce2a57 100644
--- [file java]
+++ [file java]
@@ -29,8 +29,9 @@
  */
 public final class WorkerReceiving extends WorkerState {
     @JsonCreator
-    public WorkerReceiving(@JsonProperty(""spec"") TaskSpec spec) {
-        super(spec);
+    public WorkerReceiving(@JsonProperty(""taskId"") String taskId,
+            @JsonProperty(""spec"") TaskSpec spec) {
+        super(taskId, spec);
     }
     @Override
diff --git [file java] [file java]
index af8ee88a1ab..15e77528d62 100644
--- [file java]
+++ [file java]
@@ -39,10 +39,11 @@
     private final JsonNode status;
     @JsonCreator
-    public WorkerRunning(@JsonProperty(""spec"") TaskSpec spec,
+    public WorkerRunning(@JsonProperty(""taskId"") String taskId,
+            @JsonProperty(""spec"") TaskSpec spec,
             @JsonProperty(""startedMs"") long startedMs,
             @JsonProperty(""status"") JsonNode status) {
-        super(spec);
+        super(taskId, spec);
         this.startedMs = startedMs;
         this.status = status == null ? NullNode.instance : status;
     }
diff --git [file java] [file java]
index b568ec1f887..7a06eac5b7d 100644
--- [file java]
+++ [file java]
@@ -28,8 +28,9 @@
  */
 public final class WorkerStarting extends WorkerState {
     @JsonCreator
-    public WorkerStarting(@JsonProperty(""spec"") TaskSpec spec) {
-        super(spec);
+    public WorkerStarting(@JsonProperty(""taskId"") String taskId,
+            @JsonProperty(""spec"") TaskSpec spec) {
+        super(taskId, spec);
     }
     @Override
diff --git [file java] [file java]
index 044d719f894..6480a2410dc 100644
--- [file java]
+++ [file java]
@@ -38,12 +38,19 @@
     @JsonSubTypes.Type(value = WorkerDone.class, name = ""DONE"")
     })
 public abstract class WorkerState extends Message {
+    private final String taskId;
     private final TaskSpec spec;
-    public WorkerState(TaskSpec spec) {
+    public WorkerState(String taskId, TaskSpec spec) {
+        this.taskId = taskId;
         this.spec = spec;
     }
+    @JsonProperty
+    public String taskId() {
+        return taskId;
+    }
+
     @JsonProperty
     public TaskSpec spec() {
         return spec;
diff --git [file java] [file java]
index 9fbb3ff7306..2942e118ac6 100644
--- [file java]
+++ [file java]
@@ -39,10 +39,11 @@
     private final JsonNode status;
     @JsonCreator
-    public WorkerStopping(@JsonProperty(""spec"") TaskSpec spec,
+    public WorkerStopping(@JsonProperty(""taskId"") String taskId,
+            @JsonProperty(""spec"") TaskSpec spec,
             @JsonProperty(""startedMs"") long startedMs,
             @JsonProperty(""status"") JsonNode status) {
-        super(spec);
+        super(taskId, spec);
         this.startedMs = startedMs;
         this.status = status == null ? NullNode.instance : status;
     }
diff --git [file java] [file java]
index 61de5c98797..158e690da4b 100644
--- [file java]
+++ [file java]
@@ -36,8 +36,9 @@
 import org.apache.kafka.trogdor.rest.AgentStatusResponse;
 import org.apache.kafka.trogdor.rest.CreateWorkerRequest;
-import org.apache.kafka.trogdor.rest.CreateWorkerResponse;
+import org.apache.kafka.trogdor.rest.DestroyWorkerRequest;
 import org.apache.kafka.trogdor.rest.JsonRestServer;
+import org.apache.kafka.trogdor.rest.RequestConflictException;
 import org.apache.kafka.trogdor.rest.StopWorkerRequest;
 import org.apache.kafka.trogdor.rest.WorkerDone;
 import org.apache.kafka.trogdor.rest.WorkerRunning;
@@ -120,36 +121,47 @@ public void testAgentCreateWorkers() throws Exception {
         new ExpectedTasks().waitFor(client);
         final NoOpTaskSpec fooSpec = new NoOpTaskSpec(1000, 600000);
-        CreateWorkerResponse response = client.createWorker(new CreateWorkerRequest(""foo"", fooSpec));
-        assertEquals(fooSpec.toString(), response.spec().toString());
+        client.createWorker(new CreateWorkerRequest(0, ""foo"", fooSpec));
         new ExpectedTasks().addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerRunning(fooSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""active""))).
                 build()).
             waitFor(client);
         final NoOpTaskSpec barSpec = new NoOpTaskSpec(2000, 900000);
-        client.createWorker(new CreateWorkerRequest(""bar"", barSpec));
-        client.createWorker(new CreateWorkerRequest(""bar"", barSpec));
+        client.createWorker(new CreateWorkerRequest(1, ""bar"", barSpec));
+        client.createWorker(new CreateWorkerRequest(1, ""bar"", barSpec));
+
+        try {
+            client.createWorker(new CreateWorkerRequest(1, ""foo"", barSpec));
+            Assert.fail(""Expected RequestConflictException when re-creating a request with a different taskId."");
+        } catch (RequestConflictException exception) {
+        }
+        try {
+            client.createWorker(new CreateWorkerRequest(1, ""bar"", fooSpec));
+            Assert.fail(""Expected RequestConflictException when re-creating a request with a different spec."");
+        } catch (RequestConflictException exception) {
+        }
+
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerRunning(fooSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""active""))).
                 build()).
             addTask(new ExpectedTaskBuilder(""bar"").
-                workerState(new WorkerRunning(barSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""bar"", barSpec, 0, new TextNode(""active""))).
                 build()).
             waitFor(client);
         final NoOpTaskSpec bazSpec = new NoOpTaskSpec(1, 450000);
-        client.createWorker(new CreateWorkerRequest(""baz"", bazSpec));
+        client.createWorker(new CreateWorkerRequest(2, ""baz"", bazSpec));
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerRunning(fooSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""active""))).
                 build()).
             addTask(new ExpectedTaskBuilder(""bar"").
-                workerState(new WorkerRunning(barSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""bar"", barSpec, 0, new TextNode(""active""))).
                 build()).
             addTask(new ExpectedTaskBuilder(""baz"").
-                workerState(new WorkerRunning(bazSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""baz"", bazSpec, 0, new TextNode(""active""))).
                 build()).
             waitFor(client);
@@ -167,23 +179,23 @@ public void testAgentFinishesTasks() throws Exception {
         new ExpectedTasks().waitFor(client);
         final NoOpTaskSpec fooSpec = new NoOpTaskSpec(10, 2);
-        client.createWorker(new CreateWorkerRequest(""foo"", fooSpec));
+        client.createWorker(new CreateWorkerRequest(0, ""foo"", fooSpec));
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerRunning(fooSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""active""))).
                 build()).
             waitFor(client);
         time.sleep(1);
         final NoOpTaskSpec barSpec = new NoOpTaskSpec(2000, 900000);
-        client.createWorker(new CreateWorkerRequest(""bar"", barSpec));
+        client.createWorker(new CreateWorkerRequest(1, ""bar"", barSpec));
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerRunning(fooSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""active""))).
                 build()).
             addTask(new ExpectedTaskBuilder(""bar"").
-                workerState(new WorkerRunning(barSpec, 1, new TextNode(""active""))).
+                workerState(new WorkerRunning(""bar"", barSpec, 1, new TextNode(""active""))).
                 build()).
             waitFor(client);
@@ -191,21 +203,21 @@ public void testAgentFinishesTasks() throws Exception {
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerDone(fooSpec, 0, 2, new TextNode(""done""), """")).
+                workerState(new WorkerDone(""foo"", fooSpec, 0, 2, new TextNode(""done""), """")).
                 build()).
             addTask(new ExpectedTaskBuilder(""bar"").
-                workerState(new WorkerRunning(barSpec, 1, new TextNode(""active""))).
+                workerState(new WorkerRunning(""bar"", barSpec, 1, new TextNode(""active""))).
                 build()).
             waitFor(client);
         time.sleep(5);
-        client.stopWorker(new StopWorkerRequest(""bar""));
+        client.stopWorker(new StopWorkerRequest(1));
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerDone(fooSpec, 0, 2, new TextNode(""done""), """")).
+                workerState(new WorkerDone(""foo"", fooSpec, 0, 2, new TextNode(""done""), """")).
                 build()).
             addTask(new ExpectedTaskBuilder(""bar"").
-                workerState(new WorkerDone(barSpec, 1, 7, new TextNode(""done""), """")).
+                workerState(new WorkerDone(""bar"", barSpec, 1, 7, new TextNode(""done""), """")).
                 build()).
             waitFor(client);
@@ -224,25 +236,25 @@ public void testWorkerCompletions() throws Exception {
         SampleTaskSpec fooSpec = new SampleTaskSpec(0, 900000,
             Collections.singletonMap(""node01"", 1L), """");
-        client.createWorker(new CreateWorkerRequest(""foo"", fooSpec));
+        client.createWorker(new CreateWorkerRequest(0, ""foo"", fooSpec));
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerRunning(fooSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""active""))).
                 build()).
             waitFor(client);
         SampleTaskSpec barSpec = new SampleTaskSpec(0, 900000,
             Collections.singletonMap(""node01"", 2L), ""baz"");
-        client.createWorker(new CreateWorkerRequest(""bar"", barSpec));
+        client.createWorker(new CreateWorkerRequest(1, ""bar"", barSpec));
         time.sleep(1);
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerDone(fooSpec, 0, 1,
+                workerState(new WorkerDone(""foo"", fooSpec, 0, 1,
                     new TextNode(""halted""), """")).
                 build()).
             addTask(new ExpectedTaskBuilder(""bar"").
-                workerState(new WorkerRunning(barSpec, 0,
+                workerState(new WorkerRunning(""bar"", barSpec, 0,
                     new TextNode(""active""))).
                 build()).
             waitFor(client);
@@ -250,11 +262,11 @@ public void testWorkerCompletions() throws Exception {
         time.sleep(1);
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerDone(fooSpec, 0, 1,
+                workerState(new WorkerDone(""foo"", fooSpec, 0, 1,
                     new TextNode(""halted""), """")).
                 build()).
             addTask(new ExpectedTaskBuilder(""bar"").
-                workerState(new WorkerDone(barSpec, 0, 2,
+                workerState(new WorkerDone(""bar"", barSpec, 0, 2,
                     new TextNode(""halted""), ""baz"")).
                 build()).
             waitFor(client);
@@ -293,37 +305,84 @@ public void testKiboshFaults() throws Exception {
             Assert.assertEquals(KiboshControlFile.EMPTY, mockKibosh.read());
             FilesUnreadableFaultSpec fooSpec = new FilesUnreadableFaultSpec(0, 900000,
                 Collections.singleton(""myAgent""), mockKibosh.tempDir.getPath().toString(), ""/foo"", 123);
-            client.createWorker(new CreateWorkerRequest(""foo"", fooSpec));
+            client.createWorker(new CreateWorkerRequest(0, ""foo"", fooSpec));
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
-                    workerState(new WorkerRunning(fooSpec, 0, new TextNode(""Added fault foo""))).
+                    workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""Added fault foo""))).
                     build()).
                 waitFor(client);
             Assert.assertEquals(new KiboshControlFile(Collections.<Kibosh.KiboshFaultSpec>singletonList(
                 new KiboshFilesUnreadableFaultSpec(""/foo"", 123))), mockKibosh.read());
             FilesUnreadableFaultSpec barSpec = new FilesUnreadableFaultSpec(0, 900000,
                 Collections.singleton(""myAgent""), mockKibosh.tempDir.getPath().toString(), ""/bar"", 456);
-            client.createWorker(new CreateWorkerRequest(""bar"", barSpec));
+            client.createWorker(new CreateWorkerRequest(1, ""bar"", barSpec));
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
-                    workerState(new WorkerRunning(fooSpec, 0, new TextNode(""Added fault foo""))).build()).
+                    workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""Added fault foo""))).build()).
                 addTask(new ExpectedTaskBuilder(""bar"").
-                    workerState(new WorkerRunning(barSpec, 0, new TextNode(""Added fault bar""))).build()).
+                    workerState(new WorkerRunning(""bar"", barSpec, 0, new TextNode(""Added fault bar""))).build()).
                 waitFor(client);
             Assert.assertEquals(new KiboshControlFile(new ArrayList<Kibosh.KiboshFaultSpec>() {{
                     add(new KiboshFilesUnreadableFaultSpec(""/foo"", 123));
                     add(new KiboshFilesUnreadableFaultSpec(""/bar"", 456));
                 }}), mockKibosh.read());
             time.sleep(1);
-            client.stopWorker(new StopWorkerRequest(""foo""));
+            client.stopWorker(new StopWorkerRequest(0));
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
-                    workerState(new WorkerDone(fooSpec, 0, 1, new TextNode(""Removed fault foo""), """")).build()).
+                    workerState(new WorkerDone(""foo"", fooSpec, 0, 1, new TextNode(""Removed fault foo""), """")).build()).
                 addTask(new ExpectedTaskBuilder(""bar"").
-                    workerState(new WorkerRunning(barSpec, 0, new TextNode(""Added fault bar""))).build()).
+                    workerState(new WorkerRunning(""bar"", barSpec, 0, new TextNode(""Added fault bar""))).build()).
                 waitFor(client);
             Assert.assertEquals(new KiboshControlFile(Collections.<Kibosh.KiboshFaultSpec>singletonList(
                 new KiboshFilesUnreadableFaultSpec(""/bar"", 456))), mockKibosh.read());
         }
     }
+
+    @Test
+    public void testDestroyWorkers() throws Exception {
+        MockTime time = new MockTime(0, 0, 0);
+        MockScheduler scheduler = new MockScheduler(time);
+        Agent agent = createAgent(scheduler);
+        AgentClient client = new AgentClient.Builder().
+            maxTries(10).target(""localhost"", agent.port()).build();
+        new ExpectedTasks().waitFor(client);
+
+        final NoOpTaskSpec fooSpec = new NoOpTaskSpec(10, 5);
+        client.createWorker(new CreateWorkerRequest(0, ""foo"", fooSpec));
+        new ExpectedTasks().
+            addTask(new ExpectedTaskBuilder(""foo"").
+                workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""active""))).
+                build()).
+            waitFor(client);
+        time.sleep(1);
+
+        client.destroyWorker(new DestroyWorkerRequest(0));
+        client.destroyWorker(new DestroyWorkerRequest(0));
+        client.destroyWorker(new DestroyWorkerRequest(1));
+        new ExpectedTasks().waitFor(client);
+        time.sleep(1);
+
+        final NoOpTaskSpec fooSpec2 = new NoOpTaskSpec(100, 1);
+        client.createWorker(new CreateWorkerRequest(1, ""foo"", fooSpec2));
+        new ExpectedTasks().
+            addTask(new ExpectedTaskBuilder(""foo"").
+                workerState(new WorkerRunning(""foo"", fooSpec2, 2, new TextNode(""active""))).
+                build()).
+            waitFor(client);
+
+        time.sleep(2);
+        new ExpectedTasks().
+            addTask(new ExpectedTaskBuilder(""foo"").
+                workerState(new WorkerDone(""foo"", fooSpec2, 2, 4, new TextNode(""done""), """")).
+                build()).
+            waitFor(client);
+
+        time.sleep(1);
+        client.destroyWorker(new DestroyWorkerRequest(1));
+        new ExpectedTasks().waitFor(client);
+
+        agent.beginShutdown();
+        agent.waitForShutdown();
+    }
 };
diff --git [file java] [file java]
index 617bf34bcd9..121281f5910 100644
--- [file java]
+++ [file java]
@@ -32,6 +32,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import java.util.HashMap;
 import java.util.Map;
 import java.util.TreeMap;
@@ -184,10 +185,14 @@ public boolean conditionMet() {
                     throw new RuntimeException(e);
                 }
                 StringBuilder errors = new StringBuilder();
+                HashMap<String, WorkerState> taskIdToWorkerState = new HashMap<>();
+                for (WorkerState state : status.workers().values()) {
+                    taskIdToWorkerState.put(state.taskId(), state);
+                }
                 for (Map.Entry<String, ExpectedTask> entry : expected.entrySet()) {
                     String id = entry.getKey();
                     ExpectedTask worker = entry.getValue();
-                    String differences = worker.compare(status.workers().get(id));
+                    String differences = worker.compare(taskIdToWorkerState.get(id));
                     if (differences != null) {
                         errors.append(differences);
                     }
diff --git [file java] [file java]
index 8101d9c6e4e..c1f7490cc82 100644
--- [file java]
+++ [file java]
@@ -45,9 +45,9 @@ public void testDeserializationDoesNotProduceNulls() throws Exception {
         verify(new ProcessStopFaultSpec(0, 0, null, null));
         verify(new AgentStatusResponse(0, null));
         verify(new TasksResponse(null));
-        verify(new WorkerDone(null, 0, 0, null, null));
-        verify(new WorkerRunning(null, 0, null));
-        verify(new WorkerStopping(null, 0, null));
+        verify(new WorkerDone(null, null, 0, 0, null, null));
+        verify(new WorkerRunning(null, null, 0, null));
+        verify(new WorkerStopping(null, null, 0, null));
         verify(new ProduceBenchSpec(0, 0, null, null,
             0, 0, null, null, null, null, null, 0, 0, ""test-topic"", 1, (short) 3));
         verify(new RoundTripWorkloadSpec(0, 0, null, null, null, null, null, null,
diff --git [file java] [file java]
index 07f02c5830b..46315c27d15 100644
--- [file java]
+++ [file java]
@@ -185,7 +185,7 @@ public Void call() throws Exception {
                             }
                             if (node.coordinatorRestResource != null) {
                                 node.coordinator = new Coordinator(node.platform, scheduler,
-                                    node.coordinatorRestServer, node.coordinatorRestResource);
+                                    node.coordinatorRestServer, node.coordinatorRestResource, 0);
                             }
                         } catch (Exception e) {
                             log.error(""Unable to initialize {}"", nodeName, e);
diff --git [file java] [file java]
index 34d7ffe6106..e9434844405 100644
--- [file java]
+++ [file java]
@@ -35,6 +35,8 @@
 import org.apache.kafka.trogdor.fault.NetworkPartitionFaultSpec;
 import org.apache.kafka.trogdor.rest.CoordinatorStatusResponse;
 import org.apache.kafka.trogdor.rest.CreateTaskRequest;
+import org.apache.kafka.trogdor.rest.DestroyTaskRequest;
+import org.apache.kafka.trogdor.rest.RequestConflictException;
 import org.apache.kafka.trogdor.rest.StopTaskRequest;
 import org.apache.kafka.trogdor.rest.TaskDone;
 import org.apache.kafka.trogdor.rest.TaskPending;
@@ -57,8 +59,9 @@
 import java.util.List;
 import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.fail;
 public class CoordinatorTest {
     private static final Logger log = LoggerFactory.getLogger(CoordinatorTest.class);
@@ -96,11 +99,25 @@ public void testCreateTask() throws Exception {
                     build()).
                 waitFor(cluster.coordinatorClient());
+            // Re-creating a task with the same arguments is not an error.
+            cluster.coordinatorClient().createTask(
+                new CreateTaskRequest(""foo"", fooSpec));
+
+            // Re-creating a task with different arguments gives a RequestConflictException.
+            try {
+                NoOpTaskSpec barSpec = new NoOpTaskSpec(1000, 2000);
+                cluster.coordinatorClient().createTask(
+                    new CreateTaskRequest(""foo"", barSpec));
+                fail(""Expected to get an exception when re-creating a task with a "" +
+                    ""different task spec."");
+            } catch (RequestConflictException exception) {
+            }
+
             time.sleep(2);
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskRunning(fooSpec, 2, new TextNode(""active""))).
-                    workerState(new WorkerRunning(fooSpec, 2, new TextNode(""active""))).
+                    workerState(new WorkerRunning(""foo"", fooSpec, 2, new TextNode(""active""))).
                     build()).
                 waitFor(cluster.coordinatorClient()).
                 waitFor(cluster.agentClient(""node02""));
@@ -149,7 +166,7 @@ public void testTaskDistribution() throws Exception {
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskRunning(fooSpec, 11, status1)).
-                    workerState(new WorkerRunning(fooSpec, 11,  new TextNode(""active""))).
+                    workerState(new WorkerRunning(""foo"", fooSpec, 11,  new TextNode(""active""))).
                     build()).
                 waitFor(coordinatorClient).
                 waitFor(agentClient1).
@@ -163,7 +180,7 @@ public void testTaskDistribution() throws Exception {
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskDone(fooSpec, 11, 13,
                         """", false, status2)).
-                    workerState(new WorkerDone(fooSpec, 11, 13, new TextNode(""done""), """")).
+                    workerState(new WorkerDone(""foo"", fooSpec, 11, 13, new TextNode(""done""), """")).
                     build()).
                 waitFor(coordinatorClient).
                 waitFor(agentClient1).
@@ -206,7 +223,7 @@ public void testTaskCancellation() throws Exception {
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskRunning(fooSpec, 11, status1)).
-                    workerState(new WorkerRunning(fooSpec, 11, new TextNode(""active""))).
+                    workerState(new WorkerRunning(""foo"", fooSpec, 11, new TextNode(""active""))).
                     build()).
                 waitFor(coordinatorClient).
                 waitFor(agentClient1).
@@ -221,11 +238,68 @@ public void testTaskCancellation() throws Exception {
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskDone(fooSpec, 11, 12, """",
                         true, status2)).
-                    workerState(new WorkerDone(fooSpec, 11, 12, new TextNode(""done""), """")).
+                    workerState(new WorkerDone(""foo"", fooSpec, 11, 12, new TextNode(""done""), """")).
+                    build()).
+                waitFor(coordinatorClient).
+                waitFor(agentClient1).
+                waitFor(agentClient2);
+
+            coordinatorClient.destroyTask(new DestroyTaskRequest(""foo""));
+            new ExpectedTasks().
+                waitFor(coordinatorClient).
+                waitFor(agentClient1).
+                waitFor(agentClient2);
+        }
+    }
+
+    @Test
+    public void testTaskDestruction() throws Exception {
+        MockTime time = new MockTime(0, 0, 0);
+        Scheduler scheduler = new MockScheduler(time);
+        try (MiniTrogdorCluster cluster = new MiniTrogdorCluster.Builder().
+            addCoordinator(""node01"").
+            addAgent(""node01"").
+            addAgent(""node02"").
+            scheduler(scheduler).
+            build()) {
+            CoordinatorClient coordinatorClient = cluster.coordinatorClient();
+            AgentClient agentClient1 = cluster.agentClient(""node01"");
+            AgentClient agentClient2 = cluster.agentClient(""node02"");
+
+            new ExpectedTasks().
+                waitFor(coordinatorClient).
+                waitFor(agentClient1).
+                waitFor(agentClient2);
+
+            NoOpTaskSpec fooSpec = new NoOpTaskSpec(2, 2);
+            coordinatorClient.destroyTask(new DestroyTaskRequest(""foo""));
+            coordinatorClient.createTask(new CreateTaskRequest(""foo"", fooSpec));
+            NoOpTaskSpec barSpec = new NoOpTaskSpec(20, 20);
+            coordinatorClient.createTask(new CreateTaskRequest(""bar"", barSpec));
+            coordinatorClient.destroyTask(new DestroyTaskRequest(""bar""));
+            new ExpectedTasks().
+                addTask(new ExpectedTaskBuilder(""foo"").taskState(new TaskPending(fooSpec)).build()).
+                waitFor(coordinatorClient).
+                waitFor(agentClient1).
+                waitFor(agentClient2);
+            time.sleep(10);
+
+            ObjectNode status1 = new ObjectNode(JsonNodeFactory.instance);
+            status1.set(""node01"", new TextNode(""active""));
+            status1.set(""node02"", new TextNode(""active""));
+            new ExpectedTasks().
+                addTask(new ExpectedTaskBuilder(""foo"").
+                    taskState(new TaskRunning(fooSpec, 10, status1)).
                     build()).
                 waitFor(coordinatorClient).
                 waitFor(agentClient1).
                 waitFor(agentClient2);
+
+            coordinatorClient.destroyTask(new DestroyTaskRequest(""foo""));
+            new ExpectedTasks().
+                waitFor(coordinatorClient).
+                waitFor(agentClient1).
+                waitFor(agentClient2);
         }
     }
@@ -397,7 +471,7 @@ public void testTasksRequest() throws Exception {
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskRunning(fooSpec, 2, new TextNode(""active""))).
-                    workerState(new WorkerRunning(fooSpec, 2, new TextNode(""active""))).
+                    workerState(new WorkerRunning(""foo"", fooSpec, 2, new TextNode(""active""))).
                     build()).
                 addTask(new ExpectedTaskBuilder(""bar"").
                     taskState(new TaskPending(barSpec)).
@@ -448,7 +522,7 @@ public void testWorkersExitingAtDifferentTimes() throws Exception {
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskRunning(fooSpec, 2, status1)).
-                    workerState(new WorkerRunning(fooSpec, 2, new TextNode(""active""))).
+                    workerState(new WorkerRunning(""foo"", fooSpec, 2, new TextNode(""active""))).
                     build()).
                 waitFor(coordinatorClient).
                 waitFor(cluster.agentClient(""node02"")).
@@ -461,14 +535,14 @@ public void testWorkersExitingAtDifferentTimes() throws Exception {
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskRunning(fooSpec, 2, status2)).
-                    workerState(new WorkerRunning(fooSpec, 2, new TextNode(""active""))).
+                    workerState(new WorkerRunning(""foo"", fooSpec, 2, new TextNode(""active""))).
                     build()).
                 waitFor(coordinatorClient).
                 waitFor(cluster.agentClient(""node03""));
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskRunning(fooSpec, 2, status2)).
-                    workerState(new WorkerDone(fooSpec, 2, 12, new TextNode(""halted""), """")).
+                    workerState(new WorkerDone(""foo"", fooSpec, 2, 12, new TextNode(""halted""), """")).
                     build()).
                 waitFor(cluster.agentClient(""node02""));
diff --git [file java] [file java]
index c40f958eb8d..9c7f7522853 100644
--- [file java]
+++ [file java]
@@ -24,6 +24,8 @@
 import com.fasterxml.jackson.databind.exc.InvalidTypeIdException;
 import javax.ws.rs.NotFoundException;
 import javax.ws.rs.core.Response;
+
+import org.apache.kafka.common.errors.InvalidRequestException;
 import org.apache.kafka.common.errors.SerializationException;
 import org.junit.Test;
@@ -67,6 +69,13 @@ public void testToResponseSerializationException() {
         assertEquals(resp.getStatus(), Response.Status.BAD_REQUEST.getStatusCode());
     }
+    @Test
+    public void testToResponseInvalidRequestException() {
+        RestExceptionMapper mapper = new RestExceptionMapper();
+        Response resp = mapper.toResponse(new InvalidRequestException(""invalid request""));
+        assertEquals(resp.getStatus(), Response.Status.BAD_REQUEST.getStatusCode());
+    }
+
     @Test
     public void testToResponseUnknownException() {
         RestExceptionMapper mapper = new RestExceptionMapper();
@@ -84,7 +93,7 @@ public void testToExceptionClassNotFoundException() throws Exception {
         RestExceptionMapper.toException(Response.Status.NOT_IMPLEMENTED.getStatusCode(), ""Not Implemented"");
     }
-    @Test(expected = SerializationException.class)
+    @Test(expected = InvalidRequestException.class)
     public void testToExceptionSerializationException() throws Exception {
         RestExceptionMapper.toException(Response.Status.BAD_REQUEST.getStatusCode(), ""Bad Request"");
     }
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-6696,https://issues.apache.org/jira/browse/KAFKA-6696,https://github.com/apache/kafka/blob/2.0.0/tools/src/main/java/org/apache/kafka/trogdor/coordinator/TaskManager.java,Trogdor should support destroying tasks,NO,Trogdor should support destroying tasks.  This will make it more practical to have very long running Trogdor instances.,"** Comment 1 **
cmccabe opened a new pull request #4759: KAFKA-6696 Trogdor should support destroying tasks
URL: [link]
   KAFKA-6696 Trogdor should support destroying tasks   Implement destroying tasks and workers.  This means erasing all record of them on the Coordinator and the Agent.   Workers should be identified by unique 64-bit worker IDs, rather than by the names of the tasks they are implementing.  This ensures that when a task is destroyed and re-created with the same task ID, the old workers will be not be treated as part of the new task instance.       Fix some return results from RPCs.  In some cases RPCs were returning values that were never used.  Attempting to re-create the same task ID with different arguments should fail.  Add RequestConflictException to represent HTTP error code 409 (CONFLICT) for this scenario.   If only one worker in a task stops, don't stop all the other workers for that task, unless the worker that stopped had an error.
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
rajinisivaram closed pull request #4759: KAFKA-6696 Trogdor should support destroying tasks
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git a/checkstyle/suppressions.xml b/checkstyle/suppressions.xml
index 2767132886d..64258bf7b07 100644
--- a/checkstyle/suppressions.xml
+++ b/checkstyle/suppressions.xml
@@ -45,7 +45,7 @@
     <suppress checks=""ClassDataAbstractionCoupling""
               files=""(KafkaConsumer|ConsumerCoordinator|Fetcher|KafkaProducer|AbstractRequest|AbstractResponse|TransactionManager|KafkaAdminClient).java""/>
     <suppress checks=""ClassDataAbstractionCoupling""
-              files=""(Errors|SaslAuthenticatorTest|AgentTest).java""/>
+              files=""(Errors|SaslAuthenticatorTest|AgentTest|CoordinatorTest).java""/>
     <suppress checks=""BooleanExpressionComplexity""
               files=""(Utils|Topic|KafkaLZ4BlockOutputStream|AclData).java""/>
diff --git [file java] [file java]
index 3b5b21e68d8..0324d2d2dba 100644
--- [file java]
+++ [file java]
@@ -27,10 +27,9 @@
 import org.apache.kafka.trogdor.common.Platform;
 import org.apache.kafka.trogdor.rest.AgentStatusResponse;
 import org.apache.kafka.trogdor.rest.CreateWorkerRequest;
-import org.apache.kafka.trogdor.rest.CreateWorkerResponse;
+import org.apache.kafka.trogdor.rest.DestroyWorkerRequest;
 import org.apache.kafka.trogdor.rest.JsonRestServer;
 import org.apache.kafka.trogdor.rest.StopWorkerRequest;
-import org.apache.kafka.trogdor.rest.StopWorkerResponse;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -95,13 +94,16 @@ public AgentStatusResponse status() throws Exception {
         return new AgentStatusResponse(serverStartMs, workerManager.workerStates());
     }
-    public CreateWorkerResponse createWorker(CreateWorkerRequest req) throws Exception {
-        workerManager.createWorker(req.id(), req.spec());
-        return new CreateWorkerResponse(req.spec());
+    public void createWorker(CreateWorkerRequest req) throws Throwable {
+        workerManager.createWorker(req.workerId(), req.taskId(), req.spec());
     }
-    public StopWorkerResponse stopWorker(StopWorkerRequest req) throws Exception {
-        return new StopWorkerResponse(workerManager.stopWorker(req.id()));
+    public void stopWorker(StopWorkerRequest req) throws Throwable {
+        workerManager.stopWorker(req.workerId(), false);
+    }
+
+    public void destroyWorker(DestroyWorkerRequest req) throws Throwable {
+        workerManager.stopWorker(req.workerId(), true);
     }
     public static void main(String args) throws Exception {
diff --git [file java] [file java]
index 08769a0971d..c89011b8650 100644
--- [file java]
+++ [file java]
@@ -27,15 +27,16 @@
 import org.apache.kafka.trogdor.common.JsonUtil;
 import org.apache.kafka.trogdor.rest.AgentStatusResponse;
 import org.apache.kafka.trogdor.rest.CreateWorkerRequest;
-import org.apache.kafka.trogdor.rest.CreateWorkerResponse;
+import org.apache.kafka.trogdor.rest.DestroyWorkerRequest;
 import org.apache.kafka.trogdor.rest.Empty;
 import org.apache.kafka.trogdor.rest.JsonRestServer;
 import org.apache.kafka.trogdor.rest.JsonRestServer.HttpResponse;
 import org.apache.kafka.trogdor.rest.StopWorkerRequest;
-import org.apache.kafka.trogdor.rest.StopWorkerResponse;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import javax.ws.rs.core.UriBuilder;
+
 import static net.sourceforge.argparse4j.impl.Arguments.store;
 import static net.sourceforge.argparse4j.impl.Arguments.storeTrue;
@@ -116,20 +117,29 @@ public AgentStatusResponse status() throws Exception {
         return resp.body();
     }
-    public CreateWorkerResponse createWorker(CreateWorkerRequest request) throws Exception {
-        HttpResponse<CreateWorkerResponse> resp =
-            JsonRestServer.<CreateWorkerResponse>httpRequest(
+    public void createWorker(CreateWorkerRequest request) throws Exception {
+        HttpResponse<Empty> resp =
+            JsonRestServer.<Empty>httpRequest(
                 url(""/agent/worker/create""), ""POST"",
-                request, new TypeReference<CreateWorkerResponse>() { }, maxTries);
-        return resp.body();
+                request, new TypeReference<Empty>() { }, maxTries);
+        resp.body();
     }
-    public StopWorkerResponse stopWorker(StopWorkerRequest request) throws Exception {
-        HttpResponse<StopWorkerResponse> resp =
-            JsonRestServer.<StopWorkerResponse>httpRequest(url(
+    public void stopWorker(StopWorkerRequest request) throws Exception {
+        HttpResponse<Empty> resp =
+            JsonRestServer.<Empty>httpRequest(url(
                 ""/agent/worker/stop""), ""PUT"",
-                request, new TypeReference<StopWorkerResponse>() { }, maxTries);
-        return resp.body();
+                request, new TypeReference<Empty>() { }, maxTries);
+        resp.body();
+    }
+
+    public void destroyWorker(DestroyWorkerRequest request) throws Exception {
+        UriBuilder uriBuilder = UriBuilder.fromPath(url(""/agent/worker""));
+        uriBuilder.queryParam(""workerId"", request.workerId());
+        HttpResponse<Empty> resp =
+            JsonRestServer.<Empty>httpRequest(uriBuilder.build().toString(), ""DELETE"",
+                null, new TypeReference<Empty>() { }, maxTries);
+        resp.body();
     }
     public void invokeShutdown() throws Exception {
@@ -166,10 +176,16 @@ public static void main(String args) throws Exception {
             .help(""Create a new fault."");
         actions.addArgument(""--stop-worker"")
             .action(store())
-            .type(String.class)
+            .type(Long.class)
             .dest(""stop_worker"")
-            .metavar(""SPEC_JSON"")
-            .help(""Create a new fault."");
+            .metavar(""WORKER_ID"")
+            .help(""Stop a worker ID."");
+        actions.addArgument(""--destroy-worker"")
+            .action(store())
+            .type(Long.class)
+            .dest(""destroy_worker"")
+            .metavar(""WORKER_ID"")
+            .help(""Destroy a worker ID."");
         actions.addArgument(""--shutdown"")
             .action(storeTrue())
             .type(Boolean.class)
@@ -197,13 +213,21 @@ public static void main(String args) throws Exception {
             System.out.println(""Got agent status: "" +
                 JsonUtil.toPrettyJsonString(client.status()));
         } else if (res.getString(""create_worker"") != null) {
-            client.createWorker(JsonUtil.JSON_SERDE.
-                readValue(res.getString(""create_worker""),
-                    CreateWorkerRequest.class));
-            System.out.println(""Created fault."");
+            CreateWorkerRequest req = JsonUtil.JSON_SERDE.
+                readValue(res.getString(""create_worker""), CreateWorkerRequest.class);
+            client.createWorker(req);
+            System.out.printf(""Sent CreateWorkerRequest for worker %d%n."", req.workerId());
+        } else if (res.getString(""stop_worker"") != null) {
+            long workerId = res.getLong(""stop_worker"");
+            client.stopWorker(new StopWorkerRequest(workerId));
+            System.out.printf(""Sent StopWorkerRequest for worker %d%n."", workerId);
+        } else if (res.getString(""destroy_worker"") != null) {
+            long workerId = res.getLong(""stop_worker"");
+            client.destroyWorker(new DestroyWorkerRequest(workerId));
+            System.out.printf(""Sent DestroyWorkerRequest for worker %d%n."", workerId);
         } else if (res.getBoolean(""shutdown"")) {
             client.invokeShutdown();
-            System.out.println(""Sent shutdown request."");
+            System.out.println(""Sent ShutdownRequest."");
         } else {
             System.out.println(""You must choose an action. Type --help for help."");
             Exit.exit(1);
diff --git [file java] [file java]
index 773c580fa15..1f2ad49d2fe 100644
--- [file java]
+++ [file java]
@@ -18,22 +18,34 @@
 import org.apache.kafka.trogdor.rest.AgentStatusResponse;
 import org.apache.kafka.trogdor.rest.CreateWorkerRequest;
-import org.apache.kafka.trogdor.rest.CreateWorkerResponse;
+import org.apache.kafka.trogdor.rest.DestroyWorkerRequest;
 import org.apache.kafka.trogdor.rest.Empty;
 import org.apache.kafka.trogdor.rest.StopWorkerRequest;
-import org.apache.kafka.trogdor.rest.StopWorkerResponse;
 import javax.servlet.ServletContext;
 import javax.ws.rs.Consumes;
+import javax.ws.rs.DELETE;
+import javax.ws.rs.DefaultValue;
 import javax.ws.rs.GET;
 import javax.ws.rs.POST;
 import javax.ws.rs.PUT;
 import javax.ws.rs.Path;
 import javax.ws.rs.Produces;
+import javax.ws.rs.QueryParam;
 import javax.ws.rs.core.MediaType;
 import java.util.concurrent.atomic.AtomicReference;
-
+/**
+ * The REST resource for the Agent. This describes the RPCs which the agent can accept.
+ *
+ * RPCs should be idempotent.  This is important because if the server's response is
+ * lost, the client will simply retransmit the same request. The server's response must
+ * be the same the second time around.
+ *
+ * We return the empty JSON object {} rather than void for RPCs that have no results.
+ * This ensures that if we want to add more return results later, we can do so in a
+ * compatible way.
+ */
 @Path(""/agent"")
 @Produces(MediaType.APPLICATION_JSON)
 @Consumes(MediaType.APPLICATION_JSON)
@@ -55,14 +67,23 @@ public AgentStatusResponse getStatus() throws Throwable {
     @POST
     @Path(""/worker/create"")
-    public CreateWorkerResponse createWorker(CreateWorkerRequest req) throws Throwable {
-        return agent().createWorker(req);
+    public Empty createWorker(CreateWorkerRequest req) throws Throwable {
+        agent().createWorker(req);
+        return Empty.INSTANCE;
     }
     @PUT
     @Path(""/worker/stop"")
-    public StopWorkerResponse stopWorker(StopWorkerRequest req) throws Throwable {
-        return agent().stopWorker(req);
+    public Empty stopWorker(StopWorkerRequest req) throws Throwable {
+        agent().stopWorker(req);
+        return Empty.INSTANCE;
+    }
+
+    @DELETE
+    @Path(""/worker"")
+    public Empty destroyWorker(@DefaultValue(""0"") @QueryParam(""workerId"") long workerId) throws Throwable {
+        agent().destroyWorker(new DestroyWorkerRequest(workerId));
+        return Empty.INSTANCE;
     }
     @PUT
diff --git [file java] [file java]
index 7c8de6d3f22..59d34c90ab6 100644
--- [file java]
+++ [file java]
@@ -25,6 +25,7 @@
 import org.apache.kafka.common.utils.Utils;
 import org.apache.kafka.trogdor.common.Platform;
 import org.apache.kafka.trogdor.common.ThreadUtils;
+import org.apache.kafka.trogdor.rest.RequestConflictException;
 import org.apache.kafka.trogdor.rest.WorkerDone;
 import org.apache.kafka.trogdor.rest.WorkerRunning;
 import org.apache.kafka.trogdor.rest.WorkerStarting;
@@ -36,10 +37,12 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.Map;
 import java.util.TreeMap;
 import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutionException;
 import java.util.concurrent.Executors;
 import java.util.concurrent.Future;
 import java.util.concurrent.ScheduledExecutorService;
@@ -72,7 +75,7 @@
     /**
      * A map of task IDs to Work objects.
      */
-    private final Map<String, Worker> workers;
+    private final Map<Long, Worker> workers;
     /**
      * An ExecutorService used to schedule events in the future.
@@ -137,12 +140,15 @@ synchronized boolean shutdown() {
                 return false;
             }
             shutdown = true;
+            if (refCount == 0) {
+                this.notifyAll();
+            }
             return true;
         }
         synchronized void waitForQuiescence() throws InterruptedException {
             while ((!shutdown) || (refCount > 0)) {
-                wait();
+                this.wait();
             }
         }
     }
@@ -173,10 +179,15 @@ synchronized void waitForQuiescence() throws InterruptedException {
      * A worker which is being tracked.
      */
     class Worker {
+        /**
+         * The worker ID.
+         */
+        private final long workerId;
+
         /**
          * The task ID.
          */
-        private final String id;
+        private final String taskId;
         /**
          * The task specification.
@@ -217,7 +228,7 @@ synchronized void waitForQuiescence() throws InterruptedException {
          * If there is a task timeout scheduled, this is a future which can
          * be used to cancel it.
          */
-        private Future<TaskSpec> timeoutFuture = null;
+        private Future<Void> timeoutFuture = null;
         /**
          * A shutdown manager reference which will keep the WorkerManager
@@ -225,16 +236,26 @@ synchronized void waitForQuiescence() throws InterruptedException {
          */
         private ShutdownManager.Reference reference;
-        Worker(String id, TaskSpec spec, long now) {
-            this.id = id;
+        /**
+         * Whether we should destroy the records of this worker once it stops.
+         */
+        private boolean mustDestroy = false;
+
+        Worker(long workerId, String taskId, TaskSpec spec, long now) {
+            this.workerId = workerId;
+            this.taskId = taskId;
             this.spec = spec;
-            this.taskWorker = spec.newTaskWorker(id);
+            this.taskWorker = spec.newTaskWorker(taskId);
             this.startedMs = now;
             this.reference = shutdownManager.takeReference();
         }
-        String id() {
-            return id;
+        long workerId() {
+            return workerId;
+        }
+
+        String taskId() {
+            return taskId;
         }
         TaskSpec spec() {
@@ -244,14 +265,14 @@ TaskSpec spec() {
         WorkerState state() {
             switch (state) {
                 case STARTING:
-                    return new WorkerStarting(spec);
+                    return new WorkerStarting(taskId, spec);
                 case RUNNING:
-                    return new WorkerRunning(spec, startedMs, status.get());
+                    return new WorkerRunning(taskId, spec, startedMs, status.get());
                 case CANCELLING:
                 case STOPPING:
-                    return new WorkerStopping(spec, startedMs, status.get());
+                    return new WorkerStopping(taskId, spec, startedMs, status.get());
                 case DONE:
-                    return new WorkerDone(spec, startedMs, doneMs, status.get(), error);
+                    return new WorkerDone(taskId, spec, startedMs, doneMs, status.get(), error);
             }
             throw new RuntimeException(""unreachable"");
         }
@@ -259,7 +280,7 @@ WorkerState state() {
         void transitionToRunning() {
             state = State.RUNNING;
             timeoutFuture = scheduler.schedule(stateChangeExecutor,
-                new StopWorker(id), spec.durationMs());
+                new StopWorker(workerId, false), spec.durationMs());
         }
         void transitionToStopping() {
@@ -268,7 +289,7 @@ void transitionToStopping() {
                 timeoutFuture.cancel(false);
                 timeoutFuture = null;
             }
-            workerCleanupExecutor.submit(new CleanupWorker(this));
+            workerCleanupExecutor.submit(new HaltWorker(this));
         }
         void transitionToDone() {
@@ -279,15 +300,20 @@ void transitionToDone() {
                 reference = null;
             }
         }
+
+        @Override
+        public String toString() {
+            return String.format(""%s_%d"", taskId, workerId);
+        }
     }
-    public void createWorker(final String id, TaskSpec spec) throws Exception {
+    public void createWorker(long workerId, String taskId, TaskSpec spec) throws Throwable {
         try (ShutdownManager.Reference ref = shutdownManager.takeReference()) {
             final Worker worker = stateChangeExecutor.
-                submit(new CreateWorker(id, spec, time.milliseconds())).get();
+                submit(new CreateWorker(workerId, taskId, spec, time.milliseconds())).get();
             if (worker == null) {
                 log.info(""{}: Ignoring request to create worker {}, because there is already "" +
-                    ""a worker with that id."", nodeName, id);
+                    ""a worker with that id."", nodeName, workerId);
                 return;
             }
             KafkaFutureImpl<String> haltFuture = new KafkaFutureImpl<>();
@@ -297,9 +323,10 @@ public Void apply(String errorString) {
                     if (errorString == null)
                         errorString = """";
                     if (errorString.isEmpty()) {
-                        log.info(""{}: Worker {} is halting."", nodeName, id);
+                        log.info(""{}: Worker {} is halting."", nodeName, worker);
                     } else {
-                        log.info(""{}: Worker {} is halting with error {}"", nodeName, id, errorString);
+                        log.info(""{}: Worker {} is halting with error {}"",
+                            nodeName, worker, errorString);
                     }
                     stateChangeExecutor.submit(
                         new HandleWorkerHalting(worker, errorString, false));
@@ -309,11 +336,20 @@ public Void apply(String errorString) {
             try {
                 worker.taskWorker.start(platform, worker.status, haltFuture);
             } catch (Exception e) {
-                log.info(""{}: Worker {} start() exception"", nodeName, id, e);
+                log.info(""{}: Worker {} start() exception"", nodeName, worker, e);
                 stateChangeExecutor.submit(new HandleWorkerHalting(worker,
                     ""worker.start() exception: "" + Utils.stackTrace(e), true));
             }
             stateChangeExecutor.submit(new FinishCreatingWorker(worker));
+        } catch (ExecutionException e) {
+            if (e.getCause() instanceof RequestConflictException) {
+                log.info(""{}: request conflict while creating worker {} for task {} with spec {}."",
+                    nodeName, workerId, taskId, spec);
+            } else {
+                log.info(""{}: Error creating worker {} for task {} with spec {}"",
+                    nodeName, workerId, taskId, spec, e);
+            }
+            throw e.getCause();
         }
     }
@@ -321,27 +357,42 @@ public Void apply(String errorString) {
      * Handles a request to create a new worker.  Processed by the state change thread.
      */
     class CreateWorker implements Callable<Worker> {
-        private final String id;
+        private final long workerId;
+        private final String taskId;
         private final TaskSpec spec;
         private final long now;
-        CreateWorker(String id, TaskSpec spec, long now) {
-            this.id = id;
+        CreateWorker(long workerId, String taskId, TaskSpec spec, long now) {
+            this.workerId = workerId;
+            this.taskId = taskId;
             this.spec = spec;
             this.now = now;
         }
         @Override
         public Worker call() throws Exception {
-            Worker worker = workers.get(id);
-            if (worker != null) {
-                log.info(""{}: Task ID {} is already in use."", nodeName, id);
-                return null;
+            try {
+                Worker worker = workers.get(workerId);
+                if (worker != null) {
+                    if (!worker.taskId().equals(taskId)) {
+                        throw new RequestConflictException(""There is already a worker ID "" + workerId +
+                            "" with a different task ID."");
+                    } else if (!worker.spec().equals(spec)) {
+                        throw new RequestConflictException(""There is already a worker ID "" + workerId +
+                            "" with a different task spec."");
+                    } else {
+                        return null;
+                    }
+                }
+                worker = new Worker(workerId, taskId, spec, now);
+                workers.put(workerId, worker);
+                log.info(""{}: Created worker {} with spec {}"", nodeName, worker, spec);
+                return worker;
+            } catch (Exception e) {
+                log.info(""{}: unable to create worker {} for task {}, with spec {}"",
+                    nodeName, workerId, taskId, spec, e);
+                throw e;
             }
-            worker = new Worker(id, spec, now);
-            workers.put(id, worker);
-            log.info(""{}: Created a new worker for task {} with spec {}"", nodeName, id, spec);
-            return worker;
         }
     }
@@ -360,12 +411,12 @@ public Void call() throws Exception {
             switch (worker.state) {
                 case CANCELLING:
                     log.info(""{}: Worker {} was cancelled while it was starting up.  "" +
-                        ""Transitioning to STOPPING."", nodeName, worker.id);
+                        ""Transitioning to STOPPING."", nodeName, worker);
                     worker.transitionToStopping();
                     break;
                 case STARTING:
                     log.info(""{}: Worker {} is now RUNNING.  Scheduled to stop in {} ms."",
-                        nodeName, worker.id, worker.spec.durationMs());
+                        nodeName, worker, worker.spec.durationMs());
                     worker.transitionToRunning();
                     break;
                 default:
@@ -400,29 +451,29 @@ public Void call() throws Exception {
                 case STARTING:
                     if (startupHalt) {
                         log.info(""{}: Worker {} {} during startup.  Transitioning to DONE."",
-                            nodeName, worker.id, verb);
+                            nodeName, worker, verb);
                         worker.transitionToDone();
                     } else {
                         log.info(""{}: Worker {} {} during startup.  Transitioning to CANCELLING."",
-                            nodeName, worker.id, verb);
+                            nodeName, worker, verb);
                         worker.state = State.CANCELLING;
                     }
                     break;
                 case CANCELLING:
                     log.info(""{}: Cancelling worker {} {}.  "",
-                            nodeName, worker.id, verb);
+                            nodeName, worker, verb);
                     break;
                 case RUNNING:
                     log.info(""{}: Running worker {} {}.  Transitioning to STOPPING."",
-                        nodeName, worker.id, verb);
+                        nodeName, worker, verb);
                     worker.transitionToStopping();
                     break;
                 case STOPPING:
-                    log.info(""{}: Stopping worker {} {}."", nodeName, worker.id, verb);
+                    log.info(""{}: Stopping worker {} {}."", nodeName, worker, verb);
                     break;
                 case DONE:
                     log.info(""{}: Can't halt worker {} because it is already DONE."",
-                        nodeName, worker.id);
+                        nodeName, worker);
                     break;
             }
             return null;
@@ -432,7 +483,7 @@ public Void call() throws Exception {
     /**
      * Transitions a worker to WorkerDone.  Processed by the state change thread.
      */
-    static class CompleteWorker implements Callable<Void> {
+    class CompleteWorker implements Callable<Void> {
         private final Worker worker;
         private final String failure;
@@ -448,60 +499,79 @@ public Void call() throws Exception {
                 worker.error = failure;
             }
             worker.transitionToDone();
+            if (worker.mustDestroy) {
+                log.info(""{}: destroying worker {} with error {}"",
+                    nodeName, worker, worker.error);
+                workers.remove(worker.workerId);
+            } else {
+                log.info(""{}: completed worker {} with error {}"",
+                    nodeName, worker, worker.error);
+            }
             return null;
         }
     }
-    public TaskSpec stopWorker(String id) throws Exception {
+    public void stopWorker(long workerId, boolean mustDestroy) throws Throwable {
         try (ShutdownManager.Reference ref = shutdownManager.takeReference()) {
-            TaskSpec taskSpec = stateChangeExecutor.submit(new StopWorker(id)).get();
-            if (taskSpec == null) {
-                throw new KafkaException(""No task found with id "" + id);
-            }
-            return taskSpec;
+            stateChangeExecutor.submit(new StopWorker(workerId, mustDestroy)).get();
+        } catch (ExecutionException e) {
+            throw e.getCause();
         }
     }
     /**
      * Stops a worker.  Processed by the state change thread.
      */
-    class StopWorker implements Callable<TaskSpec> {
-        private final String id;
+    class StopWorker implements Callable<Void> {
+        private final long workerId;
+        private final boolean mustDestroy;
-        StopWorker(String id) {
-            this.id = id;
+        StopWorker(long workerId, boolean mustDestroy) {
+            this.workerId = workerId;
+            this.mustDestroy = mustDestroy;
         }
         @Override
-        public TaskSpec call() throws Exception {
-            Worker worker = workers.get(id);
+        public Void call() throws Exception {
+            Worker worker = workers.get(workerId);
             if (worker == null) {
+                log.info(""{}: Can't stop worker {} because there is no worker with that ID."",
+                    nodeName, workerId);
                 return null;
             }
+            if (mustDestroy) {
+                worker.mustDestroy = true;
+            }
             switch (worker.state) {
                 case STARTING:
                     log.info(""{}: Cancelling worker {} during its startup process."",
-                        nodeName, id);
+                        nodeName, worker);
                     worker.state = State.CANCELLING;
                     break;
                 case CANCELLING:
                     log.info(""{}: Can't stop worker {}, because it is already being "" +
-                        ""cancelled."", nodeName, id);
+                        ""cancelled."", nodeName, worker);
                     break;
                 case RUNNING:
-                    log.info(""{}: Stopping running worker {}."", nodeName, id);
+                    log.info(""{}: Stopping running worker {}."", nodeName, worker);
                     worker.transitionToStopping();
                     break;
                 case STOPPING:
                     log.info(""{}: Can't stop worker {}, because it is already "" +
-                            ""stopping."", nodeName, id);
+                            ""stopping."", nodeName, worker);
                     break;
                 case DONE:
-                    log.debug(""{}: Can't stop worker {}, because it is already done."",
-                        nodeName, id);
+                    if (worker.mustDestroy) {
+                        log.info(""{}: destroying worker {} with error {}"",
+                            nodeName, worker, worker.error);
+                        workers.remove(worker.workerId);
+                    } else {
+                        log.debug(""{}: Can't stop worker {}, because it is already done."",
+                            nodeName, worker);
+                    }
                     break;
             }
-            return worker.spec();
+            return null;
         }
     }
@@ -509,10 +579,10 @@ public TaskSpec call() throws Exception {
      * Cleans up the resources associated with a worker.  Processed by the worker
      * cleanup thread pool.
      */
-    class CleanupWorker implements Callable<Void> {
+    class HaltWorker implements Callable<Void> {
         private final Worker worker;
-        CleanupWorker(Worker worker) {
+        HaltWorker(Worker worker) {
             this.worker = worker;
         }
@@ -530,18 +600,18 @@ public Void call() throws Exception {
         }
     }
-    public TreeMap<String, WorkerState> workerStates() throws Exception {
+    public TreeMap<Long, WorkerState> workerStates() throws Exception {
         try (ShutdownManager.Reference ref = shutdownManager.takeReference()) {
             return stateChangeExecutor.submit(new GetWorkerStates()).get();
         }
     }
-    class GetWorkerStates implements Callable<TreeMap<String, WorkerState>> {
+    class GetWorkerStates implements Callable<TreeMap<Long, WorkerState>> {
         @Override
-        public TreeMap<String, WorkerState> call() throws Exception {
-            TreeMap<String, WorkerState> workerMap = new TreeMap<>();
+        public TreeMap<Long, WorkerState> call() throws Exception {
+            TreeMap<Long, WorkerState> workerMap = new TreeMap<>();
             for (Worker worker : workers.values()) {
-                workerMap.put(worker.id(), worker.state());
+                workerMap.put(worker.workerId(), worker.state());
             }
             return workerMap;
         }
@@ -562,17 +632,53 @@ public void waitForShutdown() throws Exception {
     class Shutdown implements Callable<Void> {
         @Override
         public Void call() throws Exception {
-            log.info(""{}: Shutting down WorkerManager."", platform.curNode().name());
-            for (Worker worker : workers.values()) {
-                stateChangeExecutor.submit(new StopWorker(worker.id));
+            log.info(""{}: Shutting down WorkerManager."", nodeName);
+            try {
+                stateChangeExecutor.submit(new DestroyAllWorkers()).get();
+                log.info(""{}: Waiting for shutdownManager quiescence..."", nodeName);
+                shutdownManager.waitForQuiescence();
+                workerCleanupExecutor.shutdownNow();
+                stateChangeExecutor.shutdownNow();
+                log.info(""{}: Waiting for workerCleanupExecutor to terminate..."", nodeName);
+                workerCleanupExecutor.awaitTermination(1, TimeUnit.DAYS);
+                log.info(""{}: Waiting for stateChangeExecutor to terminate..."", nodeName);
+                stateChangeExecutor.awaitTermination(1, TimeUnit.DAYS);
+                log.info(""{}: Shutting down shutdownExecutor."", nodeName);
+                shutdownExecutor.shutdown();
+            } catch (Exception e) {
+                log.info(""{}: Caught exception while shutting down WorkerManager"", nodeName, e);
+                throw e;
+            }
+            return null;
+        }
+    }
+
+    /**
+     * Begins the process of destroying all workers.  Processed by the state change thread.
+     */
+    class DestroyAllWorkers implements Callable<Void> {
+        @Override
+        public Void call() throws Exception {
+            log.info(""{}: Destroying all workers."", nodeName);
+
+            // StopWorker may remove elements from the set of worker IDs.  That might generate
+            // a ConcurrentModificationException if we were iterating over the worker ID
+            // set directly.  Therefore, we make a copy of the worker IDs here and iterate
+            // over that instead.
+            //
+            // Note that there is no possible way that more worker IDs can be added while this
+            // callable is running, because the state change executor is single-threaded.
+            ArrayList<Long> workerIds = new ArrayList<>(workers.keySet());
+
+            for (long workerId : workerIds) {
+                try {
+                    new StopWorker(workerId, true).call();
+                } catch (Exception e) {
+                    log.error(""Failed to stop worker {}"", workerId, e);
+                }
             }
-            shutdownManager.waitForQuiescence();
-            workerCleanupExecutor.shutdownNow();
-            stateChangeExecutor.shutdownNow();
-            workerCleanupExecutor.awaitTermination(1, TimeUnit.DAYS);
-            stateChangeExecutor.awaitTermination(1, TimeUnit.DAYS);
-            shutdownExecutor.shutdown();
             return null;
         }
     }
+
 }
diff --git [file java] [file java]
index 717d7c7047a..23f3ceb91b0 100644
--- [file java]
+++ [file java]
@@ -27,15 +27,16 @@
 import org.apache.kafka.trogdor.common.Platform;
 import org.apache.kafka.trogdor.rest.CoordinatorStatusResponse;
 import org.apache.kafka.trogdor.rest.CreateTaskRequest;
-import org.apache.kafka.trogdor.rest.CreateTaskResponse;
+import org.apache.kafka.trogdor.rest.DestroyTaskRequest;
 import org.apache.kafka.trogdor.rest.JsonRestServer;
 import org.apache.kafka.trogdor.rest.StopTaskRequest;
-import org.apache.kafka.trogdor.rest.StopTaskResponse;
 import org.apache.kafka.trogdor.rest.TasksRequest;
 import org.apache.kafka.trogdor.rest.TasksResponse;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import java.util.concurrent.ThreadLocalRandom;
+
 import static net.sourceforge.argparse4j.impl.Arguments.store;
 /**
@@ -72,9 +73,9 @@
      * @param resource      The AgentRestResoure to use.
      */
     public Coordinator(Platform platform, Scheduler scheduler, JsonRestServer restServer,
-                       CoordinatorRestResource resource) {
+                       CoordinatorRestResource resource, long firstWorkerId) {
         this.startTimeMs = scheduler.time().milliseconds();
-        this.taskManager = new TaskManager(platform, scheduler);
+        this.taskManager = new TaskManager(platform, scheduler, firstWorkerId);
         this.restServer = restServer;
         resource.setCoordinator(this);
     }
@@ -87,12 +88,16 @@ public CoordinatorStatusResponse status() throws Exception {
         return new CoordinatorStatusResponse(startTimeMs);
     }
-    public CreateTaskResponse createTask(CreateTaskRequest request) throws Exception {
-        return new CreateTaskResponse(taskManager.createTask(request.id(), request.spec()));
+    public void createTask(CreateTaskRequest request) throws Throwable {
+        taskManager.createTask(request.id(), request.spec());
+    }
+
+    public void stopTask(StopTaskRequest request) throws Throwable {
+        taskManager.stopTask(request.id());
     }
-    public StopTaskResponse stopTask(StopTaskRequest request) throws Exception {
-        return new StopTaskResponse(taskManager.stopTask(request.id()));
+    public void destroyTask(DestroyTaskRequest request) throws Throwable {
+        taskManager.destroyTask(request.id());
     }
     public TasksResponse tasks(TasksRequest request) throws Exception {
@@ -149,7 +154,7 @@ public static void main(String args) throws Exception {
         CoordinatorRestResource resource = new CoordinatorRestResource();
         log.info(""Starting coordinator process."");
         final Coordinator coordinator = new Coordinator(platform, Scheduler.SYSTEM,
-            restServer, resource);
+            restServer, resource, ThreadLocalRandom.current().nextLong(0, Long.MAX_VALUE / 2));
         restServer.start(resource);
         Runtime.getRuntime().addShutdownHook(new Thread() {
             @Override
diff --git [file java] [file java]
index 0677296ee3c..780ae737e0e 100644
--- [file java]
+++ [file java]
@@ -27,12 +27,11 @@
 import org.apache.kafka.trogdor.common.JsonUtil;
 import org.apache.kafka.trogdor.rest.CoordinatorStatusResponse;
 import org.apache.kafka.trogdor.rest.CreateTaskRequest;
-import org.apache.kafka.trogdor.rest.CreateTaskResponse;
+import org.apache.kafka.trogdor.rest.DestroyTaskRequest;
 import org.apache.kafka.trogdor.rest.Empty;
 import org.apache.kafka.trogdor.rest.JsonRestServer;
 import org.apache.kafka.trogdor.rest.JsonRestServer.HttpResponse;
 import org.apache.kafka.trogdor.rest.StopTaskRequest;
-import org.apache.kafka.trogdor.rest.StopTaskResponse;
 import org.apache.kafka.trogdor.rest.TasksRequest;
 import org.apache.kafka.trogdor.rest.TasksResponse;
 import org.slf4j.Logger;
@@ -116,36 +115,45 @@ public CoordinatorStatusResponse status() throws Exception {
         return resp.body();
     }
-    public CreateTaskResponse createTask(CreateTaskRequest request) throws Exception {
-        HttpResponse<CreateTaskResponse> resp =
-            JsonRestServer.<CreateTaskResponse>httpRequest(log, url(""/coordinator/task/create""), ""POST"",
-                request, new TypeReference<CreateTaskResponse>() { }, maxTries);
-        return resp.body();
+    public void createTask(CreateTaskRequest request) throws Exception {
+        HttpResponse<Empty> resp =
+            JsonRestServer.httpRequest(log, url(""/coordinator/task/create""), ""POST"",
+                request, new TypeReference<Empty>() { }, maxTries);
+        resp.body();
     }
-    public StopTaskResponse stopTask(StopTaskRequest request) throws Exception {
-        HttpResponse<StopTaskResponse> resp =
-            JsonRestServer.<StopTaskResponse>httpRequest(log, url(""/coordinator/task/stop""), ""PUT"",
-                request, new TypeReference<StopTaskResponse>() { }, maxTries);
-        return resp.body();
+    public void stopTask(StopTaskRequest request) throws Exception {
+        HttpResponse<Empty> resp =
+            JsonRestServer.httpRequest(log, url(""/coordinator/task/stop""), ""PUT"",
+                request, new TypeReference<Empty>() { }, maxTries);
+        resp.body();
+    }
+
+    public void destroyTask(DestroyTaskRequest request) throws Exception {
+        UriBuilder uriBuilder = UriBuilder.fromPath(url(""/coordinator/tasks""));
+        uriBuilder.queryParam(""taskId"", request.id());
+        HttpResponse<Empty> resp =
+            JsonRestServer.httpRequest(log, uriBuilder.build().toString(), ""DELETE"",
+                null, new TypeReference<Empty>() { }, maxTries);
+        resp.body();
     }
     public TasksResponse tasks(TasksRequest request) throws Exception {
         UriBuilder uriBuilder = UriBuilder.fromPath(url(""/coordinator/tasks""));
-        uriBuilder.queryParam(""taskId"", request.taskIds().toArray(new String));
+        uriBuilder.queryParam(""taskId"", (Object) request.taskIds().toArray(new String));
         uriBuilder.queryParam(""firstStartMs"", request.firstStartMs());
         uriBuilder.queryParam(""lastStartMs"", request.lastStartMs());
         uriBuilder.queryParam(""firstEndMs"", request.firstEndMs());
         uriBuilder.queryParam(""lastEndMs"", request.lastEndMs());
         HttpResponse<TasksResponse> resp =
-            JsonRestServer.<TasksResponse>httpRequest(log, uriBuilder.build().toString(), ""GET"",
+            JsonRestServer.httpRequest(log, uriBuilder.build().toString(), ""GET"",
                 null, new TypeReference<TasksResponse>() { }, maxTries);
         return resp.body();
     }
     public void shutdown() throws Exception {
         HttpResponse<Empty> resp =
-            JsonRestServer.<Empty>httpRequest(log, url(""/coordinator/shutdown""), ""PUT"",
+            JsonRestServer.httpRequest(log, url(""/coordinator/shutdown""), ""PUT"",
                 null, new TypeReference<Empty>() { }, maxTries);
         resp.body();
     }
@@ -185,6 +193,12 @@ public static void main(String args) throws Exception {
             .dest(""stop_task"")
             .metavar(""TASK_ID"")
             .help(""Stop a task."");
+        actions.addArgument(""--destroy-task"")
+            .action(store())
+            .type(String.class)
+            .dest(""destroy_task"")
+            .metavar(""TASK_ID"")
+            .help(""Destroy a task."");
         actions.addArgument(""--shutdown"")
             .action(storeTrue())
             .type(Boolean.class)
@@ -216,15 +230,21 @@ public static void main(String args) throws Exception {
                 JsonUtil.toPrettyJsonString(client.tasks(
                     new TasksRequest(null, 0, 0, 0, 0))));
         } else if (res.getString(""create_task"") != null) {
-            client.createTask(JsonUtil.JSON_SERDE.readValue(res.getString(""create_task""),
-                CreateTaskRequest.class));
-            System.out.println(""Created task."");
+            CreateTaskRequest req = JsonUtil.JSON_SERDE.
+                readValue(res.getString(""create_task""), CreateTaskRequest.class);
+            client.createTask(req);
+            System.out.printf(""Sent CreateTaskRequest for task %s."", req.id());
         } else if (res.getString(""stop_task"") != null) {
-            client.stopTask(new StopTaskRequest(res.getString(""stop_task"")));
-            System.out.println(""Created task."");
+            String taskId = res.getString(""stop_task"");
+            client.stopTask(new StopTaskRequest(taskId));
+            System.out.printf(""Sent StopTaskRequest for task %s.%n"", taskId);
+        } else if (res.getString(""destroy_task"") != null) {
+            String taskId = res.getString(""destroy_task"");
+            client.destroyTask(new DestroyTaskRequest(taskId));
+            System.out.printf(""Sent DestroyTaskRequest for task %s.%n"", taskId);
         } else if (res.getBoolean(""shutdown"")) {
             client.shutdown();
-            System.out.println(""Sent shutdown request."");
+            System.out.println(""Sent ShutdownRequest."");
         } else {
             System.out.println(""You must choose an action. Type --help for help."");
             Exit.exit(1);
diff --git [file java] [file java]
index b8663ec4cc3..cbfbddd7eda 100644
--- [file java]
+++ [file java]
@@ -19,15 +19,15 @@
 import org.apache.kafka.trogdor.rest.CoordinatorShutdownRequest;
 import org.apache.kafka.trogdor.rest.CoordinatorStatusResponse;
 import org.apache.kafka.trogdor.rest.CreateTaskRequest;
-import org.apache.kafka.trogdor.rest.CreateTaskResponse;
+import org.apache.kafka.trogdor.rest.DestroyTaskRequest;
 import org.apache.kafka.trogdor.rest.Empty;
 import org.apache.kafka.trogdor.rest.StopTaskRequest;
-import org.apache.kafka.trogdor.rest.StopTaskResponse;
 import org.apache.kafka.trogdor.rest.TasksRequest;
 import org.apache.kafka.trogdor.rest.TasksResponse;
 import javax.servlet.ServletContext;
 import javax.ws.rs.Consumes;
+import javax.ws.rs.DELETE;
 import javax.ws.rs.DefaultValue;
 import javax.ws.rs.GET;
 import javax.ws.rs.POST;
@@ -39,7 +39,18 @@
 import java.util.List;
 import java.util.concurrent.atomic.AtomicReference;
-
+/**
+ * The REST resource for the Coordinator. This describes the RPCs which the coordinator
+ * can accept.
+ *
+ * RPCs should be idempotent.  This is important because if the server's response is
+ * lost, the client will simply retransmit the same request. The server's response must
+ * be the same the second time around.
+ *
+ * We return the empty JSON object {} rather than void for RPCs that have no results.
+ * This ensures that if we want to add more return results later, we can do so in a
+ * compatible way.
+ */
 @Path(""/coordinator"")
 @Produces(MediaType.APPLICATION_JSON)
 @Consumes(MediaType.APPLICATION_JSON)
@@ -61,14 +72,23 @@ public CoordinatorStatusResponse status() throws Throwable {
     @POST
     @Path(""/task/create"")
-    public CreateTaskResponse createTask(CreateTaskRequest request) throws Throwable {
-        return coordinator().createTask(request);
+    public Empty createTask(CreateTaskRequest request) throws Throwable {
+        coordinator().createTask(request);
+        return Empty.INSTANCE;
     }
     @PUT
     @Path(""/task/stop"")
-    public StopTaskResponse stopTask(StopTaskRequest request) throws Throwable {
-        return coordinator().stopTask(request);
+    public Empty stopTask(StopTaskRequest request) throws Throwable {
+        coordinator().stopTask(request);
+        return Empty.INSTANCE;
+    }
+
+    @DELETE
+    @Path(""/tasks"")
+    public Empty destroyTask(@DefaultValue("""") @QueryParam(""taskId"") String taskId) throws Throwable {
+        coordinator().destroyTask(new DestroyTaskRequest(taskId));
+        return Empty.INSTANCE;
     }
     @GET
diff --git [file java] [file java]
index 91ef9c2928a..3f0075e598a 100644
--- [file java]
+++ [file java]
@@ -79,13 +79,16 @@
     private static final long HEARTBEAT_DELAY_MS = 1000L;
     class ManagedWorker {
-        private final String id;
+        private final long workerId;
+        private final String taskId;
         private final TaskSpec spec;
         private boolean shouldRun;
         private WorkerState state;
-        ManagedWorker(String id, TaskSpec spec, boolean shouldRun, WorkerState state) {
-            this.id = id;
+        ManagedWorker(long workerId, String taskId, TaskSpec spec,
+                      boolean shouldRun, WorkerState state) {
+            this.workerId = workerId;
+            this.taskId = taskId;
             this.spec = spec;
             this.shouldRun = shouldRun;
             this.state = state;
@@ -93,19 +96,24 @@
         void tryCreate() {
             try {
-                client.createWorker(new CreateWorkerRequest(id, spec));
+                client.createWorker(new CreateWorkerRequest(workerId, taskId, spec));
             } catch (Throwable e) {
-                log.error(""{}: error creating worker {}."", node.name(), id, e);
+                log.error(""{}: error creating worker {}."", node.name(), this, e);
             }
         }
         void tryStop() {
             try {
-                client.stopWorker(new StopWorkerRequest(id));
+                client.stopWorker(new StopWorkerRequest(workerId));
             } catch (Throwable e) {
-                log.error(""{}: error stopping worker {}."", node.name(), id, e);
+                log.error(""{}: error stopping worker {}."", node.name(), this, e);
             }
         }
+
+        @Override
+        public String toString() {
+            return String.format(""%s_%d"", taskId, workerId);
+        }
     }
     /**
@@ -126,7 +134,7 @@ void tryStop() {
     /**
      * Maps task IDs to worker structures.
      */
-    private final Map<String, ManagedWorker> workers;
+    private final Map<Long, ManagedWorker> workers;
     /**
      * An executor service which manages the thread dedicated to this node.
@@ -196,24 +204,25 @@ public void run() {
                 }
                 // Identify workers which we think should be running, but which do not appear
                 // in the agent's response.  We need to send startWorker requests for these.
-                for (Map.Entry<String, ManagedWorker> entry : workers.entrySet()) {
-                    String id = entry.getKey();
-                    if (!agentStatus.workers().containsKey(id)) {
+                for (Map.Entry<Long, ManagedWorker> entry : workers.entrySet()) {
+                    Long workerId = entry.getKey();
+                    if (!agentStatus.workers().containsKey(workerId)) {
                         ManagedWorker worker = entry.getValue();
                         if (worker.shouldRun) {
                             worker.tryCreate();
                         }
                     }
                 }
-                for (Map.Entry<String, WorkerState> entry : agentStatus.workers().entrySet()) {
-                    String id = entry.getKey();
+                for (Map.Entry<Long, WorkerState> entry : agentStatus.workers().entrySet()) {
+                    long workerId = entry.getKey();
                     WorkerState state = entry.getValue();
-                    ManagedWorker worker = workers.get(id);
+                    ManagedWorker worker = workers.get(workerId);
                     if (worker == null) {
                         // Identify tasks which are running, but which we don't know about.
                         // Add these to the NodeManager as tasks that should not be running.
-                        log.warn(""{}: scheduling unknown worker {} for stopping."", node.name(), id);
-                        workers.put(id, new ManagedWorker(id, state.spec(), false, state));
+                        log.warn(""{}: scheduling unknown worker with ID {} for stopping."", node.name(), workerId);
+                        workers.put(workerId, new ManagedWorker(workerId, state.taskId(),
+                            state.spec(), false, state));
                     } else {
                         // Handle workers which need to be stopped.
                         if (state instanceof WorkerStarting || state instanceof WorkerRunning) {
@@ -227,7 +236,7 @@ public void run() {
                         } else {
                             log.info(""{}: worker state changed from {} to {}"", node.name(), worker.state, state);
                             worker.state = state;
-                            taskManager.updateWorkerState(node.name(), worker.id, state);
+                            taskManager.updateWorkerState(node.name(), worker.workerId, state);
                         }
                     }
                 }
@@ -240,34 +249,39 @@ public void run() {
     /**
      * Create a new worker.
      *
-     * @param id                    The new worker id.
+     * @param workerId              The new worker id.
+     * @param taskId                The new task id.
      * @param spec                  The task specification to use with the new worker.
      */
-    public void createWorker(String id, TaskSpec spec) {
-        executor.submit(new CreateWorker(id, spec));
+    public void createWorker(long workerId, String taskId, TaskSpec spec) {
+        executor.submit(new CreateWorker(workerId, taskId, spec));
     }
     /**
      * Starts a worker.
      */
     class CreateWorker implements Callable<Void> {
-        private final String id;
+        private final long workerId;
+        private final String taskId;
         private final TaskSpec spec;
-        CreateWorker(String id, TaskSpec spec) {
-            this.id = id;
+        CreateWorker(long workerId, String taskId, TaskSpec spec) {
+            this.workerId = workerId;
+            this.taskId = taskId;
             this.spec = spec;
         }
         @Override
         public Void call() throws Exception {
-            ManagedWorker worker = workers.get(id);
+            ManagedWorker worker = workers.get(workerId);
             if (worker != null) {
-                log.error(""{}: there is already a worker for task {}."", node.name(), id);
+                log.error(""{}: there is already a worker {} with ID {}."",
+                    node.name(), worker, workerId);
                 return null;
             }
-            log.info(""{}: scheduling worker {} to start."", node.name(), id);
-            workers.put(id, new ManagedWorker(id, spec, true, new WorkerReceiving(spec)));
+            worker = new ManagedWorker(workerId, taskId, spec, true, new WorkerReceiving(taskId, spec));
+            log.info(""{}: scheduling worker {} to start."", node.name(), worker);
+            workers.put(workerId, worker);
             rescheduleNextHeartbeat(0);
             return null;
         }
@@ -276,41 +290,72 @@ public Void call() throws Exception {
     /**
      * Stop a worker.
      *
-     * @param id                    The id of the worker to stop.
+     * @param workerId              The id of the worker to stop.
      */
-    public void stopWorker(String id) {
-        executor.submit(new StopWorker(id));
+    public void stopWorker(long workerId) {
+        executor.submit(new StopWorker(workerId));
     }
     /**
      * Stops a worker.
      */
     class StopWorker implements Callable<Void> {
-        private final String id;
+        private final long workerId;
-        StopWorker(String id) {
-            this.id = id;
+        StopWorker(long workerId) {
+            this.workerId = workerId;
         }
         @Override
         public Void call() throws Exception {
-            ManagedWorker worker = workers.get(id);
+            ManagedWorker worker = workers.get(workerId);
             if (worker == null) {
-                log.error(""{}: can't stop non-existent worker {}."", node.name(), id);
+                log.error(""{}: unable to locate worker to stop with ID {}."", node.name(), workerId);
                 return null;
             }
             if (!worker.shouldRun) {
-                log.error(""{}: The worker for task {} is already scheduled to stop."",
-                    node.name(), id);
+                log.error(""{}: Worker {} is already scheduled to stop."",
+                    node.name(), worker);
                 return null;
             }
-            log.info(""{}: scheduling worker {} on {} to stop."", node.name(), id);
+            log.info(""{}: scheduling worker {} to stop."", node.name(), worker);
             worker.shouldRun = false;
             rescheduleNextHeartbeat(0);
             return null;
         }
     }
+    /**
+     * Destroy a worker.
+     *
+     * @param workerId              The id of the worker to destroy.
+     */
+    public void destroyWorker(long workerId) {
+        executor.submit(new DestroyWorker(workerId));
+    }
+
+    /**
+     * Destroys a worker.
+     */
+    class DestroyWorker implements Callable<Void> {
+        private final long workerId;
+
+        DestroyWorker(long workerId) {
+            this.workerId = workerId;
+        }
+
+        @Override
+        public Void call() throws Exception {
+            ManagedWorker worker = workers.remove(workerId);
+            if (worker == null) {
+                log.error(""{}: unable to locate worker to destroy with ID {}."", node.name(), workerId);
+                return null;
+            }
+            rescheduleNextHeartbeat(0);
+            return null;
+        }
+    }
+
     public void beginShutdown(boolean stopNode) {
         executor.shutdownNow();
         if (stopNode) {
diff --git [file java] [file java]
index 7e19c8b34ae..74082bdb60a 100644
--- [file java]
+++ [file java]
@@ -21,6 +21,7 @@
 import com.fasterxml.jackson.databind.node.JsonNodeFactory;
 import com.fasterxml.jackson.databind.node.ObjectNode;
 import org.apache.kafka.common.KafkaException;
+import org.apache.kafka.common.errors.InvalidRequestException;
 import org.apache.kafka.common.utils.Scheduler;
 import org.apache.kafka.common.utils.Time;
 import org.apache.kafka.common.utils.Utils;
@@ -28,6 +29,7 @@
 import org.apache.kafka.trogdor.common.Node;
 import org.apache.kafka.trogdor.common.Platform;
 import org.apache.kafka.trogdor.common.ThreadUtils;
+import org.apache.kafka.trogdor.rest.RequestConflictException;
 import org.apache.kafka.trogdor.rest.TaskDone;
 import org.apache.kafka.trogdor.rest.TaskPending;
 import org.apache.kafka.trogdor.rest.TaskRunning;
@@ -106,12 +108,22 @@
      */
     private final Map<String, NodeManager> nodeManagers;
+    /**
+     * The states of all workers.
+     */
+    private final Map<Long, WorkerState> workerStates = new HashMap<>();
+
     /**
      * True if the TaskManager is shut down.
      */
     private AtomicBoolean shutdown = new AtomicBoolean(false);
-    TaskManager(Platform platform, Scheduler scheduler) {
+    /**
+     * The ID to use for the next worker.  Only accessed by the state change thread.
+     */
+    private long nextWorkerId;
+
+    TaskManager(Platform platform, Scheduler scheduler, long firstWorkerId) {
         this.platform = platform;
         this.scheduler = scheduler;
         this.time = scheduler.time();
@@ -119,6 +131,7 @@
         this.executor = Executors.newSingleThreadScheduledExecutor(
             ThreadUtils.createThreadFactory(""TaskManagerStateThread"", false));
         this.nodeManagers = new HashMap<>();
+        this.nextWorkerId = firstWorkerId;
         for (Node node : platform.topology().nodes().values()) {
             if (Node.Util.getTrogdorAgentPort(node) > 0) {
                 this.nodeManagers.put(node.name(), new NodeManager(node, this));
@@ -178,9 +191,9 @@
         private Future<?> startFuture = null;
         /**
-         * The states of the workers involved with this task.
+         * Maps node names to worker IDs.
          */
-        public Map<String, WorkerState> workerStates = new TreeMap<>();
+        public TreeMap<String, Long> workerIds = new TreeMap<>();
         /**
          * If this is non-empty, a message describing how this task failed.
@@ -240,38 +253,42 @@ TaskState taskState() {
                 case PENDING:
                     return new TaskPending(spec);
                 case RUNNING:
-                    return new TaskRunning(spec, startedMs, getCombinedStatus(workerStates));
+                    return new TaskRunning(spec, startedMs, getCombinedStatus());
                 case STOPPING:
-                    return new TaskStopping(spec, startedMs, getCombinedStatus(workerStates));
+                    return new TaskStopping(spec, startedMs, getCombinedStatus());
                 case DONE:
-                    return new TaskDone(spec, startedMs, doneMs, error, cancelled, getCombinedStatus(workerStates));
+                    return new TaskDone(spec, startedMs, doneMs, error, cancelled, getCombinedStatus());
             }
             throw new RuntimeException(""unreachable"");
         }
-        TreeSet<String> activeWorkers() {
-            TreeSet<String> workerNames = new TreeSet<>();
-            for (Map.Entry<String, WorkerState> entry : workerStates.entrySet()) {
-                if (!entry.getValue().done()) {
-                    workerNames.add(entry.getKey());
+        private JsonNode getCombinedStatus() {
+            if (workerIds.size() == 1) {
+                return workerStates.get(workerIds.values().iterator().next()).status();
+            } else {
+                ObjectNode objectNode = new ObjectNode(JsonNodeFactory.instance);
+                for (Map.Entry<String, Long> entry : workerIds.entrySet()) {
+                    String nodeName = entry.getKey();
+                    Long workerId = entry.getValue();
+                    WorkerState state = workerStates.get(workerId);
+                    JsonNode node = state.status();
+                    if (node != null) {
+                        objectNode.set(nodeName, node);
+                    }
                 }
+                return objectNode;
             }
-            return workerNames;
         }
-    }
-    private static final JsonNode getCombinedStatus(Map<String, WorkerState> states) {
-        if (states.size() == 1) {
-            return states.values().iterator().next().status();
-        } else {
-            ObjectNode objectNode = new ObjectNode(JsonNodeFactory.instance);
-            for (Map.Entry<String, WorkerState> entry : states.entrySet()) {
-                JsonNode node = entry.getValue().status();
-                if (node != null) {
-                    objectNode.set(entry.getKey(), node);
+        TreeMap<String, Long> activeWorkerIds() {
+            TreeMap<String, Long> activeWorkerIds = new TreeMap<>();
+            for (Map.Entry<String, Long> entry : workerIds.entrySet()) {
+                WorkerState workerState = workerStates.get(entry.getValue());
+                if (!workerState.done()) {
+                    activeWorkerIds.put(entry.getKey(), entry.getValue());
                 }
             }
-            return objectNode;
+            return activeWorkerIds;
         }
     }
@@ -280,27 +297,21 @@ private static final JsonNode getCombinedStatus(Map<String, WorkerState> states)
      *
      * @param id                    The ID of the task to create.
      * @param spec                  The specification of the task to create.
-     *
-     * @return                      The specification of the task with the given ID.
-     *                              Note that if there was already a task with the given ID,
-     *                              this may be different from the specification that was
-     *                              requested.
      */
-    public TaskSpec createTask(final String id, TaskSpec spec)
-            throws ExecutionException, InterruptedException {
-        final TaskSpec existingSpec = executor.submit(new CreateTask(id, spec)).get();
-        if (existingSpec != null) {
-            log.info(""Ignoring request to create task {}, because there is already "" +
-                ""a task with that id."", id);
-            return existingSpec;
+    public void createTask(final String id, TaskSpec spec)
+            throws Throwable {
+        try {
+            executor.submit(new CreateTask(id, spec)).get();
+        } catch (ExecutionException e) {
+            log.info(""createTask(id={}, spec={}) error"", id, spec, e);
+            throw e.getCause();
         }
-        return spec;
     }
     /**
      * Handles a request to create a new task.  Processed by the state change thread.
      */
-    class CreateTask implements Callable<TaskSpec> {
+    class CreateTask implements Callable<Void> {
         private final String id;
         private final TaskSpec spec;
@@ -310,11 +321,18 @@ public TaskSpec createTask(final String id, TaskSpec spec)
         }
         @Override
-        public TaskSpec call() throws Exception {
+        public Void call() throws Exception {
+            if (id.isEmpty()) {
+                throw new InvalidRequestException(""Invalid empty ID in createTask request."");
+            }
             ManagedTask task = tasks.get(id);
             if (task != null) {
-                log.info(""Task ID {} is already in use."", id);
-                return task.spec;
+                if (!task.spec.equals(spec)) {
+                    throw new RequestConflictException(""Task ID "" + id + "" already "" +
+                        ""exists, and has a different spec "" + task.spec);
+                }
+                log.info(""Task {} already exists with spec {}"", id, spec);
+                return null;
             }
             TaskController controller = null;
             String failure = null;
@@ -374,8 +392,10 @@ public Void call() throws Exception {
             task.state = ManagedTaskState.RUNNING;
             task.startedMs = time.milliseconds();
             for (String workerName : nodeNames) {
-                task.workerStates.put(workerName, new WorkerReceiving(task.spec));
-                nodeManagers.get(workerName).createWorker(task.id, task.spec);
+                long workerId = nextWorkerId++;
+                task.workerIds.put(workerName, workerId);
+                workerStates.put(workerId, new WorkerReceiving(task.id, task.spec));
+                nodeManagers.get(workerName).createWorker(workerId, task.id, task.spec);
             }
             return null;
         }
@@ -385,18 +405,20 @@ public Void call() throws Exception {
      * Stop a task.
      *
      * @param id                    The ID of the task to stop.
-     * @return                      The specification of the task which was stopped, or null if there
-     *                              was no task found with the given ID.
      */
-    public TaskSpec stopTask(final String id) throws ExecutionException, InterruptedException {
-        final TaskSpec spec = executor.submit(new CancelTask(id)).get();
-        return spec;
+    public void stopTask(final String id) throws Throwable {
+        try {
+            executor.submit(new CancelTask(id)).get();
+        } catch (ExecutionException e) {
+            log.info(""stopTask(id={}) error"", id, e);
+            throw e.getCause();
+        }
     }
     /**
      * Handles cancelling a task.  Processed by the state change thread.
      */
-    class CancelTask implements Callable<TaskSpec> {
+    class CancelTask implements Callable<Void> {
         private final String id;
         CancelTask(String id) {
@@ -404,7 +426,10 @@ public TaskSpec stopTask(final String id) throws ExecutionException, Interrupted
         }
         @Override
-        public TaskSpec call() throws Exception {
+        public Void call() throws Exception {
+            if (id.isEmpty()) {
+                throw new InvalidRequestException(""Invalid empty ID in stopTask request."");
+            }
             ManagedTask task = tasks.get(id);
             if (task == null) {
                 log.info(""Can't cancel non-existent task {}."", id);
@@ -420,16 +445,21 @@ public TaskSpec call() throws Exception {
                     break;
                 case RUNNING:
                     task.cancelled = true;
-                    TreeSet<String> activeWorkers = task.activeWorkers();
-                    if (activeWorkers.isEmpty()) {
-                        log.info(""Task {} is now complete with error: {}"", id, task.error);
+                    TreeMap<String, Long> activeWorkerIds = task.activeWorkerIds();
+                    if (activeWorkerIds.isEmpty()) {
+                        if (task.error.isEmpty()) {
+                            log.info(""Task {} is now complete with no errors."", id);
+                        } else {
+                            log.info(""Task {} is now complete with error: {}"", id, task.error);
+                        }
                         task.doneMs = time.milliseconds();
                         task.state = ManagedTaskState.DONE;
                     } else {
-                        for (String workerName : activeWorkers) {
-                            nodeManagers.get(workerName).stopWorker(id);
+                        for (Map.Entry<String, Long> entry : activeWorkerIds.entrySet()) {
+                            nodeManagers.get(entry.getKey()).stopWorker(entry.getValue());
                         }
-                        log.info(""Cancelling task {} on worker(s): {}"", id, Utils.join(activeWorkers, "", ""));
+                        log.info(""Cancelling task {} with worker(s) {}"",
+                            id, Utils.mkString(activeWorkerIds, """", """", "" = "", "", ""));
                         task.state = ManagedTaskState.STOPPING;
                     }
                     break;
@@ -440,7 +470,48 @@ public TaskSpec call() throws Exception {
                     log.info(""Can't cancel task {} because it is already done."", id);
                     break;
             }
-            return task.spec;
+            return null;
+        }
+    }
+
+    public void destroyTask(String id) throws Throwable {
+        try {
+            executor.submit(new DestroyTask(id)).get();
+        } catch (ExecutionException e) {
+            log.info(""destroyTask(id={}) error"", id, e);
+            throw e.getCause();
+        }
+    }
+
+    /**
+     * Handles destroying a task.  Processed by the state change thread.
+     */
+    class DestroyTask implements Callable<Void> {
+        private final String id;
+
+        DestroyTask(String id) {
+            this.id = id;
+        }
+
+        @Override
+        public Void call() throws Exception {
+            if (id.isEmpty()) {
+                throw new InvalidRequestException(""Invalid empty ID in destroyTask request."");
+            }
+            ManagedTask task = tasks.remove(id);
+            if (task == null) {
+                log.info(""Can't destroy task {}: no such task found."", id);
+                return null;
+            }
+            log.info(""Destroying task {}."", id);
+            task.clearStartFuture();
+            for (Map.Entry<String, Long> entry : task.workerIds.entrySet()) {
+                long workerId = entry.getValue();
+                workerStates.remove(workerId);
+                String nodeName = entry.getKey();
+                nodeManagers.get(nodeName).destroyWorker(workerId);
+            }
+            return null;
         }
     }
@@ -448,38 +519,48 @@ public TaskSpec call() throws Exception {
      * Update the state of a particular agent's worker.
      *
      * @param nodeName      The node where the agent is running.
-     * @param id            The worker name.
+     * @param workerId      The worker ID.
      * @param state         The worker state.
      */
-    public void updateWorkerState(String nodeName, String id, WorkerState state) {
-        executor.submit(new UpdateWorkerState(nodeName, id, state));
+    public void updateWorkerState(String nodeName, long workerId, WorkerState state) {
+        executor.submit(new UpdateWorkerState(nodeName, workerId, state));
     }
+    /**
+     * Updates the state of a worker.  Process by the state change thread.
+     */
     class UpdateWorkerState implements Callable<Void> {
         private final String nodeName;
-        private final String id;
-        private final WorkerState state;
+        private final long workerId;
+        private final WorkerState nextState;
-        UpdateWorkerState(String nodeName, String id, WorkerState state) {
+        UpdateWorkerState(String nodeName, long workerId, WorkerState nextState) {
             this.nodeName = nodeName;
-            this.id = id;
-            this.state = state;
+            this.workerId = workerId;
+            this.nextState = nextState;
         }
         @Override
         public Void call() throws Exception {
-            ManagedTask task = tasks.get(id);
-            if (task == null) {
-                log.error(""Can't update worker state unknown worker {} on node {}"",
-                    id, nodeName);
-                return null;
-            }
-            WorkerState prevState = task.workerStates.get(nodeName);
-            log.debug(""Task {}: Updating worker state for {} from {} to {}."",
-                id, nodeName, prevState, state);
-            task.workerStates.put(nodeName, state);
-            if (state.done() && (!prevState.done())) {
-                handleWorkerCompletion(task, nodeName, (WorkerDone) state);
+            try {
+                WorkerState prevState = workerStates.get(workerId);
+                if (prevState == null) {
+                    throw new RuntimeException(""Unable to find workerId "" + workerId);
+                }
+                ManagedTask task = tasks.get(prevState.taskId());
+                if (task == null) {
+                    throw new RuntimeException(""Unable to find taskId "" + prevState.taskId());
+                }
+                log.debug(""Task {}: Updating worker state for {} on {} from {} to {}."",
+                    task.id, workerId, nodeName, prevState, nextState);
+                workerStates.put(workerId, nextState);
+                if (nextState.done() && (!prevState.done())) {
+                    handleWorkerCompletion(task, nodeName, (WorkerDone) nextState);
+                }
+            } catch (Exception e) {
+                log.error(""Error updating worker state for {} on {}.  Stopping worker."",
+                    workerId, nodeName, e);
+                nodeManagers.get(nodeName).stopWorker(workerId);
             }
             return null;
         }
@@ -501,19 +582,19 @@ private void handleWorkerCompletion(ManagedTask task, String nodeName, WorkerDon
                 nodeName, task.id, state.error(), JsonUtil.toJsonString(state.status()));
             task.maybeSetError(state.error());
         }
-        if (task.activeWorkers().isEmpty()) {
+        TreeMap<String, Long> activeWorkerIds = task.activeWorkerIds();
+        if (activeWorkerIds.isEmpty()) {
             task.doneMs = time.milliseconds();
             task.state = ManagedTaskState.DONE;
             log.info(""{}: Task {} is now complete on {} with error: {}"",
-                nodeName, task.id, Utils.join(task.workerStates.keySet(), "", ""),
+                nodeName, task.id, Utils.join(task.workerIds.keySet(), "", ""),
                 task.error.isEmpty() ? ""(none)"" : task.error);
         } else if ((task.state == ManagedTaskState.RUNNING) && (!task.error.isEmpty())) {
-            TreeSet<String> activeWorkers = task.activeWorkers();
             log.info(""{}: task {} stopped with error {}.  Stopping worker(s): {}"",
-                nodeName, task.id, task.error, Utils.join(activeWorkers, "", ""));
+                nodeName, task.id, task.error, Utils.mkString(activeWorkerIds, ""{"", ""}"", "": "", "", ""));
             task.state = ManagedTaskState.STOPPING;
-            for (String workerName : activeWorkers) {
-                nodeManagers.get(workerName).stopWorker(task.id);
+            for (Map.Entry<String, Long> entry : activeWorkerIds.entrySet()) {
+                nodeManagers.get(entry.getKey()).stopWorker(entry.getValue());
             }
         }
     }
@@ -525,6 +606,9 @@ public TasksResponse tasks(TasksRequest request) throws ExecutionException, Inte
         return executor.submit(new GetTasksResponse(request)).get();
     }
+    /**
+     * Gets information about the tasks being managed.  Processed by the state change thread.
+     */
     class GetTasksResponse implements Callable<TasksResponse> {
         private final TasksRequest request;
diff --git [file java] [file java]
index c505e75e3ff..d41a54b2881 100644
--- [file java]
+++ [file java]
@@ -27,13 +27,13 @@
  */
 public class AgentStatusResponse extends Message {
     private final long serverStartMs;
-    private final TreeMap<String, WorkerState> workers;
+    private final TreeMap<Long, WorkerState> workers;
     @JsonCreator
     public AgentStatusResponse(@JsonProperty(""serverStartMs"") long serverStartMs,
-            @JsonProperty(""workers"") TreeMap<String, WorkerState> workers) {
+            @JsonProperty(""workers"") TreeMap<Long, WorkerState> workers) {
         this.serverStartMs = serverStartMs;
-        this.workers = workers == null ? new TreeMap<String, WorkerState>() : workers;
+        this.workers = workers == null ? new TreeMap<Long, WorkerState>() : workers;
     }
     @JsonProperty
@@ -42,7 +42,7 @@ public long serverStartMs() {
     }
     @JsonProperty
-    public TreeMap<String, WorkerState> workers() {
+    public TreeMap<Long, WorkerState> workers() {
         return workers;
     }
 }
diff --git [file java] [file java]
index 9f6e8dcf0d2..4acc943251e 100644
--- [file java]
+++ [file java]
@@ -25,19 +25,27 @@
  * A request to the Trogdor agent to create a worker.
  */
 public class CreateWorkerRequest extends Message {
-    private final String id;
+    private final long workerId;
+    private final String taskId;
     private final TaskSpec spec;
     @JsonCreator
-    public CreateWorkerRequest(@JsonProperty(""id"") String id,
+    public CreateWorkerRequest(@JsonProperty(""workerId"") long workerId,
+            @JsonProperty(""taskId"") String taskId,
             @JsonProperty(""spec"") TaskSpec spec) {
-        this.id = id;
+        this.workerId = workerId;
+        this.taskId = taskId;
         this.spec = spec;
     }
     @JsonProperty
-    public String id() {
-        return id;
+    public long workerId() {
+        return workerId;
+    }
+
+    @JsonProperty
+    public String taskId() {
+        return taskId;
     }
     @JsonProperty
diff --git [file java] [file java]
deleted file mode 100644
index 9e068eccd7a..00000000000
--- [file java]
+++ /dev/null
@@ -1,39 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with
- * this work for additional information regarding copyright ownership.
- * The ASF licenses this file to You under the Apache License, Version 2.0
- * (the ""License""); you may not use this file except in compliance with
- * the License. You may obtain a copy of the License at
- *
- *    [link]
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an ""AS IS"" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.kafka.trogdor.rest;
-
-import com.fasterxml.jackson.annotation.JsonCreator;
-import com.fasterxml.jackson.annotation.JsonProperty;
-import org.apache.kafka.trogdor.task.TaskSpec;
-
-/**
- * A response from the Trogdor agent about creating a worker.
- */
-public class CreateWorkerResponse extends Message {
-    private final TaskSpec spec;
-
-    @JsonCreator
-    public CreateWorkerResponse(@JsonProperty(""spec"") TaskSpec spec) {
-        this.spec = spec;
-    }
-
-    @JsonProperty
-    public TaskSpec spec() {
-        return spec;
-    }
-}
diff --git [file java] [file java]
similarity index 74%
rename from [file java]
rename to [file java]
index 7d5b4687db3..d782d5d1cfb 100644
--- [file java]
+++ [file java]
@@ -19,21 +19,20 @@
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import org.apache.kafka.trogdor.task.TaskSpec;
 /**
- * A response from the Trogdor agent about stopping a worker.
+ * A request to the Trogdor coordinator to delete all memory of a task.
  */
-public class StopWorkerResponse extends Message {
-    private final TaskSpec spec;
+public class DestroyTaskRequest extends Message {
+    private final String id;
     @JsonCreator
-    public StopWorkerResponse(@JsonProperty(""spec"") TaskSpec spec) {
-        this.spec = spec;
+    public DestroyTaskRequest(@JsonProperty(""id"") String id) {
+        this.id = id;
     }
     @JsonProperty
-    public TaskSpec spec() {
-        return spec;
+    public String id() {
+        return id;
     }
 }
diff --git [file java] [file java]
similarity index 75%
rename from [file java]
rename to [file java]
index f344dc9666a..e5a8969d4cd 100644
--- [file java]
+++ [file java]
@@ -19,21 +19,20 @@
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
-import org.apache.kafka.trogdor.task.TaskSpec;
 /**
- * A response from the Trogdor coordinator about stopping a task.
+ * A request to the Trogdor agent to delete all memory of a task.
  */
-public class StopTaskResponse extends Message {
-    private final TaskSpec spec;
+public class DestroyWorkerRequest extends Message {
+    private final long workerId;
     @JsonCreator
-    public StopTaskResponse(@JsonProperty(""spec"") TaskSpec spec) {
-        this.spec = spec;
+    public DestroyWorkerRequest(@JsonProperty(""workerId"") long workerId) {
+        this.workerId = workerId;
     }
     @JsonProperty
-    public TaskSpec spec() {
-        return spec;
+    public long workerId() {
+        return workerId;
     }
 }
diff --git [file java] [file java]
similarity index 64%
rename from [file java]
rename to [file java]
index 54ea0f23c97..2701f6af8f9 100644
--- [file java]
+++ [file java]
@@ -17,23 +17,17 @@
 package org.apache.kafka.trogdor.rest;
-import com.fasterxml.jackson.annotation.JsonCreator;
-import com.fasterxml.jackson.annotation.JsonProperty;
-import org.apache.kafka.trogdor.task.TaskSpec;
-
 /**
- * A response from the Trogdor coordinator about creating a task.
+ * Indicates that a given request got an HTTP error 409: CONFLICT.
  */
-public class CreateTaskResponse extends Message {
-    private final TaskSpec spec;
+public class RequestConflictException extends RuntimeException {
+    private static final long serialVersionUID = 1L;
-    @JsonCreator
-    public CreateTaskResponse(@JsonProperty(""spec"") TaskSpec spec) {
-        this.spec = spec;
+    public RequestConflictException(String message) {
+        super(message);
     }
-    @JsonProperty
-    public TaskSpec spec() {
-        return spec;
+    public RequestConflictException() {
+        super();
     }
 }
diff --git [file java] [file java]
index f62a775fad9..57c54ec04d8 100644
--- [file java]
+++ [file java]
@@ -18,6 +18,7 @@
 import com.fasterxml.jackson.databind.JsonMappingException;
 import com.fasterxml.jackson.databind.exc.InvalidTypeIdException;
+import org.apache.kafka.common.errors.InvalidRequestException;
 import org.apache.kafka.common.errors.SerializationException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -38,6 +39,8 @@ public Response toResponse(Throwable e) {
         }
         if (e instanceof NotFoundException) {
             return buildResponse(Response.Status.NOT_FOUND, e);
+        } else if (e instanceof InvalidRequestException) {
+            return buildResponse(Response.Status.BAD_REQUEST, e);
         } else if (e instanceof InvalidTypeIdException) {
             return buildResponse(Response.Status.NOT_IMPLEMENTED, e);
         } else if (e instanceof JsonMappingException) {
@@ -46,6 +49,8 @@ public Response toResponse(Throwable e) {
             return buildResponse(Response.Status.NOT_IMPLEMENTED, e);
         } else if (e instanceof SerializationException) {
             return buildResponse(Response.Status.BAD_REQUEST, e);
+        } else if (e instanceof RequestConflictException) {
+            return buildResponse(Response.Status.CONFLICT, e);
         } else {
             return buildResponse(Response.Status.INTERNAL_SERVER_ERROR, e);
         }
@@ -57,7 +62,9 @@ public static Exception toException(int code, String msg) throws Exception {
         } else if (code == Response.Status.NOT_IMPLEMENTED.getStatusCode()) {
             throw new ClassNotFoundException(msg);
         } else if (code == Response.Status.BAD_REQUEST.getStatusCode()) {
-            throw new SerializationException(msg);
+            throw new InvalidRequestException(msg);
+        } else if (code == Response.Status.CONFLICT.getStatusCode()) {
+            throw new RequestConflictException(msg);
         } else {
             throw new RuntimeException(msg);
         }
diff --git [file java] [file java]
index 3287801d303..704a961f99e 100644
--- [file java]
+++ [file java]
@@ -28,7 +28,7 @@
     @JsonCreator
     public StopTaskRequest(@JsonProperty(""id"") String id) {
-        this.id = id;
+        this.id = (id == null) ? """" : id;
     }
     @JsonProperty
diff --git [file java] [file java]
index 54c689adfcd..c1dcff363c8 100644
--- [file java]
+++ [file java]
@@ -24,15 +24,15 @@
  * A request to the Trogdor agent to stop a worker.
  */
 public class StopWorkerRequest extends Message {
-    private final String id;
+    private final long workerId;
     @JsonCreator
-    public StopWorkerRequest(@JsonProperty(""id"") String id) {
-        this.id = id;
+    public StopWorkerRequest(@JsonProperty(""workerId"") long workerId) {
+        this.workerId = workerId;
     }
     @JsonProperty
-    public String id() {
-        return id;
+    public long workerId() {
+        return workerId;
     }
 }
diff --git [file java] [file java]
index 500d3c6a0c2..5f773bba5c4 100644
--- [file java]
+++ [file java]
@@ -49,12 +49,13 @@
     private final String error;
     @JsonCreator
-    public WorkerDone(@JsonProperty(""spec"") TaskSpec spec,
+    public WorkerDone(@JsonProperty(""taskId"") String taskId,
+            @JsonProperty(""spec"") TaskSpec spec,
             @JsonProperty(""startedMs"") long startedMs,
             @JsonProperty(""doneMs"") long doneMs,
             @JsonProperty(""status"") JsonNode status,
             @JsonProperty(""error"") String error) {
-        super(spec);
+        super(taskId, spec);
         this.startedMs = startedMs;
         this.doneMs = doneMs;
         this.status = status == null ? NullNode.instance : status;
diff --git [file java] [file java]
index 70687743f74..1babcce2a57 100644
--- [file java]
+++ [file java]
@@ -29,8 +29,9 @@
  */
 public final class WorkerReceiving extends WorkerState {
     @JsonCreator
-    public WorkerReceiving(@JsonProperty(""spec"") TaskSpec spec) {
-        super(spec);
+    public WorkerReceiving(@JsonProperty(""taskId"") String taskId,
+            @JsonProperty(""spec"") TaskSpec spec) {
+        super(taskId, spec);
     }
     @Override
diff --git [file java] [file java]
index af8ee88a1ab..15e77528d62 100644
--- [file java]
+++ [file java]
@@ -39,10 +39,11 @@
     private final JsonNode status;
     @JsonCreator
-    public WorkerRunning(@JsonProperty(""spec"") TaskSpec spec,
+    public WorkerRunning(@JsonProperty(""taskId"") String taskId,
+            @JsonProperty(""spec"") TaskSpec spec,
             @JsonProperty(""startedMs"") long startedMs,
             @JsonProperty(""status"") JsonNode status) {
-        super(spec);
+        super(taskId, spec);
         this.startedMs = startedMs;
         this.status = status == null ? NullNode.instance : status;
     }
diff --git [file java] [file java]
index b568ec1f887..7a06eac5b7d 100644
--- [file java]
+++ [file java]
@@ -28,8 +28,9 @@
  */
 public final class WorkerStarting extends WorkerState {
     @JsonCreator
-    public WorkerStarting(@JsonProperty(""spec"") TaskSpec spec) {
-        super(spec);
+    public WorkerStarting(@JsonProperty(""taskId"") String taskId,
+            @JsonProperty(""spec"") TaskSpec spec) {
+        super(taskId, spec);
     }
     @Override
diff --git [file java] [file java]
index 044d719f894..6480a2410dc 100644
--- [file java]
+++ [file java]
@@ -38,12 +38,19 @@
     @JsonSubTypes.Type(value = WorkerDone.class, name = ""DONE"")
     })
 public abstract class WorkerState extends Message {
+    private final String taskId;
     private final TaskSpec spec;
-    public WorkerState(TaskSpec spec) {
+    public WorkerState(String taskId, TaskSpec spec) {
+        this.taskId = taskId;
         this.spec = spec;
     }
+    @JsonProperty
+    public String taskId() {
+        return taskId;
+    }
+
     @JsonProperty
     public TaskSpec spec() {
         return spec;
diff --git [file java] [file java]
index 9fbb3ff7306..2942e118ac6 100644
--- [file java]
+++ [file java]
@@ -39,10 +39,11 @@
     private final JsonNode status;
     @JsonCreator
-    public WorkerStopping(@JsonProperty(""spec"") TaskSpec spec,
+    public WorkerStopping(@JsonProperty(""taskId"") String taskId,
+            @JsonProperty(""spec"") TaskSpec spec,
             @JsonProperty(""startedMs"") long startedMs,
             @JsonProperty(""status"") JsonNode status) {
-        super(spec);
+        super(taskId, spec);
         this.startedMs = startedMs;
         this.status = status == null ? NullNode.instance : status;
     }
diff --git [file java] [file java]
index 61de5c98797..158e690da4b 100644
--- [file java]
+++ [file java]
@@ -36,8 +36,9 @@
 import org.apache.kafka.trogdor.rest.AgentStatusResponse;
 import org.apache.kafka.trogdor.rest.CreateWorkerRequest;
-import org.apache.kafka.trogdor.rest.CreateWorkerResponse;
+import org.apache.kafka.trogdor.rest.DestroyWorkerRequest;
 import org.apache.kafka.trogdor.rest.JsonRestServer;
+import org.apache.kafka.trogdor.rest.RequestConflictException;
 import org.apache.kafka.trogdor.rest.StopWorkerRequest;
 import org.apache.kafka.trogdor.rest.WorkerDone;
 import org.apache.kafka.trogdor.rest.WorkerRunning;
@@ -120,36 +121,47 @@ public void testAgentCreateWorkers() throws Exception {
         new ExpectedTasks().waitFor(client);
         final NoOpTaskSpec fooSpec = new NoOpTaskSpec(1000, 600000);
-        CreateWorkerResponse response = client.createWorker(new CreateWorkerRequest(""foo"", fooSpec));
-        assertEquals(fooSpec.toString(), response.spec().toString());
+        client.createWorker(new CreateWorkerRequest(0, ""foo"", fooSpec));
         new ExpectedTasks().addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerRunning(fooSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""active""))).
                 build()).
             waitFor(client);
         final NoOpTaskSpec barSpec = new NoOpTaskSpec(2000, 900000);
-        client.createWorker(new CreateWorkerRequest(""bar"", barSpec));
-        client.createWorker(new CreateWorkerRequest(""bar"", barSpec));
+        client.createWorker(new CreateWorkerRequest(1, ""bar"", barSpec));
+        client.createWorker(new CreateWorkerRequest(1, ""bar"", barSpec));
+
+        try {
+            client.createWorker(new CreateWorkerRequest(1, ""foo"", barSpec));
+            Assert.fail(""Expected RequestConflictException when re-creating a request with a different taskId."");
+        } catch (RequestConflictException exception) {
+        }
+        try {
+            client.createWorker(new CreateWorkerRequest(1, ""bar"", fooSpec));
+            Assert.fail(""Expected RequestConflictException when re-creating a request with a different spec."");
+        } catch (RequestConflictException exception) {
+        }
+
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerRunning(fooSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""active""))).
                 build()).
             addTask(new ExpectedTaskBuilder(""bar"").
-                workerState(new WorkerRunning(barSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""bar"", barSpec, 0, new TextNode(""active""))).
                 build()).
             waitFor(client);
         final NoOpTaskSpec bazSpec = new NoOpTaskSpec(1, 450000);
-        client.createWorker(new CreateWorkerRequest(""baz"", bazSpec));
+        client.createWorker(new CreateWorkerRequest(2, ""baz"", bazSpec));
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerRunning(fooSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""active""))).
                 build()).
             addTask(new ExpectedTaskBuilder(""bar"").
-                workerState(new WorkerRunning(barSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""bar"", barSpec, 0, new TextNode(""active""))).
                 build()).
             addTask(new ExpectedTaskBuilder(""baz"").
-                workerState(new WorkerRunning(bazSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""baz"", bazSpec, 0, new TextNode(""active""))).
                 build()).
             waitFor(client);
@@ -167,23 +179,23 @@ public void testAgentFinishesTasks() throws Exception {
         new ExpectedTasks().waitFor(client);
         final NoOpTaskSpec fooSpec = new NoOpTaskSpec(10, 2);
-        client.createWorker(new CreateWorkerRequest(""foo"", fooSpec));
+        client.createWorker(new CreateWorkerRequest(0, ""foo"", fooSpec));
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerRunning(fooSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""active""))).
                 build()).
             waitFor(client);
         time.sleep(1);
         final NoOpTaskSpec barSpec = new NoOpTaskSpec(2000, 900000);
-        client.createWorker(new CreateWorkerRequest(""bar"", barSpec));
+        client.createWorker(new CreateWorkerRequest(1, ""bar"", barSpec));
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerRunning(fooSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""active""))).
                 build()).
             addTask(new ExpectedTaskBuilder(""bar"").
-                workerState(new WorkerRunning(barSpec, 1, new TextNode(""active""))).
+                workerState(new WorkerRunning(""bar"", barSpec, 1, new TextNode(""active""))).
                 build()).
             waitFor(client);
@@ -191,21 +203,21 @@ public void testAgentFinishesTasks() throws Exception {
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerDone(fooSpec, 0, 2, new TextNode(""done""), """")).
+                workerState(new WorkerDone(""foo"", fooSpec, 0, 2, new TextNode(""done""), """")).
                 build()).
             addTask(new ExpectedTaskBuilder(""bar"").
-                workerState(new WorkerRunning(barSpec, 1, new TextNode(""active""))).
+                workerState(new WorkerRunning(""bar"", barSpec, 1, new TextNode(""active""))).
                 build()).
             waitFor(client);
         time.sleep(5);
-        client.stopWorker(new StopWorkerRequest(""bar""));
+        client.stopWorker(new StopWorkerRequest(1));
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerDone(fooSpec, 0, 2, new TextNode(""done""), """")).
+                workerState(new WorkerDone(""foo"", fooSpec, 0, 2, new TextNode(""done""), """")).
                 build()).
             addTask(new ExpectedTaskBuilder(""bar"").
-                workerState(new WorkerDone(barSpec, 1, 7, new TextNode(""done""), """")).
+                workerState(new WorkerDone(""bar"", barSpec, 1, 7, new TextNode(""done""), """")).
                 build()).
             waitFor(client);
@@ -224,25 +236,25 @@ public void testWorkerCompletions() throws Exception {
         SampleTaskSpec fooSpec = new SampleTaskSpec(0, 900000,
             Collections.singletonMap(""node01"", 1L), """");
-        client.createWorker(new CreateWorkerRequest(""foo"", fooSpec));
+        client.createWorker(new CreateWorkerRequest(0, ""foo"", fooSpec));
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerRunning(fooSpec, 0, new TextNode(""active""))).
+                workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""active""))).
                 build()).
             waitFor(client);
         SampleTaskSpec barSpec = new SampleTaskSpec(0, 900000,
             Collections.singletonMap(""node01"", 2L), ""baz"");
-        client.createWorker(new CreateWorkerRequest(""bar"", barSpec));
+        client.createWorker(new CreateWorkerRequest(1, ""bar"", barSpec));
         time.sleep(1);
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerDone(fooSpec, 0, 1,
+                workerState(new WorkerDone(""foo"", fooSpec, 0, 1,
                     new TextNode(""halted""), """")).
                 build()).
             addTask(new ExpectedTaskBuilder(""bar"").
-                workerState(new WorkerRunning(barSpec, 0,
+                workerState(new WorkerRunning(""bar"", barSpec, 0,
                     new TextNode(""active""))).
                 build()).
             waitFor(client);
@@ -250,11 +262,11 @@ public void testWorkerCompletions() throws Exception {
         time.sleep(1);
         new ExpectedTasks().
             addTask(new ExpectedTaskBuilder(""foo"").
-                workerState(new WorkerDone(fooSpec, 0, 1,
+                workerState(new WorkerDone(""foo"", fooSpec, 0, 1,
                     new TextNode(""halted""), """")).
                 build()).
             addTask(new ExpectedTaskBuilder(""bar"").
-                workerState(new WorkerDone(barSpec, 0, 2,
+                workerState(new WorkerDone(""bar"", barSpec, 0, 2,
                     new TextNode(""halted""), ""baz"")).
                 build()).
             waitFor(client);
@@ -293,37 +305,84 @@ public void testKiboshFaults() throws Exception {
             Assert.assertEquals(KiboshControlFile.EMPTY, mockKibosh.read());
             FilesUnreadableFaultSpec fooSpec = new FilesUnreadableFaultSpec(0, 900000,
                 Collections.singleton(""myAgent""), mockKibosh.tempDir.getPath().toString(), ""/foo"", 123);
-            client.createWorker(new CreateWorkerRequest(""foo"", fooSpec));
+            client.createWorker(new CreateWorkerRequest(0, ""foo"", fooSpec));
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
-                    workerState(new WorkerRunning(fooSpec, 0, new TextNode(""Added fault foo""))).
+                    workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""Added fault foo""))).
                     build()).
                 waitFor(client);
             Assert.assertEquals(new KiboshControlFile(Collections.<Kibosh.KiboshFaultSpec>singletonList(
                 new KiboshFilesUnreadableFaultSpec(""/foo"", 123))), mockKibosh.read());
             FilesUnreadableFaultSpec barSpec = new FilesUnreadableFaultSpec(0, 900000,
                 Collections.singleton(""myAgent""), mockKibosh.tempDir.getPath().toString(), ""/bar"", 456);
-            client.createWorker(new CreateWorkerRequest(""bar"", barSpec));
+            client.createWorker(new CreateWorkerRequest(1, ""bar"", barSpec));
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
-                    workerState(new WorkerRunning(fooSpec, 0, new TextNode(""Added fault foo""))).build()).
+                    workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""Added fault foo""))).build()).
                 addTask(new ExpectedTaskBuilder(""bar"").
-                    workerState(new WorkerRunning(barSpec, 0, new TextNode(""Added fault bar""))).build()).
+                    workerState(new WorkerRunning(""bar"", barSpec, 0, new TextNode(""Added fault bar""))).build()).
                 waitFor(client);
             Assert.assertEquals(new KiboshControlFile(new ArrayList<Kibosh.KiboshFaultSpec>() {{
                     add(new KiboshFilesUnreadableFaultSpec(""/foo"", 123));
                     add(new KiboshFilesUnreadableFaultSpec(""/bar"", 456));
                 }}), mockKibosh.read());
             time.sleep(1);
-            client.stopWorker(new StopWorkerRequest(""foo""));
+            client.stopWorker(new StopWorkerRequest(0));
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
-                    workerState(new WorkerDone(fooSpec, 0, 1, new TextNode(""Removed fault foo""), """")).build()).
+                    workerState(new WorkerDone(""foo"", fooSpec, 0, 1, new TextNode(""Removed fault foo""), """")).build()).
                 addTask(new ExpectedTaskBuilder(""bar"").
-                    workerState(new WorkerRunning(barSpec, 0, new TextNode(""Added fault bar""))).build()).
+                    workerState(new WorkerRunning(""bar"", barSpec, 0, new TextNode(""Added fault bar""))).build()).
                 waitFor(client);
             Assert.assertEquals(new KiboshControlFile(Collections.<Kibosh.KiboshFaultSpec>singletonList(
                 new KiboshFilesUnreadableFaultSpec(""/bar"", 456))), mockKibosh.read());
         }
     }
+
+    @Test
+    public void testDestroyWorkers() throws Exception {
+        MockTime time = new MockTime(0, 0, 0);
+        MockScheduler scheduler = new MockScheduler(time);
+        Agent agent = createAgent(scheduler);
+        AgentClient client = new AgentClient.Builder().
+            maxTries(10).target(""localhost"", agent.port()).build();
+        new ExpectedTasks().waitFor(client);
+
+        final NoOpTaskSpec fooSpec = new NoOpTaskSpec(10, 5);
+        client.createWorker(new CreateWorkerRequest(0, ""foo"", fooSpec));
+        new ExpectedTasks().
+            addTask(new ExpectedTaskBuilder(""foo"").
+                workerState(new WorkerRunning(""foo"", fooSpec, 0, new TextNode(""active""))).
+                build()).
+            waitFor(client);
+        time.sleep(1);
+
+        client.destroyWorker(new DestroyWorkerRequest(0));
+        client.destroyWorker(new DestroyWorkerRequest(0));
+        client.destroyWorker(new DestroyWorkerRequest(1));
+        new ExpectedTasks().waitFor(client);
+        time.sleep(1);
+
+        final NoOpTaskSpec fooSpec2 = new NoOpTaskSpec(100, 1);
+        client.createWorker(new CreateWorkerRequest(1, ""foo"", fooSpec2));
+        new ExpectedTasks().
+            addTask(new ExpectedTaskBuilder(""foo"").
+                workerState(new WorkerRunning(""foo"", fooSpec2, 2, new TextNode(""active""))).
+                build()).
+            waitFor(client);
+
+        time.sleep(2);
+        new ExpectedTasks().
+            addTask(new ExpectedTaskBuilder(""foo"").
+                workerState(new WorkerDone(""foo"", fooSpec2, 2, 4, new TextNode(""done""), """")).
+                build()).
+            waitFor(client);
+
+        time.sleep(1);
+        client.destroyWorker(new DestroyWorkerRequest(1));
+        new ExpectedTasks().waitFor(client);
+
+        agent.beginShutdown();
+        agent.waitForShutdown();
+    }
 };
diff --git [file java] [file java]
index 617bf34bcd9..121281f5910 100644
--- [file java]
+++ [file java]
@@ -32,6 +32,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+import java.util.HashMap;
 import java.util.Map;
 import java.util.TreeMap;
@@ -184,10 +185,14 @@ public boolean conditionMet() {
                     throw new RuntimeException(e);
                 }
                 StringBuilder errors = new StringBuilder();
+                HashMap<String, WorkerState> taskIdToWorkerState = new HashMap<>();
+                for (WorkerState state : status.workers().values()) {
+                    taskIdToWorkerState.put(state.taskId(), state);
+                }
                 for (Map.Entry<String, ExpectedTask> entry : expected.entrySet()) {
                     String id = entry.getKey();
                     ExpectedTask worker = entry.getValue();
-                    String differences = worker.compare(status.workers().get(id));
+                    String differences = worker.compare(taskIdToWorkerState.get(id));
                     if (differences != null) {
                         errors.append(differences);
                     }
diff --git [file java] [file java]
index 8101d9c6e4e..c1f7490cc82 100644
--- [file java]
+++ [file java]
@@ -45,9 +45,9 @@ public void testDeserializationDoesNotProduceNulls() throws Exception {
         verify(new ProcessStopFaultSpec(0, 0, null, null));
         verify(new AgentStatusResponse(0, null));
         verify(new TasksResponse(null));
-        verify(new WorkerDone(null, 0, 0, null, null));
-        verify(new WorkerRunning(null, 0, null));
-        verify(new WorkerStopping(null, 0, null));
+        verify(new WorkerDone(null, null, 0, 0, null, null));
+        verify(new WorkerRunning(null, null, 0, null));
+        verify(new WorkerStopping(null, null, 0, null));
         verify(new ProduceBenchSpec(0, 0, null, null,
             0, 0, null, null, null, null, null, 0, 0, ""test-topic"", 1, (short) 3));
         verify(new RoundTripWorkloadSpec(0, 0, null, null, null, null, null, null,
diff --git [file java] [file java]
index 07f02c5830b..46315c27d15 100644
--- [file java]
+++ [file java]
@@ -185,7 +185,7 @@ public Void call() throws Exception {
                             }
                             if (node.coordinatorRestResource != null) {
                                 node.coordinator = new Coordinator(node.platform, scheduler,
-                                    node.coordinatorRestServer, node.coordinatorRestResource);
+                                    node.coordinatorRestServer, node.coordinatorRestResource, 0);
                             }
                         } catch (Exception e) {
                             log.error(""Unable to initialize {}"", nodeName, e);
diff --git [file java] [file java]
index 34d7ffe6106..e9434844405 100644
--- [file java]
+++ [file java]
@@ -35,6 +35,8 @@
 import org.apache.kafka.trogdor.fault.NetworkPartitionFaultSpec;
 import org.apache.kafka.trogdor.rest.CoordinatorStatusResponse;
 import org.apache.kafka.trogdor.rest.CreateTaskRequest;
+import org.apache.kafka.trogdor.rest.DestroyTaskRequest;
+import org.apache.kafka.trogdor.rest.RequestConflictException;
 import org.apache.kafka.trogdor.rest.StopTaskRequest;
 import org.apache.kafka.trogdor.rest.TaskDone;
 import org.apache.kafka.trogdor.rest.TaskPending;
@@ -57,8 +59,9 @@
 import java.util.List;
 import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
 import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.fail;
 public class CoordinatorTest {
     private static final Logger log = LoggerFactory.getLogger(CoordinatorTest.class);
@@ -96,11 +99,25 @@ public void testCreateTask() throws Exception {
                     build()).
                 waitFor(cluster.coordinatorClient());
+            // Re-creating a task with the same arguments is not an error.
+            cluster.coordinatorClient().createTask(
+                new CreateTaskRequest(""foo"", fooSpec));
+
+            // Re-creating a task with different arguments gives a RequestConflictException.
+            try {
+                NoOpTaskSpec barSpec = new NoOpTaskSpec(1000, 2000);
+                cluster.coordinatorClient().createTask(
+                    new CreateTaskRequest(""foo"", barSpec));
+                fail(""Expected to get an exception when re-creating a task with a "" +
+                    ""different task spec."");
+            } catch (RequestConflictException exception) {
+            }
+
             time.sleep(2);
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskRunning(fooSpec, 2, new TextNode(""active""))).
-                    workerState(new WorkerRunning(fooSpec, 2, new TextNode(""active""))).
+                    workerState(new WorkerRunning(""foo"", fooSpec, 2, new TextNode(""active""))).
                     build()).
                 waitFor(cluster.coordinatorClient()).
                 waitFor(cluster.agentClient(""node02""));
@@ -149,7 +166,7 @@ public void testTaskDistribution() throws Exception {
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskRunning(fooSpec, 11, status1)).
-                    workerState(new WorkerRunning(fooSpec, 11,  new TextNode(""active""))).
+                    workerState(new WorkerRunning(""foo"", fooSpec, 11,  new TextNode(""active""))).
                     build()).
                 waitFor(coordinatorClient).
                 waitFor(agentClient1).
@@ -163,7 +180,7 @@ public void testTaskDistribution() throws Exception {
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskDone(fooSpec, 11, 13,
                         """", false, status2)).
-                    workerState(new WorkerDone(fooSpec, 11, 13, new TextNode(""done""), """")).
+                    workerState(new WorkerDone(""foo"", fooSpec, 11, 13, new TextNode(""done""), """")).
                     build()).
                 waitFor(coordinatorClient).
                 waitFor(agentClient1).
@@ -206,7 +223,7 @@ public void testTaskCancellation() throws Exception {
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskRunning(fooSpec, 11, status1)).
-                    workerState(new WorkerRunning(fooSpec, 11, new TextNode(""active""))).
+                    workerState(new WorkerRunning(""foo"", fooSpec, 11, new TextNode(""active""))).
                     build()).
                 waitFor(coordinatorClient).
                 waitFor(agentClient1).
@@ -221,11 +238,68 @@ public void testTaskCancellation() throws Exception {
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskDone(fooSpec, 11, 12, """",
                         true, status2)).
-                    workerState(new WorkerDone(fooSpec, 11, 12, new TextNode(""done""), """")).
+                    workerState(new WorkerDone(""foo"", fooSpec, 11, 12, new TextNode(""done""), """")).
+                    build()).
+                waitFor(coordinatorClient).
+                waitFor(agentClient1).
+                waitFor(agentClient2);
+
+            coordinatorClient.destroyTask(new DestroyTaskRequest(""foo""));
+            new ExpectedTasks().
+                waitFor(coordinatorClient).
+                waitFor(agentClient1).
+                waitFor(agentClient2);
+        }
+    }
+
+    @Test
+    public void testTaskDestruction() throws Exception {
+        MockTime time = new MockTime(0, 0, 0);
+        Scheduler scheduler = new MockScheduler(time);
+        try (MiniTrogdorCluster cluster = new MiniTrogdorCluster.Builder().
+            addCoordinator(""node01"").
+            addAgent(""node01"").
+            addAgent(""node02"").
+            scheduler(scheduler).
+            build()) {
+            CoordinatorClient coordinatorClient = cluster.coordinatorClient();
+            AgentClient agentClient1 = cluster.agentClient(""node01"");
+            AgentClient agentClient2 = cluster.agentClient(""node02"");
+
+            new ExpectedTasks().
+                waitFor(coordinatorClient).
+                waitFor(agentClient1).
+                waitFor(agentClient2);
+
+            NoOpTaskSpec fooSpec = new NoOpTaskSpec(2, 2);
+            coordinatorClient.destroyTask(new DestroyTaskRequest(""foo""));
+            coordinatorClient.createTask(new CreateTaskRequest(""foo"", fooSpec));
+            NoOpTaskSpec barSpec = new NoOpTaskSpec(20, 20);
+            coordinatorClient.createTask(new CreateTaskRequest(""bar"", barSpec));
+            coordinatorClient.destroyTask(new DestroyTaskRequest(""bar""));
+            new ExpectedTasks().
+                addTask(new ExpectedTaskBuilder(""foo"").taskState(new TaskPending(fooSpec)).build()).
+                waitFor(coordinatorClient).
+                waitFor(agentClient1).
+                waitFor(agentClient2);
+            time.sleep(10);
+
+            ObjectNode status1 = new ObjectNode(JsonNodeFactory.instance);
+            status1.set(""node01"", new TextNode(""active""));
+            status1.set(""node02"", new TextNode(""active""));
+            new ExpectedTasks().
+                addTask(new ExpectedTaskBuilder(""foo"").
+                    taskState(new TaskRunning(fooSpec, 10, status1)).
                     build()).
                 waitFor(coordinatorClient).
                 waitFor(agentClient1).
                 waitFor(agentClient2);
+
+            coordinatorClient.destroyTask(new DestroyTaskRequest(""foo""));
+            new ExpectedTasks().
+                waitFor(coordinatorClient).
+                waitFor(agentClient1).
+                waitFor(agentClient2);
         }
     }
@@ -397,7 +471,7 @@ public void testTasksRequest() throws Exception {
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskRunning(fooSpec, 2, new TextNode(""active""))).
-                    workerState(new WorkerRunning(fooSpec, 2, new TextNode(""active""))).
+                    workerState(new WorkerRunning(""foo"", fooSpec, 2, new TextNode(""active""))).
                     build()).
                 addTask(new ExpectedTaskBuilder(""bar"").
                     taskState(new TaskPending(barSpec)).
@@ -448,7 +522,7 @@ public void testWorkersExitingAtDifferentTimes() throws Exception {
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskRunning(fooSpec, 2, status1)).
-                    workerState(new WorkerRunning(fooSpec, 2, new TextNode(""active""))).
+                    workerState(new WorkerRunning(""foo"", fooSpec, 2, new TextNode(""active""))).
                     build()).
                 waitFor(coordinatorClient).
                 waitFor(cluster.agentClient(""node02"")).
@@ -461,14 +535,14 @@ public void testWorkersExitingAtDifferentTimes() throws Exception {
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskRunning(fooSpec, 2, status2)).
-                    workerState(new WorkerRunning(fooSpec, 2, new TextNode(""active""))).
+                    workerState(new WorkerRunning(""foo"", fooSpec, 2, new TextNode(""active""))).
                     build()).
                 waitFor(coordinatorClient).
                 waitFor(cluster.agentClient(""node03""));
             new ExpectedTasks().
                 addTask(new ExpectedTaskBuilder(""foo"").
                     taskState(new TaskRunning(fooSpec, 2, status2)).
-                    workerState(new WorkerDone(fooSpec, 2, 12, new TextNode(""halted""), """")).
+                    workerState(new WorkerDone(""foo"", fooSpec, 2, 12, new TextNode(""halted""), """")).
                     build()).
                 waitFor(cluster.agentClient(""node02""));
diff --git [file java] [file java]
index c40f958eb8d..9c7f7522853 100644
--- [file java]
+++ [file java]
@@ -24,6 +24,8 @@
 import com.fasterxml.jackson.databind.exc.InvalidTypeIdException;
 import javax.ws.rs.NotFoundException;
 import javax.ws.rs.core.Response;
+
+import org.apache.kafka.common.errors.InvalidRequestException;
 import org.apache.kafka.common.errors.SerializationException;
 import org.junit.Test;
@@ -67,6 +69,13 @@ public void testToResponseSerializationException() {
         assertEquals(resp.getStatus(), Response.Status.BAD_REQUEST.getStatusCode());
     }
+    @Test
+    public void testToResponseInvalidRequestException() {
+        RestExceptionMapper mapper = new RestExceptionMapper();
+        Response resp = mapper.toResponse(new InvalidRequestException(""invalid request""));
+        assertEquals(resp.getStatus(), Response.Status.BAD_REQUEST.getStatusCode());
+    }
+
     @Test
     public void testToResponseUnknownException() {
         RestExceptionMapper mapper = new RestExceptionMapper();
@@ -84,7 +93,7 @@ public void testToExceptionClassNotFoundException() throws Exception {
         RestExceptionMapper.toException(Response.Status.NOT_IMPLEMENTED.getStatusCode(), ""Not Implemented"");
     }
-    @Test(expected = SerializationException.class)
+    @Test(expected = InvalidRequestException.class)
     public void testToExceptionSerializationException() throws Exception {
         RestExceptionMapper.toException(Response.Status.BAD_REQUEST.getStatusCode(), ""Bad Request"");
     }
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-6195,https://issues.apache.org/jira/browse/KAFKA-6195,https://github.com/apache/kafka/blob/2.1.0/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java,DNS alias support for secured connections,NO,"It seems clients can't use a dns alias in front of a secured Kafka cluster.

So applications can only specify a list of hosts or IPs in bootstrap.servers instead of an alias encompassing all cluster nodes.

Using an alias in bootstrap.servers results in the following error : 

javax.security.sasl.SaslException: An error: (java.security.PrivilegedActionException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Fail to create credential. (63) - No service creds)]) occurred when evaluating SASL token received from the Kafka Broker. Kafka Client will go to AUTH_FAILED state. [Caused by javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Fail to create credential. (63) - No service creds)]]

When using SASL/Kerberos authentication, the kafka server principal is of the form kafka@kafka/broker1.hostname.com@EXAMPLE.COM

Kerberos requires that the hosts can be resolved by their FQDNs.

During SASL handshake, the client will create a SASL token and then send it to kafka for auth.
But to create a SASL token the client first needs to be able to validate that the broker's kerberos is a valid one.

There are 3 potential options :

1. Creating a single kerberos principal not linked to a host but to an alias and reference it in the broker jaas file.
But I think the kerberos infrastructure would refuse to validate it, so the SASL handshake would still fail

2. Modify the client bootstrap mechanism to detect whether bootstrap.servers contains a dns alias. If it does, resolve and expand the alias to retrieve all hostnames behind it and add them to the list of nodes.
This could be done by modifying parseAndValidateAddresses() in ClientUtils

3. Add a cluster.alias parameter that would be handled by the logic above. Having another parameter to avoid confusion on how bootstrap.servers works behind the scene.

Thoughts ?
I would be happy to contribute the change for any of the options.

I believe the ability to use a dns alias instead of static lists of brokers would bring good deployment flexibility.
","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]

** Comment 4 **
Thanks for taking a look ! Yes exactly !  your code looks very similar to what I tried and tested.On my side I had                    for (InetAddress inetAddress : inetAddresses) {                        String resolvedCanonicalName = inetAddress.getCanonicalHostName();                        InetSocketAddress address = new InetSocketAddress(resolvedCanonicalName, port);                        if (address.isUnresolved()) {                            log.warn(""Removing server {} from {} as DNS resolution failed for {}"", url, CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, host);                        } else {                            addresses.add(address);                        }                    }What rationale was behind your change ? Was it just to support dns names ?I see it wasn't merged in the end so was there unforeseen issues coming with it ?

** Comment 5 **
[Comment excluded]

** Comment 6 **
[Comment excluded]

** Comment 7 **
[Comment excluded]

** Comment 8 **
lepolac opened a new pull request #4485: KAFKA-6195: Resolve DNS aliases in bootstrap.server
URL: [link]
   Change described in KIP-235   [link]   I license the work to the Apache Kafka project under the project's open source license.----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 9 **
[Comment excluded]

** Comment 10 **
[Comment excluded]

** Comment 11 **
 Sorry for the delay, I was out till today. I had a look at the PR. I think it will break SSL hostname verification when IP addresses are used instead of hostnames. At the moment, we can create certificates with broker IP address in the SubjectAlternativeName. Clients using IP addresses in the bootstrap server list connect successfully since the address used for connection is the same as the address in the certificate. With the changes in the PR, clients will do reverse DNS lookup and match the hostname returned by the lookup against the IP address in the certificate and fail SSL handshake.This is a useful feature and the code makes sense, but I think it needs to be optional. Is there some way we can specify that a reverse DNS lookup is required (eg. in the bootstrap server list itself as part of the URL)?

** Comment 12 **
[Comment excluded]

** Comment 13 **
[Comment excluded]

** Comment 14 **
[Comment excluded]

** Comment 15 **
[Comment excluded]

** Comment 16 **
rajinisivaram closed pull request #4485: KAFKA-6195: Resolve DNS aliases in bootstrap.server
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git [file java] [file java]
similarity index 88%
rename from [file java]
rename to [file java]
index 4a013b96ff8..96d47c344a0 100644
--- [file java]
+++ [file java]
@@ -14,14 +14,15 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
-package org.apache.kafka.common.config;
+package org.apache.kafka.clients;
 import java.util.Locale;
 public enum ClientDnsLookup {
     DEFAULT(""default""),
-    USE_ALL_DNS_IPS(""use_all_dns_ips"");
+    USE_ALL_DNS_IPS(""use_all_dns_ips""),
+    RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY(""resolve_canonical_bootstrap_servers_only"");
     private String clientDnsLookup;
diff --git [file java] [file java]
index 1661ea39695..dce5f3fb65c 100644
--- [file java]
+++ [file java]
@@ -17,7 +17,6 @@
 package org.apache.kafka.clients;
 import org.apache.kafka.common.config.AbstractConfig;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.config.ConfigException;
 import org.apache.kafka.common.config.SaslConfigs;
 import org.apache.kafka.common.network.ChannelBuilder;
@@ -42,10 +41,12 @@
 public final class ClientUtils {
     private static final Logger log = LoggerFactory.getLogger(ClientUtils.class);
-    private ClientUtils() {}
+    private ClientUtils() {
+    }
-    public static List<InetSocketAddress> parseAndValidateAddresses(List<String> urls) {
+    public static List<InetSocketAddress> parseAndValidateAddresses(List<String> urls, String clientDnsLookup) {
         List<InetSocketAddress> addresses = new ArrayList<>();
+        ClientDnsLookup clientDnsLookupBehaviour = ClientDnsLookup.forConfig(clientDnsLookup);
         for (String url : urls) {
             if (url url.isEmpty()) {
                 try {
@@ -54,15 +55,30 @@ private ClientUtils() {}
                     if (host == null || port == null)
                         throw new ConfigException(""Invalid url in "" + CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG + "": "" + url);
-                    InetSocketAddress address = new InetSocketAddress(host, port);
-
-                    if (address.isUnresolved()) {
-                        log.warn(""Removing server {} from {} as DNS resolution failed for {}"", url, CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, host);
+                    if (clientDnsLookupBehaviour == ClientDnsLookup.RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY) {
+                        InetAddress inetAddresses = InetAddress.getAllByName(host);
+                        for (InetAddress inetAddress : inetAddresses) {
+                            String resolvedCanonicalName = inetAddress.getCanonicalHostName();
+                            InetSocketAddress address = new InetSocketAddress(resolvedCanonicalName, port);
+                            if (address.isUnresolved()) {
+                                log.warn(""Couldn't resolve server {} from {} as DNS resolution of the canonical hostname [} failed for {}"", url, CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, resolvedCanonicalName, host);
+                            } else {
+                                addresses.add(address);
+                            }
+                        }
                     } else {
-                        addresses.add(address);
+                        InetSocketAddress address = new InetSocketAddress(host, port);
+                        if (address.isUnresolved()) {
+                            log.warn(""Couldn't resolve server {} from {} as DNS resolution failed for {}"", url, CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, host);
+                        } else {
+                            addresses.add(address);
+                        }
                     }
+
                 } catch (IllegalArgumentException e) {
                     throw new ConfigException(""Invalid port in "" + CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG + "": "" + url);
+                } catch (UnknownHostException e) {
+                    throw new ConfigException(""Unknown host in "" + CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG + "": "" + url);
                 }
             }
         }
diff --git [file java] [file java]
index b697de73c85..f198533525b 100644
--- [file java]
+++ [file java]
@@ -18,7 +18,6 @@
 import java.util.concurrent.ThreadLocalRandom;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.errors.AuthenticationException;
 import java.net.InetAddress;
diff --git [file java] [file java]
index b179f0272e8..c8e2357b0a6 100644
--- [file java]
+++ [file java]
@@ -41,6 +41,11 @@
                                                        + ""discover the full cluster membership (which may change dynamically), this list need not contain the full set of ""
                                                        + ""servers (you may want more than one, though, in case a server is down)."";
+    public static final String CLIENT_DNS_LOOKUP_CONFIG = ""client.dns.lookup"";
+    public static final String CLIENT_DNS_LOOKUP_DOC = ""<p>Controls how the client uses DNS lookups.</p><p>If set to <code>use_all_dns_ips</code> then, when the lookup returns multiple IP addresses for a hostname,""
+            + "" they will all be attempted to connect to before failing the connection. Applies to both bootstrap and advertised servers.</p>""
+            + ""<p>If the value is <code>resolve_canonical_bootstrap_servers_only</code> each entry will be resolved and expanded into a list of canonical names.</p>"";
+
     public static final String METADATA_MAX_AGE_CONFIG = ""metadata.max.age.ms"";
     public static final String METADATA_MAX_AGE_DOC = ""The period of time in milliseconds after which we force a refresh of metadata even if we haven't seen any partition leadership changes to proactively discover any new brokers or partitions."";
@@ -93,9 +98,6 @@
                                                          + ""elapses the client will resend the request if necessary or fail the request if ""
                                                          + ""retries are exhausted."";
-    public static final String CLIENT_DNS_LOOKUP_CONFIG = ""client.dns.lookup"";
-    public static final String CLIENT_DNS_LOOKUP_DOC = ""<p>Controls how the client uses DNS lookups.</p><p>If set to <code>use_all_dns_ips</code> then, when the lookup returns multiple IP addresses for a hostname,""
-                                                        + "" they will all be attempted to connect to before failing the connection. Applies to both bootstrap and advertised servers.</p>"";
     /**
      * Postprocess the configuration so that exponential backoff is disabled when reconnect backoff
diff --git [file java] [file java]
index 7ea05f69f3b..c6f0c0b3fb6 100644
--- [file java]
+++ [file java]
@@ -19,7 +19,6 @@
 import org.apache.kafka.common.Cluster;
 import org.apache.kafka.common.Node;
 import org.apache.kafka.common.TopicPartition;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.errors.AuthenticationException;
 import org.apache.kafka.common.errors.UnsupportedVersionException;
 import org.apache.kafka.common.metrics.Sensor;
diff --git [file java] [file java]
index a051de29fb7..47c76ac36af 100644
--- [file java]
+++ [file java]
@@ -17,9 +17,9 @@
 package org.apache.kafka.clients.admin;
+import org.apache.kafka.clients.ClientDnsLookup;
 import org.apache.kafka.clients.CommonClientConfigs;
 import org.apache.kafka.common.config.AbstractConfig;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.config.ConfigDef;
 import org.apache.kafka.common.config.ConfigDef.Importance;
 import org.apache.kafka.common.config.ConfigDef.Type;
@@ -43,6 +43,12 @@
     public static final String BOOTSTRAP_SERVERS_CONFIG = CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG;
     private static final String BOOTSTRAP_SERVERS_DOC = CommonClientConfigs.BOOTSTRAP_SERVERS_DOC;
+    /**
+     * <code>client.dns.lookup</code>
+     */
+    public static final String CLIENT_DNS_LOOKUP_CONFIG = CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG;
+    private static final String CLIENT_DNS_LOOKUP_DOC = CommonClientConfigs.CLIENT_DNS_LOOKUP_DOC;
+
     /**
      * <code>reconnect.backoff.ms</code>
      */
@@ -159,12 +165,14 @@
                                         in(Sensor.RecordingLevel.INFO.toString(), Sensor.RecordingLevel.DEBUG.toString()),
                                         Importance.LOW,
                                         METRICS_RECORDING_LEVEL_DOC)
-                                .define(CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG,
+                                .define(CLIENT_DNS_LOOKUP_CONFIG,
                                         Type.STRING,
                                         ClientDnsLookup.DEFAULT.toString(),
-                                        in(ClientDnsLookup.DEFAULT.toString(), ClientDnsLookup.USE_ALL_DNS_IPS.toString()),
+                                        in(ClientDnsLookup.DEFAULT.toString(),
+                                           ClientDnsLookup.USE_ALL_DNS_IPS.toString(),
+                                           ClientDnsLookup.RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY.toString()),
                                         Importance.MEDIUM,
-                                        CommonClientConfigs.CLIENT_DNS_LOOKUP_DOC)
+                                        CLIENT_DNS_LOOKUP_DOC)
                                 // security support
                                 .define(SECURITY_PROTOCOL_CONFIG,
                                         Type.STRING,
diff --git [file java] [file java]
index 86e14476625..c8418c173f4 100644
--- [file java]
+++ [file java]
@@ -18,10 +18,10 @@
 package org.apache.kafka.clients.admin;
 import org.apache.kafka.clients.ApiVersions;
+import org.apache.kafka.clients.ClientDnsLookup;
 import org.apache.kafka.clients.ClientRequest;
 import org.apache.kafka.clients.ClientResponse;
 import org.apache.kafka.clients.ClientUtils;
-import org.apache.kafka.clients.CommonClientConfigs;
 import org.apache.kafka.clients.KafkaClient;
 import org.apache.kafka.clients.NetworkClient;
 import org.apache.kafka.clients.StaleMetadataException;
@@ -46,7 +46,6 @@
 import org.apache.kafka.common.acl.AclBinding;
 import org.apache.kafka.common.acl.AclBindingFilter;
 import org.apache.kafka.common.annotation.InterfaceStability;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.config.ConfigResource;
 import org.apache.kafka.common.errors.ApiException;
 import org.apache.kafka.common.errors.AuthenticationException;
@@ -362,7 +361,7 @@ static KafkaAdminClient createInternal(AdminClientConfig config, TimeoutProcesso
                 config.getInt(AdminClientConfig.SEND_BUFFER_CONFIG),
                 config.getInt(AdminClientConfig.RECEIVE_BUFFER_CONFIG),
                 (int) TimeUnit.HOURS.toMillis(1),
-                ClientDnsLookup.forConfig(config.getString(CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG)),
+                ClientDnsLookup.forConfig(config.getString(AdminClientConfig.CLIENT_DNS_LOOKUP_CONFIG)),
                 time,
                 true,
                 apiVersions,
@@ -414,7 +413,8 @@ private KafkaAdminClient(AdminClientConfig config,
         this.time = time;
         this.metadataManager = metadataManager;
         List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(
-            config.getList(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG));
+            config.getList(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG),
+            config.getString(AdminClientConfig.CLIENT_DNS_LOOKUP_CONFIG));
         metadataManager.update(Cluster.bootstrap(addresses), time.milliseconds());
         this.metrics = metrics;
         this.client = client;
diff --git [file java] [file java]
index a1c9dc250aa..795a762a494 100644
--- [file java]
+++ [file java]
@@ -16,9 +16,9 @@
  */
 package org.apache.kafka.clients.consumer;
+import org.apache.kafka.clients.ClientDnsLookup;
 import org.apache.kafka.clients.CommonClientConfigs;
 import org.apache.kafka.common.config.AbstractConfig;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.config.ConfigDef;
 import org.apache.kafka.common.config.ConfigDef.Importance;
 import org.apache.kafka.common.config.ConfigDef.Type;
@@ -90,6 +90,9 @@
      */
     public static final String BOOTSTRAP_SERVERS_CONFIG = CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG;
+    /** <code>client.dns.lookup</code> */
+    public static final String CLIENT_DNS_LOOKUP_CONFIG = CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG;
+
     /**
      * <code>enable.auto.commit</code>
      */
@@ -258,7 +261,7 @@
             "" return the LSO"";
     public static final String DEFAULT_ISOLATION_LEVEL = IsolationLevel.READ_UNCOMMITTED.toString().toLowerCase(Locale.ROOT);
-    
+
     static {
         CONFIG = new ConfigDef().define(BOOTSTRAP_SERVERS_CONFIG,
                                         Type.LIST,
@@ -266,6 +269,14 @@
                                         new ConfigDef.NonNullValidator(),
                                         Importance.HIGH,
                                         CommonClientConfigs.BOOTSTRAP_SERVERS_DOC)
+                                .define(CLIENT_DNS_LOOKUP_CONFIG,
+                                        Type.STRING,
+                                        ClientDnsLookup.DEFAULT.toString(),
+                                        in(ClientDnsLookup.DEFAULT.toString(),
+                                           ClientDnsLookup.USE_ALL_DNS_IPS.toString(),
+                                           ClientDnsLookup.RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY.toString()),
+                                        Importance.MEDIUM,
+                                        CommonClientConfigs.CLIENT_DNS_LOOKUP_DOC)
                                 .define(GROUP_ID_CONFIG, Type.STRING, """", Importance.HIGH, GROUP_ID_DOC)
                                 .define(SESSION_TIMEOUT_MS_CONFIG,
                                         Type.INT,
@@ -453,12 +464,6 @@
                                         in(IsolationLevel.READ_COMMITTED.toString().toLowerCase(Locale.ROOT), IsolationLevel.READ_UNCOMMITTED.toString().toLowerCase(Locale.ROOT)),
                                         Importance.MEDIUM,
                                         ISOLATION_LEVEL_DOC)
-                                .define(CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG,
-                                        Type.STRING,
-                                        ClientDnsLookup.DEFAULT.toString(),
-                                        in(ClientDnsLookup.DEFAULT.toString(), ClientDnsLookup.USE_ALL_DNS_IPS.toString()),
-                                        Importance.MEDIUM,
-                                        CommonClientConfigs.CLIENT_DNS_LOOKUP_DOC)
                                 // security support
                                 .define(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,
                                         Type.STRING,
diff --git [file java] [file java]
index e79ff07cff0..4061373c450 100644
--- [file java]
+++ [file java]
@@ -17,8 +17,8 @@
 package org.apache.kafka.clients.consumer;
 import org.apache.kafka.clients.ApiVersions;
+import org.apache.kafka.clients.ClientDnsLookup;
 import org.apache.kafka.clients.ClientUtils;
-import org.apache.kafka.clients.CommonClientConfigs;
 import org.apache.kafka.clients.Metadata;
 import org.apache.kafka.clients.NetworkClient;
 import org.apache.kafka.clients.consumer.internals.ConsumerCoordinator;
@@ -36,7 +36,6 @@
 import org.apache.kafka.common.MetricName;
 import org.apache.kafka.common.PartitionInfo;
 import org.apache.kafka.common.TopicPartition;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.errors.InterruptException;
 import org.apache.kafka.common.errors.TimeoutException;
 import org.apache.kafka.common.internals.ClusterResourceListeners;
@@ -710,8 +709,10 @@ private KafkaConsumer(ConsumerConfig config,
             ClusterResourceListeners clusterResourceListeners = configureClusterResourceListeners(keyDeserializer, valueDeserializer, reporters, interceptorList);
             this.metadata = new Metadata(retryBackoffMs, config.getLong(ConsumerConfig.METADATA_MAX_AGE_CONFIG),
                     true, false, clusterResourceListeners);
-            List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(config.getList(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG));
-            this.metadata.update(Cluster.bootstrap(addresses), Collections.emptySet(), 0);
+            List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(
+                    config.getList(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG),
+                    config.getString(ConsumerConfig.CLIENT_DNS_LOOKUP_CONFIG));
+            this.metadata.update(Cluster.bootstrap(addresses), Collections.<String>emptySet(), 0);
             String metricGrpPrefix = ""consumer"";
             ConsumerMetrics metricsRegistry = new ConsumerMetrics(metricsTags.keySet(), ""consumer"");
             ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(config);
@@ -732,7 +733,7 @@ private KafkaConsumer(ConsumerConfig config,
                     config.getInt(ConsumerConfig.SEND_BUFFER_CONFIG),
                     config.getInt(ConsumerConfig.RECEIVE_BUFFER_CONFIG),
                     config.getInt(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG),
-                    ClientDnsLookup.forConfig(config.getString(CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG)),
+                    ClientDnsLookup.forConfig(config.getString(ConsumerConfig.CLIENT_DNS_LOOKUP_CONFIG)),
                     time,
                     true,
                     new ApiVersions(),
diff --git [file java] [file java]
index 465e60f449c..c68a014619a 100644
--- [file java]
+++ [file java]
@@ -28,8 +28,8 @@
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReference;
 import org.apache.kafka.clients.ApiVersions;
+import org.apache.kafka.clients.ClientDnsLookup;
 import org.apache.kafka.clients.ClientUtils;
-import org.apache.kafka.clients.CommonClientConfigs;
 import org.apache.kafka.clients.KafkaClient;
 import org.apache.kafka.clients.Metadata;
 import org.apache.kafka.clients.NetworkClient;
@@ -49,7 +49,6 @@
 import org.apache.kafka.common.MetricName;
 import org.apache.kafka.common.PartitionInfo;
 import org.apache.kafka.common.TopicPartition;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.config.ConfigException;
 import org.apache.kafka.common.errors.ApiException;
 import org.apache.kafka.common.errors.AuthenticationException;
@@ -407,7 +406,9 @@ public KafkaProducer(Properties properties, Serializer<K> keySerializer, Seriali
                     apiVersions,
                     transactionManager,
                     new BufferPool(this.totalMemorySize, config.getInt(ProducerConfig.BATCH_SIZE_CONFIG), metrics, time, PRODUCER_METRIC_GROUP_NAME));
-            List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(config.getList(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG));
+            List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(
+                    config.getList(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG),
+                    config.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG));
             if (metadata != null) {
                 this.metadata = metadata;
             } else {
@@ -449,7 +450,7 @@ Sender newSender(LogContext logContext, KafkaClient kafkaClient, Metadata metada
                 producerConfig.getInt(ProducerConfig.SEND_BUFFER_CONFIG),
                 producerConfig.getInt(ProducerConfig.RECEIVE_BUFFER_CONFIG),
                 requestTimeoutMs,
-                ClientDnsLookup.forConfig(producerConfig.getString(CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG)),
+                ClientDnsLookup.forConfig(producerConfig.getString(ProducerConfig.CLIENT_DNS_LOOKUP_CONFIG)),
                 time,
                 true,
                 apiVersions,
@@ -496,7 +497,9 @@ private static int configureDeliveryTimeout(ProducerConfig config, Logger log) {
     }
     private static TransactionManager configureTransactionState(ProducerConfig config, LogContext logContext, Logger log) {
+
         TransactionManager transactionManager = null;
+
         boolean userConfiguredIdempotence = false;
         if (config.originals().containsKey(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG))
             userConfiguredIdempotence = true;
diff --git [file java] [file java]
index f08159d2e37..c63477d97dc 100644
--- [file java]
+++ [file java]
@@ -16,10 +16,10 @@
  */
 package org.apache.kafka.clients.producer;
+import org.apache.kafka.clients.ClientDnsLookup;
 import org.apache.kafka.clients.CommonClientConfigs;
 import org.apache.kafka.clients.producer.internals.DefaultPartitioner;
 import org.apache.kafka.common.config.AbstractConfig;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.config.ConfigDef;
 import org.apache.kafka.common.config.ConfigDef.Importance;
 import org.apache.kafka.common.config.ConfigDef.Type;
@@ -52,6 +52,9 @@
     /** <code>bootstrap.servers</code> */
     public static final String BOOTSTRAP_SERVERS_CONFIG = CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG;
+    /** <code>client.dns.lookup</code> */
+    public static final String CLIENT_DNS_LOOKUP_CONFIG = CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG;
+
     /** <code>metadata.max.age.ms</code> */
     public static final String METADATA_MAX_AGE_CONFIG = CommonClientConfigs.METADATA_MAX_AGE_CONFIG;
     private static final String METADATA_MAX_AGE_DOC = CommonClientConfigs.METADATA_MAX_AGE_DOC;
@@ -239,6 +242,14 @@
     static {
         CONFIG = new ConfigDef().define(BOOTSTRAP_SERVERS_CONFIG, Type.LIST, Collections.emptyList(), new ConfigDef.NonNullValidator(), Importance.HIGH, CommonClientConfigs.BOOTSTRAP_SERVERS_DOC)
+                                .define(CLIENT_DNS_LOOKUP_CONFIG,
+                                        Type.STRING,
+                                        ClientDnsLookup.DEFAULT.toString(),
+                                        in(ClientDnsLookup.DEFAULT.toString(),
+                                           ClientDnsLookup.USE_ALL_DNS_IPS.toString(),
+                                           ClientDnsLookup.RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY.toString()),
+                                        Importance.MEDIUM,
+                                        CommonClientConfigs.CLIENT_DNS_LOOKUP_DOC)
                                 .define(BUFFER_MEMORY_CONFIG, Type.LONG, 32 * 1024 * 1024L, atLeast(0L), Importance.HIGH, BUFFER_MEMORY_DOC)
                                 .define(RETRIES_CONFIG, Type.INT, Integer.MAX_VALUE, between(0, Integer.MAX_VALUE), Importance.HIGH, RETRIES_DOC)
                                 .define(ACKS_CONFIG,
@@ -347,13 +358,7 @@
                                         null,
                                         new ConfigDef.NonEmptyString(),
                                         Importance.LOW,
-                                        TRANSACTIONAL_ID_DOC)
-                                .define(CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG,
-                                        Type.STRING,
-                                        ClientDnsLookup.DEFAULT.toString(),
-                                        in(ClientDnsLookup.DEFAULT.toString(), ClientDnsLookup.USE_ALL_DNS_IPS.toString()),
-                                        Importance.MEDIUM,
-                                        CommonClientConfigs.CLIENT_DNS_LOOKUP_DOC);
+                                        TRANSACTIONAL_ID_DOC);
     }
     @Override
diff --git [file java] [file java]
index bea464f14a5..35f52a93c1c 100644
--- [file java]
+++ [file java]
@@ -16,7 +16,6 @@
  */
 package org.apache.kafka.clients;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.config.ConfigException;
 import org.junit.Test;
 import static org.junit.Assert.assertEquals;
@@ -31,27 +30,46 @@
 public class ClientUtilsTest {
+
     @Test
-    public void testParseAndValidateAddresses() {
-        check(""127.0.0.1:8000"");
-        check(""mydomain.com:8080"");
-        check("":8000"");
-        check("":1234"", ""mydomain.com:10000"");
-        List<InetSocketAddress> validatedAddresses = check(""some.invalid.hostname.foo.bar.local:9999"", ""mydomain.com:10000"");
+    public void testParseAndValidateAddresses() throws UnknownHostException {
+        checkWithoutLookup(""127.0.0.1:8000"");
+        checkWithoutLookup(""localhost:8080"");
+        checkWithoutLookup("":8000"");
+        checkWithoutLookup("":1234"", ""localhost:10000"");
+        List<InetSocketAddress> validatedAddresses = checkWithoutLookup(""localhost:10000"");
         assertEquals(1, validatedAddresses.size());
         InetSocketAddress onlyAddress = validatedAddresses.get(0);
-        assertEquals(""mydomain.com"", onlyAddress.getHostName());
+        assertEquals(""localhost"", onlyAddress.getHostName());
         assertEquals(10000, onlyAddress.getPort());
     }
+    @Test
+    public void testParseAndValidateAddressesWithReverseLookup() {
+        checkWithoutLookup(""127.0.0.1:8000"");
+        checkWithoutLookup(""localhost:8080"");
+        checkWithoutLookup("":8000"");
+        checkWithoutLookup("":1234"", ""localhost:10000"");
+        List<InetSocketAddress> validatedAddresses = checkWithLookup(Arrays.asList(""example.com:10000""));
+        assertEquals(2, validatedAddresses.size());
+        InetSocketAddress address = validatedAddresses.get(0);
+        assertEquals(""93.184.216.34"", address.getHostName());
+        assertEquals(10000, address.getPort());
+    }
+
+    @Test(expected = IllegalArgumentException.class)
+    public void testInvalidConfig() {
+        ClientUtils.parseAndValidateAddresses(Arrays.asList(""localhost:10000""), ""random.value"");
+    }
+
     @Test(expected = ConfigException.class)
     public void testNoPort() {
-        check(""127.0.0.1"");
+        checkWithoutLookup(""127.0.0.1"");
     }
     @Test(expected = ConfigException.class)
     public void testOnlyBadHostname() {
-        check(""some.invalid.hostname.foo.bar.local:9999"");
+        checkWithoutLookup(""some.invalid.hostname.foo.bar.local:9999"");
     }
     @Test
@@ -87,7 +105,12 @@ public void testResolveDnsLookupAllIps() throws UnknownHostException {
         assertEquals(2, ClientUtils.resolve(""kafka.apache.org"", ClientDnsLookup.USE_ALL_DNS_IPS).size());
     }
-    private List<InetSocketAddress> check(String... url) {
-        return ClientUtils.parseAndValidateAddresses(Arrays.asList(url));
+    private List<InetSocketAddress> checkWithoutLookup(String... url) {
+        return ClientUtils.parseAndValidateAddresses(Arrays.asList(url), ClientDnsLookup.DEFAULT.toString());
+    }
+
+    private List<InetSocketAddress> checkWithLookup(List<String> url) {
+        return ClientUtils.parseAndValidateAddresses(url, ClientDnsLookup.RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY.toString());
     }
+
 }
diff --git [file java] [file java]
index d4a2a55dd4b..23edaa999ef 100644
--- [file java]
+++ [file java]
@@ -27,7 +27,6 @@
 import java.net.InetAddress;
 import java.net.UnknownHostException;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.errors.AuthenticationException;
 import org.apache.kafka.common.utils.MockTime;
 import org.junit.Before;
diff --git [file java] [file java]
index aaf827feee3..8abe9a40085 100644
--- [file java]
+++ [file java]
@@ -19,7 +19,6 @@
 import org.apache.kafka.common.Cluster;
 import org.apache.kafka.common.Node;
 import org.apache.kafka.common.TopicPartition;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.network.NetworkReceive;
 import org.apache.kafka.common.protocol.ApiKeys;
 import org.apache.kafka.common.protocol.CommonFields;
diff --git [file java] [file java]
index 42deee7c204..94d8d5b1afb 100644
--- [file java]
+++ [file java]
@@ -17,6 +17,7 @@
 package org.apache.kafka.clients.consumer.internals;
 import org.apache.kafka.clients.ApiVersions;
+import org.apache.kafka.clients.ClientDnsLookup;
 import org.apache.kafka.clients.ClientRequest;
 import org.apache.kafka.clients.ClientUtils;
 import org.apache.kafka.clients.FetchSessionHandler;
@@ -36,7 +37,6 @@
 import org.apache.kafka.common.Node;
 import org.apache.kafka.common.PartitionInfo;
 import org.apache.kafka.common.TopicPartition;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.errors.InvalidTopicException;
 import org.apache.kafka.common.errors.RecordTooLargeException;
 import org.apache.kafka.common.errors.SerializationException;
@@ -2735,7 +2735,7 @@ private void testGetOffsetsForTimesWithError(Errors errorForP0,
         String topicName2 = ""topic2"";
         TopicPartition t2p0 = new TopicPartition(topicName2, 0);
         // Expect a metadata refresh.
-        metadata.update(Cluster.bootstrap(ClientUtils.parseAndValidateAddresses(Collections.singletonList(""1.1.1.1:1111""))),
+        metadata.update(Cluster.bootstrap(ClientUtils.parseAndValidateAddresses(Collections.singletonList(""1.1.1.1:1111""), ClientDnsLookup.DEFAULT.toString())),
                         Collections.<String>emptySet(),
                         time.milliseconds());
diff --git [file java] [file java]
index 6d4d78cb70f..23ca2aeaed7 100644
--- [file java]
+++ [file java]
@@ -32,6 +32,7 @@
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicReference;
 import org.apache.kafka.clients.ApiVersions;
+import org.apache.kafka.clients.ClientDnsLookup;
 import org.apache.kafka.clients.ClientRequest;
 import org.apache.kafka.clients.Metadata;
 import org.apache.kafka.clients.MockClient;
@@ -44,7 +45,6 @@
 import org.apache.kafka.common.MetricNameTemplate;
 import org.apache.kafka.common.Node;
 import org.apache.kafka.common.TopicPartition;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.errors.ClusterAuthorizationException;
 import org.apache.kafka.common.errors.NetworkException;
 import org.apache.kafka.common.errors.OutOfOrderSequenceException;
diff --git [file java] [file java]
index aac9fb24441..be3a70991f0 100644
--- [file java]
+++ [file java]
@@ -16,9 +16,9 @@
  */
 package org.apache.kafka.connect.runtime;
+import org.apache.kafka.clients.ClientDnsLookup;
 import org.apache.kafka.clients.CommonClientConfigs;
 import org.apache.kafka.common.config.AbstractConfig;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.config.ConfigDef;
 import org.apache.kafka.common.config.ConfigDef.Importance;
 import org.apache.kafka.common.config.ConfigDef.Type;
@@ -61,6 +61,9 @@
             + ""than one, though, in case a server is down)."";
     public static final String BOOTSTRAP_SERVERS_DEFAULT = ""localhost:9092"";
+    public static final String CLIENT_DNS_LOOKUP_CONFIG = CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG;
+    public static final String CLIENT_DNS_LOOKUP_DOC = CommonClientConfigs.CLIENT_DNS_LOOKUP_DOC;
+
     public static final String KEY_CONVERTER_CLASS_CONFIG = ""key.converter"";
     public static final String KEY_CONVERTER_CLASS_DOC =
             ""Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka."" +
@@ -223,6 +226,14 @@ protected static ConfigDef baseConfigDef() {
         return new ConfigDef()
                 .define(BOOTSTRAP_SERVERS_CONFIG, Type.LIST, BOOTSTRAP_SERVERS_DEFAULT,
                         Importance.HIGH, BOOTSTRAP_SERVERS_DOC)
+                .define(CLIENT_DNS_LOOKUP_CONFIG,
+                        Type.STRING,
+                        ClientDnsLookup.DEFAULT.toString(),
+                        in(ClientDnsLookup.DEFAULT.toString(),
+                           ClientDnsLookup.USE_ALL_DNS_IPS.toString(),
+                           ClientDnsLookup.RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY.toString()),
+                        Importance.MEDIUM,
+                        CLIENT_DNS_LOOKUP_DOC)
                 .define(KEY_CONVERTER_CLASS_CONFIG, Type.CLASS,
                         Importance.HIGH, KEY_CONVERTER_CLASS_DOC)
                 .define(VALUE_CONVERTER_CLASS_CONFIG, Type.CLASS,
@@ -278,13 +289,7 @@ protected static ConfigDef baseConfigDef() {
                         Collections.emptyList(),
                         Importance.LOW, CONFIG_PROVIDERS_DOC)
                 .define(REST_EXTENSION_CLASSES_CONFIG, Type.LIST, """",
-                        Importance.LOW, REST_EXTENSION_CLASSES_DOC)
-                .define(CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG,
-                        Type.STRING,
-                        ClientDnsLookup.DEFAULT.toString(),
-                        in(ClientDnsLookup.DEFAULT.toString(), ClientDnsLookup.USE_ALL_DNS_IPS.toString()),
-                        Importance.MEDIUM,
-                        CommonClientConfigs.CLIENT_DNS_LOOKUP_DOC);    
+                        Importance.LOW, REST_EXTENSION_CLASSES_DOC);
     }
     private void logInternalConverterDeprecationWarnings(Map<String, String> props) {
diff --git [file java] [file java]
index de8e8b27b74..5725ff5f554 100644
--- [file java]
+++ [file java]
@@ -17,6 +17,7 @@
 package org.apache.kafka.connect.runtime.distributed;
 import org.apache.kafka.clients.ApiVersions;
+import org.apache.kafka.clients.ClientDnsLookup;
 import org.apache.kafka.clients.ClientUtils;
 import org.apache.kafka.clients.CommonClientConfigs;
 import org.apache.kafka.clients.Metadata;
@@ -24,7 +25,6 @@
 import org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient;
 import org.apache.kafka.common.Cluster;
 import org.apache.kafka.common.KafkaException;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.metrics.JmxReporter;
 import org.apache.kafka.common.metrics.MetricConfig;
 import org.apache.kafka.common.metrics.Metrics;
@@ -96,7 +96,9 @@ public WorkerGroupMember(DistributedConfig config,
             this.metrics = new Metrics(metricConfig, reporters, time);
             this.retryBackoffMs = config.getLong(CommonClientConfigs.RETRY_BACKOFF_MS_CONFIG);
             this.metadata = new Metadata(retryBackoffMs, config.getLong(CommonClientConfigs.METADATA_MAX_AGE_CONFIG), true);
-            List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(config.getList(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG));
+            List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(
+                    config.getList(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG),
+                    config.getString(CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG));
             this.metadata.update(Cluster.bootstrap(addresses), Collections.<String>emptySet(), 0);
             String metricGrpPrefix = ""connect"";
             ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(config);
diff --git a/core/src/main/scala/kafka/admin/AdminClient.scala b/core/src/main/scala/kafka/admin/AdminClient.scala
index 5876b6ec460..aaa09035b50 100644
--- a/core/src/main/scala/kafka/admin/AdminClient.scala
+++ b/core/src/main/scala/kafka/admin/AdminClient.scala
@@ -23,6 +23,7 @@ import kafka.coordinator.group.GroupOverview
 import kafka.utils.Logging
 import org.apache.kafka.clients._
 import org.apache.kafka.clients.consumer.internals.{ConsumerNetworkClient, ConsumerProtocol, RequestFuture}
+import org.apache.kafka.common.config.ConfigDef.ValidString._
 import org.apache.kafka.common.config.ConfigDef.{Importance, Type}
 import org.apache.kafka.common.config.{AbstractConfig, ConfigDef}
 import org.apache.kafka.common.errors.{AuthenticationException, TimeoutException}
@@ -39,7 +40,6 @@ import org.apache.kafka.common.{Cluster, Node, TopicPartition}
 import scala.collection.JavaConverters._
 import scala.util.{Failure, Success, Try}
-import org.apache.kafka.common.config.ClientDnsLookup
 /**
   * A Scala administrative client for Kafka which supports managing and inspecting topics, brokers,
@@ -386,6 +386,14 @@ object AdminClient {
         Type.LIST,
         Importance.HIGH,
         CommonClientConfigs.BOOTSTRAP_SERVERS_DOC)
+      .define(CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG,
+        Type.STRING,
+        ClientDnsLookup.DEFAULT.toString,
+        in(ClientDnsLookup.DEFAULT.toString,
+           ClientDnsLookup.USE_ALL_DNS_IPS.toString,
+           ClientDnsLookup.RESOLVE_CANONICAL_BOOTSTRAP_SERVERS_ONLY.toString),
+        Importance.MEDIUM,
+        CommonClientConfigs.CLIENT_DNS_LOOKUP_DOC)
       .define(
         CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,
         ConfigDef.Type.STRING,
@@ -429,7 +437,8 @@ object AdminClient {
     val retryBackoffMs = config.getLong(CommonClientConfigs.RETRY_BACKOFF_MS_CONFIG)
     val brokerUrls = config.getList(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG)
-    val brokerAddresses = ClientUtils.parseAndValidateAddresses(brokerUrls)
+    val clientDnsLookup = config.getString(CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG)
+    val brokerAddresses = ClientUtils.parseAndValidateAddresses(brokerUrls, clientDnsLookup)
     val bootstrapCluster = Cluster.bootstrap(brokerAddresses)
     metadata.update(bootstrapCluster, Collections.emptySet(), 0)
diff --git a/core/src/main/scala/kafka/controller/ControllerChannelManager.scala b/core/src/main/scala/kafka/controller/ControllerChannelManager.scala
index 86b6f94bdd3..7002219efd2 100755
--- a/core/src/main/scala/kafka/controller/ControllerChannelManager.scala
+++ b/core/src/main/scala/kafka/controller/ControllerChannelManager.scala
@@ -39,8 +39,6 @@ import org.apache.kafka.common.{KafkaException, Node, TopicPartition}
 import scala.collection.JavaConverters._
 import scala.collection.mutable.HashMap
 import scala.collection.{Set, mutable}
-import org.apache.kafka.common.config.ClientDnsLookup
-
 object ControllerChannelManager {
   val QueueSizeMetricName = ""QueueSize""
diff --git a/core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerChannelManager.scala b/core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerChannelManager.scala
index d0e765c5a41..bd25d94e916 100644
--- a/core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerChannelManager.scala
+++ b/core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerChannelManager.scala
@@ -36,7 +36,6 @@ import java.util.concurrent.{BlockingQueue, ConcurrentHashMap, LinkedBlockingQue
 import collection.JavaConverters._
 import scala.collection.{concurrent, immutable}
-import org.apache.kafka.common.config.ClientDnsLookup
 object TransactionMarkerChannelManager {
   def apply(config: KafkaConfig,
diff --git a/core/src/main/scala/kafka/server/KafkaServer.scala b/core/src/main/scala/kafka/server/KafkaServer.scala
index 841ea8272fa..bef0663c26f 100755
--- a/core/src/main/scala/kafka/server/KafkaServer.scala
+++ b/core/src/main/scala/kafka/server/KafkaServer.scala
@@ -37,7 +37,7 @@ import kafka.security.CredentialProvider
 import kafka.security.auth.Authorizer
 import kafka.utils._
 import kafka.zk.{BrokerInfo, KafkaZkClient}
-import org.apache.kafka.clients.{ApiVersions, ManualMetadataUpdater, NetworkClient, NetworkClientUtils}
+import org.apache.kafka.clients.{ApiVersions, ClientDnsLookup, ManualMetadataUpdater, NetworkClient, NetworkClientUtils}
 import org.apache.kafka.common.internals.ClusterResourceListeners
 import org.apache.kafka.common.metrics.{JmxReporter, Metrics, _}
 import org.apache.kafka.common.network._
@@ -51,7 +51,6 @@ import org.apache.kafka.common.{ClusterResource, Node}
 import scala.collection.JavaConverters._
 import scala.collection.{Map, Seq, mutable}
-import org.apache.kafka.common.config.ClientDnsLookup
 object KafkaServer {
   // Copy the subset of properties that are relevant to Logs
diff --git a/core/src/main/scala/kafka/server/ReplicaFetcherBlockingSend.scala b/core/src/main/scala/kafka/server/ReplicaFetcherBlockingSend.scala
index d15fdae6b97..4e642f3cda2 100644
--- a/core/src/main/scala/kafka/server/ReplicaFetcherBlockingSend.scala
+++ b/core/src/main/scala/kafka/server/ReplicaFetcherBlockingSend.scala
@@ -30,7 +30,6 @@ import org.apache.kafka.common.Node
 import org.apache.kafka.common.requests.AbstractRequest.Builder
 import scala.collection.JavaConverters._
-import org.apache.kafka.common.config.ClientDnsLookup
 trait BlockingSend {
diff --git a/core/src/main/scala/kafka/tools/ReplicaVerificationTool.scala b/core/src/main/scala/kafka/tools/ReplicaVerificationTool.scala
index 7871015f3fb..1f87d7acfec 100644
--- a/core/src/main/scala/kafka/tools/ReplicaVerificationTool.scala
+++ b/core/src/main/scala/kafka/tools/ReplicaVerificationTool.scala
@@ -43,7 +43,6 @@ import org.apache.kafka.common.utils.{LogContext, Time}
 import org.apache.kafka.common.{Node, TopicPartition}
 import scala.collection.JavaConverters._
-import org.apache.kafka.common.config.ClientDnsLookup
 /**
  * For verifying the consistency among replicas.
diff --git [file java] [file java]
index 74ab234cdbe..9f15696fe71 100644
--- [file java]
+++ [file java]
@@ -20,8 +20,8 @@
 import com.fasterxml.jackson.annotation.JsonCreator;
 import com.fasterxml.jackson.annotation.JsonProperty;
 import org.apache.kafka.clients.ApiVersions;
+import org.apache.kafka.clients.ClientDnsLookup;
 import org.apache.kafka.clients.ClientUtils;
-import org.apache.kafka.clients.CommonClientConfigs;
 import org.apache.kafka.clients.ManualMetadataUpdater;
 import org.apache.kafka.clients.NetworkClient;
 import org.apache.kafka.clients.NetworkClientUtils;
@@ -30,7 +30,6 @@
 import org.apache.kafka.clients.producer.ProducerConfig;
 import org.apache.kafka.common.Cluster;
 import org.apache.kafka.common.Node;
-import org.apache.kafka.common.config.ClientDnsLookup;
 import org.apache.kafka.common.internals.KafkaFutureImpl;
 import org.apache.kafka.common.metrics.Metrics;
 import org.apache.kafka.common.network.ChannelBuilder;
@@ -127,7 +126,8 @@ public void run() {
                 WorkerUtils.addConfigsToProperties(props, spec.commonClientConf(), spec.commonClientConf());
                 AdminClientConfig conf = new AdminClientConfig(props);
                 List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(
-                    conf.getList(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG));
+                        conf.getList(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG),
+                        conf.getString(AdminClientConfig.CLIENT_DNS_LOOKUP_CONFIG));
                 ManualMetadataUpdater updater = new ManualMetadataUpdater(Cluster.bootstrap(addresses).nodes());
                 while (true) {
                     if (doneFuture.isDone()) {
@@ -182,7 +182,7 @@ private boolean attemptConnection(AdminClientConfig conf,
                                     4096,
                                     4096,
                                     1000,
-                                    ClientDnsLookup.forConfig(conf.getString(CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG)),
+                                    ClientDnsLookup.forConfig(conf.getString(AdminClientConfig.CLIENT_DNS_LOOKUP_CONFIG)),
                                     Time.SYSTEM,
                                     false,
                                     new ApiVersions(),
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
#6299,https://github.com/apache/kafka/pull/6299,https://github.com/apache/kafka/blob/2.2.0/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java,null,NO,null,null
KAFKA-7478,https://issues.apache.org/jira/browse/KAFKA-7478,https://github.com/apache/kafka/blob/2.2.0/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/OAuthBearerLoginModule.java,Reduce OAuthBearerLoginModule verbosity,NO,"The OAuthBearerLoginModule is pretty verbose by default and this fills logs in with too much information. It would be nice if we could reduce the verbosity by default and let the user opt in to inspect these debug-friendly messages
{code:java}
[INFO] 2018-10-03 16:58:11,986 [qtp1137078855-1798] org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule login - Login succeeded; invoke commit() to commit it; current committed token count=0 
[INFO] 2018-10-03 16:58:11,986 [qtp1137078855-1798] org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule commit - Committing my token; current committed token count = 0 
[INFO] 2018-10-03 16:58:11,986 [qtp1137078855-1798] org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule commit - Done committing my token; committed token count is now 1
[INFO] 2018-10-03 16:58:11,986 [qtp1137078855-1798] org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin login - Successfully logged in.
{code}","** Comment 1 **
stanislavkozlovski opened a new pull request #5738: KAFKA-7478: Reduce default logging verbosity in OAuthBearerLoginModule
URL: [link]
   The default `OAuthBearerLoginModule` is too noisy. My reasoning is that:   - Successful logins should be shown in `DEBUG`   - DEBUG should not be too noisy either   - Login aborted should be in `DEBUG` since authentication failures are always logged
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
rajinisivaram closed pull request #5738: KAFKA-7478: Reduce default logging verbosity in OAuthBearerLoginModule
URL: [link]
This is a PR merged from a forked repository.
As GitHub hides the original diff on merge, it is displayed below for
the sake of provenance:
As this is a foreign pull request (from a fork), the diff is supplied
below (as it won't show otherwise due to GitHub magic):
diff --git [file java] [file java]
index e3a78103560..e7976b55506 100644
--- [file java]
+++ [file java]
@@ -305,7 +305,7 @@ public boolean login() throws LoginException {
             log.debug(""Logged in without a token, this login cannot be used to establish client connections"");
         loginState = LoginState.LOGGED_IN_NOT_COMMITTED;
-        log.info(""Login succeeded; invoke commit() to commit it; current committed token count={}"",
+        log.debug(""Login succeeded; invoke commit() to commit it; current committed token count={}"",
                 committedTokenCount());
         return true;
     }
@@ -340,7 +340,7 @@ private void identifyExtensions() throws LoginException {
             throw new LoginException(""An internal error occurred while retrieving SASL extensions from callback handler"");
         } catch (UnsupportedCallbackException e) {
             extensionsRequiringCommit = EMPTY_EXTENSIONS;
-            log.info(""CallbackHandler {} does not support SASL extensions. No extensions will be added"", callbackHandler.getClass().getName());
+            log.debug(""CallbackHandler {} does not support SASL extensions. No extensions will be added"", callbackHandler.getClass().getName());
         }
         if (extensionsRequiringCommit ==  null) {
             log.error(""SASL Extensions cannot be null. Check whether your callback handler is explicitly setting them as null."");
@@ -354,12 +354,11 @@ public boolean logout() {
             throw new IllegalStateException(
                     ""Cannot call logout() immediately after login(); need to first invoke commit() or abort()"");
         if (loginState != LoginState.COMMITTED) {
-            if (log.isDebugEnabled())
-                log.debug(""Nothing here to log out"");
+            log.debug(""Nothing here to log out"");
             return false;
         }
         if (myCommittedToken != null) {
-            log.info(""Logging out my token; current committed token count = {}"", committedTokenCount());
+            log.trace(""Logging out my token; current committed token count = {}"", committedTokenCount());
             for (Iterator<Object> iterator = subject.getPrivateCredentials().iterator(); iterator.hasNext(); ) {
                 Object privateCredential = iterator.next();
                 if (privateCredential == myCommittedToken) {
@@ -368,15 +367,15 @@ public boolean logout() {
                     break;
                 }
             }
-            log.info(""Done logging out my token; committed token count is now {}"", committedTokenCount());
+            log.debug(""Done logging out my token; committed token count is now {}"", committedTokenCount());
         } else
             log.debug(""No tokens to logout for this login"");
         if (myCommittedExtensions != null) {
-            log.info(""Logging out my extensions"");
+            log.trace(""Logging out my extensions"");
             if (subject.getPublicCredentials().removeIf(e -> myCommittedExtensions == e))
                 myCommittedExtensions = null;
-            log.info(""Done logging out my extensions"");
+            log.debug(""Done logging out my extensions"");
         } else
             log.debug(""No extensions to logout for this login"");
@@ -387,17 +386,16 @@ public boolean logout() {
     @Override
     public boolean commit() {
         if (loginState != LoginState.LOGGED_IN_NOT_COMMITTED) {
-            if (log.isDebugEnabled())
-                log.debug(""Nothing here to commit"");
+            log.debug(""Nothing here to commit"");
             return false;
         }
         if (tokenRequiringCommit != null) {
-            log.info(""Committing my token; current committed token count = {}"", committedTokenCount());
+            log.trace(""Committing my token; current committed token count = {}"", committedTokenCount());
             subject.getPrivateCredentials().add(tokenRequiringCommit);
             myCommittedToken = tokenRequiringCommit;
             tokenRequiringCommit = null;
-            log.info(""Done committing my token; committed token count is now {}"", committedTokenCount());
+            log.debug(""Done committing my token; committed token count is now {}"", committedTokenCount());
         } else
             log.debug(""No tokens to commit, this login cannot be used to establish client connections"");
@@ -414,7 +412,7 @@ public boolean commit() {
     @Override
     public boolean abort() {
         if (loginState == LoginState.LOGGED_IN_NOT_COMMITTED) {
-            log.info(""Login aborted"");
+            log.debug(""Login aborted"");
             tokenRequiringCommit = null;
             extensionsRequiringCommit = null;
             loginState = LoginState.NOT_LOGGED_IN;
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-7798,https://issues.apache.org/jira/browse/KAFKA-7798,https://github.com/apache/kafka/blob/2.2.0/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java,Expose embedded client context from KafkaStreams threadMetadata,NO,"A KafkaStreams client today contains multiple embedded clients: producer, consumer and admin client. Currently these client's context like client id are not exposed via KafkaStreams. This ticket proposes to expose those context information at the per-thread basis (since each thread has its own embedded clients) via ThreadMetadata.

This also has an interplay with KIP-345: as we add group.instance.id in that KIP, this information should also be exposed as well.

Cf: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-414%3A+Expose+Embedded+ClientIds+in+Kafka+Streams]","** Comment 1 **
[Comment excluded]

** Comment 2 **
guozhangwang commented on pull request #6107: KAFKA-7798: expose embedded client context
URL: [link]
   1. Add consumer / restoreConsumer / producer(s) / admin client ids via ThreadMetadata; for producerIds, if EOS is turned on add the list of task-producer-ids, otherwise it is a singleton of thread-producer-id.   2. Consolidate the logic of generating clientIds from thread names and client id into StreamThread.   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 3 **
[Comment excluded]

** Comment 4 **
[Comment excluded]

** Comment 5 **
mjsax commented on pull request #6107: KAFKA-7798: Expose embedded clientIds
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-7921,https://issues.apache.org/jira/browse/KAFKA-7921,https://github.com/apache/kafka/blob/2.2.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java,Instable KafkaStreamsTest,NO,"{{KafkaStreamsTest}} failed multiple times, eg,
{quote}java.lang.AssertionError: Condition not met within timeout 15000. Streams never started.
at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:365)
at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:325)
at org.apache.kafka.streams.KafkaStreamsTest.shouldThrowOnCleanupWhileRunning(KafkaStreamsTest.java:556){quote}
or
{quote}java.lang.AssertionError: Condition not met within timeout 15000. Streams never started.
at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:365)
at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:325)
at org.apache.kafka.streams.KafkaStreamsTest.testStateThreadClose(KafkaStreamsTest.java:255){quote}
 
The preserved logs are as follows:

{quote}[2019-02-12 07:02:17,198] INFO Kafka version: 2.3.0-SNAPSHOT (org.apache.kafka.common.utils.AppInfoParser:109)
[2019-02-12 07:02:17,198] INFO Kafka commitId: 08036fa4b1e5b822 (org.apache.kafka.common.utils.AppInfoParser:110)
[2019-02-12 07:02:17,199] INFO stream-client [clientId] State transition from CREATED to REBALANCING (org.apache.kafka.streams.KafkaStreams:263)
[2019-02-12 07:02:17,200] INFO stream-thread [clientId-StreamThread-238] Starting (org.apache.kafka.streams.processor.internals.StreamThread:767)
[2019-02-12 07:02:17,200] INFO stream-client [clientId] State transition from REBALANCING to PENDING_SHUTDOWN (org.apache.kafka.streams.KafkaStreams:263)
[2019-02-12 07:02:17,200] INFO stream-thread [clientId-StreamThread-239] Starting (org.apache.kafka.streams.processor.internals.StreamThread:767)
[2019-02-12 07:02:17,200] INFO stream-thread [clientId-StreamThread-238] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread:214)
[2019-02-12 07:02:17,200] INFO stream-thread [clientId-StreamThread-239] State transition from CREATED to STARTING (org.apache.kafka.streams.processor.internals.StreamThread:214)
[2019-02-12 07:02:17,200] INFO stream-thread [clientId-StreamThread-238] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread:1192)
[2019-02-12 07:02:17,201] INFO stream-thread [clientId-StreamThread-238] State transition from STARTING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread:214)
[2019-02-12 07:02:17,201] INFO stream-thread [clientId-StreamThread-239] Informed to shut down (org.apache.kafka.streams.processor.internals.StreamThread:1192)
[2019-02-12 07:02:17,201] INFO stream-thread [clientId-StreamThread-239] State transition from STARTING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread:214)
[2019-02-12 07:02:17,205] INFO Cluster ID: J8uJhiTKQx-Y_i9LzT0iLg (org.apache.kafka.clients.Metadata:365)
[2019-02-12 07:02:17,205] INFO Cluster ID: J8uJhiTKQx-Y_i9LzT0iLg (org.apache.kafka.clients.Metadata:365)
[2019-02-12 07:02:17,205] INFO [Consumer clientId=clientId-StreamThread-238-consumer, groupId=appId] Discovered group coordinator localhost:36122 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:675)
[2019-02-12 07:02:17,205] INFO [Consumer clientId=clientId-StreamThread-239-consumer, groupId=appId] Discovered group coordinator localhost:36122 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:675)
[2019-02-12 07:02:17,206] INFO [Consumer clientId=clientId-StreamThread-238-consumer, groupId=appId] Revoking previously assigned partitions [] (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:458)
[2019-02-12 07:02:17,206] INFO [Consumer clientId=clientId-StreamThread-239-consumer, groupId=appId] Revoking previously assigned partitions [] (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:458)
[2019-02-12 07:02:17,206] INFO [Consumer clientId=clientId-StreamThread-238-consumer, groupId=appId] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:491)
[2019-02-12 07:02:17,206] INFO [Consumer clientId=clientId-StreamThread-239-consumer, groupId=appId] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:491)
[2019-02-12 07:02:17,208] INFO [Consumer clientId=clientId-StreamThread-239-consumer, groupId=appId] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:491)
[2019-02-12 07:02:17,208] INFO [Consumer clientId=clientId-StreamThread-238-consumer, groupId=appId] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:491)
[2019-02-12 07:02:17,278] INFO Cluster ID: J8uJhiTKQx-Y_i9LzT0iLg (org.apache.kafka.clients.Metadata:365)
[2019-02-12 07:02:17,293] INFO Cluster ID: J8uJhiTKQx-Y_i9LzT0iLg (org.apache.kafka.clients.Metadata:365)
[2019-02-12 07:02:17,301] INFO stream-thread [clientId-StreamThread-239] Shutting down (org.apache.kafka.streams.processor.internals.StreamThread:1206)
[2019-02-12 07:02:17,301] INFO [Consumer clientId=clientId-StreamThread-239-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer:1042)
[2019-02-12 07:02:17,301] INFO stream-thread [clientId-StreamThread-238] Shutting down (org.apache.kafka.streams.processor.internals.StreamThread:1206)
[2019-02-12 07:02:17,301] INFO [Consumer clientId=clientId-StreamThread-238-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer:1042)
[2019-02-12 07:02:17,302] INFO [Producer clientId=clientId-StreamThread-238-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1139)
[2019-02-12 07:02:17,301] INFO [Producer clientId=clientId-StreamThread-239-producer] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1139)
[2019-02-12 07:02:17,863] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:18,766] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:19,769] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:20,872] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:21,775] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:22,678] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:23,882] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:24,885] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:25,888] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:26,991] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:28,095] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:28,998] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:29,901] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:31,004] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:32,108] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:33,311] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:34,515] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:35,718] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:36,921] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:37,924] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:38,927] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:40,029] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:41,232] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:42,235] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:43,337] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:44,340] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:45,442] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:46,444] WARN [AdminClient clientId=adminclient-233] Connection to node 0 (localhost/127.0.0.1:41539) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient:722)
[2019-02-12 07:02:47,305] WARN [Consumer clientId=clientId-StreamThread-239-consumer, groupId=appId] Close timed out with 1 pending requests to coordinator, terminating client connections (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:800)
[2019-02-12 07:02:47,307] INFO stream-thread [clientId-StreamThread-239] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread:214)
[2019-02-12 07:02:47,307] INFO stream-thread [clientId-StreamThread-239] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread:1226)
[2019-02-12 07:02:47,308] WARN [Consumer clientId=clientId-StreamThread-238-consumer, groupId=appId] Close timed out with 1 pending requests to coordinator, terminating client connections (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:800)
[2019-02-12 07:02:47,313] INFO stream-thread [clientId-StreamThread-238] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread:214)
[2019-02-12 07:02:47,313] INFO stream-thread [clientId-StreamThread-238] Shutdown complete (org.apache.kafka.streams.processor.internals.StreamThread:1226)
[2019-02-12 07:02:47,315] INFO stream-client [clientId] State transition from PENDING_SHUTDOWN to NOT_RUNNING (org.apache.kafka.streams.KafkaStreams:263)
[2019-02-12 07:02:47,315] INFO stream-client [clientId] Streams client stopped completely (org.apache.kafka.streams.KafkaStreams:899)
[2019-02-12 07:02:47,316] INFO stream-client [clientId] Already in the pending shutdown state, wait to complete shutdown (org.apache.kafka.streams.KafkaStreams:849)
[2019-02-12 07:02:47,316] INFO stream-client [clientId] Streams client stopped completely (org.apache.kafka.streams.KafkaStreams:899){quote}
 
Note, that the instance goes from
{quote}[2019-02-12 07:02:17,199] INFO stream-client [clientId] State transition from CREATED to REBALANCING (org.apache.kafka.streams.KafkaStreams:263){quote}
to
{quote}[2019-02-12 07:02:17,200] INFO stream-client [clientId] State transition from REBALANCING to PENDING_SHUTDOWN (org.apache.kafka.streams.KafkaStreams:263){quote}
in the very beginning. It's unclear why this happens. Note, that the log before the copied snipped is unfortunately not complete (thank you Jenkins):
{quote}...[truncated 1593438 chars]...{quote}
Later, `AdminClient` seems to not be able to connect to the brokers (also unclear why) and the test times out. If the `AdminClient` issue is related to the first issue is unclear atm).","** Comment 1 **
Note that in a recent commit we've updated {{InternalTopicManager}} so that if topic creation / list topics return fatal errors it would be logged. So if we did not find it in the logs it means not related to admin-client creating topics.The ""informed shutdown"" can only be triggered from two places: 1) KafkaStreams#close() call, which should not be the case (there's no caller at that time). or 2):{code}if (streamThread.assignmentErrorCode.get() == StreamsPartitionAssignor.Error.INCOMPLETE_SOURCE_TOPIC_METADATA.code()) {                log.debug(""Received error code {} - shutdown"", streamThread.assignmentErrorCode.get());                streamThread.shutdown();                streamThread.setStateListener(null);                return;            }{code}This indicates that the source topics are not available yet:{code}for (final InternalTopologyBuilder.TopicsInfo topicsInfo : topicGroups.values()) {            for (final String topic : topicsInfo.sourceTopics) {                if (!topicsInfo.repartitionSourceTopics.keySet().contains(topic) &&                    !metadata.topics().contains(topic)) {                    return errorAssignment(clientsMetadata, topic, Error.INCOMPLETE_SOURCE_TOPIC_METADATA.code);                }            }            for (final InternalTopicConfig topic: topicsInfo.repartitionSourceTopics.values()) {                repartitionTopicMetadata.put(topic.name(), new InternalTopicMetadata(topic));            }        }{code}I'd suggest we upgrade DEBUG to ERROR logs on the above places when setting the error, as well as when receiving the error code to confirm. And in this case, inside {{KafkaStreamsTest}} we should add waitForCondition to wait for source topics to be successfully created. Cc 

** Comment 2 **
Can we do a ""two phase"" approach here? First only change the log level and wait until the test fails again to see if it's really a correct root cause analysis? And afterwards put the fix into the test?

** Comment 3 **
[Comment excluded]

** Comment 4 **
vvcephei commented on pull request #6262: KAFKA-7921: log at error level for missing source topic
URL: [link]
   This condition is a fatal error, so error level is warranted, to provide more context on why Streams shuts down.   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 5 **
guozhangwang commented on pull request #6262: KAFKA-7921: log at error level for missing source topic
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 6 **
 Test failed again. Can you have a look:  

** Comment 7 **
[Comment excluded]

** Comment 8 **
I haven't seen a failure since we merged the PR to capture logs.Checked: 

** Comment 9 **
Let's re-open this if we see another failure.

** Comment 10 **
[Comment excluded]
"
KAFKA-8429,https://issues.apache.org/jira/browse/KAFKA-8429,https://github.com/apache/kafka/blob/2.3.0/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java,Consumer should handle offset change while OffsetForLeaderEpoch is inflight,NO,"It is possible for the offset of a partition to be changed while we are in the middle of validation. If the OffsetForLeaderEpoch request is in-flight and the offset changes, we need to redo the validation after it returns.","** Comment 1 **
hachikuji commented on pull request #6811: KAFKA-8429; Handle offset change when OffsetForLeaderEpoch inflight
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-8429,https://issues.apache.org/jira/browse/KAFKA-8429,https://github.com/apache/kafka/blob/2.3.0/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java,Consumer should handle offset change while OffsetForLeaderEpoch is inflight,NO,"It is possible for the offset of a partition to be changed while we are in the middle of validation. If the OffsetForLeaderEpoch request is in-flight and the offset changes, we need to redo the validation after it returns.","** Comment 1 **
hachikuji commented on pull request #6811: KAFKA-8429; Handle offset change when OffsetForLeaderEpoch inflight
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-9140,https://issues.apache.org/jira/browse/KAFKA-9140,https://github.com/apache/kafka/blob/2.4.0/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java,Consumer gets stuck rejoining the group indefinitely,NO,"There seems to be a race condition that is now causing a rejoining member to potentially get stuck infinitely initiating a rejoin. The relevant client logs are attached (streams.log.tgz; all others attachments are broker logs), but basically it repeats this message (and nothing else) continuously until killed/shutdown:

 
{code:java}
[2019-11-05 01:53:54,699] INFO [Consumer clientId=StreamsUpgradeTest-a4c1cff8-7883-49cd-82da-d2cdfc33a2f0-StreamThread-1-consumer, groupId=StreamsUpgradeTest] Generation data was cleared by heartbeat thread. Initiating rejoin. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator)
{code}
 

The message that appears was added as part of the bugfix ([PR 7460|https://github.com/apache/kafka/pull/7460]) for this related race condition: KAFKA-8104.

This issue was uncovered by the Streams version probing upgrade test, which fails with a varying frequency. Here is the rate of failures for different system test runs so far:

trunk (cooperative): 1/1 and 2/10 failures

2.4 (cooperative) : 0/10 and 1/15 failures

trunk (eager): 0/10 failures

I've kicked off some high-repeat runs to complete overnight and hopefully shed more light.

Note that I have also kicked off runs of both 2.4 and trunk with the PR for KAFKA-8104 reverted. Both of them saw 2/10 failures, due to hitting the bug that was fixed by [PR 7460|https://github.com/apache/kafka/pull/7460]. It is therefore unclear whether [PR 7460|https://github.com/apache/kafka/pull/7460] introduced another or a new race condition/bug, or merely uncovered an existing one that previously would have first failed due to KAFKA-8104.

 ","** Comment 1 **
guozhangwang commented on pull request #7647: KAFKA-9140: Also reset join future when generation was reset in order to re-join
URL: [link]
   Otherwise the join-group would not be resend and we'd just fall into the endless loop.   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
guozhangwang commented on pull request #7647: KAFKA-9140: Also reset join future when generation was reset in order to re-join
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
#7659,https://github.com/apache/kafka/pull/7659,https://github.com/apache/kafka/blob/2.4.0/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java,null,NO,null,null
KAFKA-9051,https://issues.apache.org/jira/browse/KAFKA-9051,https://github.com/apache/kafka/blob/2.4.0/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java,Source task source offset reads can block graceful shutdown,NO,"When source tasks request source offsets from the framework, this results in a call to [Future.get()|https://github.com/apache/kafka/blob/8966d066bd2f80c6d8f270423e7e9982097f97b9/connect/runtime/src/main/java/org/apache/kafka/connect/storage/OffsetStorageReaderImpl.java#L79] with no timeout. In distributed workers, the future is blocked on a successful [read to the end|https://github.com/apache/kafka/blob/8966d066bd2f80c6d8f270423e7e9982097f97b9/connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaOffsetBackingStore.java#L136] of the source offsets topic, which in turn will [poll that topic indefinitely|https://github.com/apache/kafka/blob/8966d066bd2f80c6d8f270423e7e9982097f97b9/connect/runtime/src/main/java/org/apache/kafka/connect/util/KafkaBasedLog.java#L287] until the latest messages for every partition of that topic have been consumed.

This normally completes in a reasonable amount of time. However, if the connectivity between the Connect worker and the Kafka cluster is degraded or dropped in the middle of one of these reads, it will block until connectivity is restored and the request completes successfully.

If a task is stopped (due to a manual restart via the REST API, a rebalance, worker shutdown, etc.) while blocked on a read of source offsets during its {{start}} method, not only will it fail to gracefully stop, but the framework [will not even invoke its stop method|https://github.com/apache/kafka/blob/8966d066bd2f80c6d8f270423e7e9982097f97b9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java#L183] until its {{start}} method (and, as a result, the source offset read request) [has completed|https://github.com/apache/kafka/blob/8966d066bd2f80c6d8f270423e7e9982097f97b9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java#L202-L206]. This prevents the task from being able to clean up any resources it has allocated and can lead to OOM errors, excessive thread creation, and other problems.

 

I've confirmed that this affects every release of Connect back through 1.0 at least; I've tagged the most recent bug fix release of every major/minor version from then on in the {{Affects Version/s}} field to avoid just putting every version in that field.","** Comment 1 **
C0urante commented on pull request #7532: KAFKA-9051: Prematurely complete source offset read requests for stopped tasks
URL: [link]
   ([link]   The changes here cause source tasks which are blocked on source offset read requests to become immediately unblocked when they are scheduled for shutdown, which should allow them to complete their `start` method (if they are blocked inside if), which in turn should allow the framework to safely invoke their `stop` method and allow them to clean up allocated resources.   The source offsets returned to the task in this case may be either stale or missing entirely; however, this seems preferable to throwing an exception and potentially corrupting the state of the task badly enough that its then unable to clean up resources in its `stop` method due to, e.g., NPEs.   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
rhauch commented on pull request #7532: KAFKA-9051: Prematurely complete source offset read requests for stopped tasks
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 3 **
[Comment excluded]
"
KAFKA-9231,https://issues.apache.org/jira/browse/KAFKA-9231,https://github.com/apache/kafka/blob/2.4.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractTask.java,Streams Threads may die from recoverable errors with EOS enabled,YES,"While testing Streams in EOS mode under frequent and heavy network partitions, I've encountered the following error, leading to thread death:

{noformat}
[2019-11-26 04:54:02,650] ERROR [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] Encountered the following unexpected Kafka exception during processing, this usually indicate Streams internal errors: (org.apache.kafka.streams.processor.internals.StreamThread)
org.apache.kafka.streams.errors.StreamsException: stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] Failed to rebalance.
	at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:852)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:739)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:698)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:671)
Caused by: org.apache.kafka.streams.errors.StreamsException: stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] failed to suspend stream tasks
	at org.apache.kafka.streams.processor.internals.TaskManager.suspendActiveTasksAndState(TaskManager.java:253)
	at org.apache.kafka.streams.processor.internals.StreamsRebalanceListener.onPartitionsRevoked(StreamsRebalanceListener.java:116)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.invokePartitionsRevoked(ConsumerCoordinator.java:291)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onLeavePrepare(ConsumerCoordinator.java:707)
	at org.apache.kafka.clients.consumer.KafkaConsumer.unsubscribe(KafkaConsumer.java:1073)
	at org.apache.kafka.streams.processor.internals.StreamThread.enforceRebalance(StreamThread.java:716)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:710)
	... 1 more
Caused by: org.apache.kafka.streams.errors.ProcessorStateException: task [1_1] Failed to flush state store KSTREAM-AGGREGATE-STATE-STORE-0000000007
	at org.apache.kafka.streams.processor.internals.ProcessorStateManager.flush(ProcessorStateManager.java:279)
	at org.apache.kafka.streams.processor.internals.AbstractTask.flushState(AbstractTask.java:175)
	at org.apache.kafka.streams.processor.internals.StreamTask.flushState(StreamTask.java:581)
	at org.apache.kafka.streams.processor.internals.StreamTask.commit(StreamTask.java:535)
	at org.apache.kafka.streams.processor.internals.StreamTask.suspend(StreamTask.java:660)
	at org.apache.kafka.streams.processor.internals.StreamTask.suspend(StreamTask.java:628)
	at org.apache.kafka.streams.processor.internals.AssignedStreamsTasks.suspendRunningTasks(AssignedStreamsTasks.java:145)
	at org.apache.kafka.streams.processor.internals.AssignedStreamsTasks.suspendOrCloseTasks(AssignedStreamsTasks.java:128)
	at org.apache.kafka.streams.processor.internals.TaskManager.suspendActiveTasksAndState(TaskManager.java:246)
	... 7 more
Caused by: org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker.
[2019-11-26 04:54:02,650] INFO [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] State transition from PARTITIONS_REVOKED to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
[2019-11-26 04:54:02,650] INFO [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] Shutting down (org.apache.kafka.streams.processor.internals.StreamThread)
[2019-11-26 04:54:02,650] INFO [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] [Consumer clientId=stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
[2019-11-26 04:54:02,653] INFO [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
{noformat}

Elsewhere in the code, we catch ProducerFencedExceptions and trigger a rebalance instead of killing the thread. It seems like one possible avenue has slipped through the cracks.","** Comment 1 **
vvcephei commented on pull request #7748: KAFKA-9231: Streams: Rebalance when fenced in suspend
URL: [link]
   We missed a branch in which we might catch a ProducerFencedException. It should always be converted to a TaskMigratedException.   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
In addition to the error I listed above (ProducerFencedException), I have also observed the following recoverable exceptions leading to threads dying:UnknownProducerIdException:org.apache.kafka.streams.errors.StreamsException: task  Abort sending since an error caught with a previous record (timestamp 1574960233670) to topic stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000019-changelog due to org.apache.kafka.common.errors.UnknownProducerIdException: This exception is raised by the broker if it could not locate the producer metadata associated with the producerId in question. This could happen if, for instance, the producer's records were deleted because their retention time had elapsed. Once the last records of the producerId are removed, the producer's metadata is removed from the broker, and future appends by the producer will return this exception.	at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.recordSendError([file java]:143)	at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.access$500([file java]:51)	at org.apache.kafka.streams.processor.internals.RecordCollectorImpl$1.onCompletion([file java]:202)	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion([file java]:1348)	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks([file java]:230)	at org.apache.kafka.clients.producer.internals.ProducerBatch.done([file java]:196)	at org.apache.kafka.clients.producer.internals.Sender.failBatch([file java]:730)	at org.apache.kafka.clients.producer.internals.Sender.failBatch([file java]:716)	at org.apache.kafka.clients.producer.internals.Sender.completeBatch([file java]:674)	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse([file java]:596)	at org.apache.kafka.clients.producer.internals.Sender.access$100([file java]:74)	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete([file java]:798)	at org.apache.kafka.clients.ClientResponse.onComplete([file java]:109)	at org.apache.kafka.clients.NetworkClient.completeResponses([file java]:562)	at org.apache.kafka.clients.NetworkClient.poll([file java]:554)	at org.apache.kafka.clients.producer.internals.Sender.runOnce([file java]:335)	at org.apache.kafka.clients.producer.internals.Sender.run([file java]:244)	at java.lang.Thread.run([file java]:748) Caused by: org.apache.kafka.common.errors.UnknownProducerIdException: This exception is raised by the broker if it could not locate the producer metadata associated with the producerId in question. This could happen if, for instance, the producer's records were deleted because their retention time had elapsed. Once the last records of the producerId are removed, the producer's metadata is removed from the broker, and future appends by the producer will return this exception.and an internal assertion:java.lang.IllegalStateException: RocksDB metrics recorder for store ""KSTREAM-AGGREGATE-STATE-STORE-0000000049"" of task 3_1 has already been added. This is a bug in Kafka Streams.	at org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecordingTrigger.addMetricsRecorder([file java]:30)	at org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecorder.addStatistics([file java]:93)	at org.apache.kafka.streams.state.internals.RocksDBStore.maybeSetUpMetricsRecorder([file java]:205)	at org.apache.kafka.streams.state.internals.RocksDBStore.openDB([file java]:191)	at org.apache.kafka.streams.state.internals.RocksDBStore.init([file java]:227)	at org.apache.kafka.streams.state.internals.WrappedStateStore.init([file java]:48)	at org.apache.kafka.streams.state.internals.ChangeLoggingKeyValueBytesStore.init([file java]:44)	at org.apache.kafka.streams.state.internals.WrappedStateStore.init([file java]:48)	at org.apache.kafka.streams.state.internals.CachingKeyValueStore.init([file java]:58)	at org.apache.kafka.streams.state.internals.WrappedStateStore.init([file java]:48)	at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.lambda$init$0([file java]:203)	at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.measureLatency([file java]:356)	at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.init([file java]:201)	at org.apache.kafka.streams.processor.internals.AbstractTask.registerStateStores([file java]:211)	at org.apache.kafka.streams.processor.internals.StreamTask.initializeStateStores([file java]:323)	at org.apache.kafka.streams.processor.internals.AssignedTasks.initializeNewTasks([file java]:76)	at org.apache.kafka.streams.processor.internals.TaskManager.updateNewAndRestoringTasks([file java]:385)	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce([file java]:769)	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop([file java]:698)	at org.apache.kafka.streams.processor.internals.StreamThread.run([file java]:671) INFO  stream-thread  State transition from PARTITIONS_ASSIGNED to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)This is a specific internal assertion. What happened there is that the RocksDBStore initializer registers itself with the metric collector before fully creating the store. The store creation failed, but it was still registered, leading to a situation where the store could no longer be created at all.My testing setup is to run a Streams application processing data at a modest rate (~ 20K records/sec) in a three-instance configuration, each of which have three StreamsThreads. I'm introducing network partitions separating each instance one at a time from the brokers, at an incidence of about one network partition per hour, each lasting up to 5 minutes.I've observed all of these exceptions to kill Streams threads within a few occurrences of network partitioning. Note that Streams is configured with appropriate timeouts/retries to tolerate network interruptions lasting longer than 5 mintues, so any thread deaths are unexpected. All the thread deaths I've observed are the results of bugs in the Streams exception handling code.There are two main categories of exception:1. ProducerFencedException and UnknownProducerIdException . While they reflect different root causes, both of these are expected with EOS enabled, if the producer is silent for too long and ceases to be considered a valid member by the broker. Streams is supposed to handle this situation by rejoining the group (which includes discarding and re-creating its Producers). These were uncovered by repeated rebalances ultimately caused by the injected network partitions.2. IllegalStateException. A specific internal assertion revealed buggy store initialization logic. This was also uncovered by repeated rebalances ultimately caused by the injected network partitions.I have addressed all of these bugs in my PR [link]I'm proposing to consider this a Blocker for the 2.4.0 release. It is both severe (the exceptions above have reliably caused my Streams cluster to die completely within about 12-24 hours), and it is a regression. To determine the latter claim, I ran the exact same workload with the exact same scenario using Kafka Streams 2.3. It should be noted that I still observed threads to die on that branch, but *only* due to the UnknownProducerId exception. So, there is some overlap with what I'm seeing on 2.4, but aside from that one cause, the fact that Streams is losing threads at a high rate from both ProducerFencedExceptions and its own internal assertion (IllegalStateException) leads me to think that Streams would be less stable in production using EOS on 2.4 than it was on 2.3.For completeness, note that I've run the same test on 2.4 and 2.3 _without_ EOS, and Streams is quite stable. Also note, that the exceptions killing my applications seem to be directly caused by frequent rebalances and network interruptions. For users running EOS Streams in reliable and stable conditions, I do not expect them to suffer thread deaths.I know that everyone is waiting for the much-delayed 2.4.0 release, so I'm not taking a hard stance on it, but from where I'm sitting, the situation seems to warrant a new RC once the fix is prepared. Also note, a new RC was just announced this morning, so assuming I can merge my PR on Monday, we're only setting the release back a couple of extra days.Note that we have not previously tested Streams under these conditions, which is why we're discovering these bugs so late in the game.

** Comment 3 **
vvcephei commented on pull request #7748: KAFKA-9231: Streams Threads may die from recoverable errors with EOS enabled
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]
"
KAFKA-9231,https://issues.apache.org/jira/browse/KAFKA-9231,https://github.com/apache/kafka/blob/2.4.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasks.java,Streams Threads may die from recoverable errors with EOS enabled,YES,"While testing Streams in EOS mode under frequent and heavy network partitions, I've encountered the following error, leading to thread death:

{noformat}
[2019-11-26 04:54:02,650] ERROR [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] Encountered the following unexpected Kafka exception during processing, this usually indicate Streams internal errors: (org.apache.kafka.streams.processor.internals.StreamThread)
org.apache.kafka.streams.errors.StreamsException: stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] Failed to rebalance.
	at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:852)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:739)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:698)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:671)
Caused by: org.apache.kafka.streams.errors.StreamsException: stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] failed to suspend stream tasks
	at org.apache.kafka.streams.processor.internals.TaskManager.suspendActiveTasksAndState(TaskManager.java:253)
	at org.apache.kafka.streams.processor.internals.StreamsRebalanceListener.onPartitionsRevoked(StreamsRebalanceListener.java:116)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.invokePartitionsRevoked(ConsumerCoordinator.java:291)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onLeavePrepare(ConsumerCoordinator.java:707)
	at org.apache.kafka.clients.consumer.KafkaConsumer.unsubscribe(KafkaConsumer.java:1073)
	at org.apache.kafka.streams.processor.internals.StreamThread.enforceRebalance(StreamThread.java:716)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:710)
	... 1 more
Caused by: org.apache.kafka.streams.errors.ProcessorStateException: task [1_1] Failed to flush state store KSTREAM-AGGREGATE-STATE-STORE-0000000007
	at org.apache.kafka.streams.processor.internals.ProcessorStateManager.flush(ProcessorStateManager.java:279)
	at org.apache.kafka.streams.processor.internals.AbstractTask.flushState(AbstractTask.java:175)
	at org.apache.kafka.streams.processor.internals.StreamTask.flushState(StreamTask.java:581)
	at org.apache.kafka.streams.processor.internals.StreamTask.commit(StreamTask.java:535)
	at org.apache.kafka.streams.processor.internals.StreamTask.suspend(StreamTask.java:660)
	at org.apache.kafka.streams.processor.internals.StreamTask.suspend(StreamTask.java:628)
	at org.apache.kafka.streams.processor.internals.AssignedStreamsTasks.suspendRunningTasks(AssignedStreamsTasks.java:145)
	at org.apache.kafka.streams.processor.internals.AssignedStreamsTasks.suspendOrCloseTasks(AssignedStreamsTasks.java:128)
	at org.apache.kafka.streams.processor.internals.TaskManager.suspendActiveTasksAndState(TaskManager.java:246)
	... 7 more
Caused by: org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker.
[2019-11-26 04:54:02,650] INFO [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] State transition from PARTITIONS_REVOKED to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
[2019-11-26 04:54:02,650] INFO [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] Shutting down (org.apache.kafka.streams.processor.internals.StreamThread)
[2019-11-26 04:54:02,650] INFO [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] [Consumer clientId=stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
[2019-11-26 04:54:02,653] INFO [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
{noformat}

Elsewhere in the code, we catch ProducerFencedExceptions and trigger a rebalance instead of killing the thread. It seems like one possible avenue has slipped through the cracks.","** Comment 1 **
vvcephei commented on pull request #7748: KAFKA-9231: Streams: Rebalance when fenced in suspend
URL: [link]
   We missed a branch in which we might catch a ProducerFencedException. It should always be converted to a TaskMigratedException.   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
In addition to the error I listed above (ProducerFencedException), I have also observed the following recoverable exceptions leading to threads dying:UnknownProducerIdException:org.apache.kafka.streams.errors.StreamsException: task  Abort sending since an error caught with a previous record (timestamp 1574960233670) to topic stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000019-changelog due to org.apache.kafka.common.errors.UnknownProducerIdException: This exception is raised by the broker if it could not locate the producer metadata associated with the producerId in question. This could happen if, for instance, the producer's records were deleted because their retention time had elapsed. Once the last records of the producerId are removed, the producer's metadata is removed from the broker, and future appends by the producer will return this exception.	at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.recordSendError([file java]:143)	at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.access$500([file java]:51)	at org.apache.kafka.streams.processor.internals.RecordCollectorImpl$1.onCompletion([file java]:202)	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion([file java]:1348)	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks([file java]:230)	at org.apache.kafka.clients.producer.internals.ProducerBatch.done([file java]:196)	at org.apache.kafka.clients.producer.internals.Sender.failBatch([file java]:730)	at org.apache.kafka.clients.producer.internals.Sender.failBatch([file java]:716)	at org.apache.kafka.clients.producer.internals.Sender.completeBatch([file java]:674)	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse([file java]:596)	at org.apache.kafka.clients.producer.internals.Sender.access$100([file java]:74)	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete([file java]:798)	at org.apache.kafka.clients.ClientResponse.onComplete([file java]:109)	at org.apache.kafka.clients.NetworkClient.completeResponses([file java]:562)	at org.apache.kafka.clients.NetworkClient.poll([file java]:554)	at org.apache.kafka.clients.producer.internals.Sender.runOnce([file java]:335)	at org.apache.kafka.clients.producer.internals.Sender.run([file java]:244)	at java.lang.Thread.run([file java]:748) Caused by: org.apache.kafka.common.errors.UnknownProducerIdException: This exception is raised by the broker if it could not locate the producer metadata associated with the producerId in question. This could happen if, for instance, the producer's records were deleted because their retention time had elapsed. Once the last records of the producerId are removed, the producer's metadata is removed from the broker, and future appends by the producer will return this exception.and an internal assertion:java.lang.IllegalStateException: RocksDB metrics recorder for store ""KSTREAM-AGGREGATE-STATE-STORE-0000000049"" of task 3_1 has already been added. This is a bug in Kafka Streams.	at org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecordingTrigger.addMetricsRecorder([file java]:30)	at org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecorder.addStatistics([file java]:93)	at org.apache.kafka.streams.state.internals.RocksDBStore.maybeSetUpMetricsRecorder([file java]:205)	at org.apache.kafka.streams.state.internals.RocksDBStore.openDB([file java]:191)	at org.apache.kafka.streams.state.internals.RocksDBStore.init([file java]:227)	at org.apache.kafka.streams.state.internals.WrappedStateStore.init([file java]:48)	at org.apache.kafka.streams.state.internals.ChangeLoggingKeyValueBytesStore.init([file java]:44)	at org.apache.kafka.streams.state.internals.WrappedStateStore.init([file java]:48)	at org.apache.kafka.streams.state.internals.CachingKeyValueStore.init([file java]:58)	at org.apache.kafka.streams.state.internals.WrappedStateStore.init([file java]:48)	at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.lambda$init$0([file java]:203)	at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.measureLatency([file java]:356)	at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.init([file java]:201)	at org.apache.kafka.streams.processor.internals.AbstractTask.registerStateStores([file java]:211)	at org.apache.kafka.streams.processor.internals.StreamTask.initializeStateStores([file java]:323)	at org.apache.kafka.streams.processor.internals.AssignedTasks.initializeNewTasks([file java]:76)	at org.apache.kafka.streams.processor.internals.TaskManager.updateNewAndRestoringTasks([file java]:385)	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce([file java]:769)	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop([file java]:698)	at org.apache.kafka.streams.processor.internals.StreamThread.run([file java]:671) INFO  stream-thread  State transition from PARTITIONS_ASSIGNED to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)This is a specific internal assertion. What happened there is that the RocksDBStore initializer registers itself with the metric collector before fully creating the store. The store creation failed, but it was still registered, leading to a situation where the store could no longer be created at all.My testing setup is to run a Streams application processing data at a modest rate (~ 20K records/sec) in a three-instance configuration, each of which have three StreamsThreads. I'm introducing network partitions separating each instance one at a time from the brokers, at an incidence of about one network partition per hour, each lasting up to 5 minutes.I've observed all of these exceptions to kill Streams threads within a few occurrences of network partitioning. Note that Streams is configured with appropriate timeouts/retries to tolerate network interruptions lasting longer than 5 mintues, so any thread deaths are unexpected. All the thread deaths I've observed are the results of bugs in the Streams exception handling code.There are two main categories of exception:1. ProducerFencedException and UnknownProducerIdException . While they reflect different root causes, both of these are expected with EOS enabled, if the producer is silent for too long and ceases to be considered a valid member by the broker. Streams is supposed to handle this situation by rejoining the group (which includes discarding and re-creating its Producers). These were uncovered by repeated rebalances ultimately caused by the injected network partitions.2. IllegalStateException. A specific internal assertion revealed buggy store initialization logic. This was also uncovered by repeated rebalances ultimately caused by the injected network partitions.I have addressed all of these bugs in my PR [link]I'm proposing to consider this a Blocker for the 2.4.0 release. It is both severe (the exceptions above have reliably caused my Streams cluster to die completely within about 12-24 hours), and it is a regression. To determine the latter claim, I ran the exact same workload with the exact same scenario using Kafka Streams 2.3. It should be noted that I still observed threads to die on that branch, but *only* due to the UnknownProducerId exception. So, there is some overlap with what I'm seeing on 2.4, but aside from that one cause, the fact that Streams is losing threads at a high rate from both ProducerFencedExceptions and its own internal assertion (IllegalStateException) leads me to think that Streams would be less stable in production using EOS on 2.4 than it was on 2.3.For completeness, note that I've run the same test on 2.4 and 2.3 _without_ EOS, and Streams is quite stable. Also note, that the exceptions killing my applications seem to be directly caused by frequent rebalances and network interruptions. For users running EOS Streams in reliable and stable conditions, I do not expect them to suffer thread deaths.I know that everyone is waiting for the much-delayed 2.4.0 release, so I'm not taking a hard stance on it, but from where I'm sitting, the situation seems to warrant a new RC once the fix is prepared. Also note, a new RC was just announced this morning, so assuming I can merge my PR on Monday, we're only setting the release back a couple of extra days.Note that we have not previously tested Streams under these conditions, which is why we're discovering these bugs so late in the game.

** Comment 3 **
vvcephei commented on pull request #7748: KAFKA-9231: Streams Threads may die from recoverable errors with EOS enabled
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]
"
KAFKA-9231,https://issues.apache.org/jira/browse/KAFKA-9231,https://github.com/apache/kafka/blob/2.4.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java,Streams Threads may die from recoverable errors with EOS enabled,YES,"While testing Streams in EOS mode under frequent and heavy network partitions, I've encountered the following error, leading to thread death:

{noformat}
[2019-11-26 04:54:02,650] ERROR [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] Encountered the following unexpected Kafka exception during processing, this usually indicate Streams internal errors: (org.apache.kafka.streams.processor.internals.StreamThread)
org.apache.kafka.streams.errors.StreamsException: stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] Failed to rebalance.
	at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:852)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:739)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:698)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:671)
Caused by: org.apache.kafka.streams.errors.StreamsException: stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] failed to suspend stream tasks
	at org.apache.kafka.streams.processor.internals.TaskManager.suspendActiveTasksAndState(TaskManager.java:253)
	at org.apache.kafka.streams.processor.internals.StreamsRebalanceListener.onPartitionsRevoked(StreamsRebalanceListener.java:116)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.invokePartitionsRevoked(ConsumerCoordinator.java:291)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onLeavePrepare(ConsumerCoordinator.java:707)
	at org.apache.kafka.clients.consumer.KafkaConsumer.unsubscribe(KafkaConsumer.java:1073)
	at org.apache.kafka.streams.processor.internals.StreamThread.enforceRebalance(StreamThread.java:716)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:710)
	... 1 more
Caused by: org.apache.kafka.streams.errors.ProcessorStateException: task [1_1] Failed to flush state store KSTREAM-AGGREGATE-STATE-STORE-0000000007
	at org.apache.kafka.streams.processor.internals.ProcessorStateManager.flush(ProcessorStateManager.java:279)
	at org.apache.kafka.streams.processor.internals.AbstractTask.flushState(AbstractTask.java:175)
	at org.apache.kafka.streams.processor.internals.StreamTask.flushState(StreamTask.java:581)
	at org.apache.kafka.streams.processor.internals.StreamTask.commit(StreamTask.java:535)
	at org.apache.kafka.streams.processor.internals.StreamTask.suspend(StreamTask.java:660)
	at org.apache.kafka.streams.processor.internals.StreamTask.suspend(StreamTask.java:628)
	at org.apache.kafka.streams.processor.internals.AssignedStreamsTasks.suspendRunningTasks(AssignedStreamsTasks.java:145)
	at org.apache.kafka.streams.processor.internals.AssignedStreamsTasks.suspendOrCloseTasks(AssignedStreamsTasks.java:128)
	at org.apache.kafka.streams.processor.internals.TaskManager.suspendActiveTasksAndState(TaskManager.java:246)
	... 7 more
Caused by: org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker.
[2019-11-26 04:54:02,650] INFO [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] State transition from PARTITIONS_REVOKED to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
[2019-11-26 04:54:02,650] INFO [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] Shutting down (org.apache.kafka.streams.processor.internals.StreamThread)
[2019-11-26 04:54:02,650] INFO [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] [Consumer clientId=stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
[2019-11-26 04:54:02,653] INFO [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
{noformat}

Elsewhere in the code, we catch ProducerFencedExceptions and trigger a rebalance instead of killing the thread. It seems like one possible avenue has slipped through the cracks.","** Comment 1 **
vvcephei commented on pull request #7748: KAFKA-9231: Streams: Rebalance when fenced in suspend
URL: [link]
   We missed a branch in which we might catch a ProducerFencedException. It should always be converted to a TaskMigratedException.   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
In addition to the error I listed above (ProducerFencedException), I have also observed the following recoverable exceptions leading to threads dying:UnknownProducerIdException:org.apache.kafka.streams.errors.StreamsException: task  Abort sending since an error caught with a previous record (timestamp 1574960233670) to topic stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000019-changelog due to org.apache.kafka.common.errors.UnknownProducerIdException: This exception is raised by the broker if it could not locate the producer metadata associated with the producerId in question. This could happen if, for instance, the producer's records were deleted because their retention time had elapsed. Once the last records of the producerId are removed, the producer's metadata is removed from the broker, and future appends by the producer will return this exception.	at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.recordSendError([file java]:143)	at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.access$500([file java]:51)	at org.apache.kafka.streams.processor.internals.RecordCollectorImpl$1.onCompletion([file java]:202)	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion([file java]:1348)	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks([file java]:230)	at org.apache.kafka.clients.producer.internals.ProducerBatch.done([file java]:196)	at org.apache.kafka.clients.producer.internals.Sender.failBatch([file java]:730)	at org.apache.kafka.clients.producer.internals.Sender.failBatch([file java]:716)	at org.apache.kafka.clients.producer.internals.Sender.completeBatch([file java]:674)	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse([file java]:596)	at org.apache.kafka.clients.producer.internals.Sender.access$100([file java]:74)	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete([file java]:798)	at org.apache.kafka.clients.ClientResponse.onComplete([file java]:109)	at org.apache.kafka.clients.NetworkClient.completeResponses([file java]:562)	at org.apache.kafka.clients.NetworkClient.poll([file java]:554)	at org.apache.kafka.clients.producer.internals.Sender.runOnce([file java]:335)	at org.apache.kafka.clients.producer.internals.Sender.run([file java]:244)	at java.lang.Thread.run([file java]:748) Caused by: org.apache.kafka.common.errors.UnknownProducerIdException: This exception is raised by the broker if it could not locate the producer metadata associated with the producerId in question. This could happen if, for instance, the producer's records were deleted because their retention time had elapsed. Once the last records of the producerId are removed, the producer's metadata is removed from the broker, and future appends by the producer will return this exception.and an internal assertion:java.lang.IllegalStateException: RocksDB metrics recorder for store ""KSTREAM-AGGREGATE-STATE-STORE-0000000049"" of task 3_1 has already been added. This is a bug in Kafka Streams.	at org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecordingTrigger.addMetricsRecorder([file java]:30)	at org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecorder.addStatistics([file java]:93)	at org.apache.kafka.streams.state.internals.RocksDBStore.maybeSetUpMetricsRecorder([file java]:205)	at org.apache.kafka.streams.state.internals.RocksDBStore.openDB([file java]:191)	at org.apache.kafka.streams.state.internals.RocksDBStore.init([file java]:227)	at org.apache.kafka.streams.state.internals.WrappedStateStore.init([file java]:48)	at org.apache.kafka.streams.state.internals.ChangeLoggingKeyValueBytesStore.init([file java]:44)	at org.apache.kafka.streams.state.internals.WrappedStateStore.init([file java]:48)	at org.apache.kafka.streams.state.internals.CachingKeyValueStore.init([file java]:58)	at org.apache.kafka.streams.state.internals.WrappedStateStore.init([file java]:48)	at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.lambda$init$0([file java]:203)	at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.measureLatency([file java]:356)	at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.init([file java]:201)	at org.apache.kafka.streams.processor.internals.AbstractTask.registerStateStores([file java]:211)	at org.apache.kafka.streams.processor.internals.StreamTask.initializeStateStores([file java]:323)	at org.apache.kafka.streams.processor.internals.AssignedTasks.initializeNewTasks([file java]:76)	at org.apache.kafka.streams.processor.internals.TaskManager.updateNewAndRestoringTasks([file java]:385)	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce([file java]:769)	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop([file java]:698)	at org.apache.kafka.streams.processor.internals.StreamThread.run([file java]:671) INFO  stream-thread  State transition from PARTITIONS_ASSIGNED to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)This is a specific internal assertion. What happened there is that the RocksDBStore initializer registers itself with the metric collector before fully creating the store. The store creation failed, but it was still registered, leading to a situation where the store could no longer be created at all.My testing setup is to run a Streams application processing data at a modest rate (~ 20K records/sec) in a three-instance configuration, each of which have three StreamsThreads. I'm introducing network partitions separating each instance one at a time from the brokers, at an incidence of about one network partition per hour, each lasting up to 5 minutes.I've observed all of these exceptions to kill Streams threads within a few occurrences of network partitioning. Note that Streams is configured with appropriate timeouts/retries to tolerate network interruptions lasting longer than 5 mintues, so any thread deaths are unexpected. All the thread deaths I've observed are the results of bugs in the Streams exception handling code.There are two main categories of exception:1. ProducerFencedException and UnknownProducerIdException . While they reflect different root causes, both of these are expected with EOS enabled, if the producer is silent for too long and ceases to be considered a valid member by the broker. Streams is supposed to handle this situation by rejoining the group (which includes discarding and re-creating its Producers). These were uncovered by repeated rebalances ultimately caused by the injected network partitions.2. IllegalStateException. A specific internal assertion revealed buggy store initialization logic. This was also uncovered by repeated rebalances ultimately caused by the injected network partitions.I have addressed all of these bugs in my PR [link]I'm proposing to consider this a Blocker for the 2.4.0 release. It is both severe (the exceptions above have reliably caused my Streams cluster to die completely within about 12-24 hours), and it is a regression. To determine the latter claim, I ran the exact same workload with the exact same scenario using Kafka Streams 2.3. It should be noted that I still observed threads to die on that branch, but *only* due to the UnknownProducerId exception. So, there is some overlap with what I'm seeing on 2.4, but aside from that one cause, the fact that Streams is losing threads at a high rate from both ProducerFencedExceptions and its own internal assertion (IllegalStateException) leads me to think that Streams would be less stable in production using EOS on 2.4 than it was on 2.3.For completeness, note that I've run the same test on 2.4 and 2.3 _without_ EOS, and Streams is quite stable. Also note, that the exceptions killing my applications seem to be directly caused by frequent rebalances and network interruptions. For users running EOS Streams in reliable and stable conditions, I do not expect them to suffer thread deaths.I know that everyone is waiting for the much-delayed 2.4.0 release, so I'm not taking a hard stance on it, but from where I'm sitting, the situation seems to warrant a new RC once the fix is prepared. Also note, a new RC was just announced this morning, so assuming I can merge my PR on Monday, we're only setting the release back a couple of extra days.Note that we have not previously tested Streams under these conditions, which is why we're discovering these bugs so late in the game.

** Comment 3 **
vvcephei commented on pull request #7748: KAFKA-9231: Streams Threads may die from recoverable errors with EOS enabled
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]
"
KAFKA-9231,https://issues.apache.org/jira/browse/KAFKA-9231,https://github.com/apache/kafka/blob/2.4.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java,Streams Threads may die from recoverable errors with EOS enabled,YES,"While testing Streams in EOS mode under frequent and heavy network partitions, I've encountered the following error, leading to thread death:

{noformat}
[2019-11-26 04:54:02,650] ERROR [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] Encountered the following unexpected Kafka exception during processing, this usually indicate Streams internal errors: (org.apache.kafka.streams.processor.internals.StreamThread)
org.apache.kafka.streams.errors.StreamsException: stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] Failed to rebalance.
	at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:852)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:739)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:698)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:671)
Caused by: org.apache.kafka.streams.errors.StreamsException: stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] failed to suspend stream tasks
	at org.apache.kafka.streams.processor.internals.TaskManager.suspendActiveTasksAndState(TaskManager.java:253)
	at org.apache.kafka.streams.processor.internals.StreamsRebalanceListener.onPartitionsRevoked(StreamsRebalanceListener.java:116)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.invokePartitionsRevoked(ConsumerCoordinator.java:291)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onLeavePrepare(ConsumerCoordinator.java:707)
	at org.apache.kafka.clients.consumer.KafkaConsumer.unsubscribe(KafkaConsumer.java:1073)
	at org.apache.kafka.streams.processor.internals.StreamThread.enforceRebalance(StreamThread.java:716)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:710)
	... 1 more
Caused by: org.apache.kafka.streams.errors.ProcessorStateException: task [1_1] Failed to flush state store KSTREAM-AGGREGATE-STATE-STORE-0000000007
	at org.apache.kafka.streams.processor.internals.ProcessorStateManager.flush(ProcessorStateManager.java:279)
	at org.apache.kafka.streams.processor.internals.AbstractTask.flushState(AbstractTask.java:175)
	at org.apache.kafka.streams.processor.internals.StreamTask.flushState(StreamTask.java:581)
	at org.apache.kafka.streams.processor.internals.StreamTask.commit(StreamTask.java:535)
	at org.apache.kafka.streams.processor.internals.StreamTask.suspend(StreamTask.java:660)
	at org.apache.kafka.streams.processor.internals.StreamTask.suspend(StreamTask.java:628)
	at org.apache.kafka.streams.processor.internals.AssignedStreamsTasks.suspendRunningTasks(AssignedStreamsTasks.java:145)
	at org.apache.kafka.streams.processor.internals.AssignedStreamsTasks.suspendOrCloseTasks(AssignedStreamsTasks.java:128)
	at org.apache.kafka.streams.processor.internals.TaskManager.suspendActiveTasksAndState(TaskManager.java:246)
	... 7 more
Caused by: org.apache.kafka.common.errors.ProducerFencedException: Producer attempted an operation with an old epoch. Either there is a newer producer with the same transactionalId, or the producer's transaction has been expired by the broker.
[2019-11-26 04:54:02,650] INFO [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] State transition from PARTITIONS_REVOKED to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)
[2019-11-26 04:54:02,650] INFO [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] Shutting down (org.apache.kafka.streams.processor.internals.StreamThread)
[2019-11-26 04:54:02,650] INFO [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] [Consumer clientId=stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2-restore-consumer, groupId=null] Unsubscribed all topics or patterns and assigned partitions (org.apache.kafka.clients.consumer.KafkaConsumer)
[2019-11-26 04:54:02,653] INFO [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] stream-thread [stream-soak-test-4290196e-d805-4acd-9f78-b459cc7e99ee-StreamThread-2] State transition from PENDING_SHUTDOWN to DEAD (org.apache.kafka.streams.processor.internals.StreamThread)
{noformat}

Elsewhere in the code, we catch ProducerFencedExceptions and trigger a rebalance instead of killing the thread. It seems like one possible avenue has slipped through the cracks.","** Comment 1 **
vvcephei commented on pull request #7748: KAFKA-9231: Streams: Rebalance when fenced in suspend
URL: [link]
   We missed a branch in which we might catch a ProducerFencedException. It should always be converted to a TaskMigratedException.   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
In addition to the error I listed above (ProducerFencedException), I have also observed the following recoverable exceptions leading to threads dying:UnknownProducerIdException:org.apache.kafka.streams.errors.StreamsException: task  Abort sending since an error caught with a previous record (timestamp 1574960233670) to topic stream-soak-test-KSTREAM-AGGREGATE-STATE-STORE-0000000019-changelog due to org.apache.kafka.common.errors.UnknownProducerIdException: This exception is raised by the broker if it could not locate the producer metadata associated with the producerId in question. This could happen if, for instance, the producer's records were deleted because their retention time had elapsed. Once the last records of the producerId are removed, the producer's metadata is removed from the broker, and future appends by the producer will return this exception.	at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.recordSendError([file java]:143)	at org.apache.kafka.streams.processor.internals.RecordCollectorImpl.access$500([file java]:51)	at org.apache.kafka.streams.processor.internals.RecordCollectorImpl$1.onCompletion([file java]:202)	at org.apache.kafka.clients.producer.KafkaProducer$InterceptorCallback.onCompletion([file java]:1348)	at org.apache.kafka.clients.producer.internals.ProducerBatch.completeFutureAndFireCallbacks([file java]:230)	at org.apache.kafka.clients.producer.internals.ProducerBatch.done([file java]:196)	at org.apache.kafka.clients.producer.internals.Sender.failBatch([file java]:730)	at org.apache.kafka.clients.producer.internals.Sender.failBatch([file java]:716)	at org.apache.kafka.clients.producer.internals.Sender.completeBatch([file java]:674)	at org.apache.kafka.clients.producer.internals.Sender.handleProduceResponse([file java]:596)	at org.apache.kafka.clients.producer.internals.Sender.access$100([file java]:74)	at org.apache.kafka.clients.producer.internals.Sender$1.onComplete([file java]:798)	at org.apache.kafka.clients.ClientResponse.onComplete([file java]:109)	at org.apache.kafka.clients.NetworkClient.completeResponses([file java]:562)	at org.apache.kafka.clients.NetworkClient.poll([file java]:554)	at org.apache.kafka.clients.producer.internals.Sender.runOnce([file java]:335)	at org.apache.kafka.clients.producer.internals.Sender.run([file java]:244)	at java.lang.Thread.run([file java]:748) Caused by: org.apache.kafka.common.errors.UnknownProducerIdException: This exception is raised by the broker if it could not locate the producer metadata associated with the producerId in question. This could happen if, for instance, the producer's records were deleted because their retention time had elapsed. Once the last records of the producerId are removed, the producer's metadata is removed from the broker, and future appends by the producer will return this exception.and an internal assertion:java.lang.IllegalStateException: RocksDB metrics recorder for store ""KSTREAM-AGGREGATE-STATE-STORE-0000000049"" of task 3_1 has already been added. This is a bug in Kafka Streams.	at org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecordingTrigger.addMetricsRecorder([file java]:30)	at org.apache.kafka.streams.state.internals.metrics.RocksDBMetricsRecorder.addStatistics([file java]:93)	at org.apache.kafka.streams.state.internals.RocksDBStore.maybeSetUpMetricsRecorder([file java]:205)	at org.apache.kafka.streams.state.internals.RocksDBStore.openDB([file java]:191)	at org.apache.kafka.streams.state.internals.RocksDBStore.init([file java]:227)	at org.apache.kafka.streams.state.internals.WrappedStateStore.init([file java]:48)	at org.apache.kafka.streams.state.internals.ChangeLoggingKeyValueBytesStore.init([file java]:44)	at org.apache.kafka.streams.state.internals.WrappedStateStore.init([file java]:48)	at org.apache.kafka.streams.state.internals.CachingKeyValueStore.init([file java]:58)	at org.apache.kafka.streams.state.internals.WrappedStateStore.init([file java]:48)	at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.lambda$init$0([file java]:203)	at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.measureLatency([file java]:356)	at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.init([file java]:201)	at org.apache.kafka.streams.processor.internals.AbstractTask.registerStateStores([file java]:211)	at org.apache.kafka.streams.processor.internals.StreamTask.initializeStateStores([file java]:323)	at org.apache.kafka.streams.processor.internals.AssignedTasks.initializeNewTasks([file java]:76)	at org.apache.kafka.streams.processor.internals.TaskManager.updateNewAndRestoringTasks([file java]:385)	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce([file java]:769)	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop([file java]:698)	at org.apache.kafka.streams.processor.internals.StreamThread.run([file java]:671) INFO  stream-thread  State transition from PARTITIONS_ASSIGNED to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread)This is a specific internal assertion. What happened there is that the RocksDBStore initializer registers itself with the metric collector before fully creating the store. The store creation failed, but it was still registered, leading to a situation where the store could no longer be created at all.My testing setup is to run a Streams application processing data at a modest rate (~ 20K records/sec) in a three-instance configuration, each of which have three StreamsThreads. I'm introducing network partitions separating each instance one at a time from the brokers, at an incidence of about one network partition per hour, each lasting up to 5 minutes.I've observed all of these exceptions to kill Streams threads within a few occurrences of network partitioning. Note that Streams is configured with appropriate timeouts/retries to tolerate network interruptions lasting longer than 5 mintues, so any thread deaths are unexpected. All the thread deaths I've observed are the results of bugs in the Streams exception handling code.There are two main categories of exception:1. ProducerFencedException and UnknownProducerIdException . While they reflect different root causes, both of these are expected with EOS enabled, if the producer is silent for too long and ceases to be considered a valid member by the broker. Streams is supposed to handle this situation by rejoining the group (which includes discarding and re-creating its Producers). These were uncovered by repeated rebalances ultimately caused by the injected network partitions.2. IllegalStateException. A specific internal assertion revealed buggy store initialization logic. This was also uncovered by repeated rebalances ultimately caused by the injected network partitions.I have addressed all of these bugs in my PR [link]I'm proposing to consider this a Blocker for the 2.4.0 release. It is both severe (the exceptions above have reliably caused my Streams cluster to die completely within about 12-24 hours), and it is a regression. To determine the latter claim, I ran the exact same workload with the exact same scenario using Kafka Streams 2.3. It should be noted that I still observed threads to die on that branch, but *only* due to the UnknownProducerId exception. So, there is some overlap with what I'm seeing on 2.4, but aside from that one cause, the fact that Streams is losing threads at a high rate from both ProducerFencedExceptions and its own internal assertion (IllegalStateException) leads me to think that Streams would be less stable in production using EOS on 2.4 than it was on 2.3.For completeness, note that I've run the same test on 2.4 and 2.3 _without_ EOS, and Streams is quite stable. Also note, that the exceptions killing my applications seem to be directly caused by frequent rebalances and network interruptions. For users running EOS Streams in reliable and stable conditions, I do not expect them to suffer thread deaths.I know that everyone is waiting for the much-delayed 2.4.0 release, so I'm not taking a hard stance on it, but from where I'm sitting, the situation seems to warrant a new RC once the fix is prepared. Also note, a new RC was just announced this morning, so assuming I can merge my PR on Monday, we're only setting the release back a couple of extra days.Note that we have not previously tested Streams under these conditions, which is why we're discovering these bugs so late in the game.

** Comment 3 **
vvcephei commented on pull request #7748: KAFKA-9231: Streams Threads may die from recoverable errors with EOS enabled
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]
"
#7691,https://github.com/apache/kafka/pull/7691,https://github.com/apache/kafka/blob/2.4.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java,null,NO,null,null
KAFKA-8933,https://issues.apache.org/jira/browse/KAFKA-8933,https://github.com/apache/kafka/blob/2.4.1/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java,An unhandled SSL handshake exception in polling event - needed a retry logic,NO,"Already client is connected and during polling event, SSL handshake failure happened. it led to leaving the co-ordinator. Even on SSL handshake failure which was actually intermittent issue, polling should have some resilient and retry the polling. Leaving group caused all instances of clients to drop and left the messages in Kafka for long time until re-subscribe the kafka topic manually.

 

 
{noformat}
2019-09-06 04:03:09,016 ERROR [reactive-kafka-xxxx] org.apache.kafka.clients.NetworkClient [Consumer clientId=aaa, groupId=bbb] Connection to node 150 (host:port) failed authentication due to: SSL handshake failed
2019-09-06 04:03:09,021 ERROR [reactive-kafka-xxxx]  reactor.kafka.receiver.internals.DefaultKafkaReceiver Unexpected exception
java.lang.NullPointerException: null
 at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.handleCompletedMetadataResponse(NetworkClient.java:1012) ~[kafka-clients-2.2.1.jar!/:?]
 at org.apache.kafka.clients.NetworkClient.handleCompletedReceives(NetworkClient.java:822) ~[kafka-clients-2.2.1.jar!/:?]
 at org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:544) ~[kafka-clients-2.2.1.jar!/:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:265) ~[kafka-clients-2.2.1.jar!/:?]
 at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:236) ~[kafka-clients-2.2.1.jar!/:?]
 at org.apache.kafka.clients.consumer.KafkaConsumer.pollForFetches(KafkaConsumer.java:1256) ~[kafka-clients-2.2.1.jar!/:?]
 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1200) ~[kafka-clients-2.2.1.jar!/:?]
 at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1176) ~[kafka-clients-2.2.1.jar!/:?]
 at reactor.kafka.receiver.internals.DefaultKafkaReceiver$PollEvent.run(DefaultKafkaReceiver.java:470) ~[reactor-kafka-1.1.1.RELEASE.jar!/:1.1.1.RELEASE]
 at reactor.kafka.receiver.internals.DefaultKafkaReceiver.doEvent(DefaultKafkaReceiver.java:401) ~[reactor-kafka-1.1.1.RELEASE.jar!/:1.1.1.RELEASE]
 at reactor.kafka.receiver.internals.DefaultKafkaReceiver.lambda$start$14(DefaultKafkaReceiver.java:335) ~[reactor-kafka-1.1.1.RELEASE.jar!/:1.1.1.RELEASE]
 at reactor.core.publisher.LambdaSubscriber.onNext(LambdaSubscriber.java:130) ~[reactor-core-3.2.10.RELEASE.jar!/:3.2.10.RELEASE]
 at reactor.core.publisher.FluxPublishOn$PublishOnSubscriber.runAsync(FluxPublishOn.java:398) ~[reactor-core-3.2.10.RELEASE.jar!/:3.2.10.RELEASE]
 at reactor.core.publisher.FluxPublishOn$PublishOnSubscriber.run(FluxPublishOn.java:484) ~[reactor-core-3.2.10.RELEASE.jar!/:3.2.10.RELEASE]
 at reactor.kafka.receiver.internals.KafkaSchedulers$EventScheduler.lambda$decorate$1(KafkaSchedulers.java:100) ~[reactor-kafka-1.1.1.RELEASE.jar!/:1.1.1.RELEASE]
 at reactor.core.scheduler.WorkerTask.call(WorkerTask.java:84) ~[reactor-core-3.2.10.RELEASE.jar!/:3.2.10.RELEASE]
 at reactor.core.scheduler.WorkerTask.call(WorkerTask.java:37) ~[reactor-core-3.2.10.RELEASE.jar!/:3.2.10.RELEASE]
 at org.springframework.cloud.sleuth.instrument.async.TraceCallable.call(TraceCallable.java:70) ~[spring-cloud-sleuth-core-2.1.1.RELEASE.jar!/:2.1.1.RELEASE]
 at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]
 at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304) ~[?:?]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[?:?]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[?:?]
 at java.lang.Thread.run(Thread.java:834) [?:?]

2019-09-06 04:03:09,023 INFO  [reactive-kafka-xxxx] org.apache.kafka.clients.consumer.internals.AbstractCoordinator [Consumer clientId=aaa, groupId=bbb] Member x_13-081e61ec-1509-4e0e-819e-58063d1ce8f6 sending LeaveGroup request to coordinator{noformat}
 ","** Comment 1 **
We saw a similar exception in 2.4/2.5:Nov 5 18:10:26 mirrormaker2-6c5bbffffc-jx85h mirrormaker2 ERROR  Failure during poll. (org.apache.kafka.connect.mirror.MirrorSourceTask:159)java.lang.NullPointerException at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.handleCompletedMetadataResponse([file java]:1071) at org.apache.kafka.clients.NetworkClient.handleCompletedReceives([file java]:847) at org.apache.kafka.clients.NetworkClient.poll([file java]:549) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll([file java]:262) at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.pollNoWakeup([file java]:303) at org.apache.kafka.clients.consumer.KafkaConsumer.poll([file java]:1249) at org.apache.kafka.clients.consumer.KafkaConsumer.poll([file java]:1211) at org.apache.kafka.connect.mirror.MirrorSourceTask.poll([file java]:137) at org.apache.kafka.connect.runtime.WorkerSourceTask.poll([file java]:259) at org.apache.kafka.connect.runtime.WorkerSourceTask.execute([file java]:226) at org.apache.kafka.connect.runtime.WorkerTask.doRun([file java]:177) at org.apache.kafka.connect.runtime.WorkerTask.run([file java]:227) at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) at java.util.concurrent.FutureTask.run(Unknown Source) at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.lang.Thread.run(Unknown Source){code}Prior to the exception, the client was disconnected. In NetworkClient.processDisconnection() several paths can lead to inProgressRequestVersion being set to null. If there's a MetadataRequest in flight to another node at the time, then the exception is hit when handling the response as inProgressRequestVersion is unboxed to an int.I was able to reproduce reliably with the following setup: - 2 brokers with SASL - consumer consuming from broker 0 - make broker 0 drop consumer connection - the consumer gets:INFO :  Error sending fetch request (sessionId=735517, epoch=2) to node 0: {}.org.apache.kafka.common.errors.DisconnectException{code} - make broker 0 fail authentication - consumer gets:INFO :  Failed authentication with localhost/127.0.0.1 (Authentication failed, invalid credentials)ERROR :  Connection to node 0 (localhost/127.0.0.1:9093) failed authentication due to: Authentication failed, invalid credentials{code} - force metadata refresh by consumer, for example: consumer.partitionsFor(""topic-that-does-not-exist""); - the consumer gets:java.lang.NullPointerException    at org.apache.kafka.clients.NetworkClient$DefaultMetadataUpdater.handleCompletedMetadataResponse([file java]:1073)    at org.apache.kafka.clients.NetworkClient.handleCompletedReceives([file java]:847)    at org.apache.kafka.clients.NetworkClient.poll([file java]:549)    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll([file java]:262)    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll([file java]:233)    at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll([file java]:212)    at org.apache.kafka.clients.consumer.internals.Fetcher.getTopicMetadata([file java]:368)    at org.apache.kafka.clients.consumer.KafkaConsumer.partitionsFor([file java]:1930)    at org.apache.kafka.clients.consumer.KafkaConsumer.partitionsFor([file java]:1898)    at main.ConsumerTest2.main([file java]:37){code}I'm not super familair with this code path but the following patch helped:diff --git [file java] [file java]index d782df865..d3119f132 100644--- [file java]+++ [file java]@@ -1067,6 +1069,9 @@ public class NetworkClient implements KafkaClient {             if (response.brokers().isEmpty()) {                 log.trace(""Ignoring empty metadata response with correlation id {}."", requestHeader.correlationId());                 this.metadata.failedUpdate(now, null);+            } else if (inProgressRequestVersion == null) {+                log.warn(""Ignoring metadata response ..."");+                this.metadata.failedUpdate(now, null);             } else {                 this.metadata.update(inProgressRequestVersion, response, now);             }{code} is this a blocker?cc  

** Comment 2 **
[Comment excluded]

** Comment 3 **
hachikuji commented on pull request #7682: KAFKA-8933; Fix NPE in DefaultMetadataUpdater after authentication failure
URL: [link]
   This patch fixes an NPE in DefaultMetadataUpdater due to an inconsistency in event expectations. Whenever there is an authentication failure, we were treating it as a failed update even if was from a separate connection from an inflight metadata request. This patch fixes the problem by making the `MetadataUpdater` api clearer in terms of the events that are handled.   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 4 **
hachikuji commented on pull request #7682: KAFKA-8933; Fix NPE in DefaultMetadataUpdater after authentication failure
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 5 **
[Comment excluded]
"
#7502,https://github.com/apache/kafka/pull/7502,https://github.com/apache/kafka/blob/2.4.1/clients/src/main/java/org/apache/kafka/common/metrics/Metrics.java,null,NO,null,null
KAFKA-9074,https://issues.apache.org/jira/browse/KAFKA-9074,https://github.com/apache/kafka/blob/2.4.1/connect/api/src/main/java/org/apache/kafka/connect/data/Values.java,Connect's Values class does not parse time or timestamp values from string literals,NO,"The `Values.parseString(String)` method that returns a `SchemaAndValue` is not able to parse a string that contains a time or timestamp literal into a logical time or timestamp value. This is likely because the `:` is a delimiter for the internal parser, and so literal values such as `2019-08-23T14:34:54.346Z` and `14:34:54.346Z` are separated into multiple tokens before matching the pattern.

The colon can be escaped to prevent the unexpected tokenization, but then the literal string contains the backslash character before each colon, and again the pattern matching for the time and timestamp literal strings fails to match.

This should be backported as far back as possible: the `Values` class was introduced in AK 1.1.0.","** Comment 1 **
rhauch commented on pull request #7568: KAFKA-9074: Correct Connect’s `Values.parseString` to properly parse a time and timestamp literal
URL: [link]
   Time and timestamp literal strings contain a `:` character, but the internal parser used in the `Values.parseString(String)` method tokenizes on the colon character to tokenize and parse map entries. The colon could be escaped, but then the backslash character used to escape the colon is not removed and the parser fails to match the literal as a time or timestamp value.   This fix corrects the parsing logic to properly parse timestamp and time literal strings whose colon characters are either escaped or unescaped. Additional unit tests were added to first verify the incorrect behavior and then to validate the correction.   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
rhauch commented on pull request #7568: KAFKA-9074: Correct Connect’s `Values.parseString` to properly parse a time and timestamp literal
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 3 **
[Comment excluded]
"
KAFKA-9540,https://issues.apache.org/jira/browse/KAFKA-9540,https://github.com/apache/kafka/blob/2.4.1/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStandbyTasks.java,"Application getting ""Could not find the standby task 0_4 while closing it"" error",YES,"Because of this the following line, there is a possibility that some standby tasks might not be created:

https://github.com/apache/kafka/blob/2.4.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L436

Then causing this line to not adding the task to standby task list:

https://github.com/apache/kafka/blob/2.4.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L299

But this line assumes that all standby tasks are to be created and add it to the standby list:

https://github.com/apache/kafka/blob/2.4.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java#L168

This results in user getting this error message on the next PARTITION_ASSIGNMENT state:

{noformat}
Could not find the standby task 0_4 while closing it (org.apache.kafka.streams.processor.internals.AssignedStandbyTasks:74)
{noformat}

But the harm caused by this issue is minimal: No standby task for some partitions. And it is recreated on the next rebalance anyway. So, I suggest lowering this message to WARN. Or probably check to WARN when standby task could not be created.","** Comment 1 **
Hey , thanks for the ticket. Your analysis is correct that this should not be logged as an error, since the cause is a completely valid situation: a standby task is not created if there are no state stores for it to actually work on. That particular standby task will actually never get created, so whichever thread ends up with this task will always hit this upon closing it. It's probably ok to go all the way down to debug, since warn might still suggest to users that something is wrong.I think this is actually fixed in trunk already due to some significant refactoring of the task management code. But I can quick together a quick PR to bump down the log level on 2.4/2.5 (won't make it into 2.5.0 but may get into 2.4.1)Of course the real fix would be for the assignor to be smart enough not to assign these ghost standbys to begin with. We should be able to fix that up as part of KIP-441 

** Comment 2 **
ableegoldman commented on pull request #8092: KAFKA-9540: Move ""Could not find the standby task while closing it"" log to debug level
URL: [link]
   As described in the ticket, this message is logged at the error level but only indicates that a standby task was not created (as is the case if its subtopology is stateless). Moving this to debug level, and clarifying the implications in the log level.   Targeting this PR against 2.4, as the issue is incidentally fixed in trunk as part of the tech debt cleanup. We should also merge this fix to 2.5 but need to wait for the release, since this is obviously not a blocker
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 3 **
guozhangwang commented on pull request #8092: KAFKA-9540: Move ""Could not find the standby task while closing it"" log to debug level
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 4 **
[Comment excluded]
"
KAFKA-9530,https://issues.apache.org/jira/browse/KAFKA-9530,https://github.com/apache/kafka/blob/2.5.0/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java,Flaky Test kafka.admin.DescribeConsumerGroupTest.testDescribeGroupWithShortInitializationTimeout,NO,"[https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/4570/testReport/junit/kafka.admin/DescribeConsumerGroupTest/testDescribeGroupWithShortInitializationTimeout/]

 
{noformat}
Error Messagejava.lang.AssertionError: assertion failedStacktracejava.lang.AssertionError: assertion failed
	at scala.Predef$.assert(Predef.scala:267)
	at kafka.admin.DescribeConsumerGroupTest.testDescribeGroupWithShortInitializationTimeout(DescribeConsumerGroupTest.scala:585)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at jdk.internal.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:118)
	at jdk.internal.reflect.GeneratedMethodAccessor27.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:182)
	at org.gradle.internal.remote.internal.hub.MessageHubBackedObjectConnection$DispatchWrapper.dispatch(MessageHubBackedObjectConnection.java:164)
	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:412)
	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
	at java.base/java.lang.Thread.run(Thread.java:834)
Standard OutputGROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
test.group      foo             0          0               0               0               -               -               -

GROUP               TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
test.group--offsets foo             0          0               0               0               -               -               -

GROUP                     COORDINATOR (ID)          ASSIGNMENT-STRATEGY  STATE           #MEMBERS
test.group--state         localhost:45389 (0)                            Empty           0{noformat}","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]

** Comment 4 **
I managed to reproduce the failure with 1 run (out of 96) locally - the issue is that DisconnectException gets raised:java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.DisconnectException: Cancelled findCoordinator request with correlation id 3 due to node 0 being disconnected....kafka.admin.ConsumerGroupCommand$ConsumerGroupService.$anonfun$describeConsumerGroups$1(ConsumerGroupCommand.scala:497)...kafka.admin.ConsumerGroupCommand$ConsumerGroupService.describeConsumerGroups(ConsumerGroupCommand.scala:496)...kafka.admin.ConsumerGroupCommand$ConsumerGroupService.collectGroupsMembers(ConsumerGroupCommand.scala:552)kafka.admin.ConsumerGroupCommand$ConsumerGroupService.describeGroups(ConsumerGroupCommand.scala:318)kafka.admin.DescribeConsumerGroupTest.testDescribeGroupWithShortInitializationTimeout(DescribeConsumerGroupTest.scala:582){code}-----------It seems like the AdminClient handles timed out calls by disconnecting () - this adds a disconnected flag to the response which  we then handle via a DisconnectedException ()

** Comment 5 **
{quote}java.lang.AssertionError: expected:<class org.apache.kafka.common.errors.TimeoutException> but was:<class org.apache.kafka.common.errors.DisconnectException> at org.junit.Assert.fail([file java]:89) at org.junit.Assert.failNotEquals([file java]:835) at org.junit.Assert.assertEquals([file java]:120) at org.junit.Assert.assertEquals([file java]:146) at kafka.admin.DescribeConsumerGroupTest.testDescribeGroupWithShortInitializationTimeout(DescribeConsumerGroupTest.scala:585){quote}

** Comment 6 **
hachikuji commented on pull request #8154: KAFKA-9530; Fix flaky test `testDescribeGroupWithShortInitializationTimeout`
URL: [link]
   This should fix the flakiness with this test case. With a short timeout, the call may fail and the client might disconnect. Rather than exposing the underlying retriable error, we give the user a TimeoutException.   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 7 **
hachikuji commented on pull request #8154: KAFKA-9530; Fix flaky test `testDescribeGroupWithShortInitializationTimeout`
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-7251,https://issues.apache.org/jira/browse/KAFKA-7251,https://github.com/apache/kafka/blob/2.5.0/clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java,Add support for TLS 1.3,NO,"Java 11 adds support for TLS 1.3. We should support this after we add support for Java 11.

Related issues:

[https://bugs.openjdk.java.net/browse/JDK-8206170]

[https://bugs.openjdk.java.net/browse/JDK-8206178]

[https://bugs.openjdk.java.net/browse/JDK-8208538]

[https://bugs.openjdk.java.net/browse/JDK-8207009]

[https://bugs.openjdk.java.net/browse/JDK-8209893]

 

 ","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
rajinisivaram commented on pull request #7804: KAFKA-7251; Add support for TLS 1.3
URL: [link]
   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-9086,https://issues.apache.org/jira/browse/KAFKA-9086,https://github.com/apache/kafka/blob/2.5.0/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamSessionWindowAggregate.java,Refactor Processor Node Streams Metrics,NO,Refactor processor node metrics as described in KIP-444. ,"** Comment 1 **
cadonna commented on pull request #7615: KAFKA-9086: Refactor processor-node-level metrics
URL: [link]
   * Refactors metrics according to KIP-444   * Introduces `ProcessorNodeMetrics` as a central provider for processor node metrics   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
bbejeck commented on pull request #7615: KAFKA-9086: Refactor processor-node-level metrics
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-9086,https://issues.apache.org/jira/browse/KAFKA-9086,https://github.com/apache/kafka/blob/2.5.0/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamWindowAggregate.java,Refactor Processor Node Streams Metrics,NO,Refactor processor node metrics as described in KIP-444. ,"** Comment 1 **
cadonna commented on pull request #7615: KAFKA-9086: Refactor processor-node-level metrics
URL: [link]
   * Refactors metrics according to KIP-444   * Introduces `ProcessorNodeMetrics` as a central provider for processor node metrics   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
bbejeck commented on pull request #7615: KAFKA-9086: Refactor processor-node-level metrics
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
#7691,https://github.com/apache/kafka/pull/7691,https://github.com/apache/kafka/blob/2.5.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStandbyTasks.java,null,NO,null,null
KAFKA-9431,https://issues.apache.org/jira/browse/KAFKA-9431,https://github.com/apache/kafka/blob/2.5.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java,Expose API in KafkaStreams to fetch all local offset lags,NO,,"** Comment 1 **
vinothchandar commented on pull request #7961:  Expose API in KafkaStreams to fetch all local offset lags
URL: [link]
    - Adds KafkaStreams#allLocalOffsetLags(), which returns lag information of all active/standby tasks local to a streams instance    - LagInfo class encapsulates the current position in the changelog, endoffset in the changelog and their difference as lag    - Lag information is a mere estimate; it can over-estimate (source topic optimization), or under-estimate.    - Each call to allLocalOffsetLags() generates a metadata call to Kafka brokers, so caution advised    - Unit and Integration tests added.   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
vvcephei commented on pull request #7961: KAFKA-9431: Expose API in KafkaStreams to fetch all local offset lags
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-8980,https://issues.apache.org/jira/browse/KAFKA-8980,https://github.com/apache/kafka/blob/2.5.0/streams/src/main/java/org/apache/kafka/streams/state/internals/AbstractRocksDBSegmentedBytesStore.java,Refactor State-Store-level Metrics,NO,Refactor state-store-level metrics as proposed in KIP-444.,"** Comment 1 **
cadonna commented on pull request #7584: KAFKA-8980: Refactor state-store-level streams metrics
URL: [link]
   - Refactors metrics according to KIP-444   - Introduces `StateStoreMetrics` as a central provider for state store metrics   - Adds metric scope (a.k.a. store type) to the in-memory suppression buffer   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
bbejeck commented on pull request #7584: KAFKA-8980: Refactor state-store-level streams metrics
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-8980,https://issues.apache.org/jira/browse/KAFKA-8980,https://github.com/apache/kafka/blob/2.5.0/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java,Refactor State-Store-level Metrics,NO,Refactor state-store-level metrics as proposed in KIP-444.,"** Comment 1 **
cadonna commented on pull request #7584: KAFKA-8980: Refactor state-store-level streams metrics
URL: [link]
   - Refactors metrics according to KIP-444   - Introduces `StateStoreMetrics` as a central provider for state store metrics   - Adds metric scope (a.k.a. store type) to the in-memory suppression buffer   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 2 **
bbejeck commented on pull request #7584: KAFKA-8980: Refactor state-store-level streams metrics
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-9540,https://issues.apache.org/jira/browse/KAFKA-9540,https://github.com/apache/kafka/blob/2.5.1/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedStandbyTasks.java,"Application getting ""Could not find the standby task 0_4 while closing it"" error",YES,"Because of this the following line, there is a possibility that some standby tasks might not be created:

https://github.com/apache/kafka/blob/2.4.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L436

Then causing this line to not adding the task to standby task list:

https://github.com/apache/kafka/blob/2.4.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L299

But this line assumes that all standby tasks are to be created and add it to the standby list:

https://github.com/apache/kafka/blob/2.4.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java#L168

This results in user getting this error message on the next PARTITION_ASSIGNMENT state:

{noformat}
Could not find the standby task 0_4 while closing it (org.apache.kafka.streams.processor.internals.AssignedStandbyTasks:74)
{noformat}

But the harm caused by this issue is minimal: No standby task for some partitions. And it is recreated on the next rebalance anyway. So, I suggest lowering this message to WARN. Or probably check to WARN when standby task could not be created.","** Comment 1 **
Hey , thanks for the ticket. Your analysis is correct that this should not be logged as an error, since the cause is a completely valid situation: a standby task is not created if there are no state stores for it to actually work on. That particular standby task will actually never get created, so whichever thread ends up with this task will always hit this upon closing it. It's probably ok to go all the way down to debug, since warn might still suggest to users that something is wrong.I think this is actually fixed in trunk already due to some significant refactoring of the task management code. But I can quick together a quick PR to bump down the log level on 2.4/2.5 (won't make it into 2.5.0 but may get into 2.4.1)Of course the real fix would be for the assignor to be smart enough not to assign these ghost standbys to begin with. We should be able to fix that up as part of KIP-441 

** Comment 2 **
ableegoldman commented on pull request #8092: KAFKA-9540: Move ""Could not find the standby task while closing it"" log to debug level
URL: [link]
   As described in the ticket, this message is logged at the error level but only indicates that a standby task was not created (as is the case if its subtopology is stateless). Moving this to debug level, and clarifying the implications in the log level.   Targeting this PR against 2.4, as the issue is incidentally fixed in trunk as part of the tech debt cleanup. We should also merge this fix to 2.5 but need to wait for the release, since this is obviously not a blocker
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 3 **
guozhangwang commented on pull request #8092: KAFKA-9540: Move ""Could not find the standby task while closing it"" log to debug level
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 4 **
[Comment excluded]
"
#8702,https://github.com/apache/kafka/pull/8702,https://github.com/apache/kafka/blob/2.6.0/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java,null,NO,null,null
KAFKA-9848,https://issues.apache.org/jira/browse/KAFKA-9848,https://github.com/apache/kafka/blob/2.6.0/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java,Avoid triggering scheduled rebalance delay when task assignment fails but Connect workers remain in the group,YES,"There are cases where a Connect worker does not receive its tasks assignments successfully after a rebalance but will still remain in the group. For example when a SyncGroup response is lost, a worker will not get its expected assignments but will rejoin the group immediately and will trigger another rebalance. 

With incremental cooperative rebalancing, tasks assignments that are computed and sent by the leader but are not received by any of the members are marked as lost assignments in the subsequent rebalance. The presence of lost assignments activates the scheduled rebalance delay (property) and the missing tasks are not assigned until this delay expires.


This situation can be improved in two cases: 
a) When it's the leader that failed to receive the new assignments from the broker coordinator (for example if the SyncGroup request or response was lost). If this worker remains the leader of the group in the subsequent rebalance round, it can detect that the previous assignment was not successfully applied by checking what's the expected generation.

b) If one or more regular members did not receive their assignments successfully, but have joined the latest round of rebalancing, they can be assigned the tasks that remain unassigned from the previous assignment immediately without these tasks being marked as lost. The leader can detect that by checking that some tasks seem lost since the previous assignment but also the number of workers is unchanged between the two rounds of rebalancing. In this case, the leader can go ahead and assign the missing tasks as new tasks immediately.",
KAFKA-10134,https://issues.apache.org/jira/browse/KAFKA-10134,https://github.com/apache/kafka/blob/2.6.0/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java,High CPU issue during rebalance in Kafka consumer after upgrading to 2.5,NO,"We want to utilize the new rebalance protocol to mitigate the stop-the-world effect during the rebalance as our tasks are long running task.

But after the upgrade when we try to kill an instance to let rebalance happen when there is some load(some are long running tasks >30S) there, the CPU will go sky-high. It reads ~700% in our metrics so there should be several threads are in a tight loop. We have several consumer threads consuming from different partitions during the rebalance. This is reproducible in both the new CooperativeStickyAssignor and old eager rebalance rebalance protocol. The difference is that with old eager rebalance rebalance protocol used the high CPU usage will dropped after the rebalance done. But when using cooperative one, it seems the consumers threads are stuck on something and couldn't finish the rebalance so the high CPU usage won't drop until we stopped our load. Also a small load without long running task also won't cause continuous high CPU usage as the rebalance can finish in that case.

 

""executor.kafka-consumer-executor-4"" #124 daemon prio=5 os_prio=0 cpu=76853.07ms elapsed=841.16s tid=0x00007fe11f044000 nid=0x1f4 runnable  [0x00007fe119aab000]""executor.kafka-consumer-executor-4"" #124 daemon prio=5 os_prio=0 cpu=76853.07ms elapsed=841.16s tid=0x00007fe11f044000 nid=0x1f4 runnable  [0x00007fe119aab000]   java.lang.Thread.State: RUNNABLE at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:467) at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1275) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1241) at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1216) at

 

By debugging into the code we found it looks like the clients are  in a loop on finding the coordinator.

I also tried the old rebalance protocol for the new version the issue still exists but the CPU will be back to normal when the rebalance is done.

Also tried the same on the 2.4.1 which seems don't have this issue. So it seems related something changed between 2.4.1 and 2.5.0.

 ","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
Cooperative:ConsumerConfig values: 	allow.auto.create.topics = true	auto.commit.interval.ms = 5000	auto.offset.reset = latest	bootstrap.servers = 	check.crcs = true	client.dns.lookup = default	client.id = 	client.rack = 	connections.max.idle.ms = 540000	default.api.timeout.ms = 60000	enable.auto.commit = false	exclude.internal.topics = true	fetch.max.bytes = 52428800	fetch.max.wait.ms = 500	fetch.min.bytes = 1	group.id = xxx-consumer-group	group.instance.id = null	heartbeat.interval.ms = 3000	interceptor.classes = 	internal.leave.group.on.close = true	isolation.level = read_uncommitted	key.deserializer = class com.cisco.wx2.kafka.serialization.SimpleKafkaDeserializer	max.partition.fetch.bytes = 1048576	max.poll.interval.ms = 1800000	max.poll.records = 10	metadata.max.age.ms = 300000	metric.reporters = 	metrics.num.samples = 2	metrics.recording.level = INFO	metrics.sample.window.ms = 30000	partition.assignment.strategy = 	receive.buffer.bytes = 65536	reconnect.backoff.max.ms = 1000	reconnect.backoff.ms = 50	request.timeout.ms = 30000	retry.backoff.ms = 100	sasl.client.callback.handler.class = null	sasl.jaas.config = null	sasl.kerberos.kinit.cmd = /usr/bin/kinit	sasl.kerberos.min.time.before.relogin = 60000	sasl.kerberos.service.name = null	sasl.kerberos.ticket.renew.jitter = 0.05	sasl.kerberos.ticket.renew.window.factor = 0.8	sasl.login.callback.handler.class = null	sasl.login.class = null	sasl.login.refresh.buffer.seconds = 300	sasl.login.refresh.min.period.seconds = 60	sasl.login.refresh.window.factor = 0.8	sasl.login.refresh.window.jitter = 0.05	sasl.mechanism = GSSAPI	security.protocol = SSL	security.providers = null	send.buffer.bytes = 131072	session.timeout.ms = 30000	ssl.cipher.suites = null	ssl.enabled.protocols = 	ssl.endpoint.identification.algorithm = https	ssl.key.password = null	ssl.keymanager.algorithm = SunX509	ssl.keystore.location = null	ssl.keystore.password = null	ssl.keystore.type = JKS	ssl.protocol = TLS	ssl.provider = null	ssl.secure.random.implementation = null	ssl.trustmanager.algorithm = PKIX	ssl.truststore.location = null	ssl.truststore.password = null	ssl.truststore.type = JKS	value.deserializer = class com.cisco.wx2.kafka.serialization.SimpleKafkaDeserializerEager:ConsumerConfig values: 	allow.auto.create.topics = true	auto.commit.interval.ms = 5000	auto.offset.reset = latest	bootstrap.servers = 	check.crcs = true	client.dns.lookup = default	client.id = 	client.rack = 	connections.max.idle.ms = 540000	default.api.timeout.ms = 60000	enable.auto.commit = false	exclude.internal.topics = true	fetch.max.bytes = 52428800	fetch.max.wait.ms = 500	fetch.min.bytes = 1	group.id = xxx-consumer-group	group.instance.id = null	heartbeat.interval.ms = 3000	interceptor.classes = 	internal.leave.group.on.close = true	isolation.level = read_uncommitted	key.deserializer = class com.cisco.wx2.kafka.serialization.SimpleKafkaDeserializer	max.partition.fetch.bytes = 1048576	max.poll.interval.ms = 1800000	max.poll.records = 10	metadata.max.age.ms = 300000	metric.reporters = 	metrics.num.samples = 2	metrics.recording.level = INFO	metrics.sample.window.ms = 30000	partition.assignment.strategy = 	receive.buffer.bytes = 65536	reconnect.backoff.max.ms = 1000	reconnect.backoff.ms = 50	request.timeout.ms = 30000	retry.backoff.ms = 100	sasl.client.callback.handler.class = null	sasl.jaas.config = null	sasl.kerberos.kinit.cmd = /usr/bin/kinit	sasl.kerberos.min.time.before.relogin = 60000	sasl.kerberos.service.name = null	sasl.kerberos.ticket.renew.jitter = 0.05	sasl.kerberos.ticket.renew.window.factor = 0.8	sasl.login.callback.handler.class = null	sasl.login.class = null	sasl.login.refresh.buffer.seconds = 300	sasl.login.refresh.min.period.seconds = 60	sasl.login.refresh.window.factor = 0.8	sasl.login.refresh.window.jitter = 0.05	sasl.mechanism = GSSAPI	security.protocol = SSL	security.providers = null	send.buffer.bytes = 131072	session.timeout.ms = 30000	ssl.cipher.suites = null	ssl.enabled.protocols = 	ssl.endpoint.identification.algorithm = https	ssl.key.password = null	ssl.keymanager.algorithm = SunX509	ssl.keystore.location = null	ssl.keystore.password = null	ssl.keystore.type = JKS	ssl.protocol = TLS	ssl.provider = null	ssl.secure.random.implementation = null	ssl.trustmanager.algorithm = PKIX	ssl.truststore.location = null	ssl.truststore.password = null	ssl.truststore.type = JKS	value.deserializer = class com.cisco.wx2.kafka.serialization.SimpleKafkaDeserializerWith 2.5.1 we can reproduce the high CPU issue with both eager and cooperative rebalancing protocol but not in 2.4.1. The difference between eager and cooperative with 2.5.1 is that for eager rebalance the CPU can go back to normal after the rebalance is done but for cooperative it seems it stuck on rebalancing and never ends.

** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]

** Comment 6 **
we have experienced same issue,one easy way to reproduce is: 1. start both kafka server and java consumer thread 2. stop kafkathen you should observe consumer thread keeps busy cpu loop and cause very high cputhe reason is due to java consumer 2.5.0 code change// code placeholderorg.apache.kafka.clients.consumer.KafkaConsumer  line: 1235if (includeMetadataInTimeout) {    // try to update assignment metadata BUT do not need to block on the timer,    // since even if we are 1) in the middle of a rebalance or 2) have partitions    // with unknown starting positions we may still want to return some data    // as long as there are some partitions fetchable; NOTE we always use a timer with 0ms    // to never block on completing the rebalance procedure if there's any    updateAssignmentMetadataIfNeeded(time.timer(0L));}{code}if it fails to fetch metadata, this line never blocks, and it run thru and keep cpu busy loop, due to // code placeholderwhile (timer.notExpired());{code}please help on this issue, this is blocker for us to upgrade to java client 2.5.0it's particular bad if we deploy both kafka/java consumer in same VM/(k8s node) if something wrong make kafka hiccup, all java consumers cause cpu high, and make kafka even slower to recover (like restart by k8s), and eventually make entire node/VM not be able to going forward. (java consumers keep busy loop to wait kafka ready, kafka has too little cpu to move on) 

** Comment 7 **
do {    client.maybeTriggerWakeup();    if (includeMetadataInTimeout) {        // try to update assignment metadata BUT do not need to block on the timer,        // since even if we are 1) in the middle of a rebalance or 2) have partitions        // with unknown starting positions we may still want to return some data        // as long as there are some partitions fetchable; NOTE we always use a timer with 0ms        // to never block on completing the rebalance procedure if there's any        updateAssignmentMetadataIfNeeded(time.timer(0L));    } else {        while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) {            log.warn(""Still waiting for metadata"");        }    }    final Map<TopicPartition, List<ConsumerRecord<K, V>>> records = pollForFetches(timer);    if (!records.isEmpty()) {        // before returning the fetched records, we can send off the next round of fetches        // and avoid block waiting for their responses to enable pipelining while the user        // is handling the fetched records.        //        // NOTE: since the consumed position has already been updated, we must not allow        // wakeups or any other errors to be triggered prior to returning the fetched records.        if (fetcher.sendFetches() > 0 || client.hasPendingRequests()) {            client.transmitSends();        }        return this.interceptors.onConsume(new ConsumerRecords<>(records));    }} while (timer.notExpired());{code}from the current design, i guess one possible fix is to add exponentially retry if metadata is not available and nothing returned by pollForFetches(timer), until timer expired.then let outside application code to call consumer.poll(timeout) again 

** Comment 8 **
[Comment excluded]

** Comment 9 **
[Comment excluded]

** Comment 10 **
[Comment excluded]

** Comment 11 **
[Comment excluded]

** Comment 12 **
something like this fix my issues, but i am not sure whether this is right thing to do to fit bigger picture// poll for new data until the timeout expiresMap<TopicPartition, List<ConsumerRecord<K, V>>> records = null;do {    client.maybeTriggerWakeup();    if (includeMetadataInTimeout) {        // try to update assignment metadata BUT do not need to block on the timer if we still have        // some assigned partitions, since even if we are 1) in the middle of a rebalance        // or 2) have partitions with unknown starting positions we may still want to return some data        // as long as there are some partitions fetchable; NOTE we always use a timer with 0ms        // to never block on completing the rebalance procedure if there's any        if (subscriptions.fetchablePartitions(tp -> true).isEmpty() || records == null || records.isEmpty()) {            updateAssignmentMetadataIfNeeded(timer);        } else {            final Timer updateMetadataTimer = time.timer(0L);            updateAssignmentMetadataIfNeeded(updateMetadataTimer);            timer.update(updateMetadataTimer.currentTimeMs());        }    } else {        while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) {            log.warn(""Still waiting for metadata"");        }    }    records = pollForFetches(timer);    if (!records.isEmpty()) {        // before returning the fetched records, we can send off the next round of fetches        // and avoid block waiting for their responses to enable pipelining while the user        // is handling the fetched records.        //        // NOTE: since the consumed position has already been updated, we must not allow        // wakeups or any other errors to be triggered prior to returning the fetched records.        if (fetcher.sendFetches() > 0 || client.hasPendingRequests()) {            client.transmitSends();        }        return this.interceptors.onConsume(new ConsumerRecords<>(records));    }} while (timer.notExpired());{code}

** Comment 13 **
[Comment excluded]

** Comment 14 **
[Comment excluded]

** Comment 15 **
[Comment excluded]

** Comment 16 **
Hi, for ""What's still puzzling me is that, even in the second branch, since we always keep calling `timer.update` then we should still eventually exit the while loop with `timer.expired`. So why would we observe that it blocks inside the while-loop forever is not clear to me.""yes, it will exit while loop eventually, but usually the app code is like following (at least in my case)while (!shutdown) {    try {        ConsumerRecords<byte, byte> records = consumer.poll(Duration.ofSeconds(30));        if (records.isEmpty()) continue;        processRecords(records);    } catch (Throwable e) {        if (shutdown) break;        logger.error(""failed to pull message, retry in 10 seconds"", e);        Threads.sleepRoughly(Duration.ofSeconds(10));    }}{code}so during 30s, if kafak is down in the middle, since fetchablePartitions return non-empty, the consumer.poll keeps busy loop,and as soon as it exists, the application usually will try to poll immediately.in application level, sure i can put delay if poll return empty, but still it will trigger high cpu time to time, and timeout passed into consumer.poll can't be high, say if i use 5 secs,in the second case, the thread acts like, high cpu 5sec, -> sleep 5s -> 100%cpu 5s, (which is still not considered as healthy behavior) 

** Comment 17 **
[Comment excluded]

** Comment 18 **
[Comment excluded]

** Comment 19 **
[Comment excluded]

** Comment 20 **
[Comment excluded]

** Comment 21 **
[Comment excluded]

** Comment 22 **
[Comment excluded]

** Comment 23 **
[Comment excluded]

** Comment 24 **
[Comment excluded]

** Comment 25 **
[Comment excluded]

** Comment 26 **
[Comment excluded]

** Comment 27 **
[Comment excluded]

** Comment 28 **
[Comment excluded]

** Comment 29 **
[Comment excluded]

** Comment 30 **
[Comment excluded]

** Comment 31 **
, I have three brokers and 10 consumers. When I restart one of consumers, some of other consumers will be with high CPU issue. // from [file java] (a fine fix)// private ConsumerRecords<K, V> poll(final Timer timer, final boolean includeMetadataInTimeout);                if (includeMetadataInTimeout) {                    // try to update assignment metadata BUT do not need to block on the timer if we still have                    // some assigned partitions, since even if we are 1) in the middle of a rebalance                    // or 2) have partitions with unknown starting positions we may still want to return some data                    // as long as there are some partitions fetchable; NOTE we always use a timer with 0ms                    // to never block on completing the rebalance procedure if there's any                    if (subscriptions.fetchablePartitions(tp -> true).isEmpty()) {                        updateAssignmentMetadataIfNeeded(timer);                    } else {                        final Timer updateMetadataTimer = time.timer(0L);                        updateAssignmentMetadataIfNeeded(updateMetadataTimer);                        timer.update(updateMetadataTimer.currentTimeMs());                    }                } else {                    while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE))) {                        log.warn(""Still waiting for metadata"");                    }                }{code}// from [file java] (last commit)// private ConsumerRecords<K, V> poll(final Timer timer, final boolean includeMetadataInTimeout);               if (includeMetadataInTimeout) {                    // try to update assignment metadata BUT do not need to block on the timer for join group                    updateAssignmentMetadataIfNeeded(timer, false);                } else {                    while (!updateAssignmentMetadataIfNeeded(time.timer(Long.MAX_VALUE), true)) {                        log.warn(""Still waiting for metadata"");                    }                }{code}Per the above two commits I have one question about `updateAssignmentMetadataIfNeeded(timer, false);` why the second parameter is false? Per my understanding when it's false, actually it's same as before `updateAssignmentMetadataIfNeeded(time.timer(0L));` I've tested w/ true, it looks fine for us and I'm thinking when it's w/ true, the behavior is similar to .

** Comment 32 **
 Just to clarify are you working with  on the same issue?The rationale for the final fix is that we only need `timer.timer(0L)` for join-group, but not for others, for example even if the flag is set to false we would still use the original timer trying to discover the coordinator etc, because our setting was based on the observation that when the coordinator is not available, we are spending busy loops looking for it.From your description, your case actually is not the same as my setup or 's, i.e. the coordinator is fine, but all the partitions are revoked and hence you have none to fetch from while looping for the join-group request to complete. I've prepared a new PR that adds the fetchable logic back in a more efficient way, LMK if it works for you: [link]

** Comment 33 **
[Comment excluded]

** Comment 34 **
  I tried to reproduce your high CPU with {{three brokers and 10 consumers, and restart one of consumers}} but I failed to do that locally.I've prepared a patch just improving on some log4j entries as we suspect your issue maybe related to heartbeats: [link] Could you try it out while reproducing the issue, and share your logs (you'd have to enable it to at least ERROR level) here so we can further understand the root cause?

** Comment 35 **
 refer to the attached file (consumer5.log.2020-07-22.log), the last re-join happened at "" 10:52:41.247  INFO  o.a.k.c.consumer.internals.AbstractCoordinator -  (Re-)joining group"" because there is one consumer trying to join. interesting, this time looks better than before, but still spend more than one min.

** Comment 36 **
[Comment excluded]

** Comment 37 **
 Did your run include both the log4j improvement and the other PR depending on fetchable partitions to do long polling?

** Comment 38 **
BTW I found that the main latency during rebalance is on discovering the coordinator while we keep getting ""Join group failed with org.apache.kafka.common.errors.DisconnectException"" and it kept retrying for about a minute. But I think you did not shutdown the broker in your experiment, is there anything else happening that cause that broker node to be not reachable?

** Comment 39 **
 I'm using logback and PR: [link] included as well.  yeah, just to restart consumer w/o broker change, both brokers &consumers were running on my local laptop, not found any other issue.

** Comment 40 **
[Comment excluded]

** Comment 41 **
[Comment excluded]

** Comment 42 **
What I did not see from my local run is the following:{code}Join group failed with org.apache.kafka.common.errors.DisconnectException{code}Which indicates that the socket connecting to the brokers cannot be established, and the consumer has to retry discovering (the same) broker, and then re-connect to it, and somehow after one minute or two the issue goes away itself. If you did not restart the broker during that time, then I can only think of transient network issues..Actually, could you try only patching the logback PR but not the 9011 PR (i.e. let it to falls into busy loop) and upload the logs so I can also check what's causing the busy loops as well?

** Comment 43 **
The long time joining group is related to *max.poll.records*, looks like rejoin is trigged in *poll()*, it means only when processing complete and *poll()* be called. And *DisconnectException* should not be a real socket error, before leader consumer issues a rejoin request, looks like rejoin request from other consumers will be failed w/ *DisconnectException*.

** Comment 44 **
[Comment excluded]

** Comment 45 **
What's in PR #9011 was meant to help debug the remaining issue, we know it was not merged. I reopened the Jira to avoid confusion. , thoughts on the remaining issue?

** Comment 46 **
I think I'd need more information to further investigate this issue.  could you apply [link] only (this is for improved log4j entries) on top of 2.6 to reproduce this issue and then upload the enhanced log file?

** Comment 47 **
 sure, I'll porting PR #9038 on 2.6.0 and share log with u. thx.please refer to the attached file: consumer3.log.2020-08-20.log

** Comment 48 **
 Thanks for the new log files, it has been very helpful for me to nail down the root causes and I will refine an existing WIP PR [link] as a final fix for this. Please stay tuned.

** Comment 49 **
[Comment excluded]

** Comment 50 **
[Comment excluded]

** Comment 51 **
 I've tested against PR #8834, it works fine for our scenario. appreciate.

** Comment 52 **
[Comment excluded]

** Comment 53 **
[Comment excluded]

** Comment 54 **
 thanks, I've tested clients changes in PR #8834 against legacy Kafka server, it works fine as well. thanks for your response. 

** Comment 55 **
I'd like to see the title of this bug clarified: It is worse than just ""High CPU usage"".  I have a couple of Kafka Streams apps with a high number of tasks/threads and this issue is causing infinite rebalance loops where the entire *cluster stops processing and cannot successfully rebalance*.  This causes hard downtime.  I've had to roll back to 2.4.1.Edit: clarification: tested with 2.6.0.  Have not tested 2.5.I'm currently working on building the patch, will test.Late to the party since you've already got the fix in progress, but in case it helps, I'd like to share what I'm seeing:The rebalance failures seems to be associated the TimeoutExceptions, DisconnectionExceptions and other side effects as noted in earlier comments.  When many StreamThreads are all spinning, then each time a rebalance is attempted, when there are a large number of threads it is likely that _some_ thread will fail, and the rebalance never succeeds.  The downward spiral begins as ConsumerThreads become ""fenced"" and it triggers a full (not incremental) rebalance, and eventually all data flow gets blocked.  I've tried different combinations of session.timeout.ms, rebalance.timeout.ms, max.poll.time.ms, default.api.timeout.ms (as recommended in the text of the timeout exceptions) to no avail.Of my applications, the ones that are affected include * one stateless app with num.stream.threads=24.  With more than 1 instance (2-4x=48-96 threads), it will often never rebalance correctly, or only after multiple attempts (30+ minutes).   * one stateful app with 36 partitions of large-ish (500MB-1GB each) state stores which can take a while to restore.  This app successfully starts if I shut down all instances, delete state stores, set initial rebalance delay, and start all up simultaneously – but if any instance restarts or I attempt to scale up later, then rebalance will never succeed.  Additionally, when state stores are reassigned, there are ""LockExceptions"" (DEBUG level logs) in a tight loop, and the state stores fail to be closed cleanly, which forces the restore process to begin all over again.  The only way I can successfully do a rolling restart is if I use static membership and increase the session timeout.  If there is only a single instance of the app, then it works with no problems (but this is not a solution as I need multiple instances for scale).Other side effects: the tight loop logs several DEBUG logs, which filled up log storage and caused pod evictions, which caused state stores to become invalid and restore (workaround: disable this logging).Additionally, have seen the following exceptions sporadically, not sure if these are separate bugs:{{2020-08-31T00:40:47.786Z ERROR Uncaught stream processing error! KafkaStreamsConfiguration java.lang.IllegalStateException: There are insufficient bytes available to read assignment from the sync-group response (actual byte size 0) , this is not expected; it is possible that the leader's assign function is buggy and did not return any assignment for this member, or *because static member is configured and the protocol is buggy* hence did not get the assignment for this member}} {{    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete([file java]:367)}} {{    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded([file java]:440)}} {{    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup([file java]:359)}} {{    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll([file java]:513)}} {{    at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded([file java]:1268)}} {{    at org.apache.kafka.clients.consumer.KafkaConsumer.poll([file java]:1230)}} {{    at org.apache.kafka.clients.consumer.KafkaConsumer.poll([file java]:1210)}} {{    at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests([file java]:766)}} {{    at org.apache.kafka.streams.processor.internals.StreamThread.runOnce([file java]:624)}} {{    at org.apache.kafka.streams.processor.internals.StreamThread.runLoop([file java]:551)}} {{    at org.apache.kafka.streams.processor.internals.StreamThread.run([file java]:510)}}{{2020-09-03T15:53:17.524Z ERROR Uncaught stream processing error! KafkaStreamsConfiguration java.lang.IllegalStateException: Active task 3_0 should have been suspended}} {{    at org.apache.kafka.streams.processor.internals.TaskManager.handleAssignment([file java]:281)}} {{    ... 13 common frames omitted}} {{Wrapped by: java.lang.RuntimeException: Unexpected failure to close 1 task(s) ]. First unexpected exception (for task 3_0) follows.}} {{    at org.apache.kafka.streams.processor.internals.TaskManager.handleAssignment([file java]:349)}} {{    at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.onAssignment([file java]:1428)}} {{    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.invokeOnAssignment([file java]:279)}} {{    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete([file java]:421)}} {{    ... 10 common frames omitted}} {{Wrapped by: org.apache.kafka.common.KafkaException: User rebalance callback throws an error}} {{    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete([file java]:436)}} {{    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded([file java]:440)}} {{    at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup([file java]:359)}} {{    at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll([file java]:513)}} {{    at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded([file java]:1268)}} {{    at org.apache.kafka.clients.consumer.KafkaConsumer.poll([file java]:1230)}} {{    at org.apache.kafka.clients.consumer.KafkaConsumer.poll([file java]:1210)}} {{    at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests([file java]:766)}} {{    at org.apache.kafka.streams.processor.internals.StreamThread.runOnce([file java]:628)}} {{    at org.apache.kafka.streams.processor.internals.StreamThread.runLoop([file java]:551)}} {{    at org.apache.kafka.streams.processor.internals.StreamThread.run([file java]:510)}}

** Comment 56 **
Hey ,It looks like you might have run into a few distinct issues: the rebalancing problems, the ""insufficient bytes available"" IllegalStateException, and the ""Active task 3_0 should have been suspended"" IllegalStateException.The rebalancing seems to point to this issue, as the full fix did not make it into 2.6.0 in time. It would be great if you could test out the patch and see if that helps (building from  specifically, which is not yet merged). The patch I linked also includes a fix for KAFKA-10122, another cause of unnecessary rebalances.For the two IllegalStateException issues, could you open separate tickets? They seem unrelated to this, and to each other, but definitely merit a closer look. Any logs you have from the time of the exceptions would help a lot. Thanks!

** Comment 57 **
[Comment excluded]

** Comment 58 **
[Comment excluded]

** Comment 59 **
[Comment excluded]
"
KAFKA-10249,https://issues.apache.org/jira/browse/KAFKA-10249,https://github.com/apache/kafka/blob/2.6.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java,In-memory stores are skipped when checkpointing but not skipped when reading the checkpoint,NO,"As the title suggests, offsets for in-memory stores (including the suppression buffer) are not written to the checkpoint file. However, when reading from the checkpoint file during task initialization, we do not check StateStore#persistent. We attempt to look up the offsets for in-memory stores in the checkpoint file, and obviously do not find them.

With eos we have to conclude that the existing state is dirty and thus throw a TaskCorruptedException. So pretty much any task with in-memory state will always hit this exception when reinitializing from the checkpoint, forcing it to clear the entire state directory and build up all of its state again from scratch (both persistent and in-memory).

This is especially unfortunate for KIP-441, as we will hit this any time a task is moved from one thread to another.","** Comment 1 **
[Comment excluded]
"
KAFKA-10263,https://issues.apache.org/jira/browse/KAFKA-10263,https://github.com/apache/kafka/blob/2.6.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java,Do not create standbys for those revoking active tasks if it is not stateful,NO,"Today in StreamsPartitionAssignor, if an intended active tasks is not yet revoked from the old owner we would not give it to the newly assigned owner, but instead we would assign it as a standby task to the new owner to let it start restoring a bit early.

However, if that task is not stateful, then there's no point trying to let it restore at all. This should be avoided in the assignor.",
KAFKA-10758,https://issues.apache.org/jira/browse/KAFKA-10758,https://github.com/apache/kafka/blob/2.6.1/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java,Kafka Streams consuming from a pattern goes to PENDING_SHUTDOWN when adding a new topic,NO,"I have a simple Kafka Stream app that consumes from multiple input topics using the _stream_ function that accepts a Pattern ([link|https://kafka.apache.org/26/javadoc/org/apache/kafka/streams/StreamsBuilder.html#stream-java.util.regex.Pattern-]).
 
Whenever I add a new topic that matches the pattern the kafka stream state goes to REBALANCING -> ERROR -> PENDING_SHUTDOWN .
If I restart the app it correctly starts reading again without problems.
It is by design? Should I handle this and simply restart the app?
 
Kafka Stream version is 2.6.0.
The error is the following:
{code:java}
ERROR o.a.k.s.p.i.ProcessorTopology - Set of source nodes do not match:
sourceNodesByName = [KSTREAM-SOURCE-0000000003, KSTREAM-SOURCE-0000000002]
sourceTopicsByName = [KSTREAM-SOURCE-0000000000, KSTREAM-SOURCE-0000000014, KSTREAM-SOURCE-0000000003, KSTREAM-SOURCE-0000000002]
org.apache.kafka.common.KafkaException: User rebalance callback throws an error
  at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:436)
  at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:440)
  at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:359)
  at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:513)
  at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1268)
  at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1230)
  at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1210)
  at org.apache.kafka.streams.processor.internals.StreamThread.pollRequests(StreamThread.java:766)
  at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:624)
  at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:551)
  at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:510)
 Caused by: java.lang.IllegalStateException: Tried to update source topics but source nodes did not match
  at org.apache.kafka.streams.processor.internals.ProcessorTopology.updateSourceTopics(ProcessorTopology.java:151)
  at org.apache.kafka.streams.processor.internals.AbstractTask.update(AbstractTask.java:109)
  at org.apache.kafka.streams.processor.internals.StreamTask.update(StreamTask.java:514)
  at org.apache.kafka.streams.processor.internals.TaskManager.updateInputPartitionsAndResume(TaskManager.java:397)
  at org.apache.kafka.streams.processor.internals.TaskManager.handleAssignment(TaskManager.java:261)
  at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.onAssignment(StreamsPartitionAssignor.java:1428)
  at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.invokeOnAssignment(ConsumerCoordinator.java:279)
  at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.onJoinComplete(ConsumerCoordinator.java:421)
  ... 10 common frames omitted
 KafkaStream state is ERROR
 17:28:53.200 [datalake-StreamThread-1] ERROR o.apache.kafka.streams.KafkaStreams - stream-client [datalake] All stream threads have died. The instance will be in error state and should be closed.
 ============> User rebalance callback throws an error
 KafkaStream state is PENDING_SHUTDOWN
{code}
 
 ","** Comment 1 **
[Comment excluded]

** Comment 2 **
 here the details requested:I have this code:private val inputCommandsStream =    streamsBuilder.stream](Pattern.compile(""^ingestion\\.datalake\\..+\\..+\\.commands$""))private val inputEventStream =    streamsBuilder.stream](Pattern.compile(""^ingestion\\.datalake\\..+\\..+\\.events"")){code}And here the topology:Topologies:   Sub-topology: 0    Source: KSTREAM-SOURCE-0000000000 (topics: )      --> KSTREAM-PROCESSOR-0000000001    Processor: KSTREAM-PROCESSOR-0000000001 (stores: )      --> none      <-- KSTREAM-SOURCE-0000000000  Sub-topology: 1    Source: KSTREAM-SOURCE-0000000002 (topics: ^ingestion\.datalake\..+\..+\.commands$)      --> KSTREAM-LEFTJOIN-0000000006    Processor: KSTREAM-LEFTJOIN-0000000006 (stores: )      --> KSTREAM-MAP-0000000010, KSTREAM-SINK-0000000007      <-- KSTREAM-SOURCE-0000000002    Source: KSTREAM-SOURCE-0000000003 (topics: ^ingestion\.datalake\..+\..+\.events)      --> KSTREAM-FILTER-0000000004    Processor: KSTREAM-FILTER-0000000004 (stores: )      --> KSTREAM-AGGREGATE-0000000005      <-- KSTREAM-SOURCE-0000000003    Processor: KSTREAM-AGGREGATE-0000000005 (stores: )      --> KTABLE-TOSTREAM-0000000008      <-- KSTREAM-FILTER-0000000004    Processor: KSTREAM-MAP-0000000010 (stores: )      --> KSTREAM-FILTER-0000000013      <-- KSTREAM-LEFTJOIN-0000000006    Processor: KSTREAM-FILTER-0000000013 (stores: )      --> KSTREAM-SINK-0000000012      <-- KSTREAM-MAP-0000000010    Processor: KTABLE-TOSTREAM-0000000008 (stores: )      --> KSTREAM-SINK-0000000009      <-- KSTREAM-AGGREGATE-0000000005    Sink: KSTREAM-SINK-0000000007 (extractor class: service.streaming.EventStreamTopicNameExtractor@20801cbb)      <-- KSTREAM-LEFTJOIN-0000000006    Sink: KSTREAM-SINK-0000000009 (extractor class: service.streaming.SnapshotStreamTopicNameExtractor@1c240cf2)      <-- KTABLE-TOSTREAM-0000000008    Sink: KSTREAM-SINK-0000000012 (topic: KSTREAM-TOTABLE-0000000011-repartition)      <-- KSTREAM-FILTER-0000000013  Sub-topology: 2    Source: KSTREAM-SOURCE-0000000014 (topics: )      --> KSTREAM-TOTABLE-0000000011    Processor: KSTREAM-TOTABLE-0000000011 (stores: )      --> none      <-- KSTREAM-SOURCE-0000000014{code}It is a work in progress, for sure to be optimized, but I don't understand the reason for the error.thanks!

** Comment 3 **
Also, to be more precise, the error doesn't happen exactly when I add the topic. But I think it happens when the app try to refresh the topics information. I think every 5 mins by default. Right?

** Comment 4 **
Hey , thanks for submitting this ticket. Looks like there's a bug in the topic update logic. I've opened a PR which we should be able to get into the 2.6.1 and 2.7.0 releases.Just to answer some of your other questions: yes, this error wouldn't appear right after the topic was created but only once the Streams app refreshed its topic metadata (5min default as you said). Yes, it should be safe to just restart the application after hitting this error as a workaround. And no, this was not by design :) Sorry for the trouble. Obviously I'd recommend upgrading to 2.6.1 to get the fix once it's been released, but for now you should be ok to just ignore it and start up the application again. You'll only hit this error once, since after the restart there's no need to update anything

** Comment 5 **
[Comment excluded]
"
#9757,https://github.com/apache/kafka/pull/9757,https://github.com/apache/kafka/blob/2.7.0/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java,null,NO,null,null
KAFKA-10839,https://issues.apache.org/jira/browse/KAFKA-10839,https://github.com/apache/kafka/blob/2.7.0/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerCoordinator.java,Improve consumer group coordinator unavailable message,NO,"When a consumer encounters an issue that triggers marking a coordinator as unknown, the error message it prints does not give context about the error that triggered it.
{noformat}
log.info(""Group coordinator {} is unavailable or invalid, will attempt rediscovery"", this.coordinator);{noformat}
These may be triggered by response errors or the coordinator becoming disconnected. We should improve this error message to make the cause clear.",
KAFKA-9274,https://issues.apache.org/jira/browse/KAFKA-9274,https://github.com/apache/kafka/blob/2.7.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopicManager.java,Gracefully handle timeout exceptions on Kafka Streams,NO,"Right now streams don't treat timeout exception as retriable in general by throwing it to the application level. If not handled by the user, this would kill the stream thread unfortunately.

In fact, timeouts happen mostly due to network issue or server side unavailability. Hard failure on client seems to be an over-kill.

We would like to discuss what's the best practice to handle timeout exceptions on Streams. The current state is still brainstorming and consolidate all the cases that contain timeout exception within this ticket.

This ticket is backed by KIP-572: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-572%3A+Improve+timeouts+and+retries+in+Kafka+Streams]","** Comment 1 **
[Comment excluded]

** Comment 2 **
Thanks for starting this, !One thing we should document is that the current approach is actually intentional. Streams essentially delegates the handling of transient failures into the clients (Consumer and Producer and Admin). Accordingly, the current advice is to set the client timeouts high enough to tolerate transient outages.However, I agree with you that this is not the best approach. Setting the timeout high enough to be resilient to most network outages has the perverse effect of allowing StreamThreads to get hung up on an individual operation for that amount of time, harming the overall ability of the system to make progress on all inputs. For example, what if only one broker is unhealthy/unavailable? Rather than sit around indefinitely waiting to commit on the partition that broker is leader for, we could attempt to make progress on other tasks, and then try to commit the stuck one again later.Another issue is self-healing. Suppose that the broker cluster becomes unavailable. If we're running an application in which some or all of the threads will die from timeouts, then whenever the broker's operators _do_ bring it back up, we would have to actually restart the application to recover. Maybe this doesn't seem so bad, but it you're in a large organization operating 100 Streams apps, it sounds like a pretty big pain to me.Conversely, if Streams were to just log a warning each time it got a timeout, but continue trying to make progress, then we would know that there is something wrong (because we're monitoring the app for warnings), so we could alert the broker's operators. However, once the issue is resolved, our app will just automatically pick right back up where it left off.At a very high level, I'd propose the following failure handling protocol:1. *retriable error*: A transient failure (like a timeout exception). We just put the task to the side, and try to do work for the next task. When we come back around to the current task, we should try to repeat the operation. I.e., if it was a timeout on commit, we should try to commit again when we come back around, as opposed to just continuing to process that task without committing.2. *recoverable error*: A permanent failure (like getting fenced), than we can recover from by re-joining the cluster. We try to close and re-open the producer, or initiate a rebalance to re-join the cluster, depending on the exact nature of the failure.3. *fatal error*: A permanent failure that requires human intervention to resolve. Something like discovering that we're in the middle of upgrading to an incompatible topology, or that the application itself is invalid for some other reason. Attempting to continue could result in data corruption, so we should just shut down the whole application so that someone can figure out what's wrong and fix it.

** Comment 3 **
[Comment excluded]

** Comment 4 **
[Comment excluded]

** Comment 5 **
guozhangwang commented on pull request #8060: KAFKA-9274: Gracefully handle timeout exception 
URL: [link]
   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 6 **
guozhangwang commented on pull request #8060: KAFKA-9274: Gracefully handle timeout exception
URL: [link]
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 7 **
mjsax commented on pull request #8120: KAFKA-9274: Graceful handle TimeoutException in GlobalStateManager
URL: [link]
   *More detailed description of your change,   if necessary. The PR title and PR message become   the squashed commit message, so use a separate   comment to ping reviewers.*   *Summary of testing strategy (including rationale)   for the feature or bug fix. Unit and/or integration   tests are expected for any behaviour change and   system tests should be considered for larger changes.*   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org


** Comment 8 **
mjsax commented on pull request #8122: KAFKA-9274: Add retries for handling TimeoutExceptions
URL: [link]
   *More detailed description of your change,   if necessary. The PR title and PR message become   the squashed commit message, so use a separate   comment to ping reviewers.*   *Summary of testing strategy (including rationale)   for the feature or bug fix. Unit and/or integration   tests are expected for any behaviour change and   system tests should be considered for larger changes.*   ### Committer Checklist (excluded from commit message)   -  Verify design and implementation    -  Verify test coverage and CI build status   -  Verify documentation (including upgrade notes)----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
For queries about this service, please contact Infrastructure at:
users@infra.apache.org

"
KAFKA-10755,https://issues.apache.org/jira/browse/KAFKA-10755,https://github.com/apache/kafka/blob/2.7.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java,Should consider commit latency when computing next commit timestamp,NO,"In 2.6, we reworked the main processing/commit loop in `StreamThread` and introduced a regression, by _not_ updating the current time after committing. This implies that we compute the next commit timestamp too low (ie, too early).

For small commit intervals and high commit latency (like in EOS), this big may lead to an increased commit frequency and fewer processed records between two commits, and thus to reduced throughput.

For example, assume that the commit interval is 100ms and the commit latency is 50ms, and we start the commit at timestamp 10000. The commit finishes at 10050, and the next commit should happen at 10150. However, if we don't update the current timestamp, we incorrectly compute the next commit time as 10100, ie, 50ms too early, and we have only 50ms to process data instead of the intended 100ms.

In the worst case, if the commit latency is larger than the commit interval, it would imply that we commit after processing a single record per task.","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]
"
KAFKA-12152,https://issues.apache.org/jira/browse/KAFKA-12152,https://github.com/apache/kafka/blob/2.7.1/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java,Idempotent Producer does not reset the sequence number of partitions without in-flight batches,NO,"When a `OutOfOrderSequenceException` error is received by an idempotent producer for a partition, the producer bumps its epoch, adjusts the sequence number and the epoch of the in-flight batches of the partitions affected by the `OutOfOrderSequenceException` error. This happens in `TransactionManager#bumpIdempotentProducerEpoch`.

The remaining partitions are treated separately. When the last in-flight batch of a given partition is completed, the sequence number is reset. This happens in `TransactionManager#handleCompletedBatch`.

However, when a given partition does not have in-flight batches when the producer epoch is bumped, its sequence number is not reset. It results in having subsequent producer request to use the new producer epoch with the old sequence number and to be rejected by the broker.",
KAFKA-12462,https://issues.apache.org/jira/browse/KAFKA-12462,https://github.com/apache/kafka/blob/2.7.1/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java,Threads in PENDING_SHUTDOWN entering a rebalance can cause an illegal state exception ,NO,"A thread was removed, sending it to the PENDING_SHUTDOWN state, but went through a rebalance before completing the shutdown.
{code:java}
// [2021-03-07 04:33:39,385] DEBUG [i-07430efc31ad166b7-StreamThread-6] stream-thread [i-07430efc31ad166b7-StreamThread-6] Ignoring request to transit from PENDING_SHUTDOWN to PARTITIONS_REVOKED: only DEAD state is a valid next state (org.apache.kafka.streams.processor.internals.StreamThread)
{code}
Inside StreamsRebalanceListener#onPartitionsRevoked, we have
{code:java}
// 
if (streamThread.setState(State.PARTITIONS_REVOKED) != null && !partitions.isEmpty())
    taskManager.handleRevocation(partitions);
{code}
Since PENDING_SHUTDOWN → PARTITIONS_REVOKED is a disallowed transition, we never invoke TaskManager#handleRevocation. Currently handleRevocation is responsible for preparing any active tasks for close, including committing offsets and writing the checkpoint as well as suspending the task. We can’t close the task in handleRevocation since we still support EAGER rebalancing, which invokes handleRevocation at the beginning of a rebalance on all tasks.

The tasks that are actually revoked will be closed during TaskManager#handleAssignment . The IllegalStateException is specifically because we don’t suspend the task before attempting to close it, and the direct transition from RUNNING → CLOSED is forbidden.","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]

** Comment 4 **
[Comment excluded]
"
#9489,https://github.com/apache/kafka/pull/9489,https://github.com/apache/kafka/blob/2.7.1/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java,null,NO,null,null
KAFKA-10632,https://issues.apache.org/jira/browse/KAFKA-10632,https://github.com/apache/kafka/blob/2.8.0/clients/src/main/java/org/apache/kafka/common/metrics/stats/Percentiles.java,Raft client should push all committed data to listeners,NO,"We would like to move to a push model for sending committed data to the state machine. This simplifies the state machine a bit since it does not need to track its own position and poll for new data. It also allows the raft layer to ensure that all committed data up to the state of a leader epoch has been sent before allowing the state machine to begin sending writes. Finally, it allows us to take advantage of optimizations. For example, we can save the need to re-read writes that have been sent to the leader; instead, we can retain the data in memory and push it to the state machine after it becomes committed.",
KAFKA-10835,https://issues.apache.org/jira/browse/KAFKA-10835,https://github.com/apache/kafka/blob/2.8.0/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java,Replace Runnable and Callable overrides with lambdas in Connect,NO,We've been using Java 8 for sometime now of course. Replacing the overrides from the pre-Java 8 era will simplify some parts of the code and will reduce verbosity. ,
KAFKA-12270,https://issues.apache.org/jira/browse/KAFKA-12270,https://github.com/apache/kafka/blob/2.8.0/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java,Kafka Connect may fail a task when racing to create topic,YES,"When a source connector configured with many tasks and to use the new topic creation feature is run, it is possible that multiple tasks will attempt to write to the same topic, will see that the topic does not exist, and then race to create the topic. The topic is only created once, but some tasks might fail with:
{code:java}
org.apache.kafka.connect.errors.ConnectException: Task failed to create new topic (name=TOPICX, numPartitions=8, replicationFactor=3, replicasAssignments=null, configs={cleanup.policy=delete}). Ensure that the task is authorized to create topics or that the topic exists and restart the task
  at org.apache.kafka.connect.runtime.WorkerSourceTask.maybeCreateTopic(WorkerSourceTask.java:436)
  at org.apache.kafka.connect.runtime.WorkerSourceTask.sendRecords(WorkerSourceTask.java:364)
  at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:264)
  at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:185)
  at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:235)
... {code}
The reason appears to be that the WorkerSourceTask throws an exception if the topic creation failed, and does not account for the fact that the topic may have been created between the time the WorkerSourceTask lists existing topics and tries to create the topic.

 

See in particular: [https://github.com/apache/kafka/blob/5c562efb2d76407011ea88c1ca1b2355079935bc/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java#L415-L423]

 

This is only an issue when using topic creation settings in the source connector configuration, and when running multiple tasks that write to the same topic.

The workaround is to create the topics manually before starting the connector, or to simply restart the failed tasks using the REST API.","** Comment 1 **
[Comment excluded]

** Comment 2 **
I don't think it's because the exception is retriable, because the `TopicAdmin.createTopics(...)` method explicitly catches a `TopicExistsException` and simply returns a response without the topic name, signaling that the topic was not created. I think the response is adequate to know whether my request created the topic (or topics), but it's less than ideal if the caller simply wants to know whether the topic ""was created or already existed"".We can't easily change the response, but we could potentially add an overloaded method that takes a flag as to whether existing topics should be included in the response. I'm just not sure that's worth it. WDYT?BTW, I've added a PR that keeps the TopicAdmin methods the same and instead just re-describes the topic. This should be an infrequent occurrence, but I'm happy to eliminate the re-describe if you think that's a better approach.

** Comment 3 **
Ok, I didn't really like that the first PR added another admin client call to (re)describe the topic in question when the `createTopic(...)` method returned false, meaning the topic was not created (because it already existed by the time the create topic request was made/received by Kafka).So, I created an alternative PR that changes how the `TopicAdmin` creates a topic and returns precisely which topic names were created AND which were found to already exist. This allows the `WorkerSourceTask` to know exactly what happend and to log it accordingly.This new method in `TopicAdmin` is called `createOrFindTopics(...)`, and it is the old implementation of `createTopics(...)` with only slight modification; the previously-existing `createTopics(...)` and `createTopic(...)` methods were changed to delegate to the new method. Thus the behavior of the existing methods remains unchanged (for the multiple places where they are called), but we get the more precise results in `WorkerSourceTask`.

** Comment 4 **
[Comment excluded]
"
#10256,https://github.com/apache/kafka/pull/10256,https://github.com/apache/kafka/blob/2.8.0/raft/src/main/java/org/apache/kafka/raft/KafkaRaftClient.java,null,NO,null,null
KAFKA-13037,https://issues.apache.org/jira/browse/KAFKA-13037,https://github.com/apache/kafka/blob/2.8.1/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java,"""Thread state is already PENDING_SHUTDOWN"" log spam",YES,"KAFKA-12462 introduced a [change|https://github.com/apache/kafka/commit/4fe4cdc4a61cbac8e070a8b5514403235194015b#diff-76f629d0df8bd30b2593cbcf4a2dc80de3167ebf55ef8b5558e6e6285a057496R722] that increased this ""Thread state is already {}"" logger to info instead of debug. We are running into a problem with our streams apps when they hit an unrecoverable exception that shuts down the streams thread, we get this log printed about 50,000 times per second per thread. I am guessing it is once per record we have queued up when the exception happens? We have temporarily raised the StreamThread logger to WARN instead of INFO to avoid the spam, but we do miss the other good logs we get on INFO in that class. Could this log be reverted back to debug? Thank you! ","** Comment 1 **
Hey , would you be interested in submitting a PR for this? I completely agree that logging this at INFO on every iteration is wildly inappropriate, I just didn't push it at the time since I figured someone would file a ticket if it was really bothering people. And here we are :) 

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]
"
KAFKA-12257,https://issues.apache.org/jira/browse/KAFKA-12257,https://github.com/apache/kafka/blob/3.0.0/clients/src/main/java/org/apache/kafka/clients/Metadata.java, Consumer mishandles topics deleted and recreated with the same name,NO,"In KAFKA-7738, caching of leader epochs (KIP-320) was added to o.a.k.c.Metadata to ignore metadata responses with epochs smaller than the last seen epoch.

The current implementation can cause problems in cases where a consumer is subscribed to a topic that has been deleted and then recreated with the same name. This is something seen more often in consumers that subscribe to a multitude of topics using a wildcard.

Currently, when a topic is deleted and the Fetcher receives UNKNOWN_TOPIC_OR_PARTITION, the leader epoch is not cleared. If at a later time while the consumer is still running a topic is created with the same name, the leader epochs are set to 0 for the new topics partitions, and are likely smaller than those for the previous topic. For example, if a broker had restarted during the lifespan of the previous topic, the leader epoch would be at least 1 or 2. In this case the metadata will be ignored since it is incorrectly considered stale. Of course, the user will sometimes get lucky, and if a topic was only recently created so that the epoch is still 0, no problem will occur on recreation. The issue is also not seen when consumers happen to have been restarted in between deletion and recreation.

The most common side effect of the new metadata being disregarded is that the new partitions end up assigned but the Fetcher is unable to fetch data because it does not know the leaders. When recreating a topic with the same name it is likely that the partition leaders are not the same as for the previous topic, and the number of partitions may even be different. Besides not being able to retrieve data for the new topic, there is a more sinister side effect of the Fetcher triggering a metadata update after the fetch fails. The subsequent update will again ignore the topic's metadata if the leader epoch is still smaller than the cached value. This metadata refresh loop can continue indefinitely and with a sufficient number of consumers may even put a strain on a cluster since the requests are occurring in a tight loop. This can also be hard for clients to identify since there is nothing logged by default that would indicate what's happening. Both the Metadata class's logging of ""_Not replacing existing epoch_"", and the Fetcher's logging of ""_Leader for partition <T-P> is unknown_"" are at DEBUG level.

A second possible side effect was observed where if the consumer is acting as leader of the group and happens to not have any current data for the previous topic, e.g. it was cleared due to a metadata error from a broker failure, then the new topic's partitions may simply end up unassigned within the group. This is because while the subscription list contains the recreated topic the metadata for it was previously ignored due to the leader epochs. In this case the user would see logs such as:
{noformat}
WARN o.a.k.c.c.i.ConsumerCoordinator [Consumer clientId=myClientId, groupId=myGroup] The following subscribed topics are not assigned to any members: [myTopic]{noformat}
Interestingly, I believe the Producer is less affected by this problem since o.a.k.c.p.i.ProducerMetadata explicitly clears knowledge of its topics in retainTopics() after each metadata expiration. ConsumerMetadata does no such thing.

To reproduce this issue:
 # Turn on DEBUG logging, e.g. for org.apache.kafka.clients.consumer and org.apache.kafka.clients.Metadata
 # Begin a consumer for a topic (or multiple topics)
 # Restart a broker that happens to be a leader for one of the topic's partitions
 # Delete the topic
 # Create another topic with the same name
 # Publish data for the new topic
 # The consumer will not receive data for the new topic, and there will be a high rate of metadata requests.
 # The issue can be corrected by restarting the consumer or restarting brokers until leader epochs are large enough

I believe KIP-516 (unique topic ids) will likely fix this problem, since after those changes the leader epoch map should be keyed off of the topic id, rather than the name.

One possible workaround with the current version of Kafka is to add code to onPartitionsRevoked() to manually clear leader epochs before each rebalance, e.g.
{code:java}
Map<TopicPartition, Integer> emptyLeaderEpochs = new HashMap<>();
ConsumerMetadata metadata = (ConsumerMetadata)FieldUtils.readField(consumer, ""metadata"", 
true);
FieldUtils.writeField(metadata, ""lastSeenLeaderEpochs"", emptyLeaderEpochs, true);{code}
This is not really recommended of course, since besides modifying private consumer state, it defeats the purpose of epochs! It does in a sense revert the consumer to pre-2.2 behavior before leader epochs existed.","** Comment 1 **
With the changes from  there some options to fix this bug. This ticket mentions using the topic ID as a key for the leader epoch map, but another option that might be a bit simpler to implement is checking for a new topic ID in the request.There is already work ongoing to update the Fetch path to use topic IDs () and this will include storing the topic ID in the consumer's metadata cache.  Upon receiving a new metadata response, we can check if the topic ID matches the ID already stored in the cache and set a flag if it has changed. Then, in `updateLatestMetadata`, we have code that checks the epoch:if (currentEpoch == null || newEpoch >= currentEpoch) {    log.debug(""Updating last seen epoch for partition {} from {} to epoch {} from new metadata"", tp, currentEpoch, newEpoch);    lastSeenLeaderEpochs.put(tp, newEpoch);    return Optional.of(partitionMetadata);{code}If we include an or ( || ) for the changed topic ID, I believe this will achieve the behavior this ticket is looking for. The lastSeenEpoch will be reset to the epoch of the new topic and we will use the new partitionMetadata.

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]
"
KAFKA-13276,https://issues.apache.org/jira/browse/KAFKA-13276,https://github.com/apache/kafka/blob/3.0.0/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java,Public DescribeConsumerGroupsResult constructor refers to KafkaFutureImpl,NO,The new public DescribeConsumerGroupsResult constructor refers to the non-public API KafkaFutureImpl,
KAFKA-12578,https://issues.apache.org/jira/browse/KAFKA-12578,https://github.com/apache/kafka/blob/3.0.0/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslServerAuthenticator.java,Remove deprecated security classes/methods,NO,,
KAFKA-9726,https://issues.apache.org/jira/browse/KAFKA-9726,https://github.com/apache/kafka/blob/3.0.0/connect/runtime/src/test/java/org/apache/kafka/connect/util/clusters/EmbeddedKafkaCluster.java,IdentityReplicationPolicy for MM2 to mimic MM1,NO,"Per KIP-382, we should support MM2 in ""legacy mode"", i.e. with behavior similar to MM1. A key requirement for this is a ReplicationPolicy that does not rename topics.","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]

** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]
"
KAFKA-13173,https://issues.apache.org/jira/browse/KAFKA-13173,https://github.com/apache/kafka/blob/3.0.0/metadata/src/main/java/org/apache/kafka/controller/QuorumController.java,KRaft controller does not handle simultaneous broker expirations correctly,NO,"In `ReplicationControlManager.fenceStaleBrokers`, we find all of the current stale replicas and attempt to remove them from the ISR. However, when multiple expirations occur at once, we do not properly accumulate the ISR changes. For example, I ran a test where the ISR of a partition was initialized to [1, 2, 3]. Then I triggered a timeout of replicas 2 and 3 at the same time. The records that were generated by `fenceStaleBrokers` were the following:

{code}
ApiMessageAndVersion(PartitionChangeRecord(partitionId=0, topicId=_seg8hBuSymBHUQ1sMKr2g, isr=[1, 3], leader=1, replicas=null, removingReplicas=null, addingReplicas=null) at version 0), ApiMessageAndVersion(FenceBrokerRecord(id=2, epoch=102) at version 0), 
ApiMessageAndVersion(PartitionChangeRecord(partitionId=0, topicId=_seg8hBuSymBHUQ1sMKr2g, isr=[1, 2], leader=1, replicas=null, removingReplicas=null, addingReplicas=null) at version 0), 
ApiMessageAndVersion(FenceBrokerRecord(id=3, epoch=103) at version 0)]
{code}

First the ISR is shrunk to [1, 3] as broker 2 is fenced. We also see the record to fence broker 2. Then the ISR is modified to [1, 2] as the fencing of broker 3 is handled. So we did not account for the fact that we had already fenced broker 2 in the request. 

A simple solution for now is to change the logic to handle fencing only one broker at a time. ","** Comment 1 **
I upgraded this bug to a blocker because I think it can result in data loss. For example, in the example above, the second ISR change would be interpreted as an expansion, but there may have been committed writes to the log between the two ISR changes which were not reflected in the expansion.
"
KAFKA-12779,https://issues.apache.org/jira/browse/KAFKA-12779,https://github.com/apache/kafka/blob/3.0.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/StateDirectory.java,TaskMetadata should return actual TaskId rather than plain String,NO,"Not sure why this was encoded as a String field instead of using the public TaskId class. We should use an actual TaskId object, especially as we may add additional fields that increase the complexity and parsing of the taskId.

[KIP-740: Use TaskId instead of String for the taskId field in TaskMetadata|https://cwiki.apache.org/confluence/display/KAFKA/KIP-740%3A+Use+TaskId+instead+of+String+for+the+taskId+field+in+TaskMetadata]",
KAFKA-13488,https://issues.apache.org/jira/browse/KAFKA-13488,https://github.com/apache/kafka/blob/3.0.1/clients/src/main/java/org/apache/kafka/clients/Metadata.java,Producer fails to recover if topic gets deleted (and gets auto-created),YES,"Producer currently fails to produce messages to a topic if the topic is deleted and gets auto-created OR is created manually during the lifetime of the producer (and certain other conditions are met - leaderEpochs of deleted topic > 0).

 

To reproduce, these are the steps which can be carried out:

0) A cluster with 2 brokers 0 and 1 with auto.topic.create=true.

1) Create a topic T with 2 partitions P0-> (0,1), P1-> (0,1)

2) Reassign the partitions such that P0-> (1,0), P1-> (1,0).

2) Create a producer P and send few messages which land on all the TPs of topic T.

3) Delete the topic T

4) Immediately, send a new message from producer P, this message will be failed to send and eventually timed out.

A test-case which fails with the above steps is added at the end as well as a patch file.

 

This happens after leaderEpoch (KIP-320) was introduced in the MetadataResponse KAFKA-7738. There is a solution attempted to fix this issue in KAFKA-12257, but the solution has a bug due to which the above use-case still fails.

 

*Issue in the solution of KAFKA-12257:*
{code:java}
// org.apache.kafka.clients.Metadata.handleMetadataResponse():
       ...
        Map<String, Uuid> topicIds = new HashMap<>();
        Map<String, Uuid> oldTopicIds = cache.topicIds();
        for (MetadataResponse.TopicMetadata metadata : metadataResponse.topicMetadata()) {
            String topicName = metadata.topic();
            Uuid topicId = metadata.topicId();
            topics.add(topicName);
            // We can only reason about topic ID changes when both IDs are valid, so keep oldId null unless the new metadata contains a topic ID
            Uuid oldTopicId = null;
            if (!Uuid.ZERO_UUID.equals(topicId)) {
                topicIds.put(topicName, topicId);
                oldTopicId = oldTopicIds.get(topicName);
            } else {
                 topicId = null;
            }
    ...
} {code}
With every new call to {{{}handleMetadataResponse{}}}(), {{cache.topicIds()}} gets created afresh. When a topic is deleted and created immediately soon afterwards (because of auto.create being true), producer's call to {{MetadataRequest}} for the deleted topic T will result in a {{UNKNOWN_TOPIC_OR_PARTITION}} or {{LEADER_NOT_AVAILABLE}} error {{MetadataResponse}} depending on which point of topic recreation metadata is being asked at. In the case of errors, TopicId returned back in the response is {{{}Uuid.ZERO_UUID{}}}. As seen in the above logic, if the topicId received is ZERO, the method removes the earlier topicId entry from the cache.

Now, when a non-Error Metadata Response does come back for the newly created topic T, it will have a non-ZERO topicId now but the leaderEpoch for the partitions will mostly be ZERO. This situation will lead to rejection of the new MetadataResponse if the older LeaderEpoch was >0 (for more details, refer to KAFKA-12257). Because of the rejection of the metadata, producer will never get to know the new Leader of the TPs of the newly created topic.

 

{{*}} 1. Solution / Fix (Preferred){*}:
Client's metadata should keep on remembering the old topicId till:
1) response for the TP has ERRORs
2) topicId entry was already present in the cache earlier
3) retain time is not expired
{code:java}
--- a/clients/src/main/java/org/apache/kafka/clients/Metadata.java
+++ b/clients/src/main/java/org/apache/kafka/clients/Metadata.java
@@ -336,6 +336,10 @@ public class Metadata implements Closeable {
                 topicIds.put(topicName, topicId);
                 oldTopicId = oldTopicIds.get(topicName);
             } else {
+                // Retain the old topicId for comparison with newer TopicId created later. This is only needed till retainMs
+                if (metadata.error() != Errors.NONE && oldTopicIds.get(topicName) != null && retainTopic(topicName, false, nowMs))
+                    topicIds.put(topicName, oldTopicIds.get(topicName));
+                else
                     topicId = null;
             }

{code}
{{*}} 2. Alternative Solution / Fix {{*}}:
To allow updates to LeaderEpoch when originalTopicId was {{{}null{}}}. This is less desirable as when cluster moves from no topic IDs to using topic IDs, we will count this topic as new and update LeaderEpoch irrespective of whether newEpoch was greater than current or not.
{code:java}
@@ -394,7 +398,7 @@ public class Metadata implements Closeable {
         if (hasReliableLeaderEpoch && partitionMetadata.leaderEpoch.isPresent()) {
             int newEpoch = partitionMetadata.leaderEpoch.get();
             Integer currentEpoch = lastSeenLeaderEpochs.get(tp);
-            if (topicId != null && oldTopicId != null && !topicId.equals(oldTopicId)) {
+            if (topicId != null && !topicId.equals(oldTopicId)) {
                 // If both topic IDs were valid and the topic ID changed, update the metadata
                 log.info(""Resetting the last seen epoch of partition {} to {} since the associated topicId changed from {} to {}"",
                          tp, newEpoch, oldTopicId, topicId);
{code}
From the above discussion, i think Solution 1 would be a better solution.

–
Testcase to repro the issue:
{code:java}
  @Test
  def testSendWithTopicDeletionMidWay(): Unit = {
    val numRecords = 10

    // create topic with leader as 0 for the 2 partitions.
    createTopic(topic, Map(0 -> Seq(0, 1), 1 -> Seq(0, 1)))

    val reassignment = Map(
      new TopicPartition(topic, 0) -> Seq(1, 0),
      new TopicPartition(topic, 1) -> Seq(1, 0)
    )

    // Change leader to 1 for both the partitions to increase leader Epoch from 0 -> 1
    zkClient.createPartitionReassignment(reassignment)
    TestUtils.waitUntilTrue(() => !zkClient.reassignPartitionsInProgress,
      ""failed to remove reassign partitions path after completion"")

    val producer = createProducer(brokerList, maxBlockMs = 5 * 1000L, deliveryTimeoutMs = 20 * 1000)

    (1 to numRecords).map { i =>
      val resp = producer.send(new ProducerRecord(topic, null, (""value"" + i).getBytes(StandardCharsets.UTF_8))).get
      assertEquals(topic, resp.topic())
    }

    // start topic deletion
    adminZkClient.deleteTopic(topic)

    // Verify that the topic is deleted when no metadata request comes in
    TestUtils.verifyTopicDeletion(zkClient, topic, 2, servers)
    
    // Producer would timeout and not self-recover after topic deletion.
    val e = assertThrows(classOf[ExecutionException], () => producer.send(new ProducerRecord(topic, null, (""value"").getBytes(StandardCharsets.UTF_8))).get)
    assertEquals(classOf[TimeoutException], e.getCause.getClass)
  }
{code}
Attaching the solution proposal and test repro as a patch file.","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]

** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]

** Comment 6 **
[Comment excluded]

** Comment 7 **
[Comment excluded]

** Comment 8 **
[Comment excluded]
"
KAFKA-13472,https://issues.apache.org/jira/browse/KAFKA-13472,https://github.com/apache/kafka/blob/3.0.1/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java,Connect can lose track of last committed offsets for topic partitions after partial consumer revocation,NO,"The Connect framework tracks the last successfully-committed offsets for each topic partition that is currently assigned to the consumer of each sink task. If a sink task throws an exception from {{{}SinkTask::preCommit{}}}, the consumer is ""rewound"" by seeking to those last successfully-committed offsets for each topic partition, so that the same records can be redelivered to the task again.

With the changes from KAFKA-12487, we failed to correctly update the logic for tracking these last-committed offsets which can cause topic partitions to be missing from them after partial revocation of topic partitions from the consumer. Specifically, we make the assumption that, whenever an offset commit succeeds, the offsets that were successfully committed constitute the entirely of the last-committed offsets for the task; when a partial consumer revocation takes place, we only commit offsets for some of the topic partitions assigned to the task's producer, and this assumption fails.","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]
"
KAFKA-12487,https://issues.apache.org/jira/browse/KAFKA-12487,https://github.com/apache/kafka/blob/3.0.1/connect/runtime/src/test/java/org/apache/kafka/connect/integration/MonitorableSinkConnector.java,Sink connectors do not work with the cooperative consumer rebalance protocol,NO,"The {{ConsumerRebalanceListener}} used by the framework to respond to rebalance events in consumer groups for sink tasks is hard-coded with the assumption that the consumer performs rebalances eagerly. In other words, it assumes that whenever {{onPartitionsRevoked}} is called, all partitions have been revoked from that consumer, and whenever {{onPartitionsAssigned}} is called, the partitions passed in to that method comprise the complete set of topic partitions assigned to that consumer.

See the [WorkerSinkTask.HandleRebalance class|https://github.com/apache/kafka/blob/b96fc7892f1e885239d3290cf509e1d1bb41e7db/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L669-L730] for the specifics.

 

One issue this can cause is silently ignoring to-be-committed offsets provided by sink tasks, since the framework ignores offsets provided by tasks in their {{preCommit}} method if it does not believe that the consumer for that task is currently assigned the topic partition for that offset. See these lines in the [WorkerSinkTask::commitOffsets method|https://github.com/apache/kafka/blob/b96fc7892f1e885239d3290cf509e1d1bb41e7db/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSinkTask.java#L429-L430] for reference.

 

This may not be the only issue caused by configuring a sink connector's consumer to use cooperative rebalancing. Rigorous unit and integration testing should be added before claiming that the Connect framework supports the use of cooperative consumers with sink connectors.","** Comment 1 **
Based on recent  discussion, we'll also want to implement the   method in order to avoid task failures due to offset commit failures if/when the consumer rebalance protocol is automatically downgraded from {{COOPERATIVE}} to {{EAGER}}.

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]

** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]

** Comment 6 **
[Comment excluded]

** Comment 7 **
[Comment excluded]

** Comment 8 **
Hi . My understanding of the issue is that we are fixing a bug here. Therefore, merging it to 3.1 and 3.0 is fine for me. Please correct if it is not a bug.
"
#11449,https://github.com/apache/kafka/pull/11449,https://github.com/apache/kafka/blob/3.1.0/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java,null,NO,null,null
KAFKA-12648,https://issues.apache.org/jira/browse/KAFKA-12648,https://github.com/apache/kafka/blob/3.1.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java,Experiment with resilient isomorphic topologies,NO,"We're not ready to make this a public feature yet, but I want to start experimenting with some ways to make Streams applications more resilient in the face of isomorphic topological changes (eg adding/removing/reordering subtopologies).

If this turns out to be stable and useful, we can circle back on doing a KIP to bring this feature into the public API",
KAFKA-13669,https://issues.apache.org/jira/browse/KAFKA-13669,https://github.com/apache/kafka/blob/3.2.0/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java,Log messages for source tasks with no offsets to commit are noisy and confusing,YES,"The [messages|https://github.com/apache/kafka/blob/71cbff62b685ef2b6b3169c2e69693687ddf7b3f/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerSourceTask.java#L493-L500] that Connect emits during offset commit for source tasks that haven't accrued any new offsets since their last commit are logged at INFO level, which has confused some users who saw them and believed that something was wrong with their connector.

We introduced these fairly recently as part of the work in [https://github.com/apache/kafka/pull/11323] for KAFKA-12226.

We may want to lower the level of these messages to DEBUG in order to avoid giving users the impression that something's wrong with their connector, especially if there's a source task running that has gotten fully caught-up with the system it's polling and is simply waiting for more data to become available.","** Comment 1 **
I've experienced this messages on Kafka Datagen Source Connector using Confluent Platform (7.0.1) / Apache Kafka 3.0.1.Very confusing, thought there was an issue, but data was still been produced.Lowering the log level to DEBUG make sense to me.

** Comment 2 **
Seen this with replicator and my customer thought it was an issue. Lowering the log level to DEBUG make sense to me.
"
KAFKA-13682,https://issues.apache.org/jira/browse/KAFKA-13682,https://github.com/apache/kafka/blob/3.2.0/metadata/src/main/java/org/apache/kafka/controller/QuorumController.java,Implement auto preferred leader election in KRaft Controller,NO,,
KAFKA-13423,https://issues.apache.org/jira/browse/KAFKA-13423,https://github.com/apache/kafka/blob/3.2.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStreamThread.java,Error falsely reported on Kafka Streams app when GlobalKTables are used ,YES,"It seems that error of this form:

_ERROR streams.KafkaStreams: stream-client [testAppId-56832986-6ff0-4583-8aaf-85fafd7b4fe4] Global thread has died. The streams application or client will now close to ERROR._

are being reported when Streams are closed gracefully. This seems to be not right. See attached files for repro case 

 ",
KAFKA-9847,https://issues.apache.org/jira/browse/KAFKA-9847,https://github.com/apache/kafka/blob/3.2.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/TopologyMetadata.java,Add config to set default store type,NO,"Kafka Streams supports persistent RocksDB stores as well as in-memory stores out of the box. By default all DSL operators use persistent RocksDB stores. Currently, it is only possible to switch out RocksDB stores with in-memory store on a per operator basis what is tedious.

We propose to add a new config to set the default store type globally.

KIP-591: [https://cwiki.apache.org/confluence/display/KAFKA/KIP-591%3A+Add+Kafka+Streams+config+to+set+default+store+type]","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]
"
KAFKA-14089,https://issues.apache.org/jira/browse/KAFKA-14089,https://github.com/apache/kafka/blob/3.3.0/connect/runtime/src/test/java/org/apache/kafka/connect/integration/MonitorableSourceConnector.java,Flaky ExactlyOnceSourceIntegrationTest.testSeparateOffsetsTopic,NO,"It looks like the sequence got broken around ""65535, 65537, 65536, 65539, 65538, 65541, 65540, 65543""","** Comment 1 **
[Comment excluded]

** Comment 2 **
Thanks . We don't assert on order of records, just that the expected seqnos were present in any order, so the wonkiness around 65535 isn't actually an issue (and it's even present in the stringified representation of both the expected _and_ the actual seqno sets).After doing some Bash scrubbing on the file attached to the ticket, it looks like seqnos start to be missing (i.e., they're in the expected set but not the actual) between 114463 and 114754. Not every seqno in that range is missing, but there's 105 missing in total. After that, starting at 114755, there's 105 extra (i.e., in the actual set but not the expected) seqnos.Given that the issues crop up at the very end of the seqno set, it seems like this could be caused by non-graceful shutdown of the worker after exactly-once support is disabled, or even possibly the recently-discovered KAFKA-14079. -It's a little worrisome, though, since the results here indicate possible data loss.- Actually, on second thought, this is probably not data loss, since we're reading the records that have been produced to Kafka, but not necessarily the records whose offsets have been committed.If this was on Jenkins, do you have a link to the CI run that caused it? Or if it was encountered elsewhere, do you have any logs available? I'll try to kick off some local runs but I'm in the middle of stress-testing my laptop with the latest KIP-618 system tests and may not be able to reproduce locally.I suspect a fix for this would involve reading the last-committed offset for each task, then only checking seqnos for that task up to the seqno in that offset. But I'd like to have a better idea of what exactly is causing the failure before pulling the trigger on that, especially if it's unclean task/worker shutdown and we can find a way to fix that instead of adjusting our tests to handle sloppy shutdowns.

** Comment 3 **
[Comment excluded]

** Comment 4 **
Thanks Mickael. I put together a draft fix , although I still haven't been able to replicate the failure locally. If you have time, would you mind giving it a try and see if it has positive effects in your environment? I can also kick off several Jenkins builds by re-triggering CI runs, although that will be more time-consuming as it will run the build for the whole project instead of just Connect.
"
KAFKA-14104,https://issues.apache.org/jira/browse/KAFKA-14104,https://github.com/apache/kafka/blob/3.3.0/metadata/src/test/java/org/apache/kafka/metalog/LocalLogManager.java,Perform CRC validation on KRaft Batch Records and Snapshots,NO,"Today we stamp the BatchRecord header with a CRC [1] and verify that CRC before the log is written to disk [2]. The CRC checks should also be verified when the records are read back from disk. The same procedure should be followed for KRaft snapshots as well.

[1] [https://github.com/apache/kafka/blob/6b76c01cf895db0651e2cdcc07c2c392f00a8ceb/clients/src/main/java/org/apache/kafka/common/record/DefaultRecordBatch.java#L501=] 

[2] [https://github.com/apache/kafka/blob/679e9e0cee67e7d3d2ece204a421ea7da31d73e9/core/src/main/scala/kafka/log/UnifiedLog.scala#L1143]","** Comment 1 **
[Comment excluded]
"
KAFKA-12495,https://issues.apache.org/jira/browse/KAFKA-12495,https://github.com/apache/kafka/blob/3.4.0/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/IncrementalCooperativeAssignor.java,Unbalanced connectors/tasks distribution will happen in Connect's incremental cooperative assignor,NO,"In Kafka Connect, we implement incremental cooperative rebalance algorithm based on KIP-415 ([https://cwiki.apache.org/confluence/display/KAFKA/KIP-415%3A+Incremental+Cooperative+Rebalancing+in+Kafka+Connect)|https://cwiki.apache.org/confluence/display/KAFKA/KIP-415%3A+Incremental+Cooperative+Rebalancing+in+Kafka+Connect]. However, we have a bad assumption in the algorithm implementation, which is: after revoking rebalance completed, the member(worker) count will be the same as the previous round of reblance.

 

Let's take a look at the example in the KIP-415:

!image-2021-03-18-15-07-27-103.png|width=441,height=556!

It works well for most cases. But what if W4 added after 1st rebalance completed and before 2nd rebalance started? Let's see what will happened? Let's see this example: (we'll use 10 tasks here):

 
{code:java}
Initial group and assignment: W1([AC0, AT1, AT2, AT3, AT4, AT5, BC0, BT1, BT2, BT4, BT4, BT5])
Config topic contains: AC0, AT1, AT2, AT3, AT4, AT5, BC0, BT1, BT2, BT4, BT4, BT5
W1 is current leader
W2 joins with assignment: []
Rebalance is triggered
W3 joins while rebalance is still active with assignment: []
W1 joins with assignment: [AC0, AT1, AT2, AT3, AT4, AT5, BC0, BT1, BT2, BT4, BT4, BT5]
W1 becomes leader
W1 computes and sends assignments:
W1(delay: 0, assigned: [AC0, AT1, AT2, AT3], revoked: [AT4, AT5, BC0, BT1, BT2, BT4, BT4, BT5])
W2(delay: 0, assigned: [], revoked: [])
W3(delay: 0, assigned: [], revoked: [])

W1 stops revoked resources
W1 rejoins with assignment: [AC0, AT1, AT2, AT3]
Rebalance is triggered
W2 joins with assignment: []
W3 joins with assignment: []

// one more member joined
W4 joins with assignment: []
W1 becomes leader
W1 computes and sends assignments:

// We assigned all the previous revoked Connectors/Tasks to the new member, but we didn't revoke any more C/T in this round, which cause unbalanced distribution
W1(delay: 0, assigned: [AC0, AT1, AT2, AT3], revoked: [])
W2(delay: 0, assigned: [AT4, AT5, BC0], revoked: [])
W2(delay: 0, assigned: [BT1, BT2, BT4], revoked: [])
W2(delay: 0, assigned: [BT4, BT5], revoked: [])
{code}
Because we didn't allow to do consecutive revoke in two consecutive rebalances (under the same leader), we will have this uneven distribution under this situation. We should allow consecutive rebalance to have another round of revocation to revoke the C/T to the other members in this case.

expected:
{code:java}
Initial group and assignment: W1([AC0, AT1, AT2, AT3, AT4, AT5, BC0, BT1, BT2, BT4, BT4, BT5])
Config topic contains: AC0, AT1, AT2, AT3, AT4, AT5, BC0, BT1, BT2, BT4, BT4, BT5
W1 is current leader
W2 joins with assignment: []
Rebalance is triggered
W3 joins while rebalance is still active with assignment: []
W1 joins with assignment: [AC0, AT1, AT2, AT3, AT4, AT5, BC0, BT1, BT2, BT4, BT4, BT5]
W1 becomes leader
W1 computes and sends assignments:
W1(delay: 0, assigned: [AC0, AT1, AT2, AT3], revoked: [AT4, AT5, BC0, BT1, BT2, BT4, BT4, BT5])
W2(delay: 0, assigned: [], revoked: [])
W3(delay: 0, assigned: [], revoked: [])

W1 stops revoked resources
W1 rejoins with assignment: [AC0, AT1, AT2, AT3]
Rebalance is triggered
W2 joins with assignment: []
W3 joins with assignment: []

// one more member joined
W4 joins with assignment: []
W1 becomes leader
W1 computes and sends assignments:

// We assigned all the previous revoked Connectors/Tasks to the new member, **and also revoke some C/T** 
W1(delay: 0, assigned: [AC0, AT1, AT2], revoked: [AT3])
W2(delay: 0, assigned: [AT4, AT5, BC0], revoked: [])
W3(delay: 0, assigned: [BT1, BT2, BT4], revoked: [])
W4(delay: 0, assigned: [BT4, BT5], revoked: [])

// another round of rebalance to assign the new revoked C/T to the other members
W1 rejoins with assignment: [AC0, AT1, AT2] 
Rebalance is triggered 
W2 joins with assignment: [AT4, AT5, BC0] 
W3 joins with assignment: [BT1, BT2, BT4]
W4 joins with assignment: [BT4, BT5]

W1 becomes leader 
W1 computes and sends assignments:

// (final) We assigned all the previous revoked Connectors/Tasks to the members
W1(delay: 0, assigned: [AC0, AT1, AT2], revoked: []) 
W2(delay: 0, assigned: [AT4, AT5, BC0], revoked: []) 
W2(delay: 0, assigned: [BT1, BT2, BT4], revoked: []) 
W2(delay: 0, assigned: [BT4, BT5, AT3], revoked: [])
{code}
Note: The consumer's cooperative sticky assignor won't have this issue since we re-compute the assignment in each round.

 

Note2: this issue makes KAFKA-12283 test flaky.","** Comment 1 **
[Comment excluded]

** Comment 2 **
[Comment excluded]

** Comment 3 **
[Comment excluded]

** Comment 4 **
[Comment excluded]

** Comment 5 **
[Comment excluded]

** Comment 6 **
[Comment excluded]

** Comment 7 **
[Comment excluded]

** Comment 8 **
[Comment excluded]

** Comment 9 **
[Comment excluded]

** Comment 10 **
[Comment excluded]

** Comment 11 **
[Comment excluded]

** Comment 12 **
[Comment excluded]

** Comment 13 **
[Comment excluded]

** Comment 14 **
[Comment excluded]

** Comment 15 **
[Comment excluded]

** Comment 16 **
[Comment excluded]

** Comment 17 **
[Comment excluded]

** Comment 18 **
[Comment excluded]

** Comment 19 **
[Comment excluded]

** Comment 20 **
[Comment excluded]

** Comment 21 **
[Comment excluded]

** Comment 22 **
[Comment excluded]

** Comment 23 **
[Comment excluded]

** Comment 24 **
[Comment excluded]

** Comment 25 **
[Comment excluded]

** Comment 26 **
[Comment excluded]

** Comment 27 **
[Comment excluded]

** Comment 28 **
[Comment excluded]

** Comment 29 **
[Comment excluded]

** Comment 30 **
[Comment excluded]
"
KAFKA-14299,https://issues.apache.org/jira/browse/KAFKA-14299,https://github.com/apache/kafka/blob/3.4.0/streams/src/main/java/org/apache/kafka/streams/processor/internals/DefaultStateUpdater.java,Benchmark and stabilize state updater,NO,We need to benchmark and stabilize the separate state restoration code path.,
